<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[EDA check list]]></title>
      <url>%2F2018%2F10%2F16%2FEDA-check-list%2F</url>
      <content type="text"><![CDATA[Get domain knowledgeCheck if the data is intuitive (abnormal detection)add a feature is_incorrectUnderstand how the data was generatedIt is crucial to understand the generation process to set up a proper validation schemeTwo things to do with anonymized featuresTry to decode the featuresGuess the true meaning of the featureGuess the feature typesEach type need its own preprocessingVisualizationTools for individual features explorationHistograms plt.hist(x)Plot (index versus value) plt.plot(x, something)Statistics df.describe() or x.mean() or x.var()Other tools x.value_counts() or x.isnull()Tools for feature relationshipsPairsplt.scatter(x1, x2)pd.scatter_matrix(df)df.corr() or plt.matshow()Groups:ClusteringPlot (index vs feature statistics) df.mean().sort_values().plot()Data Cleanremove duplicated and constant featurestraintest.nunique(axis=1) == 1traintest.T.drop_duplicates()for f in categorical_feats: traintest[f] = traintest[f].factorize then traintest.T.drop_duplicates()check if same rows have same labelcheck if dataset is shuffled]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Processing Anonymized Features]]></title>
      <url>%2F2018%2F10%2F16%2FProcessing-anonymized-features%2F</url>
      <content type="text"><![CDATA[IMPORTANT: You will not be able to run this notebook at coursera platform, as the dataset is not there. The notebook is in read-only mode.But you can run the notebook locally and download the dataset using this link to explore the data interactively.1pd.set_option('max_columns', 100)Load the data12train = pd.read_csv('./train.csv')train.head().dataframe thead tr:only-child th{ text-align:right}.dataframe thead th{ text-align:left}.dataframe tbody tr th{ vertical-align:top}x0x1x2x3x4x5x6x7x8x9x10x11x12x13x14x15x16x17x18x19x20x21x22x23x24x25x26x27x28x29x30x31x32x33x34x35x36x37x38x39x40x41x42x43x44x45x46x47x48x49x50x51x52x53x54x55x56x57x58x59x60x61y0b4d8a653ea16a14a2d1706330986edca63304de0a62168d6261746600cb011-0.6887067e5c97705ae5df3eff9b91bb549494e33c63cf353694.06e40247e69617a4ad3f9718c61545bc26d08129a634e3cf3acdd9c9e0da217c99905b6513a3e3f369aba4d7f5140.579612-0.112693-0.1721911.1666671.6745380.63088937.0000001.29492255.00.16666710.00.00.0000001.09.00.01.023.03.670.121.9352.20.6250.2500.1250.0000.8130.0740.6340.5480.2353330.2649520.0000000.3333330.3333330.3333330.0000000.0000009.021467f9617a316a14a2d1706330986edca63304de0b7584c2d521746600cb0110.8708715624b8f759fa0b797a92669ea3d319f17880307418156.001ede04b4b617a4ad3f9718c61545bd342e2765fbb20e1ca068a6c8cef831b02793146992153ed659aba4d7f5128.7655032.6122852.1590914.0000001.7107141.7135380.1666670.027669109.00.00000031.00.00.0000001.0244.01.01.068.017.250.573.4524.00.4090.6190.5790.2480.3460.5410.5220.0001.7823461.3224090.0116470.3976710.2396010.2495840.0682200.033278601.042190436e52816a14a2d1706330986edca63304de0b7584c2d521746600cb0110.4376555624b8f759152af2cb2f91bb549494e33c63cf351178.0cc69cbe29a617a4ad3f9e8a040423ac82c3dbd33ee3501282b199ce7c4845f17dedd5c5c5025bd0a9aba4d7f5124.943933-0.814660-0.7083081.500000-0.512422-0.7339670.33333314.83772811.00.00000024.00.00.0000001.029.00.03.011.04.420.150.1610.21.0001.0001.0001.0001.0000.5200.5330.835-0.5865400.6724360.0000000.6060610.1212120.2121210.0606060.00000033.03343859085bc16a14a2d1706330986edca63304de0a62168d6261746600cb0110.004439f67f142e40c4dd2197c391bb549494e33c63cf3514559.06e40247e69617a4ad3f9718c61545bc26d08129a9e166b965d466f8951b0fde72a6d5cacfadc5c019aba4d7f5141.576860-0.907833-0.7617360.500000-0.627525-0.8058011.1666670.0043950.00.5000000.00.00.0000007.07.00.03.015.08.920.290.2260.80.0000.0000.0000.0000.0001.0000.0000.000-1.600326-1.8386800.0000001.0000000.0000000.0000000.0000000.0000001.044a4c3095b7516a14a2d1706330986edca63304de0b7584c2d521746600cb0110.4809777e5c97705ae071d01df591bb549494e33c63cf355777.06e40247e69617a4ad3f94b9480aa42e84655292c527b6ca8ccdd9c9e0da217c99905b60fc56ea1f09aba4d7f5131.080282-0.371787-0.3676161.6666670.2713070.01311217.3333331713.43912833.00.0000006.01.00.6666678.0108.01.04.086.01.580.052.0322.40.3480.7620.5500.3920.4890.5171.0000.6420.9609910.7909900.0201610.6451610.2580650.0362900.0403230.000000248.03Build a quick baseline123456789101112131415161718from sklearn.ensemble import RandomForestClassifier# Create a copy to work withX = train.copy()# Save and drop labelsy = train.yX = X.drop('y', axis=1)# fill NANs X = X.fillna(-999)# Label encoderfor c in train.columns[train.dtypes == 'object']: X[c] = X[c].factorize()[0] rf = RandomForestClassifier()rf.fit(X,y)RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&apos;gini&apos;, max_depth=None, max_features=&apos;auto&apos;, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=None, verbose=0, warm_start=False) 12plt.plot(rf.feature_importances_)plt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);/home/dulyanov/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u&apos;serif&apos;] not found. Falling back to DejaVu Sans (prop.get_family(), self.defaultFamily[fontext])) There is something interesting about x8.123# we see it was standard scaled, most likely, if we concat train and test, we will get exact mean=1, and std 1 print 'Mean:', train.x8.mean()print 'std:', train.x8.std()Mean: -0.000252352028622 std: 1.02328163601 12# And we see that it has a lot of repeated valuestrain.x8.value_counts().head(15)-2.984750 2770 0.480977 2569 0.610941 1828 0.654263 1759 0.567620 1746 0.697585 1691 0.524298 1639 0.740906 1628 0.394333 1610 0.437655 1513 0.351012 1450 0.264369 1429 0.307690 1401 0.221047 1372 0.784228 1293 Name: x8, dtype: int64 1234567# It's very hard to work with scaled feature, so let's try to scale them back# Let's first take a look at difference between neighbouring values in x8x8_unique = train.x8.unique()x8_unique_sorted = np.sort(x8_unique) np.diff(x8_unique_sorted)array([ 43.27826527, 38.98942817, 0.21660793, 0.04332159, 0.17328635, 0.21660793, 0.08664317, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.12996476, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.21660793, 1.16968285, 0.04332159, 0.38989428, nan]) 1234567# The most of the diffs are 0.04332159! # The data is scaled, so we don't know what was the diff value for the original feature# But let's assume it was 1.0# Let's devide all the numbers by 0.04332159 to get the right scaling# note, that feature will still have zero meannp.diff(x8_unique_sorted/0.04332159)array([ 998.99992752, 899.9999347 , 4.99999964, 0.99999993, 3.99999971, 4.99999964, 1.99999985, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 2.99999978, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 4.99999964, 26.99999804, 0.99999993, 8.99999935, nan]) 1(train.x8/0.04332159).head(10)0 -15.897530 1 20.102468 2 10.102468 3 0.102469 4 11.102468 5 -68.897526 6 10.102468 7 15.102468 8 9.102468 9 -68.897526 Name: x8, dtype: float64 1234# Ok, now we see .102468 in every value# this looks like a part of a mean that was subtracted during standard scaling# If we subtract it, the values become almost integers(train.x8/0.04332159 - .102468).head(10)0 -15.999998 1 20.000000 2 10.000000 3 0.000001 4 11.000000 5 -68.999994 6 10.000000 7 15.000000 8 9.000000 9 -68.999994 Name: x8, dtype: float64 123# let's round them x8_int = (train.x8/0.04332159 - .102468).round()x8_int.head(10)0 -16.0 1 20.0 2 10.0 3 0.0 4 11.0 5 -69.0 6 10.0 7 15.0 8 9.0 9 -69.0 Name: x8, dtype: float64 123# Ok, what's next? In fact it is not obvious how to find shift parameter, # and how to understand what the data this feature actually store# But ...1x8_int.value_counts()-69.0 2770 11.0 2569 14.0 1828 15.0 1759 13.0 1746 16.0 1691 12.0 1639 17.0 1628 9.0 1610 10.0 1513 8.0 1450 6.0 1429 7.0 1401 5.0 1372 18.0 1293 1.0 1290 4.0 1276 2.0 1250 3.0 1213 -1.0 1085 0.0 1080 -2.0 1006 -4.0 995 -3.0 976 -5.0 954 -8.0 923 -9.0 921 -6.0 906 19.0 893 -7.0 881 ... 26.0 3 -40.0 3 -41.0 3 25.0 2 -59.0 2 31.0 2 34.0 2 -46.0 2 -49.0 2 33.0 2 -42.0 2 32.0 2 37.0 2 30.0 2 -45.0 2 -54.0 1 36.0 1 -51.0 1 27.0 1 79.0 1 -47.0 1 69.0 1 70.0 1 -50.0 1 -1968.0 1 42.0 1 -63.0 1 -48.0 1 -64.0 1 35.0 1 Name: x8, Length: 99, dtype: int64 123# do you see this -1968? Doesn't it look like a year? ... So my hypothesis is that this feature is a year of birth! # Maybe it was a textbox where users enter their year of birth, and someone entered 0000 instead# The hypothesis looks plausible, isn't it?1(x8_int + 1968.0).value_counts().sort_index()0.0 1 999.0 4 1899.0 2770 1904.0 1 1905.0 1 1909.0 2 1914.0 1 1916.0 3 1917.0 1 1918.0 1 1919.0 2 1920.0 1 1921.0 1 1922.0 2 1923.0 2 1924.0 4 1925.0 4 1926.0 2 1927.0 3 1928.0 3 1929.0 4 1930.0 4 1931.0 12 1932.0 10 1933.0 7 1934.0 13 1935.0 28 1936.0 35 1937.0 35 1938.0 45 ... 1978.0 1513 1979.0 2569 1980.0 1639 1981.0 1746 1982.0 1828 1983.0 1759 1984.0 1691 1985.0 1628 1986.0 1293 1987.0 893 1988.0 624 1989.0 434 1990.0 233 1991.0 110 1992.0 31 1993.0 2 1994.0 3 1995.0 1 1998.0 2 1999.0 2 2000.0 2 2001.0 2 2002.0 2 2003.0 1 2004.0 1 2005.0 2 2010.0 1 2037.0 1 2038.0 1 2047.0 1 Name: x8, Length: 99, dtype: int64 1# After the competition ended the organisers told it was really a year of birth]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Exploratory Data Analysis]]></title>
      <url>%2F2018%2F10%2F16%2FExploratory-Data-Analysis%2F</url>
      <content type="text"><![CDATA[This is a detailed EDA of the data, shown in the second video of “Exploratory data analysis” lecture (week 2).PLEASE NOTE: the dataset cannot be published, so this notebook is read-only.Load dataIn this competition hosted by solutions.se, the task was to predict the advertisement cost for a particular ad.12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinedata_path = './data'train = pd.read_csv('%s/train.csv.gz' % data_path, parse_dates=['Date'])test = pd.read_csv('%s/test.csv.gz' % data_path, parse_dates=['Date'])Let’s look at the data (notice that the table is transposed, so we can see all feature names).1train.head().T01234AdGroupId78db03413668a0110c6921af1035aff63fda0c33cd868ebdccAdGroupName6d91d 25866 9c5942657d cb2d0 6d91d6d91d e33a0 9a99b59991 9c5946d91d 25866 9a99bAdNetworkType2sssssAveragePosition1.22111.1CampaignId273823cb71273823cb71273823cb71273823cb71273823cb71CampaignName2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140dClicks00003Conversions00000ConversionsManyPerClick00000Cost00000.94Date2014-01-01 00:00:002014-01-01 00:00:002014-01-01 00:00:002014-01-01 00:00:002014-01-01 00:00:00DestinationUrl98035d60fcc25f23cd0801f87f76395c0e89f5328888b55ddeDevicetttddFirstPageCpc1.062.940.421.750.17Impressions3214122KeywordMatchTypebbbbbKeywordTextjze 10 +uxsgk+jze +dznvgyhjclrjze 100 +gzpxykjze 10 +uxsgk 1950kjze 10 mykj +gzpxykMaxCpc0.2810.220.540.12QualityScore11111Slots_2s_2s_1s_2s_1TopOfPageCpc1.075.020.4240.25KeywordId7d20d63df9a617d4f0376e0b7024d29c2ea0cdf84c8ba7affdWe see a lot of features with not obvious names. If you search for the CampaignId, AdGroupName, AdNetworkType2 using any web search engine, you will find this dataset was exported from Google AdWords. So what is the required domain knowledge here? The knowledge of how web advertisement and Google AdWords work! After you have learned it, the features will make sense to you and you can proceed.For the sake of the story I will briefly describe Google AdWords system now. Basically every time a user queries a search engine, Google AdWords decides what ad will be shown along with the actual search results. On the other side of AdWords, the advertisers manage the ads – they can set a multiple keywords, that a user should query in order to their ad to be shown. If the keywords are set properly and are relevant to the ad, then the ad will be shown to relevant users and the ad will get clicked. Advertisers pay to Google for some type of events, happened with their ad: for example for a click event, i.e. the user saw this ad and clicked it. AdWords uses complex algorithms to decide which ad to show to a particular user with a particular search query. The advertisers can only indirectly influence AdWords decesion process by changing keywords and several other parameters. So at a high level, the task is to predict what will be the costs for the advertiser (how much he will pay to Google, column Cost) when the parameters (e.g. keywords) are changed.The ads are grouped in groups, there are features AdGroupId AdGroupName describing them. A campaign corresponds to some specific parameters that an advertiser sets. Similarly, there are ID and name features CampaignId, CampaignName. And finally there is some information about keywords: KeywordId and KeywordText. Slot is $1$ when ad is shown on top of the page, and $2$ when on the side. Device is a categorical variable and can be either “tablet”, “mobile” or “pc”. And finally the Date is just the date, for which clicks were aggregated.1test.head().T01234Id01234AdGroupId00096e761100096e761100096e761100096e761100096e7611AdGroupNamec8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dAdNetworkType2sssssAveragePosition11111CampaignIde62b4bc4c3e62b4bc4c3e62b4bc4c3e62b4bc4c3e62b4bc4c3CampaignName2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15dDate2014-06-01 00:00:002014-06-01 00:00:002014-06-01 00:00:002014-06-01 00:00:002014-06-01 00:00:00DestinationUrlf5aad09031f5aad09031f5aad09031f5aad09031f5aad09031DevicetdmtdKeywordId539778bb80539778bb80539778bb80539778bb80539778bb80KeywordMatchTypeeeeeeKeywordTexttcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhySlots_1s_1s_1s_2s_2Notice there is diffrent number of columns in test and train – our target is Cost column, but it is closly related to several other features, e.g. Clicks, Conversions. All of the related columns were deleted from the test set to avoid data leakages.Let’s analyzeAre we ready to modeling? Not yet. Take a look at this statistic:12345678print 'Train min/max date: %s / %s' % (train.Date.min().date(), train.Date.max().date())print 'Test min/max date: %s / %s' % ( test.Date.min().date(), test.Date.max().date())print ''print 'Number of days in train: %d' % ((train.Date.max() - train.Date.min()).days + 1)print 'Number of days in test: %d' % (( test.Date.max() - test.Date.min()).days + 1)print ''print 'Train shape: %d rows' % train.shape[0]print 'Test shape: %d rows' % test.shape[0]Train min/max date: 2014-01-01 / 2014-05-31 Test min/max date: 2014-06-01 / 2014-06-14 Number of days in train: 151 Number of days in test: 14 Train shape: 3493820 rows Test shape: 8951040 rows Train period is more than 10 times larger than the test period, but train set has fewer rows, how could that happen?At this point I suggest you to stop and think yourself, what could be a reason, why this did happen. Unfortunately we cannot share the data for this competition, but the information from above should be enough to get a right idea.Alternatively, you can go along for the explanation, if you want.InvestigationLet’s take a look how many rows with each date we have in train and test.1test.Date.value_counts()2014-06-02 639360 2014-06-12 639360 2014-06-09 639360 2014-06-14 639360 2014-06-01 639360 2014-06-11 639360 2014-06-08 639360 2014-06-05 639360 2014-06-10 639360 2014-06-07 639360 2014-06-04 639360 2014-06-06 639360 2014-06-03 639360 2014-06-13 639360 Name: Date, dtype: int64 12# print only first 10train.Date.value_counts().head(10)2014-01-01 36869 2014-01-04 36427 2014-01-05 36137 2014-01-02 34755 2014-01-03 34693 2014-01-06 31349 2014-04-07 30950 2014-02-09 30101 2014-01-26 29830 2014-02-08 29187 Name: Date, dtype: int64 Interesting, for the test set we have the same number of rows for every date, while in train set the number of rows is different for each day. It looks like that for each day in the test set a loop through some kind of IDs had been run. But what about train set? So far we don’t know, but let’s find the test IDs first.TestSo now we know, that there is $639360$ different IDs. It should be easy to find the columns, that form ID, because if the ID is [‘col1’, ‘col2’], then to compute the number of combinations we should just multiply the number of unique elements in each.12test_nunique = test.nunique()test_nuniqueId 8951040 AdGroupId 13548 AdGroupName 2281 AdNetworkType2 2 AveragePosition 131 CampaignId 252 CampaignName 252 Date 14 DestinationUrl 52675 Device 3 KeywordId 12285 KeywordMatchType 3 KeywordText 11349 Slot 4 dtype: int64 12345678910111213141516import itertools# This function looks for a combination of elements # with product of 639360 def find_prod(data): # combinations of not more than 5 features for n in range(1, 5): # iterate through all combinations for c in itertools.combinations(range(len(data)), n): if data[list(c)].prod() == 639360: print test_nunique.index[c] return print 'Nothing found' find_prod(test_nunique.values)Nothing found Hmm, nothing found! The problem is that some features are tied, and the number of their combinations does not equal to product of individual unique number of elements. For example it does not make sense to create all possible combinations of DestinationUrl and AdGroupId as DestinationUrl belong to exactly one AdGroupId.1test.groupby('DestinationUrl').AdGroupId.nunique()DestinationUrl 00010d62df 1 000249f717 1 00054cf3f8 1 000684bf0b 1 00072a9fa7 1 00077a6729 1 0007cc191f 1 0009388900 1 001144cae4 1 00115f6477 1 00141a299f 1 00169dc49b 1 0018b27e06 1 001b0b3d06 1 001ef8368e 1 00205e056a 1 002082ab8b 1 0020c585ea 1 0021419f7e 1 00225519cc 1 002498dc88 1 0026171436 1 00265dc4bb 1 0026833e5c 1 0027ffbad9 1 002b1deb25 1 002c55ccef 1 002e44290f 1 0030ca870e 1 0032b64beb 1 .. ffda377018 1 ffda3c412a 1 ffda5b53d6 1 ffda8c0d8c 1 ffdbf5d179 1 ffdc872fcf 1 ffde114af5 1 ffde41a800 1 ffe2fb7007 1 ffe4a040d4 1 ffe685e937 1 ffe8c3da53 1 ffe8f82e08 1 ffeb9fda9d 1 ffebd1d253 1 ffebea724f 1 ffecf398b1 1 ffecf3e7d4 1 ffed185438 1 fff02d7269 1 fff10adcb0 1 fff12e5f19 1 fff132d5bd 1 fff19836a0 1 fff3539204 1 fff4c5d255 1 fff55db78a 1 fff8c11ad9 1 fff90ea351 1 fffb248bf0 1 Name: AdGroupId, Length: 52675, dtype: int64 So, now let’s try to find ID differently. Let’s try to find a list of columns, such that threre is exazctly $639360$ unique combinations of their values in the test set (not overall). So, we want to find columns, such that:1test[columns].drop_duplicates().shape[0] == 639360We could do it with a similar loop.123456789101112131415import itertoolsdef find_ncombinations(data): # combinations of not more than 5 features for n in range(1, 5): for c in itertools.combinations(range(data.shape[1]), n): print c columns = test.columns[list(c)] if test[columns].drop_duplicates().shape[0] == 639360: print columns return print 'Nothing found' find_ncombinations(test)But it will take forever to compute. So it is easier to find the combination manually.So after some time of trials and errors I figured out, that the four features KeywordId, AdGroupId, Device, Slot form the index. The number of unique rows is exactly 639360 as we wanted to find.12columns = ['KeywordId', 'AdGroupId', 'Device', 'Slot']test[columns].drop_duplicates().shape(639360, 4) Looks reasonable. For each AdGroupId there is a distinct set of possible KeywordId’s, but Device and Slot variants are the same for each ad. And the target is to predict what will be the daily cost for using different KeywordId’s, Device type, Slot type to advertise ads from AdGroups.TrainTo this end, we found how test set was constructed, but what about the train set? Let us plot something, probably we will find it out.123import seaborn as snssns.set(palette='pastel')sns.set(font_scale=2)12# from absolute dates to relativetrain['date_diff'] = (train.Date - train.Date.min()).dt.days12345678910# group by the index, that we've foundg= train.groupby(['KeywordId', 'AdGroupId', 'Device', 'Slot'])# and for each index show average relative date versus # the number of rows with that indexplt.figure(figsize=(12,12))plt.scatter(g.date_diff.mean(),g.size(),edgecolor = 'none',alpha = 0.2, s=20, c='b')plt.xlabel('Group mean relative date')plt.ylabel('Group size')plt.title('Train');Looks interesting, isn’t it? That is something we need to explain! How the same plot looks for the test set?12# from absolute dates to relativetest['date_diff'] = (test.Date - test.Date.min()).dt.days1234567891011# group by the index, that we've foundg= test.groupby(['KeywordId', 'AdGroupId', 'Device', 'Slot'])# and for each index show average relative date versus # the number of rows with that indexplt.figure(figsize=(12,12))plt.scatter(g.date_diff.mean(),g.size(),edgecolor = 'none',alpha = 0.2, s=20, c='b')plt.xlabel('Group mean relative date')plt.ylabel('Group size')plt.ylim(-2, 30)plt.title('Test');Just a dot!Now let’s think, what we actually plotted? We grouped the data by the ID that we’ve found previously and we plotted average Date in the group versus the size of each group. We found that ID is an aggregation index – so for each date the Cost is aggreagated for each possible index. So group size shows for how many days we have Const information for each ID and mean relative date shows some information about these days.For test set it is expectable that both average date and the size of the groups are the same for each group: the size of each group is $14$ (as we have $14$ test days) and mean date is $6.5$, because for each group (index) we have $14$ different days, and $\frac{0 + 1 + \dots + 13}{14} = 6.5$.And now we can explain everything for the train set. Look at the top of the triangle: for those points (groups) we have Cost information for all the days in the train period, while on the sides we see groups, for which we have very few rows.But why for some groups we have smaller number of rows, than number of days? Let’s look at the Impressions column.1train.Impressions.value_counts()1 1602929 2 565896 3 287128 4 175197 5 119092 6 86651 7 66443 8 53007 9 42984 10 35731 11 30248 12 25950 13 22629 14 20126 15 17503 16 15682 17 14100 18 12848 19 11597 20 10724 21 9864 22 8931 23 8316 24 7953 25 7168 26 6684 27 6196 28 5863 29 5556 30 5223 ... 4978 1 15210 1 9076 1 13174 1 116535 1 4979 1 17273 1 90974 1 4976 1 5906 1 7023 1 60282 1 7955 1 13881 1 2921 1 4970 1 7019 1 17249 1 23394 1 28210 1 11116 1 15929 1 7017 1 95761 1 2923 1 15213 1 9070 1 5692 1 13162 1 13922 1 Name: Impressions, Length: 8135, dtype: int64 We never have $0$ value in Imressions column. But in reality, of course, some ads with some combination of keyword, slot, device were never shown. So this looks like a nice explanation for the data: in the train set we only have information about ads (IDs, groups) which were shown at least once. And for the test set, we, of course, want to predict Cost for every possible ID.What it means for competitors, is that if one would just fit a model on the train set as is, the predictions for the test set will be biased by a lot. The predictions will be much higher than they should be, as we are only given a specific subset of rows as train.csv file.So, before modeling we should first extend the trainset and inject rows with 0 impressions. Such change will make train set very similar to the test set and the models will generalize nicely.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据预处理相关技术]]></title>
      <url>%2F2018%2F10%2F15%2F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%2F</url>
      <content type="text"><![CDATA[数值型数据 (non-tree based model)特征预处理MinMaxScalar 不会改变数据分布StandardScalarscipy.stats.rankdatalog transform np.log(1+x)raising to the power &lt; 1 np.sqrt(x + 2/3)drop outlier（winsorization，specify upper and lower bound）融合不同预处理方法得到的特征训练一个模型或者每一种特征训练出一个模型最后做模型融合特征生成主要依据先验经验以及对数据的深刻理解例如，浮点数的小数部分单独提取出来作为特征类别数据以及有序类别数据特征预处理Label encoding (tree(or non-tree)-based model)alphabetical sorted sklearn.preprocessing.LabelEncoderorder of appearance Pandas.factorizefrequency encoding （非常适用于测试数据中包含训练数据未包含的类别）Label encoding (non-tree(or tree)-based model)one-hot encoding (sparse matrix)特征生成枚举不同的类别特征的组合形成新的类别特征 (linear models and KNN)日期数据以及坐标数据日期数据特征生成周期性数据Day number in week, month, season, yearsecond, minute, second自什么时候以来问题无关， 比如自1970年1月1日以来问题相关，比如距离下一个节假日还有多少天等等两个日期特征之间的差值坐标数据特征生成距离某些关键坐标的距离等等（需要外部数据支持）对坐标进行网格化或者聚类，然后计算每个网格中的点距离选定点的距离或者每个簇中的点距离聚类中心的距离点的密度（某一限定范围之内）区域价值，例如物价房价等（某一限定范围之内）特征预处理坐标旋转（例如45°）缺失值处理找出隐含的NaN，通过可视化数据分布填充方法-999, -1等中值，均值等尝试恢复缺失数据（线性回归）特征生成增加一个特征，是否有缺失值采用填充的缺失值进行特征生成要特别小心，一般来说若要进行特征生成，则最好不要在之前进行缺失值填充xgboost对于缺失值不敏感文本数据特征生成词袋 skearn.feature_extraction.text.CountVectorizerTF-IDF skearn.feature_extraction.text.TfidfVectorizerN-grams ngram特征预处理lowercaselemmatization (单词最原始的形式)stemmingstopwords nltkWord2Vec, Doc2vec, Glove, FastText, etcPipeline预处理Ngrams then TF-IDFor Word2Vec, etc图像数据可以结合不同层的特征图]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[About Feature Scaling and Normalization]]></title>
      <url>%2F2018%2F10%2F15%2FAbout-Feature-Scaling-and-Normalization%2F</url>
      <content type="text"><![CDATA[About standardizationThe result of standardization (or Z-score normalization) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ and $\sigma=1$where $\mu$ is the mean (average) and $\sigma$ is the standard deviation from the mean; standard scores (also called z scores) of the samples are calculated as follows:$$z = \frac{x - \mu}{\sigma}$$Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example (an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates$$\Delta w_j = - \eta \frac{\partial J}{\partial w_j} = \eta \sum_i (t^{(i)} - o^{(i)}) x_j^{(i)},$$so that$w_j := w_j + \Delta w_j$, where $\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output. Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures – in fact, tree-based classifier are probably the only classifiers where feature scaling doesn’t make a difference.In fact, the only family of algorithms that I could think of being scale-invariant are tree-based methods. Let’s take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as “is feature x_i &gt;= some_val?” Intuitively, we can see that it really doesn’t matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale – it really doesn’t matter).Some examples of algorithms where feature scaling matters are:k-nearest neighbors with an Euclidean distance measure if want all features to contribute equallyk-means (see k-nearest neighbors)logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than otherslinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.In addition, we’d also want to think about whether we want to “standardize” or “normalize” (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights “equally.” As a rule of thumb I’d say: When in doubt, just standardize the data, it shouldn’t hurt.About Min-Max scalingAn alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling(often also simply called “normalization” - a common cause for ambiguities).In this approach, the data is scaled to a fixed range - usually 0 to 1.The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.A Min-Max scaling is typically done via the following equation:$$X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}$$Z-score standardization or Min-Max scaling?“Standardization or Min-Max scaling?” - There is no obvious answer to this question: it really depends on the application.For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; but more about PCA in my previous article).However, this doesn’t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale.Standardizing and normalizing - how it can be done using scikit-learnOf course, we could make use of NumPy’s vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Python’s open-source machine learning library scikit-learn.For the following examples and discussion, we will have a look at the free “Wine” Dataset that is deposited on the UCI machine learning repository(http://archive.ics.uci.edu/ml/datasets/Wine).Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):1) Alcohol2) Malic acid…Loading the wine dataset123456789101112import pandas as pdimport numpy as npdf = pd.io.parsers.read_csv( 'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', header=None, usecols=[0,1,2] )df.columns=['Class label', 'Alcohol', 'Malic acid']df.head()Class labelAlcoholMalic acid0114.231.711113.201.782113.162.363114.371.954113.242.59As we can see in the table above, the features Alcohol (percent/volumne) and Malic acid (g/l) are measured on different scales, so that Feature Scaling is necessary important prior to any comparison or combination of these data.Standardization and Min-Max scaling1234567from sklearn import preprocessingstd_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])df_std = std_scale.transform(df[['Alcohol', 'Malic acid']])minmax_scale = preprocessing.MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])df_minmax = minmax_scale.transform(df[['Alcohol', 'Malic acid']])1234print('Mean after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_std[:,0].mean(), df_std[:,1].mean()))print('\nStandard deviation after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_std[:,0].std(), df_std[:,1].std()))12345Mean after standardization:Alcohol=0.00, Malic acid=0.00Standard deviation after standardization:Alcohol=1.00, Malic acid=1.001234print('Min-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_minmax[:,0].min(), df_minmax[:,1].min()))print('\nMax-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_minmax[:,0].max(), df_minmax[:,1].max()))12345Min-value after min-max scaling:Alcohol=0.00, Malic acid=0.00Max-value after min-max scaling:Alcohol=1.00, Malic acid=1.00Plotting1%matplotlib inline123456789101112131415161718192021222324from matplotlib import pyplot as pltdef plot(): plt.figure(figsize=(8,6)) plt.scatter(df['Alcohol'], df['Malic acid'], color='green', label='input scale', alpha=0.5) plt.scatter(df_std[:,0], df_std[:,1], color='red', label='Standardized [$$N (\mu=0, \; \sigma=1)$$]', alpha=0.3) plt.scatter(df_minmax[:,0], df_minmax[:,1], color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3) plt.title('Alcohol and Malic Acid content of the wine dataset') plt.xlabel('Alcohol') plt.ylabel('Malic Acid') plt.legend(loc='upper left') plt.grid() plt.tight_layout()plot()plt.show()The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue). In the following plot, we will zoom in into the three different axis-scales.123456789101112131415161718192021222324fig, ax = plt.subplots(3, figsize=(6,14))for a,d,l in zip(range(len(ax)), (df[['Alcohol', 'Malic acid']].values, df_std, df_minmax), ('Input scale', 'Standardized [$$N (\mu=0, \; \sigma=1)$$]', 'min-max scaled [min=0, max=1]') ): for i,c in zip(range(1,4), ('red', 'blue', 'green')): ax[a].scatter(d[df['Class label'].values == i, 0], d[df['Class label'].values == i, 1], alpha=0.5, color=c, label='Class %s' %i ) ax[a].set_title(l) ax[a].set_xlabel('Alcohol') ax[a].set_ylabel('Malic Acid') ax[a].legend(loc='upper left') ax[a].grid()plt.tight_layout()plt.show()Bottom-up approachesOf course, we can also code the equations for standardization and 0-1 Min-Max scaling “manually”. However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.E.g.,123std_scale = preprocessing.StandardScaler().fit(X_train)X_train = std_scale.transform(X_train)X_test = std_scale.transform(X_test)Below, we will perform the calculations using “pure” Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix.Vanilla Python1234567891011# Standardizationx = [1,4,5,6,6,2,3]mean = sum(x)/len(x)std_dev = (1/len(x) * sum([ (x_i - mean)**2 for x_i in x]))**0.5z_scores = [(x_i - mean)/std_dev for x_i in x]# Min-Max scalingminmax = [(x_i - min(x)) / (max(x) - min(x)) for x_i in x]NumPy12345678910import numpy as np# Standardizationx_np = np.asarray(x)z_scores_np = (x_np - x_np.mean()) / x_np.std()# Min-Max scalingnp_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())VisualizationJust to make sure that our code works correctly, let us plot the results via matplotlib.12345678910111213141516171819202122232425from matplotlib import pyplot as pltfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5))y_pos = [0 for i in range(len(x))]ax1.scatter(z_scores, y_pos, color='g')ax1.set_title('Python standardization', color='g')ax2.scatter(minmax, y_pos, color='g')ax2.set_title('Python Min-Max scaling', color='g')ax3.scatter(z_scores_np, y_pos, color='b')ax3.set_title('Python NumPy standardization', color='b')ax4.scatter(np_minmax, y_pos, color='b')ax4.set_title('Python NumPy Min-Max scaling', color='b')plt.tight_layout()for ax in (ax1, ax2, ax3, ax4): ax.get_yaxis().set_visible(False) ax.grid()plt.show()The effect of standardization on PCA in a pattern classification taskEarlier, I mentioned the Principal Component Analysis (PCA) as an example where standardization is crucial, since it is “analyzing” the variances of the different features.Now, let us see how the standardization affects PCA and a following supervised classification on the whole wine dataset.In the following section, we will go through the following steps:Reading in the datasetDividing the dataset into a separate training and test datasetStandardization of the featuresPrincipal Component Analysis (PCA) to reduce the dimensionalityTraining a naive Bayes classifierEvaluating the classification accuracy with and without standardizationReading in the dataset123456import pandas as pddf = pd.io.parsers.read_csv( 'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', header=None, )Dividing the dataset into a separate training and test datasetIn this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively.1234567from sklearn.cross_validation import train_test_splitX_wine = df.values[:,1:]y_wine = df.values[:,0]X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.30, random_state=12345)Feature Scaling - Standardization12345from sklearn import preprocessingstd_scale = preprocessing.StandardScaler().fit(X_train)X_train_std = std_scale.transform(X_train)X_test_std = std_scale.transform(X_test)Dimensionality reduction via Principal Component Analysis (PCA)Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between “preserving information” and “overfitting” for different classifiers. However, we will omit this step since we don’t want to train a perfect classifier here, but merely compare the effects of standardization.123456789101112from sklearn.decomposition import PCA# on non-standardized datapca = PCA(n_components=2).fit(X_train)X_train = pca.transform(X_train)X_test = pca.transform(X_test)# om standardized datapca_std = PCA(n_components=2).fit(X_train_std)X_train_std = pca_std.transform(X_train_std)X_test_std = pca_std.transform(X_test_std)Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity).123456789101112131415161718192021222324252627282930313233from matplotlib import pyplot as pltfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')): ax1.scatter(X_train[y_train==l, 0], X_train[y_train==l, 1], color=c, label='class %s' %l, alpha=0.5, marker=m )for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')): ax2.scatter(X_train_std[y_train==l, 0], X_train_std[y_train==l, 1], color=c, label='class %s' %l, alpha=0.5, marker=m )ax1.set_title('Transformed NON-standardized training dataset after PCA') ax2.set_title('Transformed standardized training dataset after PCA') for ax in (ax1, ax2): ax.set_xlabel('1st principal component') ax.set_ylabel('2nd principal component') ax.legend(loc='upper right') ax.grid()plt.tight_layout()plt.show()Training a naive Bayes classifierWe will use a naive Bayes classifier for the classification task. If you are not familiar with it, the term “naive” comes from the assumption that all features are “independent”.All in all, it is a simple but robust classifier based on Bayes’ ruleI don’t want to get into more detail about Bayes’ rule in this article, but if you are interested in a more detailed collection of examples, please have a look at the Statistical Patter Classification in my pattern classification repository.123456789from sklearn.naive_bayes import GaussianNB# on non-standardized datagnb = GaussianNB()fit = gnb.fit(X_train, y_train)# on standardized datagnb_std = GaussianNB()fit_std = gnb_std.fit(X_train_std, y_train)Evaluating the classification accuracy with and without standardization1234567891011from sklearn import metricspred_train = gnb.predict(X_train)print('\nPrediction accuracy for the training dataset')print('&#123;:.2%&#125;'.format(metrics.accuracy_score(y_train, pred_train)))pred_test = gnb.predict(X_test)print('\nPrediction accuracy for the test dataset')print('&#123;:.2%&#125;\n'.format(metrics.accuracy_score(y_test, pred_test)))12345Prediction accuracy for the training dataset81.45%Prediction accuracy for the test dataset64.81%123456789pred_train_std = gnb_std.predict(X_train_std)print('\nPrediction accuracy for the training dataset')print('&#123;:.2%&#125;'.format(metrics.accuracy_score(y_train, pred_train_std)))pred_test_std = gnb_std.predict(X_test_std)print('\nPrediction accuracy for the test dataset')print('&#123;:.2%&#125;\n'.format(metrics.accuracy_score(y_test, pred_test_std)))12345Prediction accuracy for the training dataset96.77%Prediction accuracy for the test dataset98.15%As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset.Appendix A: The effect of scaling and mean centering of variables prior to PCALet us think about whether it matters or not if the variables are centered for applications such as Principal Component Analysis (PCA) if the PCA is calculated from the covariance matrix (i.e., the kkprincipal components are the eigenvectors of the covariance matrix that correspond to the kk largest eigenvalues.1. Mean centering does not affect the covariance matrixHere, the rational is: If the covariance is the same whether the variables are centered or not, the result of the PCA will be the same.Let’s assume we have the 2 variables $x$ and $y$ Then the covariance between the attributes is calculated as$$\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$Let us write the centered variables as$$x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}$$The centered covariance would then be calculated as follows:$$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i’ - \bar{y}’)$$But since after centering, $\bar{x}’ = 0$ and $\bar{y}’ = 0$ we have$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} x_i’ y_i’$ which is our original covariance matrix if we resubstitute back the terms $x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}$.Even centering only one variable, e.g., xx wouldn’t affect the covariance:$$\sigma_{\text{xy}} = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i - \bar{y})$$2. Scaling of variables does affect the covariance matrixIf one variable is scaled, e.g, from pounds into kilogram (1 pound = 0.453592 kg), it does affect the covariance and therefore influences the results of a PCA.Let cc be the scaling factor for $x$Given that the “original” covariance is calculated as$$\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$the covariance after scaling would be calculated as:$$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (c \cdot x_i - c \cdot \bar{x})(y_i - \bar{y}) = \frac{c}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y}) \Rightarrow \sigma_{xy}’ = c \cdot \sigma_{xy}$$Therefore, the covariance after scaling one attribute by the constant $c$ will result in a rescaled covariance $c \sigma_{xy}$ So if we’d scaled $x$ from pounds to kilograms, the covariance between $x$ and $y$ will be 0.453592 times smaller.3. Standardizing affects the covarianceStandardization of features will have an effect on the outcome of a PCA (assuming that the variables are originally not standardized). This is because we are scaling the covariance between every pair of variables by the product of the standard deviations of each pair of variables.The equation for standardization of a variable is written as$$z = \frac{x_i - \bar{x}}{\sigma}$$The “original” covariance matrix:$$\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$And after standardizing both variables:$$x’ = \frac{x - \bar{x}}{\sigma_x} \text{ and } y’ =\frac{y - \bar{y}}{\sigma_y}$$$$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - 0)(y_i’ - 0) = \frac{1}{n-1} \sum_{i}^{n} \bigg(\frac{x - \bar{x}}{\sigma_x}\bigg)\bigg(\frac{y - \bar{y}}{\sigma_y}\bigg) = \frac{1}{(n-1) \cdot \sigma_x \sigma_y} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$$$\Rightarrow \sigma_{xy}’ = \frac{\sigma_{xy}}{\sigma_x \sigma_y}$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Install SerpentAI on Windows 10]]></title>
      <url>%2F2018%2F06%2F07%2FInstall-SerpentAI-on-Windows-10%2F</url>
      <content type="text"><![CDATA[Python EnvironmentPython 3.6+ (with Anaconda)Serpent.AI was developed taking full advantage of Python 3.6 so it is only natural that the Python requirement be for versions 3.6 and up.Installing regular Python 3.6+ isn’t exactly difficult but Serpent.AI relies on a good amount of scientific computing libraries that are extremely difficult / impossible to compile on your own on Windows. Thankfully, the Anaconda Distribution exists and takes this huge weight off our collective shoulders.Installing Anaconda 5.2.0 (Python 3.6)Download the Python 3.6 version of Anaconda 5.2.0 and run the graphical installer.The following commands are to be performed in an Anaconda Prompt with elevated privileges (Right click and Run as Administrator). It is recommended to create a shortcut to this prompt because every Python and Serpent command will have to be performed from there starting now.Creating a Conda Env for Serpent.AIconda create --name serpent python=3.6 (‘serpent’ can be replaced with another name)Creating a directory for your Serpent.AI projectsmkdir SerpentAI &amp;&amp; cd SerpentAIActivating the Conda Envconda activate serpent3rd-Party DependenciesRedisRedis is used in the framework as the in-memory store for the captured frame buffers as well as the temporary storage of analytics events. It is not meant to be compatible with Windows! Microsoft used to maintain a port but it’s been abandoned since. This being said, that Redis version is sufficient and it outperforms stuff like running it in WSL on Windows 10. It will install as a Windows Service. Make sure you set it to start automatically.Install Windows Subsystem for Linux (WSL)From Start, search for Turn Windows features on or off (type turn)Select Windows Subsystem for Linux (beta)Once installed you can run bash on Ubuntu by typing bash from a Windows Command Prompt. To install the latest version of Redis we’ll need to use a repository that maintains up-to-date packages for Ubuntu and Debian servers like https://www.dotdeb.org which you can add to Ubuntu’s apt-get sources with:1234$ echo deb http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list$ echo deb-src http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list$ sudo mv dotdeb.org.list /etc/apt/sources.list.d$ wget -q -O - http://www.dotdeb.org/dotdeb.gpg | sudo apt-key add -Then after updating our APT cache we can install Redis with:12$ sudo apt-get update$ sudo apt-get install redis-serverYou’ll then be able to launch redis with:1$ redis-server --daemonize yesWhich will run redis in the background freeing your shell so you can play with it using the redis client:12345$ redis-cli$ 127.0.0.1:6379&gt; SET foo barOK$ 127.0.0.1:6379&gt; GET foo&quot;bar&quot;Which you can connect to from within bash or from your Windows desktop using the redis-cli native Windows binary from MSOpenTech.Build Tools for Visual Studio 2017Some of the packages that will be installed alongside Serpent.AI are not pre-compiled binaries and will be need to be built from source. This is a little more problematic for Windows but with the correct C++ Build Tools for Visual Studio it all goes down smoothly.You can get the proper installer by visiting https://www.visualstudio.com/downloads/ and scrolling down to the Build Tools for Visual Studio 2017 download. Download, run, select the Visual C++ build tools section and make sure the following components are checked (VSs are not installed):Visual C++ Build Tools core featuresVC++ 2017 version 15.7 v14.14 latest v141 toolsVisual C++ 2017 Redistributable UpdateVC++ 2015.3 v14.00 (v140) toolset for desktopWindows 10 SDK (10.0.17134.0)Windows Universal CRT SDKInstalling Serpent.AIOnce all of the above had been installed and set up, you are ready to install the framework. Remember that PATH changes in Windows are not reflected in your command prompts that were opened while you made the changes. Open a fresh Anaconda prompt before continuing to avoid installation issues.Go back to the directory you created earlier for your Serpent.AI projects. Make sure you are scoped in your Conda Env.Run pip install SerpentAIThen run serpent setup to install the remaining dependencies automatically.Installing Optional ModulesIn the spirit of keeping the initial installation on the light side, some specialized / niche components with extra dependencies have been isolated from the core. It is recommended to only focus on installing them once you reach a point where you actually need them. The framework will provide a warning when a feature you are trying to use requires one of those modules.OCRA module to provide OCR functionality in your game agents.TesseractSerpent.AI leverages Tesseract for its OCR functionality. You can install Tesseract for Windows by following these steps:Visit https://github.com/UB-Mannheim/tesseract/wikiDownload the .exe for version 3Run the graphical installer (Remember the install path!)Add the path to tesseract.exe to your %PATH% environment variableYou can test your Tesseract installation by opening an Anaconda Prompt and executing tesseract --list-langs.InstallationOnce you’ve validated that Tesseract has been properly set up, you can install the module with serpent setup ocrGUIA module to allow Serpent.AI desktop app to run.KivyKivy is the GUI framework used in the framework.Once you are ready to test your Kivy, you can install the module with serpent setup gui and try to run serpent visual_debugger]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Matching Networks for One Shot Learning]]></title>
      <url>%2F2018%2F06%2F02%2FMatching-Networks-for-One-Shot-Learning%2F</url>
      <content type="text"><![CDATA[By DeepMind crew: Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan WierstraThis is a paper on one-shot learning, where we’d like to learn a class based on very few (or indeed, 1) training examples. E.g. it suffices to show a child a single giraffe, not a few hundred thousands before it can recognize more giraffes.This paper falls into a category of “duh of course” kind of paper, something very interesting, powerful, but somehow obvious only in retrospect. I like it.Suppose you’re given a single example of some class and would like to label it in test images.Observation 1: a standard approach might be to train an Exemplar SVM for this one (or few) examples vs. all the other training examples - i.e. a linear classifier. But this requires optimization.Observation 2: known non-parameteric alternatives (e.g. k-Nearest Neighbor) don’t suffer from this problem. E.g. I could immediately use a Nearest Neighbor to classify the new class without having to do any optimization whatsoever. However, NN is gross because it depends on an (arbitrarily-chosen) metric, e.g. L2 distance. Ew.Core idea: lets train a fully end-to-end nearest neighbor classifer!The training protocolAs the authors amusingly point out in the conclusion (and this is the duh of course part), “one-shot learning is much easier if you train the network to do one-shot learning”. Therefore, we want the test-time protocol (given N novel classes with only k examples each (e.g. k = 1 or 5), predict new instances to one of N classes) to exactly match the training time protocol.To create each “episode” of training from a dataset of examples then:Sample a task T from the training data, e.g. select 5 labels, and up to 5 examples per label (i.e. 5-25 examples).To form one episode sample a label set L (e.g. {cats, dogs}) and then use L to sample the support set S and a batch B of examples to evaluate loss on.The idea on high level is clear but the writing here is a bit unclear on details, of exactly how the sampling is done.The modelI find the paper’s model description slightly wordy and unclear, but basically we’re building a differentiable nearest neighbor++. The output \hat{y} for a test example \hat{x} is computed very similar to what you might see in Nearest Neighbors:where a acts as a kernel, computing the extent to which \hat{x} is similar to a training example x_i, and then the labels from the training examples (y_i) are weight-blended together accordingly. The paper doesn’t mention this but I assume for classification y_i would presumbly be one-hot vectors.Now, we’re going to embed both the training examples x_i and the test example \hat{x}, and we’ll interpret their inner products (or here a cosine similarity) as the “match”, and pass that through a softmax to get normalized mixing weights so they add up to 1. No surprises here, this is quite natural:Here c() is cosine distance, which I presume is implemented by normalizing the two input vectors to have unit L2 norm and taking a dot product. I assume the authors tried skipping the normalization too and it did worse? Anyway, now all that’s left to define is the function f (i.e. how do we embed the test example into a vector) and the function g (i.e. how do we embed each training example into a vector?).Embedding the training examples. This (the function g) is a bidirectional LSTM over the examples:i.e. encoding of i’th example x_i is a function of its “raw” embedding g’(x_i) and the embedding of its friends, communicated through the bidirectional network’s hidden states. i.e. each training example is a function of not just itself but all of its friends in the set. This is part of the ++ above, because in a normal nearest neighbor you wouldn’t change the representation of an example as a function of the other data points in the training set.It’s odd that the order is not mentioned, I assume it’s random? This is a bit gross because order matters to a bidirectional LSTM; you’d get different embeddings if you permute the examples.Embedding the test example. This (the function f) is a an LSTM that processes for a fixed amount (K time steps) and at each point also attends over the examples in the training set. The encoding is the last hidden state of the LSTM. Again, this way we’re allowing the network to change its encoding of the test example as a function of the training examples. Nifty: That looks scary at first but it’s really just a vanilla LSTM with attention where the input at each time step is constant (f’(\hat{x}), an encoding of the test example all by itself) and the hidden state is a function of previous hidden state but also a concatenated readout vector r, which we obtain by attending over the encoded training examples (encoded with g from above).Oh and I assume there is a typo in equation (5), it should say r_k = … without the -1 on LHS.ExperimentsTask: N-way k-shot learning task. i.e. we’re given k (e.g. 1 or 5) labelled examples for N classes that we have not previously trained on and asked to classify new instances into he N classes.Baselines: an “obvious” strategy of using a pretrained ConvNet and doing nearest neighbor based on the codes. An option of finetuning the network on the new examples as well (requires training and careful and strong regularization!).MANN of Santoro et al. [21]: Also a DeepMind paper, a fun NTM-like Meta-Learning approach that is fed a sequence of examples and asked to predict their labels.Siamese network of Koch et al. [11]: A siamese network that takes two examples and predicts whether they are from the same class or not with logistic regression. A test example is labeled with a nearest neighbor: with the class it matches best according to the siamese net (requires iteration over all training examples one by one). Also, this approach is less end-to-end than the one here because it requires the ad-hoc nearest neighbor matching, while here the exact end task is optimized for. It’s beautiful.Omniglot experimentsOmniglot of Lake et al. [14] is a MNIST-like scribbles dataset with 1623 characters with 20 examples each.Image encoder is a CNN with 4 modules of [3x3 CONV 64 filters, batchnorm, ReLU, 2x2 max pool]. The original image is claimed to be so resized from original 28x28 to 1x1x64, which doesn’t make sense because factor of 2 downsampling 4 times is reduction of 16, and 28/16 is a non-integer &gt;1. I’m assuming they use VALID convs?Results: Matching nets do best. Fully Conditional Embeddings (FCE) by which I mean they the “Full Context Embeddings” of Section 2.1.2 instead are not used here, mentioned to not work much better. Finetuning helps a bit on baselines but not with Matching nets (weird).The comparisons in this table are somewhat confusing:I can’t find the MANN numbers of 82.8% and 94.9% in their paper [21]; not clear where they come from. E.g. for 5 classes and 5-shot they seem to report 88.4% not 94.9% as seen here. I must be missing something.I also can’t find the numbers reported here in the Siamese Net [11] paper. As far as I can tell in their Table 2 they report one-shot accuracy, 20-way classification to be 92.0, while here it is listed as 88.1%?The results of Lake et al. [14] who proposed Omniglot are also missing from the table. If I’m understanding this correctly they report 95.2% on 1-shot 20-way, while matching nets here show 93.8%, and humans are estimated at 95.5%. That is, the results here appear weaker than those of Lake et al., but one should keep in mind that the method here is significantly more generic and does not make any assumptions about the existence of strokes, etc., and it’s a simple, single fully-differentiable blob of neural stuff.(skipping ImageNet/LM experiments as there are few surprises)ConclusionsGood paper, effectively develops a differentiable nearest neighbor trained end-to-end. It’s something new, I like it!A few concerns:A bidirectional LSTMs (not order-invariant compute) is applied over sets of training examples to encode them. The authors don’t talk about the order actually used, which presumably is random, or mention this potentially unsatisfying feature. This can be solved by using a recurrent attentional mechanism instead, as the authors are certainly aware of and as has been discussed at length in ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS, where Oriol is also the first author. I wish there was a comment on this point in the paper somewhere.The approach also gets quite a bit slower as the number of training examples grow, but once this number is large one would presumable switch over to a parameteric approach.It’s also potentially concerning that during training the method uses a specific number of examples, e.g. 5-25, so this is the number of that must also be used at test time. What happens if we want the size of our training set to grow online? It appears that we need to retrain the network because the encoder LSTM for the training data is not “used to” seeing inputs of more examples? That is unless you fall back to iteratively subsampling the training data, doing multiple inference passes and averaging, or something like that. If we don’t use FCE it can still be that the attention mechanism LSTM can still not be “used to” attending over many more examples, but it’s not clear how much this matters. An interesting experiment would be to not use FCE and try to use 100 or 1000 training examples, while only training on up to 25 (with and fithout FCE). Discussion surrounding this point would be interesting.Not clear what happened with the Omniglot experiments, with incorrect numbers for [11], [21], and the exclusion of Lake et al. [14] comparison.A baseline that is missing would in my opinion also include training of an Exemplar SVM, which is a much more powerful approach than encode-with-a-cnn-and-nearest-neighbor.​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[One Shot Learning and Siamese Networks in Keras]]></title>
      <url>%2F2018%2F06%2F02%2FOne-Shot-Learning-and-Siamese-Networks-in-Keras%2F</url>
      <content type="text"><![CDATA[Background:Conventional wisdom says that deep neural networks are really good at learning from high dimensional data like images or spoken language, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of one-shot learning - if you take a human who’s never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with astoundingly high precision.Never been inside a kitchen before? Now’s your chance to test your one shot learning ability! which of the images on the right is of the same type as the big image? Email me for the correct answer...Yet another one of the things humans can do that seemed trivial to us right up until we tried to make an algorithm do it.This ability to rapidly learn from very little data seems like it’s obviously desirable for machine learning systems to have because collecting and labelling data is expensive. I also think this is an important step on the long road towards general intelligence.Recently there have been many interesting papers about one-shot learning with neural nets and they’ve gotten some good results. This is a new area that really excites me, so I wanted to make a gentle introduction to make it more accessible to fellow newcomers to deep learning.In this post, I want to:Introduce and formulate the problem of one-shot learningDescribe benchmarks for one-shot classification and give a baseline for performanceGive an example of deep one-shot learning by partially reimplementing the model in this paper with keras.Hopefully point out some small insights that aren’t obvious to everyoneFormulating the Problem - N-way One-Shot LearningBefore we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:Our model is given a tiny labelled training set SS, which has N examples, each vectors of the same dimension with a distinct label yy.$$S={(x_1,y_1),…,(x_N,y_N)}$$It is also given $\hat{x}$, the test example it has to classify. Since exactly one example in the support set has the right class, the aim is to correctly predict which $y \in S$ is the same as $\hat{x}$ ‘s label, $\hat{y}$.There are fancier ways of defining the problem, but this one is ours. Here are some things to make note of:Real world problems might not always have the constraint that exactly one image has the correct classIt’s easy to generalize this to k-shot learning by having there be k examples for each yiyirather than just one.When N is higher, there are more possible classes that $\hat{x}$ can belong to, so it’s harder to predict the correct one.Random guessing will average $\frac{100}{n}\%$ accuracy.Here are some examples of one-shot learning tasks on the Omniglot dataset, which I’ll describe in the next section.9, 25 and 36 way one-shot learnng tasks.About the data - Omniglot! :The Omniglot dataset is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105.A few of the alphabets from the omniglot dataset. As you can see, there’s a huge variety of different symbols.If you like machine learning, you’ve probably heard of the MNIST dataset. Omniglot is sometimes referred to as the transpose of mnist, since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits. There is also data about the strokes used to create each character, but we won’t be using that. Usually, it’s split into 30 training alphabets and 20 evaluation alphabets. All those different characters make for lots of possible one-shot tasks, so it’s a really good benchmark for one-shot learning algorithms.A One-Shot Learning Baseline / 1 Nearest NeighbourThe simplest way of doing classification is with k-nearest neighbours, but since there is only one example per class we have to do 1 nearest neighbour. This is very simple, just calculate the Euclidean distance of the test example from each training example and pick the closest one:$$C(\hat{x})={\arg \min}_{c\in S}||\hat{x} − x_c||$$According to Koch et al, 1-nn gets ~28% accuracy in 20 way one shot classification on omniglot. 28% doesn’t sound great, but it’s nearly six times more accurate than random guessing(5%). This is a good baseline or “sanity check” to compare future one-shot algorithms with.Hierarchical Bayesian Program Learning from Lake et al gets 95.2% - very impressive! The ~30% of this paper which I understood was very interesting. Comparing it with deep learning results that train on raw pixels is kind of “apples and oranges” though, because:HBPL used data about the strokes, not just the raw pixelsHBPL on omniglot involved learning a generative model for strokes. The algorithm requires data with more complicated annotation, so unlike deep learning it can’t easily be tweaked to one-shot learn from raw pixels of dogs/trucks/brain scans/spatulas and other objects that aren’t made up of brushstrokes.Lake et al also says that humans get 95.5% accuracy in 20 way classification on omniglot, only beating HBPL by a tiny margin. In the spirit of nullius in verba, I tried testing myself on the 20 way tasks and managed to average 97.2%. I wasn’t always doing true one-shot learning though - I saw several symbols I recognised, since I’m familiar with the greek alphabet, hiragana and katakana. I removed those alphabets and tried again but still managed 96.7%. My hypothesis is that having to read my own terrible handwriting has endowed me with superhuman symbol recognition ability.Ways to use deep networks for one shot learning?!If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will severely overfit. Heck, even if it was a hundred shot learning a modern neural net would still probably overfit. Big neural networks have millions of parameters to adjust to their data and so they can learn a huge space of possible functions. (More formally, they have a high VC dimension, which is part of why they do so well at learning from complex data with high dimensionality.) Unfortunately this strength also appears to be their undoing for one-shot learning. When there are millions of parameters to gradient descend upon, and a staggeringly huge number of possible mappings that can be learned, how can we make a network learn one that generalizes when there’s just a single example to learn from?It’s easier for humans to one-shot learn the concept of a spatula or the letter ΘΘ because they have spent a lifetime observing and learning from similar objects. It’s not really fair to compare the performance of a human who’s spent a lifetime having to classify objects and symbols with that of a randomly initialized neural net, which imposes a very weak prior about the structure of the mapping to be learned from the data. This is why most of the one-shot learning papers I’ve seen take the approach of knowledge transfer from other tasks.Neural nets are really good at extracting useful features from structurally complex/high dimensional data, such as images. If a neural network is given training data that is similar to (but not the same as) that in the one-shot task, it might be able to learn useful features which can be used in a simple learning algorithm that doesn’t require adjusting these parameters. It still counts as one-shot learning as long as the training examples are of different classes to the examples used for one-shot testing.(NOTE: Here a feature means a “transformation of the data that is useful for learning”.)So now an interesting problem is how do we get a neural network to learn the features? The most obvious way of doing this (if there’s labelled data) is just vanilla transfer learning - train a softmax classifier on the training set, then fine-tune the weights of the last layer on the support set of the one-shot task. In practice, neural net classifiers don’t work too well for data like omniglot where there are few examples per class, and even fine tuning only the weights in the last layer is enough to overfit the support set. Still works quite a lot better than L2 distance nearest neighbour though! (See Matching Networks for One Shot learning for a comparison table of various deep one-shot learning methods and their accuracy.)There’s a better way of doing it though! Remember 1 nearest neighbour? This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok, but L2 Distance suffers from the ominous sounding curse of dimensionality and so won’t work well for data with thousands of dimensions like omniglot. Also, if you have two nearly identical images and move one over a few pixels to the right the L2 distance can go from being almost zero to being really high. L2 distance is a metric that is just woefully inadequate for this task. Deep learning to the rescue? We can use a deep convolutional network to learn some kind of similarity function that a non-parametric classifer like nearest neighbor can use.Siamese networksI originally planned to have craniopagus conjoined twins as the accompanying image for this section but ultimately decided that siamese cats would go over better..This wonderful paper is what I will be implementing in this tutorial. Koch et al’s approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.Say x1 and x2 are two images in our dataset, and let x1∘x2 mean “x1 and x2 are images with the same class”. Note that x1∘x2 is the same as x2∘x1 - this means that if we reverse the order of the inputs to the neural network, the output should be the same - p(x1∘x2) should equal p(x2∘x1). This property is called symmetry and siamese nets are designed around having it.Symmetry is important because it’s required for learning a distance metric - the distance from x1 to x2 should equal the distance x2 to x1.If we just concatenate two examples together and use them as a single input to a neural net, each example will be matrix multiplied(or convolved) with a different set of weights, which breaks symmetry. Sure it’s possible it will eventually manage to learn the exact same weights for each input, but it would be much easier to learn a single set of weights applied to both inputs. So we could propagate both inputs through identical twin neural nets with shared parameters, then use the absolute difference as the input to a linear classifier - this is essentially what a siamese net is. Two identical twins, joined at the head, hence the name.Network architectureUnfortunately, properly explaining how and why a convolutional neural net work would make this post twice as long. If you want to understand convnets work, I suggest checking out cs231n and then colah. For any non-dl people who are reading this, the best summary I can give of a CNN is this: An image is a 3D array of pixels. A convolutional layer is where you have a neuron connected to a tiny subgrid of pixels or neurons, and use copies of that neuron across all parts of the image/block to make another 3d array of neuron activations. A max pooling layer makes a block of activations spatially smaller. Lots of these stacked on top of one another can be trained with gradient descent and are really good at learning from images.I’m going to describe the architecture pretty briefly because it’s not the important part of the paper. Koch et al uses a convolutional siamese network to classify pairs of omniglot images, so the twin networks are both convolutional neural nets(CNNs). The twins each have the following architecture: convolution with 64 10x10 filters, relu -&gt; max pool -&gt; convolution with 128 7x7 filters, relu -&gt; max pool -&gt; convolution with 128 4x4 filters, relu -&gt; max pool -&gt; convolution with 256 4x4 filters. The twin networks reduce their inputs down to smaller and smaller 3d tensors, finally their is a fully connected layer with 4096 units. The absolute difference between the two vectors is used as input to a linear classifier. All up, the network has 38,951,745 parameters - 96% of which belong to the fully connected layer. This is quite a lot, so the network has high capacity to overfit, but as I show below, pairwse training means the dataset size is huge so this won’t be a problem.Hastily made architecture diagram.The output is squashed into [0,1] with a sigmoid function to make it a probability. We use the target t=1t=1 when the images have the same class and t=0t=0 for a different class. It’s trained with logistic regression. This means the loss function should be binary cross entropy between the predictions and targets. There is also a L2 weight decay term in the loss to encourage the network to learn smaller/less noisy weights and possibly improve generalization:$$L(x_1,x_2,t)=t⋅\log(p(x_1∘x_2))+(1−t)⋅\log(1−p(x_1∘x_2))+λ⋅||w||^2$$When it does a one-shot task, the siamese net simply classifies the test image as whatever image in the support set it thinks is most similar to the test image:$$C(\hat{x},S) = {\arg\max}_c P(\hat{x}∘x_c),x_c∈S$$This uses an argmax unlike nearest neighbour which uses an argmin, because a metric like L2 is higher the more “different” the examples are, but this models outputs p(x1∘x2), so we want the highest. This approach has one flaw that’s obvious to me: for any xaxa in the support set,the probability $\hat{x}∘x_a$ is independent of every other example in the support set! This means the probabilities won’t sum to 1, ignores important information, namely that the test image will be the same type as exactly one x∈S…Observation: effective dataset size in pairwise trainingEDIT: After discussing this with a PhD student at UoA, I think this bit might be overstated or even just wrong. Emperically, my implementation did overfit, even though it wasn’t trained for enough iterations to sample every possible pair, which kind of contradicts this section. I’m leaving it up in the spirit of being wrong loudly.One cool thing I noticed about training on pairs is that there are quadratically many possible pairs of images to train the model on, making it hard to overfit. Say we have CC examples each of EE classes. Since there are C⋅EC⋅E images total, the total number of possible pairs is given by$$Npairs=(C⋅E 2)=(C⋅E)!/2!(C⋅E−2)!$$For omniglot with its 20 examples of 964 training classes, this leads to 185,849,560 possible pairs, which is huge! However, the siamese network needs examples of both same and different class pairs. There are E examples per class, so there will be (E 2) pairs for every class, which means there are Nsame=(E 2)⋅C possible pairs with the same class - 183,160 pairs for omniglot. Even though 183,160 example pairs is plenty, it’s only a thousandth of the possible pairs, and the number of same-class pairs increases quadratically with E but only linearly with C. This is important because the siamese network should be given a 1:1 ratio of same-class and different-class pairs to train on - perhaps it implies that pairwise training is easier on datasets with lots of examples per class.The Code:Prefer to just play with a jupyter notebook? I got you famHere is the model definition, it should be pretty easy to follow if you’ve seen keras before. I only define the twin network’s architecture once as a Sequential() model and then call it with respect to each of two input layers, this way the same parameters are used for both inputs. Then merge them together with absolute distance and add an output layer, and compile the model with binary cross entropy loss.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2Dfrom keras.models import Model, Sequentialfrom keras.regularizers import l2from keras import backend as Kfrom keras.optimizers import SGD,Adamfrom keras.losses import binary_crossentropyimport numpy.random as rngimport numpy as npimport osimport dill as pickleimport matplotlib.pyplot as pltfrom sklearn.utils import shuffledef W_init(shape,name=None): """Initialize weights as in paper""" values = rng.normal(loc=0,scale=1e-2,size=shape) return K.variable(values,name=name)#//TODO: figure out how to initialize layer biases in keras.def b_init(shape,name=None): """Initialize bias as in paper""" values=rng.normal(loc=0.5,scale=1e-2,size=shape) return K.variable(values,name=name)input_shape = (105, 105, 1)left_input = Input(input_shape)right_input = Input(input_shape)#build convnet to use in each siamese 'leg'convnet = Sequential()convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape, kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))convnet.add(MaxPooling2D())convnet.add(Conv2D(128,(7,7),activation='relu', kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))convnet.add(MaxPooling2D())convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))convnet.add(MaxPooling2D())convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))convnet.add(Flatten())convnet.add(Dense(4096,activation="sigmoid",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))#encode each of the two inputs into a vector with the convnetencoded_l = convnet(left_input)encoded_r = convnet(right_input)#merge two encoded inputs with the l1 distance between themL1_distance = lambda x: K.abs(x[0]-x[1])both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)siamese_net = Model(input=[left_input,right_input],output=prediction)#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)optimizer = Adam(0.00006)#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworkingsiamese_net.compile(loss="binary_crossentropy",optimizer=optimizer)siamese_net.count_params()The original paper used layerwise learning rates and momentum - I skipped this because it; was kind of messy to implement in keras and the hyperparameters aren’t the interesting part of the paper. Koch et al adds examples to the dataset by distorting the images and runs experiments with a fixed training set of up to 150,000 pairs. Since that won’t fit in my computers memory, I decided to just randomly sample pairs. Loading image pairs was probably the hardest part of this to implement. Since there were 20 examples for every class, I reshaped the data into N_classes x 20 x 105 x 105 arrays, to make it easier to index by category.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Siamese_Loader: """For loading batches and testing tasks to a siamese net""" def __init__(self,Xtrain,Xval): self.Xval = Xval self.Xtrain = Xtrain self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape self.n_val,self.n_ex_val,_,_ = Xval.shape def get_batch(self,n): """Create batch of n pairs, half same class, half different class""" categories = rng.choice(self.n_classes,size=(n,),replace=False) pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)] targets=np.zeros((n,)) targets[n//2:] = 1 for i in range(n): category = categories[i] idx_1 = rng.randint(0,self.n_examples) pairs[0][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,1) idx_2 = rng.randint(0,self.n_examples) #pick images of same class for 1st half, different for 2nd category_2 = category if i &gt;= n//2 else (category + rng.randint(1,self.n_classes)) % self.n_classes pairs[1][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,1) return pairs, targets def make_oneshot_task(self,N): """Create pairs of test image, support set for testing N way one-shot learning. """ categories = rng.choice(self.n_val,size=(N,),replace=False) indices = rng.randint(0,self.n_ex_val,size=(N,)) true_category = categories[0] ex1, ex2 = rng.choice(self.n_examples,replace=False,size=(2,)) test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1) support_set = self.Xval[categories,indices,:,:] support_set[0,:,:] = self.Xval[true_category,ex2] support_set = support_set.reshape(N,self.w,self.h,1) pairs = [test_image,support_set] targets = np.zeros((N,)) targets[0] = 1 return pairs, targets def test_oneshot(self,model,N,k,verbose=0): """Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks""" pass n_correct = 0 if verbose: print("Evaluating model on &#123;&#125; unique &#123;&#125; way one-shot learning tasks ...".format(k,N)) for i in range(k): inputs, targets = self.make_oneshot_task(N) probs = model.predict(inputs) if np.argmax(probs) == 0: n_correct+=1 percent_correct = (100.0*n_correct / k) if verbose: print("Got an average of &#123;&#125;% &#123;&#125; way one-shot learning accuracy".format(percent_correct,N)) return percent_correct..And now the training loop. Nothing unusual here, except for that I monitor one-shot tasks validation accuracy to test performance, rather than loss on the validation set.12345678910111213141516171819evaluate_every = 7000loss_every=300batch_size = 32N_way = 20n_val = 550siamese_net.load_weights("PATH")best = 76.0for i in range(900000): (inputs,targets)=loader.get_batch(batch_size) loss=siamese_net.train_on_batch(inputs,targets) if i % evaluate_every == 0: val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True) if val_acc &gt;= best: print("saving") siamese_net.save('PATH') best=val_acc if i % loss_every == 0: print("iteration &#123;&#125;, training loss: &#123;:.2f&#125;,".format(i,loss))ResultsOnce the learning curve flattened out, I used the weights which got the best validation 20 way accuracy for testing. My network averaged ~83% accuracy for tasks from the evaluation set, compared to 93% in the original paper. Probably this difference is because I didn’t implement many of the performance enhancing tricks from the original paper, like layerwise learning rates/momentum, data augmentation with distortions, bayesian hyperparemeter optimization and I also probably trained for less epochs. I’m not too worried about this because this tutorial was more about introducing one-shot learning in general, than squeezing the last few % performance out of a classifier. There is no shortage of resources on that!I was curious to see how accuracy varied over different values of “N” in N way one shot learning, so I plotted it, with comparisons to 1 nearest neighbours, random guessing and training set performance.results.As you can see, it performs worse on tasks from the validaiton set than the train set, especially for high values of N, so there must be overfitting. It would be interesting to see how well traditional regularization methods like dropout work when the validation set is made of completely different classes to the training set. It works better than I expected for large N, still averaging above 65% accuracy for 50-60 way tasks.DiscussionWe’ve just trained a neural network trained to do same-different pairwise classification on symbols. More importantly, we’ve shown that it can then get reasonable accuracy in 20 way one-shot learning on symbols from unseen alphabets. Of course, this is not the only way to use deep networks for one-shot learning.As I touched on earlier, I think a major flaw of this siamese approach is that it only compares the test image to every support image individualy, when it should be comparing it to the support set as a whole. When the network compares the test image to any image x1, p(x^∘x1) is the same no matter what else is the support set. This is silly. Say you’re doing a one-shot task and you see an image that looks similar to the test image. You should be much less confident they have the same class if there is another image in the support set that also looks similar to the test image. The training objective is different to the test objective. It might work better to have a model that can compare the test image to the support set as a whole and use the constraint that only one support image has the same class.Matching Networks for One Shot learning does exactly that. Rather than learning a similarity function, they have a deep model learn a full nearest neighbour classifier end to end, training directly on oneshot tasks rather than on image pairs. Andrej Karpathy’s notes explain it much better than I can. Since you are learning a machine classifier, this can be seen as a kind of meta-learning. One-shot Learning with Memory-Augmented Neural Networks explores the connection between one-shot learning and meta learning and trains a memory augmented network on omniglot, though I confess I had trouble understanding this paper.What next?The omniglot dataset has been around since 2015, and already there are scalable ML algorithms getting within the ballpark of human level performance on certain one-shot learning tasks. Hopefully one day it will be seen as a mere “sanity check” for one-shot classification algorithms much like MNIST is for supervised learning now.Image classification is cool but I don’t think it’s the most interesting problem in machine learning. Now that we know deep one-shot learning can work pretty good, I think it would be cool to see attempts at one-shot learning for other, more exotic tasks.Ideas from one-shot learning could be used for more sample efficient reinforcement learning, especially for problems like OpenAI’s Universe, where there are lots of MDPs/environments that have similar visual features and dynamics. - It would be cool to have an RL agent that could efficiently explore a new environment after learning in similar MDPs.OpenAI’s world of bits environments.One-shot Imitation learning is one of my favourite one-shot learning papers. The goal is to have an agent learn a robust policy for solving a task from a single human demonstration of that task.This is done by:Having a neural net map from the current state and a sequence of states(the human demonstration) to an actionTraining it on pairs of human demonstrations on slightly different variants of the same task, with the goal of reproducing the second demonstration based on the first.This strikes me as a really promising path to one day having broadly applicable, learning based robots!Bringing one-shot learning to NLP tasks is a cool idea too. Matching Networks for One-Shot learning has an attempt at one-shot language modeling, filling a missing word in a test sentence given a small set of support sentences, and it seems to work pretty well. Exciting!ConclusionAnyway, thanks for reading! I hope you’ve managed to one-shot learn the concept of one-shot learning :) If not, I’d love to hear feedback or answer any questions you have!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Anaconda uses socket proxy on Windows 10]]></title>
      <url>%2F2018%2F05%2F17%2FAnaconda-uses-socket-proxy-on-Windows-10%2F</url>
      <content type="text"><![CDATA[you need to create a .condarc file in you Windows user area:1C:\Users\&lt;username&gt;\The file should contain (if you are using shadowsocks):12345678910111213channels:- defaults# Show channel URLs when displaying what is going to be downloaded and# in 'conda list'. The default is False.show_channel_urls: Trueallow_other_channels: Trueproxy_servers: http: socks5://127.0.0.1:1080 https: socks5://127.0.0.1:1080ssl_verify: FalseNoticed that you cannot create a file that begins with a dot in Windows directly.To create/rename on windows explorer, just rename to .name. - The additional dot at the end is necessary, and will be removed by Windows Explorer.To create a new file begins with a dot, on command prompt:1echo testing &gt; .name]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Paper Note: Selective Search for Object Recognition]]></title>
      <url>%2F2018%2F05%2F03%2FPaper-Note-Selective-Search-for-Object-Recognition%2F</url>
      <content type="text"><![CDATA[与 Selective Search 初次见面是在著名的物体检测论文 Rich feature hierarchies for accurate object detection and semantic segmentation ，因此，这篇论文算是阅读 R-CNN 的准备。这篇论文的标题虽然也提到了 Object Recognition ，但就创新点而言，其实在 Selective Search 。所以，这里只简单介绍 Selective Search 的思想和算法过程，对于 Object Recognition 则不再赘述。什么是 Selective SearchSelective Search，说的简单点，就是从图片中找出物体可能存在的区域。result上面这幅宇航员的图片中，那些红色的框就是 Selective Search 找出来的可能存在物体的区域。在进一步探讨它的原理之前，我们分析一下，如何判别哪些 region 属于一个物体？image seg作者在论文中用以上四幅图，分别描述了四种可能的情况：图 a ，物体之间可能存在层级关系，比如：碗里有个勺；图 b，我们可以用颜色来分开两只猫，却没法用纹理来区分；图 c，我们可以用纹理来区分变色龙，却没法用颜色来区分；图 d，轮胎是车的一部分，不是因为它们颜色相近、纹理相近，而是因为轮胎包含在车上。所以，我们没法用单一的特征来定位物体，需要综合考虑多种策略，这一点是 Selective Search 精要所在。需要考虑的问题在学习 Selective Search 算法之前，我曾在计算机视觉课上学到过关于物体（主要是人脸）检测的方法。通常来说，最常规也是最简单粗暴的方法，就是用不同尺寸的矩形框，一行一行地扫描整张图像，通过提取矩形框内的特征判断是否是待检测物体。这种方法的复杂度极高，所以又被称为 exhaustive search。在人脸识别中，由于使用了 Haar 特征，因此可以借助 Paul Viola 和 Michael Jones 两位大牛提出的积分图，使检测在常规时间内完成。但并不是每种特征都适用于积分图，尤其在神经网络中，积分图这种动态规划的思路就没什么作用了。针对传统方法的不足，Selective Search 从三个角度提出了改进：我们没法事先得知物体的大小，在传统方法中需要用不同尺寸的矩形框检测物体，防止遗漏。而 Selective Search 采用了一种具备层次结构的算法来解决这个问题；检测的时间复杂度可能会很高。Selective Search 遵循简单即是美的原则，只负责快速地生成可能是物体的区域，而不做具体的检测；另外，结合上一节提出的，采用多种先验知识来对各个区域进行简单的判别，避免一些无用的搜索，提高速度和精度。算法框架algorithm论文中给出的这个算法框架还是很详细的，这里再简单翻译一下。输入：彩色图片。输出：物体可能的位置，实际上是很多的矩形坐标。首先，我们使用这篇论文的方法将图片初始化为很多小区域 $R=r_i, \cdots, r_n$。由于我们的重点是 Selective Search，因此我直接将该论文的算法当成一个黑盒子。初始化一个相似集合为空集： $S=∅$。计算所有相邻区域之间的相似度（相似度函数之后会重点分析），放入集合 S 中，集合 S 保存的其实是一个区域对以及它们之间的相似度。找出 S 中相似度最高的区域对，将它们合并，并从 S 中删除与它们相关的所有相似度和区域对。重新计算这个新区域与周围区域的相似度，放入集合 S 中，并将这个新合并的区域放入集合 R 中。重复这个步骤直到 S 为空。从 R 中找出所有区域的 bounding box（即包围该区域的最小矩形框），这些 box 就是物体可能的区域。另外，为了提高速度，新合并区域的 feature 可以通过之前的两个区域获得，而不必重新遍历新区域的像素点进行计算。这个 feature 会被用于计算相似度。相似度计算方法相似度计算方法将直接影响合并区域的顺序，进而影响到检测结果的好坏。论文中比较了八种颜色空间的特点，在实际操作中，只选择一个颜色空间（比如：RGB 空间）进行计算。正如一开始提出的那样，我们需要综合多种信息来判断。作者将相似度度量公式分为四个子公式，称为互补相似度测量(Complementary Similarity Measures) 。这四个子公式的值都被归一化到区间 [0, 1] 内。1. 颜色相似度scolor (ri,rj)scolor (ri,rj)正如本文一开始提到的，颜色是一个很重要的区分物体的因素。论文中将每个 region 的像素按不同颜色通道统计成直方图，其中，每个颜色通道的直方图为 25 bins （比如，对于 0 ～ 255 的颜色通道来说，就每隔 9(255/25=9) 个数值统计像素数量）。这样，三个通道可以得到一个 75 维的直方图向量 $C_i={c_{i}^{1}, …, c_{i}^{n}}$，其中 n = 75。之后，我们用 L1 范数（绝对值之和）对直方图进行归一化。由直方图我们就可以计算两个区域的颜色相似度：$$s_{color}(r_i, r_j) =\sum_{k=1}^{n}{min(c_{i}^{k}, c_{j}^{k})}$$这个颜色直方图可以在合并区域的时候，很方便地传递给下一级区域。即它们合并后的区域的直方图向量为：$$C_t=\frac{size(r_i)C_i+size(r_j)C_j}{size(r_i)+size(r_j)}$$，其中$size(r_i)$ 表示区域 $r_i$ 的面积，合并后的区域为 $size(r_t)=size(r_i)+size(r_j)$。2. 纹理相似度$s_{texture}(r_i,r_j)$另一个需要考虑的因素是纹理，即图像的梯度信息。论文中对纹理的计算采用了 SIFT-like 特征，该特征借鉴了 SIFT 的计算思路，对每个颜色通道的像素点，沿周围 8 个方向计算高斯一阶导数(σ=1σ=1)，每个方向统计一个直方图（bin = 10），这样，一个颜色通道统计得到的直方图向量为 80 维，三个通道就是 240 维：$T_i={t_i^{(1)}, …, t_i^{(n)}}$，其中 n = 240。注意这个直方图要用 L1 范数归一化。然后，我们按照颜色相似度的计算思路计算两个区域的纹理相似度：$$s_{texture}(r_i, r_j) =\sum_{k=1}^{n}{min(t_{i}^{k}, t_{j}^{k})}$$3. 尺寸相似度$s_{size} (r_i,r_j)$在合并区域的时候，论文优先考虑小区域的合并，这种做法可以在一定程度上保证每次合并的区域面积都比较相似，防止大区域对小区域的逐步蚕食。这么做的理由也很简单，我们要均匀地在图片的每个角落生成不同尺寸的区域，作用相当于 exhaustive search 中用不同尺寸的矩形扫描图片。具体的相似度计算公式为：$$s_{size}(r_i, r_j)=1-\frac{size(r_i) + size(r_j)}{size(im)}$$其中，$size(im)$ 表示原图片的像素数量。4. 填充相似度$s_{fill}(r_i,r_j)$填充相似度主要用来测量两个区域之间 fit 的程度，个人觉得这一点是要解决文章最开始提出的物体之间的包含关系（比如：轮胎包含在汽车上）。在给出填充相似度的公式前，我们需要定义一个矩形区域 $BB_{ij}$，它表示包含 $r_i$ 和 $r_j$ 的最小的 bounding box。基于此，我们给出相似度计算公式为：$$s_{fill}(r_i, r_j)=1-\frac{size(BB_{ij})-size(r_i)-size(r_j)}{size(im)}$$为了高效地计算 $BB_{ij}$，我们可以在计算每个 region 的时候，都保存它们的 bounding box 的位置，这样，$BB_{ij}$ 就可以很快地由两个区域的 bounding box 推出来。5. 相似度计算公式综合上面四个子公式，我们可以得到计算相似度的最终公式：$$s(r_i, r_j) = a_1 s_{color}(r_i, r_j) +a_2s_{texture}(r_i, r_j) \\\\ +a_3s_{size}(r_i, r_j)+a_4s_{fill}(r_i, r_j)$$其中，$a_i$的取值为 0 或 1，表示某个相似度是否被采纳。Combining Locations前面我们基本完成了 Selective Search 的流程，从图片中提取出了物体可能的位置。现在，我们想完善最后一个问题，那就是给这些位置排个序。因为提取出来的矩形框数量巨大，而用户可能只需要其中的几个，这个时候我们就很有必要对这些矩形框赋予优先级，按照优先级高低返回给用户。原文中作者称这一步为 Combining Locations，我找不出合适的翻译，就姑且保留英文原文。这个排序的方法也很简单。作者先给各个 region 一个序号，前面说了，Selective Search 是一个逐步合并的层级结构，因此，我们将覆盖整个区域的 region 的序号标记为 1，合成这个区域的两个子区域的序号为 2，以此类推。但如果仅按序号排序，会存在一个漏洞，那就是区域面积大的会排在前面，为了避免这个漏洞，作者又在每个序号前乘上一个随机数 $RND∈[0,1]$，通过这个新计算出来的数值，按从小到大的顺序得出 region 最终的排序结果。参考Selective Search for Object Recognition(阅读)Efficient Graph-Based Image Segmentation本文作者： Jermmy本文链接： https://jermmy.github.io/2017/05/04/2017-5-4-paper-notes-selective-search/版权声明： 本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Micro- and Macro-average of Precision, Recall and F-Score]]></title>
      <url>%2F2018%2F04%2F27%2FMicro-and-Macro-average-of-Precision-Recall-and-F-Score%2F</url>
      <content type="text"><![CDATA[Micro-average MethodIn Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics. For example, for a set of data, the system’sTrue positive (TP1)= 12False positive (FP1)=9False negative (FN1)=3Then precision (P1) and recall (R1) will be 57.14 and 80and for a different set of data, the system’sTrue positive (TP2)= 50False positive (FP2)=23False negative (FN2)=9Then precision (P2) and recall (R2) will be 68.49 and 84.75Now, the average precision and recall of the system using the Micro-average method isMicro-average of precision = (TP1+TP2)/(TP1+TP2+FP1+FP2) = (12+50)/(12+50+9+23) = 65.96Micro-average of recall = (TP1+TP2)/(TP1+TP2+FN1+FN2) = (12+50)/(12+50+3+9) = 83.78The Micro-average F-Score will be simply the harmonic mean of these two figures.Macro-average MethodThe method is straight forward. Just take the average of the precision and recall of the system on different sets. For example, the macro-average precision and recall of the system for the given example isMacro-average precision = (P1+P2)/2 = (57.14+68.49)/2 = 62.82Macro-average recall = (R1+R2)/2 = (80+84.75)/2 = 82.25The Macro-average F-Score will be simply the harmonic mean of these two figures.SuitabilityMacro-average method can be used when you want to know how the system performs overall across the sets of data. You should not come up with any specific decision with this average.On the other hand, micro-average can be a useful measure when your dataset varies in size.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Get financial data from Tushare]]></title>
      <url>%2F2018%2F04%2F11%2FGet-financial-data-from-Tushare%2F</url>
      <content type="text"><![CDATA[IntroductionTuShare is a famous free, open source python financial data interface package. Its official home page is: TuShare - financial data interface package. The interface package now provides a large amount of financial data covering a wide range of data such as stocks, fundamentals, macros, news, etc. (Please check the official website for details) and keep updating. At present, the length of the stock’s data is three years. Although it is a bit short, it can basically meet the needs of quantitative beginners for testing.TutorialInstall and ImportYou need to install first:PandaslxmlTwo way to install tushare:pip install tusharevisit https://pypi.python.org/pypi/Tushare/, download and installHow to update:pip install tushare --upgradeImport package and view package version:123import tushareprint(tushare.__version__)Use some simple functionStock dataupdate：Many of the quotes returned by the get_hist_data function are wrong, but both get_h_data and get_k_data can be usedWe should still master how to use tushare to obtain stock market data, using the ts.get_hist_data() function whose input parameters are:code: Stock code, ie 6-digit code, or index code (sh = Shanghai index sz = Shenzhen index hs300 = CSI 300 index sz50 = SSE 50 zxb = small and medium board cyb = board)start: Start date, format YYYY-MM-DDend: End date, format YYYY-MM-DDktype: Data type, D = day k line W = week M = month 5 = 5 minutes 15 = 15 minutes 30 = 30 minutes 60 = 60 minutes, the default is Dretry_count: The number of retries after the network is abnormal. The default is 3pause: Pause seconds when retrying, default is 0Return values:date：dateopen：Opening pricehigh：Highest priceclose：Closing pricelow：Lowest pricevolume：Volumeprice_change：price fluncuationp_change：Quote changema5：5-day average pricema10：10-day average pricema20: 20-day average pricev_ma5: 5-day average volumev_ma10: 10-day average volumev_ma20: 20-day average volumeturnover: Change in hand rate [Note: Index does not have this item]Specific examples:12345678910111213141516171819202122ts.get_hist_data('600848') date open high close low volume p_change ma5 2012-01-11 6.880 7.380 7.060 6.880 14129.96 2.62 7.0602012-01-12 7.050 7.100 6.980 6.900 7895.19 -1.13 7.0202012-01-13 6.950 7.000 6.700 6.690 6611.87 -4.01 6.9132012-01-16 6.680 6.750 6.510 6.480 2941.63 -2.84 6.8132012-01-17 6.660 6.880 6.860 6.460 8642.57 5.38 6.8222012-01-18 7.000 7.300 6.890 6.880 13075.40 0.44 6.7882012-01-19 6.690 6.950 6.890 6.680 6117.32 0.00 6.7702012-01-20 6.870 7.080 7.010 6.870 6813.09 1.74 6.832date ma10 ma20 v_ma5 v_ma10 v_ma20 turnover2012-01-11 7.060 7.060 14129.96 14129.96 14129.96 0.482012-01-12 7.020 7.020 11012.58 11012.58 11012.58 0.272012-01-13 6.913 6.913 9545.67 9545.67 9545.67 0.232012-01-16 6.813 6.813 7894.66 7894.66 7894.66 0.102012-01-17 6.822 6.822 8044.24 8044.24 8044.24 0.302012-01-18 6.833 6.833 7833.33 8882.77 8882.77 0.452012-01-19 6.841 6.841 7477.76 8487.71 8487.71 0.212012-01-20 6.863 6.863 7518.00 8278.38 8278.38 0.23You can also set the start time and end time of historical data:12345678910111213ts.get_hist_data('600848',start='2015-01-05',end='2015-01-09') date open high close low volume p_change ma5 ma102015-01-05 11.160 11.390 11.260 10.890 46383.57 1.26 11.156 11.2122015-01-06 11.130 11.660 11.610 11.030 59199.93 3.11 11.182 11.1552015-01-07 11.580 11.990 11.920 11.480 86681.38 2.67 11.366 11.2512015-01-08 11.700 11.920 11.670 11.640 56845.71 -2.10 11.516 11.3492015-01-09 11.680 11.710 11.230 11.190 44851.56 -3.77 11.538 11.363 date ma20 v_ma5 v_ma10 v_ma20 turnover2015-01-05 11.198 58648.75 68429.87 97141.81 1.592015-01-06 11.382 54854.38 63401.05 98686.98 2.032015-01-07 11.543 55049.74 61628.07 103010.58 2.972015-01-08 11.647 57268.99 61376.00 105823.50 1.95Others:123456789101112ts.get_hist_data('600848', ktype='W') # Get weekly k-line datats.get_hist_data('600848', ktype='M') # Get monthly k-line datats.get_hist_data('600848', ktype='5') # Get 5 minutes k-line datats.get_hist_data('600848', ktype='15') # Get 15 minutes k-line datats.get_hist_data('600848', ktype='30') # Get 30 minutes k-line datats.get_hist_data('600848', ktype='60') # Get 60 minutes k-line datats.get_hist_data('sh'）# Get data on the Shanghai index k-line, other parameters consistent with the stocks, the same belowts.get_hist_data('sz'）# Get Shenzhen Chengzhi k line datats.get_hist_data('hs300'）# Get the CSI 300 k line datats.get_hist_data('sz50'）# Get SSE 50 Index k-line datats.get_hist_data('zxb'）# Get the k-line data of small and medium board indicests.get_hist_data('cyb'）# Get GEM Index k-line dataFundamental dataWith tushare we can also get fundamental data through ts.get_stock_basics() (shown in the results section):1234567891011121314ts.get_stock_basics()code name industry area pe outstanding totals totalAssets 300563 N神宇 通信设备 江苏 26.73 2000.00 8000.00 4.216000e+04 601882 海天精工 机床制造 浙江 26.83 5220.00 52200.00 1.877284e+05 601880 大连港 港口 辽宁 76.40 773582.00 1289453.63 3.263012e+06 300556 丝路视觉 软件服务 深圳 101.38 2780.00 11113.33 4.448248e+04 600528 中铁二局 建筑施工 四川 149.34 145920.00 145920.00 5.709568e+06 002495 佳隆股份 食品 广东 202.12 66611.13 93562.56 1.169174e+05 600917 重庆燃气 供气供热 重庆 76.87 15600.00 155600.00 8.444600e+05 002752 昇兴股份 广告包装 福建 75.14 12306.83 63000.00 2.387493e+05 002346 柘中股份 电气设备 上海 643.97 7980.00 44157.53 2.263010e+05 000680 山推股份 工程机械 山东 0.00 105694.97 124078.75 9.050701e+05...Macro dataWe use the resident consumer index as an example, which can be obtained through the ts.get_cpi() function (it will get 322 items at a time, some of them will be displayed):123456789101112131415print ts.get_cpi() month cpi0 2016.10 102.101 2016.9 101.902 2016.8 101.343 2016.7 101.774 2016.6 101.885 2016.5 102.046 2016.4 102.337 2016.3 102.308 2016.2 102.289 2016.1 101.7510 2015.12 101.64...Recent newsThe tushare package can use the ts.get_latest_news() function to view the latest news, and it will return 80. For reasons of space, we only show the first 15 here. We can see that it is all Sina Finance’s news data.12345678910111213141516171819202122232425262728293031323334353637print ts.get_latest_news(); classify title time \0 美股 “特朗普通胀”预期升温 美国国债下挫 11-14 23:10 1 美股 特朗普：脸书、推特等社交媒体助我入主白宫 11-14 23:10 2 证券 11月14日晚增减持每日速览 11-14 22:54 3 美股 财经观察：日本为何急于推动TPP批准程序 11-14 22:54 4 美股 新总统谜题：特朗普会连续加息吗？ 11-14 22:52 5 证券 神州专车财报遭质疑 增发100亿股东退出需50年 11-14 22:41 6 证券 恒大闪电杀回马枪锁仓半年 戒短炒了吗？ 11-14 22:38 7 国内财经 楼继伟力推改革做派 或加快国有资本划拨社保 11-14 22:36 8 美股 开盘：美股周一小幅高开 延续上周涨势 11-14 22:32 9 美股 喜达屋创始人：当好总统就要走中庸之道 11-14 22:24 10 证券 北京高华：将乐视网评级下调至中性 11-14 22:09 11 美股 11月14日22点交易员正关注要闻 11-14 22:02 12 美股 摩根大通：新兴市场股市、货币的前景悲观 11-14 21:55 13 国内财经 人民日报刊文谈全面深化改革这三年：啃下硬骨头 11-14 21:46 14 证券 泽平宏观：经济L型延续 地产销量回落投资超预期 11-14 21:43 15 证券 黄燕铭等五大券商大佬告诉你 2017年买点啥？ 11-14 21:41 url 0 http://finance.sina.com.cn/stock/usstock/c/201... 1 http://finance.sina.com.cn/stock/usstock/c/201... 2 http://finance.sina.com.cn/stock/y/2016-11-14/... 3 http://finance.sina.com.cn/stock/usstock/c/201... 4 http://finance.sina.com.cn/stock/usstock/c/201... 5 http://finance.sina.com.cn/stock/marketresearc... 6 http://finance.sina.com.cn/stock/marketresearc... 7 http://finance.sina.com.cn/china/gncj/2016-11-... 8 http://finance.sina.com.cn/stock/usstock/c/201... 9 http://finance.sina.com.cn/stock/usstock/c/201... 10 http://finance.sina.com.cn/stock/s/2016-11-14/... 11 http://finance.sina.com.cn/stock/usstock/c/201... 12 http://finance.sina.com.cn/stock/usstock/c/201... 13 http://finance.sina.com.cn/china/gncj/2016-11-... 14 http://finance.sina.com.cn/stock/marketresearc... 15 http://finance.sina.com.cn/stock/marketresearc...]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Some Paper Summaries of Semantic Segmentation with Deep Learning]]></title>
      <url>%2F2018%2F04%2F10%2FSome-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning%2F</url>
      <content type="text"><![CDATA[What exactly is semantic segmentation?Semantic segmentation is understanding an image at pixel level i.e, we want to assign each pixel in the image an object class. For example, check out the following images. Left: Input image. Right: It’s semantic segmentation. Source.Apart from recognizing the bike and the person riding it, we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.VOC2012 and MSCOCO are the most important datasets for semantic segmentation.What are the different approaches?Before deep learning took over computer vision, people used approaches like TextonForest and Random Forest based classifiers for semantic segmentation. As with image classification, convolutional neural networks (CNN) have had enormous success on segmentation problems.One of the popular initial deep learning approaches was patch classification where each pixel was separately classified into classes using a patch of image around it. Main reason to use patches was that classification networks usually have full connected layers and therefore required fixed size images.In 2014, Fully Convolutional Networks (FCN) by Long et al. from Berkeley, popularized CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for image of any size and was also much faster compared to the patch classification approach. Almost all the subsequent state of the art approaches on semantic segmentation adopted this paradigm.Apart from fully connected layers, one of the main problems with using CNNs for segmentation is pooling layers. Pooling layers increase the field of view and are able to aggregate the context while discarding the ‘where’ information. However, semantic segmentation requires the exact alignment of class maps and thus, needs the ‘where’ information to be preserved. Two different classes of architectures evolved in the literature to tackle this issue.First one is encoder-decoder architecture. Encoder gradually reduces the spatial dimension with pooling layers and decoder gradually recovers the object details and spatial dimension. There are usually shortcut connections from encoder to decoder to help decoder recover the object details better. U-Net is a popular architecture from this class.U-Net: An encoder-decoder architecture. Source.Architectures in the second class use what are called as dilated/atrous convolutionsand do away with pooling layers.Dilated/atrous convolutions. rate=1 is same as normal convolutions. Source.Conditional Random Field (CRF) postprocessing are usually used to improve the segmentation. CRFs are graphical models which ‘smooth’ segmentation based on the underlying image intensities. They work based on the observation that similar intensity pixels tend to be labeled as the same class. CRFs can boost scores by 1-2%.CRF illustration. (b) Unary classifiers is the segmentation input to the CRF. (c, d, e) are variants of CRF with (e) being the widely used one. Source.In the next section, I’ll summarize a few papers that represent the evolution of segmentation architectures starting from FCN. All these architectures are benchmarked on VOC2012 evaluation server.SummariesFollowing papers are summarized (in chronological order):FCNSegNetDilated ConvolutionsDeepLab (v1 &amp; v2)RefineNetPSPNetLarge Kernel MattersDeepLab v3For each of these papers, I list down their key contributions and explain them. I also show their benchmark scores (mean IOU) on VOC2012 test dataset.FCNFully Convolutional Networks for Semantic SegmentationSubmitted on 14 Nov 2014Arxiv LinkKey Contributions:Popularize the use of end to end convolutional networks for semantic segmentationRe-purpose imagenet pretrained networks for segmentationUpsample using deconvolutional layersIntroduce skip connections to improve over the coarseness of upsamplingExplanation:Key observation is that fully connected layers in classification networks can be viewed as convolutions with kernels that cover their entire input regions. This is equivalent to evaluating the original classification network on overlapping input patches but is much more efficient because computation is shared over the overlapping regions of patches. Although this observation is not unique to this paper (see overfeat, this post), it improved the state of the art on VOC2012 significantly.Fully connected layers as a convolution. Source.After convolutionalizing fully connected layers in a imagenet pretrained network like VGG, feature maps still need to be upsampled because of pooling operations in CNNs. Instead of using simple bilinear interpolation, deconvolutional layers can learn the interpolation. This layer is also known as upconvolution, full convolution, transposed convolution or fractionally-strided convolution.However, upsampling (even with deconvolutional layers) produces coarse segmentation maps because of loss of information during pooling. Therefore, shortcut/skip connections are introduced from higher resolution feature maps.Benchmarks (VOC2012):ScoreCommentSource62.2-leaderboard67.2More momentum. Not described in paperleaderboardMy Comments:This was an important contribution but state of the art has improved a lot by now though.SegNetSegNet: A Deep Convolutional Encoder-Decoder Architecture for Image SegmentationSubmitted on 2 Nov 2015Arxiv LinkKey Contributions:Maxpooling indices transferred to decoder to improve the segmentation resolution.Explanation:FCN, despite upconvolutional layers and a few shortcut connections produces coarse segmentation maps. Therefore, more shortcut connections are introduced. However, instead of copying the encoder features as in FCN, indices from maxpooling are copied. This makes SegNet more memory efficient than FCN.Segnet Architecture. Source.Benchmarks (VOC2012):ScoreCommentSource59.9-leaderboardMy comments:FCN and SegNet are one of the first encoder-decoder architectures.Benchmarks for SegNet are not good enough to be used anymore.Dilated ConvolutionsMulti-Scale Context Aggregation by Dilated ConvolutionsSubmitted on 23 Nov 2015Arxiv LinkKey Contributions:Use dilated convolutions, a convolutional layer for dense predictions.Propose ‘context module’ which uses dilated convolutions for multi scale aggregation.Explanation:Pooling helps in classification networks because receptive field increases. But this is not the best thing to do for segmentation because pooling decreases the resolution. Therefore, authors use dilated convolution layer which works like this:Dilated/Atrous Convolutions. SourceDilated convolutional layer (also called as atrous convolution in DeepLab) allows for exponential increase in field of view without decrease of spatial dimensions.Last two pooling layers from pretrained classification network (here, VGG) are removed and subsequent convolutional layers are replaced with dilated convolutions. In particular, convolutions between the pool-3 and pool-4 have dilation 2 and convolutions after pool-4 have dilation 4. With this module (called frontend module in the paper), dense predictions are obtained without any increase in number of parameters.A module (called context module in the paper) is trained separately with the outputs of frontend module as inputs. This module is a cascade of dilated convolutions of different dilations so that multi scale context is aggregated and predictions from frontend are improved.Benchmarks (VOC2012):ScoreCommentSource71.3frontendreported in the paper73.5frontend + contextreported in the paper74.7frontend + context + CRFreported in the paper75.3frontend + context + CRF-RNNreported in the paperMy comments:Note that predicted segmentation map’s size is 1/8th of that of the image. This is the case with almost all the approaches. They are interpolated to get the final segmentation map.DeepLab (v1 &amp; v2)v1 : Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFsSubmitted on 22 Dec 2014Arxiv Linkv2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFsSubmitted on 2 Jun 2016Arxiv LinkKey Contributions:Use atrous/dilated convolutions.Propose atrous spatial pyramid pooling (ASPP)Use Fully connected CRFExplanation:Atrous/Dilated convolutions increase the field of view without increasing the number of parameters. Net is modified like in dilated convolutions paper.Multiscale processing is achieved either by passing multiple rescaled versions of original images to parallel CNN branches (Image pyramid) and/or by using multiple parallel atrous convolutional layers with different sampling rates (ASPP).Structured prediction is done by fully connected CRF. CRF is trained/tuned separately as a post processing step.DeepLab2 Pipeline. Source.Benchmarks (VOC2012):ScoreCommentSource79.7ResNet-101 + atrous Convolutions + ASPP + CRFleaderboardRefineNetRefineNet: Multi-Path Refinement Networks for High-Resolution Semantic SegmentationSubmitted on 20 Nov 2016Arxiv LinkKey Contributions:Encoder-Decoder architecture with well thought-out decoder blocksAll the components follow residual connection designExplanation:Approach of using dilated/atrous convolutions are not without downsides. Dilated convolutions are computationally expensive and take a lot of memory because they have to be applied on large number of high resolution feature maps. This hampers the computation of high-res predictions. DeepLab’s predictions, for example are 1/8th the size of original input.So, the paper proposes to use encoder-decoder architecture. Encoder part is ResNet-101 blocks. Decoder has RefineNet blocks which concatenate/fuse high resolution features from encoder and low resolution features from previous RefineNet block.RefineNet Architecture. Source.Each RefineNet block has a component to fuse the multi resolution features by upsampling the lower resolution features and a component to capture context based on repeated 5 x 5 stride 1 pool layers. Each of these components employ the residual connection design following the identity map mindset.RefineNet Block. Source.Benchmarks (VOC2012):ScoreCommentSource84.2Uses CRF, Multiscale inputs, COCO pretrainingleaderboardPSPNetPyramid Scene Parsing NetworkSubmitted on 4 Dec 2016Arxiv LinkKey Contributions:Propose pyramid pooling module to aggregate the context.Use auxiliary lossExplanation:Global scene categories matter because it provides clues on the distribution of the segmentation classes. Pyramid pooling module captures this information by applying large kernel pooling layers.Dilated convolutions are used as in dilated convolutions paper to modify Resnet and a pyramid pooling module is added to it. This module concatenates the feature maps from ResNet with upsampled output of parallel pooling layers with kernels covering whole, half of and small portions of image.An auxiliary loss, additional to the loss on main branch, is applied after the fourth stage of ResNet (i.e input to pyramid pooling module). This idea was also called as intermediate supervision elsewhere.PSPNet Architecture. Source.Benchmarks (VOC2012):ScoreCommentSource85.4MSCOCO pretraining, multi scale input, no CRFleaderboard82.6no MSCOCO pretraining, multi scale input, no CRFreported in the paperLarge Kernel MattersLarge Kernel Matters – Improve Semantic Segmentation by Global Convolutional NetworkSubmitted on 8 Mar 2017Arxiv LinkKey Contributions:Propose a encoder-decoder architecture with very large kernels convolutionsExplanation:Semantic segmentation requires both segmentation and classification of the segmented objects. Since fully connected layers cannot be present in a segmentation architecture, convolutions with very large kernels are adopted instead.Another reason to adopt large kernels is that although deeper networks like ResNet have very large receptive field, studies show that the network tends to gather information from a much smaller region (valid receptive filed).Larger kernels are computationally expensive and have a lot of parameters. Therefore, k x k convolution is approximated with sum of 1 x k + k x 1 and k x 1 and 1 x k convolutions. This module is called as Global Convolutional Network (GCN) in the paper.Coming to architecture, ResNet(without any dilated convolutions) forms encoder part of the architecture while GCNs and deconvolutions form decoder. A simple residual block called Boundary Refinement (BR) is also used.GCN Architecture. Source.Benchmarks (VOC2012):ScoreCommentSource82.2-reported in the paper83.6Improved training, not described in the paperleaderboardDeepLab v3Rethinking Atrous Convolution for Semantic Image SegmentationSubmitted on 17 Jun 2017Arxiv LinkKey Contributions:Improved atrous spatial pyramid pooling (ASPP)Module which employ atrous convolutions in cascadeExplanation:ResNet model is modified to use dilated/atrous convolutions as in DeepLabv2 and dilated convolutions. Improved ASPP involves concatenation of image-level features, a 1x1 convolution and three 3x3 atrous convolutions with different rates. Batch normalization is used after each of the parallel convolutional layers.Cascaded module is a resnet block except that component convolution layers are made atrous with different rates. This module is similar to context module used in dilated convolutions paper but this is applied directly on intermediate feature maps instead of belief maps (belief maps are final CNN feature maps with channels equal to number of classes).Both the proposed models are evaluated independently and attempt to combine the both did not improve the performance. Both of them performed very similarly on val set with ASPP performing slightly better. CRF is not used.Both these models outperform the best model from DeepLabv2. Authors note that the improvement comes from the batch normalization and better way to encode multi scale context.DeepLabv3 ASPP (used for submission). Source.Benchmarks (VOC2012):ScoreCommentSource85.7used ASPP (no cascaded modules)leaderboardReblog from here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN]]></title>
      <url>%2F2018%2F04%2F10%2FA-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN%2F</url>
      <content type="text"><![CDATA[Ever since Alex Krizhevsky, Geoff Hinton, and Ilya Sutskever won ImageNet in 2012, Convolutional Neural Networks(CNNs) have become the gold standard for image classification. In fact, since then, CNNs have improved to the point where they now outperform humans on the ImageNet challenge!CNNs now outperform humans on the ImageNet challenge. The y-axis in the above graph is the error rate on ImageNet.While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.An example of an image used in the classification challenge. Note how the image is well framed and has just one object.In classification, there’s generally an image with a single object as the focus and the task is to say what that image is (see above). But when we look at the world around us, we carry out far more complex tasks.Sights in real life are often composed of a multitude of different, overlapping objects, backgrounds, and actions.We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!In image segmentation, our goal is to classify the different objects in the image, and identify their boundaries. Source: Mask R-CNN paper.Can CNNs help us with such complex tasks? Namely, given a more complicated image, can we use CNNs to identify the different objects in the image, and their boundaries? As has been shown by Ross Girshick and his peers over the last few years, the answer is conclusively yes.Goals of this PostThrough this post, we’ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they’ve evolved from one implementation to the next. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Finally, we’ll cover Mask R-CNN, a paper released recently by Facebook Research that extends such object detection techniques to provide pixel level segmentation. Here are the papers referenced in this post:R-CNN: https://arxiv.org/abs/1311.2524Fast R-CNN: https://arxiv.org/abs/1504.08083Faster R-CNN: https://arxiv.org/abs/1506.01497Mask R-CNN: https://arxiv.org/abs/1703.068702014: R-CNN - An Early Application of CNNs to Object DetectionObject detection algorithms such as R-CNN take in an image and identify the locations and classifications of the main objects in the image. Source: https://arxiv.org/abs/1311.2524.Inspired by the research of Hinton’s lab at the University of Toronto, a small team at UC Berkeley, led by Professor Jitendra Malik, asked themselves what today seems like an inevitable question:To what extent do [Krizhevsky et. al’s results] generalize to object detection?Object detection is the task of finding the different objects in an image and classifying them (as seen in the image above). The team, comprised of Ross Girshick (a name we’ll see again), Jeff Donahue, and Trevor Darrel found that this problem can be solved with Krizhevsky’s results by testing on the PASCAL VOC Challenge, a popular object detection challenge akin to ImageNet. They write,This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.Let’s now take a moment to understand how their architecture, Regions With CNNs (R-CNN) works.Understanding R-CNNThe goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image.Inputs: ImageOutputs: Bounding boxes + labels for each object in the image.But how do we find out where these bounding boxes are? R-CNN does what we might intuitively do as well - propose a bunch of boxes in the image and see if any of them actually correspond to an object.Selective Search looks through windows of multiple scales and looks for adjacent pixels that share textures, colors, or intensities. Image source: https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdfR-CNN creates these bounding boxes, or region proposals, using a process called Selective Search which you can read about here. At a high level, Selective Search (shown in the image above) looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.After creating a set of region proposals, R-CNN passes the image through a modified version of AlexNet to determine whether or not it is a valid region. Source: https://arxiv.org/abs/1311.2524.Once the proposals are created, R-CNN warps the region to a standard square size and passes it through to a modified version of AlexNet (the winning submission to ImageNet 2012 that inspired R-CNN), as shown above.On the final layer of the CNN, R-CNN adds a Support Vector Machine (SVM) that simply classifies whether this is an object, and if so what object. This is step 4 in the image above.Improving the Bounding BoxesNow, having found the object in the box, can we tighten the box to fit the true dimensions of the object? We can, and this is the final step of R-CNN. R-CNN runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result. Here are the inputs and outputs of this regression model:Inputs: sub-regions of the image corresponding to objects.Outputs: New bounding box coordinates for the object in the sub-region.So, to summarize, R-CNN is just the following steps:Generate a set of proposals for bounding boxes.Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.2015: Fast R-CNN - Speeding up and Simplifying R-CNNRoss Girshick wrote both R-CNN and Fast R-CNN. He continues to push the boundaries of Computer Vision at Facebook Research.R-CNN works really well, but is really quite slow for a few simple reasons:It requires a forward pass of the CNN (AlexNet) for every single region proposal for every single image (that’s around 2000 forward passes per image!).It has to train three different models separately - the CNN to generate image features, the classifier that predicts the class, and the regression model to tighten the bounding boxes. This makes the pipeline extremely hard to train.In 2015, Ross Girshick, the first author of R-CNN, solved both these problems, leading to the second algorithm in our short history - Fast R-CNN. Let’s now go over its main insights.Fast R-CNN Insight 1: RoI (Region of Interest) PoolingFor the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). His insight was simple — Why not run the CNN just once per image and then find a way to share that computation across the ~2000 proposals?In RoIPool, a full forward pass of the image is created and the conv features for each region of interest are extracted from the resulting forward pass. Source: Stanford’s CS231N slides by Fei Fei Li, Andrei Karpathy, and Justin Johnson.This is exactly what Fast R-CNN does using a technique known as RoIPool (Region of Interest Pooling). At its core, RoIPool shares the forward pass of a CNN for an image across its subregions. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. Then, the features in each region are pooled (usually using max pooling). So all it takes us is one pass of the original image as opposed to ~2000!Fast R-CNN Insight 2: Combine All Models into One NetworkFast R-CNN combined the CNN, classifier, and bounding box regressor into one, single network. Source: https://www.slideshare.net/simplyinsimple/detection-52781995.The second insight of Fast R-CNN is to jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three.You can see how this was done in the image above. Fast R-CNN replaced the SVM classifier with a softmax layer on top of the CNN to output a classification. It also added a linear regression layer parallel to the softmax layer to output bounding box coordinates. In this way, all the outputs needed came from one single network! Here are the inputs and outputs to this overall model:Inputs: Images with region proposals.Outputs: Object classifications of each region along with tighter bounding boxes.2016: Faster R-CNN - Speeding Up Region ProposalEven with all these advancements, there was still one remaining bottleneck in the Fast R-CNN process — the region proposer. As we saw, the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test. In Fast R-CNN, these proposals were created using Selective Search, a fairly slow process that was found to be the bottleneck of the overall process.Jian Sun, a principal researcher at Microsoft Research, led the team behind Faster R-CNN. Source: https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdpIn the middle 2015, a team at Microsoft Research composed of Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, found a way to make the region proposal step almost cost free through an architecture they (creatively) named Faster R-CNN.The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). So why not reuse those same CNN results for region proposals instead of running a separate selective search algorithm?In Faster R-CNN, a single CNN is used for region proposals, and classifications. Source: https://arxiv.org/abs/1506.01497.Indeed, this is just what the Faster R-CNN team achieved. In the image above, you can see how a single CNN is used to both carry out region proposals and classification. This way, only one CNN needs to be trained and we get region proposals almost for free! The authors write:Our observation is that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals [thus enabling nearly cost-free region proposals].Here are the inputs and outputs of their model:Inputs: Images (Notice how region proposals are not needed).Outputs: Classifications and bounding box coordinates of objects in the images.How the Regions are GeneratedLet’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Faster R-CNN adds a Fully Convolutional Network on top of the features of the CNN creating what’s known as the Region Proposal Network.The Region Proposal Network slides a window over the features of the CNN. At each window location, the network outputs a score and a bounding box per anchor (hence 4k box coordinates where k is the number of anchors). Source: https://arxiv.org/abs/1506.01497.The Region Proposal Network works by passing a sliding window over the CNN feature map and at each window, outputting k potential bounding boxes and scores for how good each of those boxes is expected to be. What do these k boxes represent?We know that the bounding boxes for people tend to be rectangular and vertical. We can use this intuition to guide our Region Proposal networks through creating an anchor of such dimensions. Image Source: http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg.Intuitively, we know that objects in an image should fit certain common aspect ratios and sizes. For instance, we know that we want some rectangular boxes that resemble the shapes of humans. Likewise, we know we won’t see many boxes that are very very thin. In such a way, we create k such common aspect ratios we call anchor boxes. For each such anchor box, we output one bounding box and score per position in the image.With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:Inputs: CNN Feature Map.Outputs: A bounding box per anchor. A score representing how likely the image in that bounding box will be an object.We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes.2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level SegmentationThe goal of image instance segmentation is to identify, at a pixel level, what the different objets in a scene are. Source: https://arxiv.org/abs/1703.06870.So far, we’ve seen how we’ve been able to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes.Can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes? This problem, known as image segmentation, is what Kaiming He and a team of researchers, including Girshick, explored at Facebook AI using an architecture known as Mask R-CNN.Kaiming He, a researcher at Facebook AI, is lead author of Mask R-CNN and also a coauthor of Faster R-CNN.Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward. Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel level segmentation?In Mask R-CNN, a Fully Convolutional Network (FCN) is added on top of the CNN features of Faster R-CNN to generate a mask (segmentation output). Notice how this is in parallel to the classification and bounding box regression network of Faster R-CNN. Source: https://arxiv.org/abs/1703.06870.Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch (in white in the above image), as before, is just a Fully Convolutional Network on top of a CNN based feature map. Here are its inputs and outputs:Inputs: CNN Feature Map.Outputs: Matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a binary mask).But the Mask R-CNN authors had to make one small adjustment to make this pipeline work as expected.RoiAlign - Realigning RoIPool to be More AccurateInstead of RoIPool, the image gets passed through RoIAlign so that the regions of the feature map selected by RoIPool correspond more precisely to the regions of the original image. This is needed because pixel level segmentation requires more fine-grained alignment than bounding boxes. Source: https://arxiv.org/abs/1703.06870.When run without modifications on the original Faster R-CNN architecture, the Mask R-CNN authors realized that the regions of the feature map selected by RoIPool were slightly misaligned from the regions of the original image. Since image segmentation requires pixel level specificity, unlike bounding boxes, this naturally led to inaccuracies.The authors were able to solve this problem by cleverly adjusting RoIPool to be more precisely aligned using a method known as RoIAlign.How do we accurately map a region of interest from the original image onto the feature map?Imagine we have an image of size 128x128 and a feature map of size 25x25. Let’s imagine we want features the region corresponding to the top-left 15x15pixels in the original image (see above). How might we select these pixels from the feature map?We know each pixel in the original image corresponds to ~ 25/128 pixels in the feature map. To select 15 pixels from the original image, we just select 15 25/128 ~= *2.93 pixels.In RoIPool, we would round this down and select 2 pixels causing a slight misalignment. However, in RoIAlign, we avoid such rounding. Instead, we use bilinear interpolation to get a precise idea of what would be at pixel 2.93. This, at a high level, is what allows us to avoid the misalignments caused by RoIPool.Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:Mask R-CNN is able to segment as well as classify the objects in an image. Source: https://arxiv.org/abs/1703.06870.CodeIf you’re interested in trying out these algorithms yourselves, here are relevant repositories:Faster R-CNNCaffe: https://github.com/rbgirshick/py-faster-rcnnPyTorch: https://github.com/longcw/faster_rcnn_pytorchMatLab: https://github.com/ShaoqingRen/faster_rcnnMask R-CNNPyTorch: https://github.com/felixgwu/mask_rcnn_pytorchTensorFlow: https://github.com/CharlesShang/FastMaskRCNNReblog from here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Understanding nested list comprehension syntax in Python]]></title>
      <url>%2F2018%2F03%2F27%2FUnderstanding-nested-list-comprehension-syntax-in-Python%2F</url>
      <content type="text"><![CDATA[List comprehensions are one of the really nice and powerful features of Python. It is actually a smart way to introduce new users to functional programming concepts (after all a list comprehension is just a combination of map and filter) and compact statements.However, one thing that always troubled me when using list comprehensions is their non intuitive syntax when nesting was needed. For example, let’s say that we just want to flatten a list of lists using a nested list comprehension:1non_flat = [ [1,2,3], [4,5,6], [7,8] ]To write that, somebody would think: For a simple list comprehension I need to write [ x for x in non_flat ] to get all its items - however I want to retrieve each element of the x list so I’ll write something like this:12&gt;&gt;&gt; [y for y in x for x in non_flat][7, 7, 7, 8, 8, 8]Well duh! At this time I’d need research google for a working list comprehension syntax and adjust it to my needs (or give up and write it as a double for loop).Here’s the correct nested list comprehension people wondering:12&gt;&gt;&gt; [y for x in non_flat for y in x][1, 2, 3, 4, 5, 6, 7, 8]What if I wanted to add a third level of nesting or an if? Well I’d just bite the bullet and use for loops!However, if you take a look at the document describing list comprehensions in python (PEP202) you’ll see the following phrase:It is proposed to allow conditional construction of list literals using for and if clauses. They would nest in the same way for loops and if statements nest now.This statement explains everything! Just think in for-loops syntax. So, If I used for loops for the previous flattening, I’d do something like:123for x in non_flat: for y in x: ywhich, if y is moved to the front and joined in one line would be the correct nested list comprehension!So that’s the way… What If I wanted to include only lists with more than 2 elements in the flattening (so [7,8] should not be included)? I’ll write it with for loops first:1234for x in non_flat: if len(x) &gt; 2 for y in x: yso by convering this to list comprehension we get:12&gt;&gt;&gt; [ y for x in non_flat if len(x) &gt; 2 for y in x ][1, 2, 3, 4, 5, 6]Success!One final, more complex example: Let’s say that we have a list of lists of words and we want to get a list of all the letters of these words along with the index of the list they belong to but only for words with more than two characters. Using the same for-loop syntax for the nested list comprehensions we’ll get:123&gt;&gt;&gt; strings = [ ['foo', 'bar'], ['baz', 'taz'], ['w', 'koko'] ]&gt;&gt;&gt; [ (letter, idx) for idx, lst in enumerate(strings) for word in lst if len(word)&gt;2 for letter in word][('f', 0), ('o', 0), ('o', 0), ('b', 0), ('a', 0), ('r', 0), ('b', 1), ('a', 1), ('z', 1), ('t', 1), ('a', 1), ('z', 1), ('k', 2), ('o', 2), ('k', 2), ('o', 2)]source blog is here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python多核编程mpi4py实践]]></title>
      <url>%2F2018%2F03%2F19%2FPython%E5%A4%9A%E6%A0%B8%E7%BC%96%E7%A8%8Bmpi4py%E5%AE%9E%E8%B7%B5%2F</url>
      <content type="text"><![CDATA[转载自这篇博文.概述​ CPU从三十多年前的8086，到十年前的奔腾，再到当下的多核i7。一开始，以单核cpu的主频为目标，架构的改良和集成电路工艺的进步使得cpu的性能高速上升，单核cpu的主频从老爷车的MHz阶段一度接近4GHz高地。然而，也因为工艺和功耗等的限制，单核cpu遇到了人生的天花板，急需转换思维，以满足无止境的性能需求。多核cpu在此登上历史舞台。给你的老爷车多加两个引擎，让你有法拉利的感觉。现时代，连手机都到处叫嚣自己有4核8核处理器的时代，PC就更不用说了。​ 扯远了，anyway，对于俺们程序员来说，如何利用如此强大的引擎完成我们的任务才是我们要考虑的。随着大规模数据处理、大规模问题和复杂系统求解需求的增加，以前的单核编程已经有心无力了。如果程序一跑就得几个小时，甚至一天，想想都无法原谅自己。那如何让自己更快的过度到高大上的多核并行编程中去呢？哈哈，广大人民的力量！​ 目前工作中我所接触到的并行处理框架主要有MPI、OpenMP和MapReduce(Hadoop)三个（CUDA属于GPU并行编程，这里不提及）。MPI和Hadoop都可以在集群中运行，而OpenMP因为共享存储结构的关系，不能在集群上运行，只能单机。另外，MPI可以让数据保留在内存中，可以为节点间的通信和数据交互保存上下文，所以能执行迭代算法，而Hadoop却不具有这个特性。因此，需要迭代的机器学习算法大多使用MPI来实现。当然了，部分机器学习算法也是可以通过设计使用Hadoop来完成的。（浅见，如果错误，希望各位不吝指出，谢谢）。​ 本文主要介绍Python环境下MPI编程的实践基础。MPI与mpi4py​ MPI是Message Passing Interface的简称，也就是消息传递。消息传递指的是并行执行的各个进程具有自己独立的堆栈和代码段，作为互不相关的多个程序独立执行，进程之间的信息交互完全通过显示地调用通信函数来完成。​ Mpi4py是构建在mpi之上的python库，使得python的数据结构可以在进程（或者多个cpu）之间进行传递。MPI的工作方式​ 很简单，就是你启动了一组MPI进程，每个进程都是执行同样的代码！然后每个进程都有一个ID，也就是rank来标记我是谁。什么意思呢？假设一个CPU是你请的一个工人，共有10个工人。你有100块砖头要搬，然后很公平，让每个工人搬10块。这时候，你把任务写到一个任务卡里面，让10个工人都执行这个任务卡中的任务，也就是搬砖！这个任务卡中的“搬砖”就是你写的代码。然后10个CPU执行同一段代码。需要注意的是，代码里面的所有变量都是每个进程独有的，虽然名字相同。​ 例如，一个脚本test.py，里面包含以下代码：123from mpi4py import MPI print("hello world'') print("my rank is: %d" %MPI.rank)​ 然后我们在命令行通过以下方式运行：​ mpirun –np 5 python test.py​ -np5 指定启动5个mpi进程来执行后面的程序。相当于对脚本拷贝了5份，每个进程运行一份，互不干扰。在运行的时候代码里面唯一的不同，就是各自的rank也就是ID不一样。所以这个代码就会打印5个hello world和5个不同的rank值，从0到4.点对点通信​ 点对点通信（Point-to-PointCommunication）的能力是信息传递系统最基本的要求。意思就是让两个进程直接可以传输数据，也就是一个发送数据，另一个接收数据。接口就两个，send和recv，来个例子：123456789101112import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() # point to point communication data_send = [comm_rank]*5 comm.send(data_send,dest=(comm_rank+1)%comm_size) data_recv =comm.recv(source=(comm_rank-1)%comm_size) print("my rank is %d, and Ireceived:" % comm_rank) print data_recv​ 启动5个进程运行以上代码，结果如下：12345678910my rank is 0, and I received: [4, 4, 4, 4, 4] my rank is 1, and I received: [0, 0, 0, 0, 0] my rank is 2, and I received: [1, 1, 1, 1, 1] my rank is 3, and I received: [2, 2, 2, 2, 2] my rank is 4, and I received: [3, 3, 3, 3, 3]​ 可以看到，每个进程都创建了一个数组，然后把它传递给下一个进程，最后的那个进程传递给第一个进程。comm_size就是mpi的进程个数，也就是-np指定的那个数。MPI.COMM_WORLD表示进程所在的通信组。​ 但这里面有个需要注意的问题，如果我们要发送的数据比较小的话，mpi会缓存我们的数据，也就是说执行到send这个代码的时候，会缓存被send的数据，然后继续执行后面的指令，而不会等待对方进程执行recv指令接收完这个数据。但是，如果要发送的数据很大，那么进程就是挂起等待，直到接收进程执行了recv指令接收了这个数据，进程才继续往下执行。所以上述的代码发送[rank]5没啥问题，如果发送[rank]500程序就会半死不活的样子了。因为所有的进程都会卡在发送这条指令，等待下一个进程发起接收的这个指令，但是进程是执行完发送的指令才能执行接收的指令，这就和死锁差不多了。所以一般，我们将其修改成以下的方式：12345678910111213141516import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() data_send = [comm_rank]*5 if comm_rank == 0: comm.send(data_send, dest=(comm_rank+1)%comm_size) if comm_rank &gt; 0: data_recv = comm.recv(source=(comm_rank-1)%comm_size) comm.send(data_send, dest=(comm_rank+1)%comm_size) if comm_rank == 0: data_recv = comm.recv(source=(comm_rank-1)%comm_size) print("my rank is %d, and Ireceived:" % comm_rank) print data_recv​ 第一个进程一开始就发送数据，其他进程一开始都是在等待接收数据，这时候进程1接收了进程0的数据，然后发送进程1的数据，进程2接收了，再发送进程2的数据……知道最后进程0接收最后一个进程的数据，从而避免了上述问题。​ 一个比较常用的方法是封一个组长，也就是一个主进程，一般是进程0作为主进程leader。主进程将数据发送给其他的进程，其他的进程处理数据，然后返回结果给进程0。换句话说，就是进程0来控制整个数据处理流程。群体通信​ 点对点通信是A发送给B，一个人将自己的秘密告诉另一个人，群体通信（Collective Communications）像是拿个大喇叭，一次性告诉所有的人。前者是一对一，后者是一对多。但是，群体通信是以更有效的方式工作的。它的原则就一个：尽量把所有的进程在所有的时刻都使用上！我们在下面的bcast小节讲述。​ 群体通信还是发送和接收两类，一个是一次性把数据发给所有人，另一个是一次性从所有人那里回收结果。广播bcast​ 将一份数据发送给所有的进程。例如我有200份数据，有10个进程，那么每个进程都会得到这200份数据。1234567891011import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) data = comm.bcast(data if comm_rank == 0else None, root=0) print 'rank %d, got:' % (comm_rank) print data​ 结果如下：12345678910rank 0, got: [0, 1, 2, 3, 4] rank 1, got: [0, 1, 2, 3, 4] rank 2, got: [0, 1, 2, 3, 4] rank 3, got: [0, 1, 2, 3, 4] rank 4, got: [0, 1, 2, 3, 4]​ Root进程自己建了一个列表，然后广播给所有的进程。这样所有的进程都拥有了这个列表。然后爱干嘛就干嘛了。​ 对广播最直观的观点是某个特定进程将数据一一发送给每个进程。假设有n个进程，那么假设我们的数据在0进程，那么0进程就需要将数据发送给剩下的n-1个进程，这是非常低效的，复杂度是O(n)。那有没有高效的方式？一个最常用也是非常高效的手段是规约树广播：收到广播数据的所有进程都参与到数据广播的过程中。首先只有一个进程有数据，然后它把它发送给第一个进程，此时有两个进程有数据；然后这两个进程都参与到下一次的广播中，这时就会有4个进程有数据，……，以此类推，每次都会有2的次方个进程有数据。通过这种规约树的广播方法，广播的复杂度降为O(log n)。这就是上面说的群体通信的高效原则：充分利用所有的进程来实现数据的发送和接收。散播scatter​ 将一份数据平分给所有的进程。例如我有200份数据，有10个进程，那么每个进程会分别得到20份数据。1234567891011121314import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) print data else: data = None local_data = comm.scatter(data, root=0) print 'rank %d, got:' % comm_rank print local_data​ 结果如下：1234567891011[0, 1, 2, 3, 4] rank 0, got: 0 rank 1, got: 1 rank 2, got: 2 rank 3, got: 3 rank 4, got: 4​ 这里root进程创建了一个list，然后将它散播给所有的进程，相当于对这个list做了划分，每个进程获得等分的数据，这里就是list的每一个数。（主要根据list的索引来划分，list索引为第i份的数据就发送给第i个进程）。如果是矩阵，那么就等分的划分行，每个进程获得相同的行数进行处理。​ 需要注意的是，MPI的工作方式是每个进程都会执行所有的代码，所以每个进程都会执行scatter这个指令，但只有root执行它的时候，它才兼备发送者和接收者的身份（root也会得到属于自己的数据），对于其他进程来说，他们都只是接收者而已。收集gather​ 那有发送，就有一起回收的函数。Gather是将所有进程的数据收集回来，合并成一个列表。下面联合scatter和gather组成一个完成的分发和收回过程：123456789101112131415161718import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) print data else: data = None local_data = comm.scatter(data, root=0) local_data = local_data * 2 print 'rank %d, got and do:' % comm_rank print local_data combine_data = comm.gather(local_data,root=0) if comm_rank == 0: printcombine_data​ 结果如下：123456789101112[0, 1, 2, 3, 4] rank 0, got and do: 0 rank 1, got and do: 2 rank 2, got and do: 4 rank 4, got and do: 8 rank 3, got and do: 6 [0, 2, 4, 6, 8]​ Root进程将数据通过scatter等分发给所有的进程，等待所有的进程都处理完后（这里只是简单的乘以2），root进程再通过gather回收他们的结果，和分发的原则一样，组成一个list。Gather还有一个变体就是allgather，可以理解为它在gather的基础上将gather的结果再bcast了一次。啥意思？意思是root进程将所有进程的结果都回收统计完后，再把整个统计结果告诉大家。这样，不仅root可以访问combine_data，所有的进程都可以访问combine_data了。规约reduce​ 规约是指不但将所有的数据收集回来，收集回来的过程中还进行了简单的计算，例如求和，求最大值等等。为什么要有这个呢？我们不是可以直接用gather全部收集回来了，再对列表求个sum或者max就可以了吗？这样不是累死组长吗？为什么不充分使用每个工人呢？规约实际上是使用规约树来实现的。例如求max，完成可以让工人两两pk后，再返回两两pk的最大值，然后再对第二层的最大值两两pk，直到返回一个最终的max给组长。组长就非常聪明的将工作分配下工人高效的完成了。这是O(n)的复杂度，下降到O(log n)（底数为2）的复杂度。1234567891011121314151617import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) print data else: data = None local_data = comm.scatter(data, root=0) local_data = local_data * 2 print 'rank %d, got and do:' % comm_rank print local_data all_sum = comm.reduce(local_data, root=0,op=MPI.SUM) if comm_rank == 0: print 'sumis:%d' % all_sum​ 结果如下：123456789101112[0, 1, 2, 3, 4] rank 0, got and do: 0 rank 1, got and do: 2 rank 2, got and do: 4 rank 3, got and do: 6 rank 4, got and do: 8 sum is:20​ 可以看到，最后可以得到一个sum值。常见用法对一个文件的多个行并行处理1234567891011121314151617181920212223242526272829303132333435363738#!usr/bin/env python #-*- coding: utf-8 -*- import sys import os import mpi4py.MPI as MPI import numpy as np # # Global variables for MPI # # instance for invoking MPI relatedfunctions comm = MPI.COMM_WORLD # the node rank in the whole community comm_rank = comm.Get_rank() # the size of the whole community, i.e.,the total number of working nodes in the MPI cluster comm_size = comm.Get_size() if __name__ == '__main__': if comm_rank == 0: sys.stderr.write("processor root starts reading data...\n") all_lines = sys.stdin.readlines() all_lines = comm.bcast(all_lines if comm_rank == 0 else None, root = 0) num_lines = len(all_lines) local_lines_offset = np.linspace(0, num_lines, comm_size +1).astype('int') local_lines = all_lines[local_lines_offset[comm_rank] :local_lines_offset[comm_rank + 1]] sys.stderr.write("%d/%d processor gets %d/%d data \n" %(comm_rank, comm_size, len(local_lines), num_lines)) cnt = 0 for line in local_lines: fields = line.strip().split('\t') cnt += 1 if cnt % 100 == 0: sys.stderr.write("processor %d has processed %d/%d lines \n" %(comm_rank, cnt, len(local_lines))) output = line.strip() + ' process every line here' print output对多个文件并行处理​ 如果我们的文件太大，例如几千万行，那么mpi是没办法将这么大的数据bcast给所有的进程的，所以我们可以先把大的文件split成小的文件，再让每个进程处理少数的文件。123456789101112131415161718192021222324252627282930313233343536373839404142#!usr/bin/env python #-*- coding: utf-8 -*- import sys import os import mpi4py.MPI as MPI import numpy as np # # Global variables for MPI # # instance for invoking MPI relatedfunctions comm = MPI.COMM_WORLD # the node rank in the whole community comm_rank = comm.Get_rank() # the size of the whole community, i.e.,the total number of working nodes in the MPI cluster comm_size = comm.Get_size() if __name__ == '__main__': if len(sys.argv) != 2: sys.stderr.write("Usage: python *.py directoty_with_files\n") sys.exit(1) path = sys.argv[1] if comm_rank == 0: file_list = os.listdir(path) sys.stderr.write("%d files\n" % len(file_list)) file_list = comm.bcast(file_list if comm_rank == 0 else None, root = 0) num_files = len(file_list) local_files_offset = np.linspace(0, num_files, comm_size +1).astype('int') local_files = file_list[local_files_offset[comm_rank] :local_files_offset[comm_rank + 1]] sys.stderr.write("%d/%d processor gets %d/%d data \n" %(comm_rank, comm_size, len(local_files), num_files)) cnt = 0 for file_name in local_files: hd = open(os.path.join(path, file_name)) for line in hd: output = line.strip() + ' process every line here' print output cnt += 1 sys.stderr.write("processor %d has processed %d/%d files \n" %(comm_rank, cnt, len(local_files))) hd.close()联合numpy对矩阵的多个行或者多列并行处理​ Mpi4py一个非常优秀的特性是完美支持numpy！12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import os, sys, time import numpy as np import mpi4py.MPI as MPI # # Global variables for MPI # # instance for invoking MPI relatedfunctions comm = MPI.COMM_WORLD # the node rank in the whole community comm_rank = comm.Get_rank() # the size of the whole community, i.e.,the total number of working nodes in the MPI cluster comm_size = comm.Get_size() # test MPI if __name__ == "__main__": #create a matrix if comm_rank == 0: all_data = np.arange(20).reshape(4, 5) print "************ data ******************" print all_data #broadcast the data to all processors all_data = comm.bcast(all_data if comm_rank == 0 else None, root = 0) #divide the data to each processor num_samples = all_data.shape[0] local_data_offset = np.linspace(0, num_samples, comm_size + 1).astype('int') #get the local data which will be processed in this processor local_data = all_data[local_data_offset[comm_rank] :local_data_offset[comm_rank + 1]] print "****** %d/%d processor gets local data ****" %(comm_rank, comm_size) print local_data #reduce to get sum of elements local_sum = local_data.sum() all_sum = comm.reduce(local_sum, root = 0, op = MPI.SUM) #process in local local_result = local_data ** 2 #gather the result from all processors and broadcast it result = comm.allgather(local_result) result = np.vstack(result) if comm_rank == 0: print "*** sum: ", all_sum print "************ result ******************" print resultMPI和mpi4py的环境搭建​ 这章放到这里是作为一个附录。我们的环境是linux，需要安装的包有python、openmpi、numpy、cpython和mpi4py，过程如下：安装Python12345tar xzvf Python-2.7.tgz cd Python-2.7 ./configure--prefix=/home/work/vis/zouxiaoyi/my_tools make make install​ 先将Python放到环境变量里面，还有Python的插件库12exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATH​ 执行python，如果看到可爱的&gt;&gt;&gt;出来，就表示成功了。按crtl+d退出安装openmpi123456wget http://www.open-mpi.org/software/ompi/v1.4/downloads/openmpi-1.4.1.tar.gz tar xzvf openmpi-1.4.1.tar.gz cd openmpi-1.4.1 ./configure--prefix=/home/work/vis/zouxiaoyi/my_tools make -j 8 make install​ 然后把bin路径加到环境变量里面：12exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATH​ 执行mpirun，如果有帮助信息打印出来，就表示安装好了。需要注意的是，我安装了几个版本都没有成功，最后安装了1.4.1这个版本才能成功，因此就看你的人品了。安装numpy和Cython​ 安装python库的方法可以参考之前的博客。过程一般如下：123tar –xgvf Cython-0.20.2.tar.gz cd Cython-0.20.2 python setup.py install​ 打开Python，import Cython，如果没有报错，就表示安装成功了安装mpi4py123tar –xgvf mpi4py_1.3.1.tar.gz cd mpi4py vi mpi.cfg​ 在68行，[openmpi]下面，将刚才已经安装好的openmpi的目录给改上。12mpi_dir = /home/work/vis/zouxiaoyi/my_tools python setup.py install​ 打开Python，import mpi4py as MPI，如果没有报错，就表示安装成功了​ 下面就可以开始属于你的并行之旅了，勇敢探索多核的乐趣吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A Visual Guide to Evolution Strategies]]></title>
      <url>%2F2018%2F01%2F29%2FA-Visual-Guide-to-Evolution-Strategies%2F</url>
      <content type="text"><![CDATA[Source post is here.Survival of the fittest.In this post I explain how evolution strategies (ES) work with the aid of a few visual examples. I try to keep the equations light, and I provide links to original articles if the reader wishes to understand more details. This is the first post in a series of articles, where I plan to show how to apply these algorithms to a range of tasks from MNIST, OpenAI Gym, Roboschool to PyBullet environments.IntroductionNeural network models are highly expressive and flexible, and if we are able to find a suitable set of model parameters, we can use neural nets to solve many challenging problems. Deep learning’s success largely comes from the ability to use the backpropagation algorithm to efficiently calculate the gradient of an objective function over each model parameter. With these gradients, we can efficiently search over the parameter space to find a solution that is often good enough for our neural net to accomplish difficult tasks.However, there are many problems where the backpropagation algorithm cannot be used. For example, in reinforcement learning (RL) problems, we can also train a neural network to make decisions to perform a sequence of actions to accomplish some task in an environment. However, it is not trivial to estimate the gradient of reward signals given to the agent in the future to an action performed by the agent right now, especially if the reward is realised many timesteps in the future. Even if we are able to calculate accurate gradients, there is also the issue of being stuck in a local optimum, which exists many for RL tasks.Stuck in a local optimum.A whole area within RL is devoted to studying this credit-assignment problem, and great progress has been made in recent years. However, credit assignment is still difficult when the reward signals are sparse. In the real world, rewards can be sparse and noisy. Sometimes we are given just a single reward, like a bonus check at the end of the year, and depending on our employer, it may be difficult to figure out exactly why it is so low. For these problems, rather than rely on a very noisy and possibly meaningless gradient estimate of the future to our policy, we might as well just ignore any gradient information, and attempt to use black-box optimisation techniques such as genetic algorithms (GA) or ES.OpenAI published a paper called Evolution Strategies as a Scalable Alternative to Reinforcement Learning where they showed that evolution strategies, while being less data efficient than RL, offer many benefits. The ability to abandon gradient calculation allows such algorithms to be evaluated more efficiently. It is also easy to distribute the computation for an ES algorithm to thousands of machines for parallel computation. By running the algorithm from scratch many times, they also showed that policies discovered using ES tend to be more diverse compared to policies discovered by RL algorithms.I would like to point out that even for the problem of identifying a machine learning model, such as designing a neural net’s architecture, is one where we cannot directly compute gradients. While RL, Evolution, GA etc., can be applied to search in the space of model architectures, in this post, I will focus only on applying these algorithms to search for parameters of a pre-defined model.What is an Evolution Strategy?Two-dimensional Rastrigin function has many local optima (Source: Wikipedia).The diagrams below are top-down plots of shifted 2D Schaffer and Rastrigin functions, two of several simple toy problems used for testing continuous black-box optimisation algorithms. Lighter regions of the plots represent higher values of $F(x,y)$. As you can see, there are many local optimums in this function. Our job is to find a set of model parameters $(x, y)$, such that $F(x,y)$ is as close as possible to the global maximum.Schaffer-2D FunctionRastrigin-2D FunctionAlthough there are many definitions of evolution strategies, we can define an evolution strategy as an algorithm that provides the user a set of candidate solutions to evaluate a problem. The evaluation is based on an objective function that takes a given solution and returns a single fitness value. Based on the fitness results of the current solutions, the algorithm will then produce the next generation of candidate solutions that is more likely to produce even better results than the current generation. The iterative process will stop once the best known solution is satisfactory for the user.Given an evolution strategy algorithm called EvolutionStrategy, we can use in the following way:solver = EvolutionStrategy()while True:# ask the ES to give us a set of candidate solutionssolutions = solver.ask()# create an array to hold the fitness results.fitness_list = np.zeros(solver.popsize)# evaluate the fitness for each given solution.for i in range(solver.popsize):fitness_list[i] = evaluate(solutions[i])# give list of fitness results back to ESsolver.tell(fitness_list)# get best parameter, fitness from ESbest_solution, best_fitness = solver.result()if best_fitness &gt; MY_REQUIRED_FITNESS:breakAlthough the size of the population is usually held constant for each generation, they don’t need to be. The ES can generate as many candidate solutions as we want, because the solutions produced by an ES are sampled from a distribution whose parameters are being updated by the ES at each generation. I will explain this sampling process with an example of a simple evolution strategy.Simple Evolution StrategyOne of the simplest evolution strategy we can imagine will just sample a set of solutions from a Normal distribution, with a mean \muμand a fixed standard deviation \sigmaσ. In our 2D problem, \mu = (\mu_x, \mu_y)μ=(μx,μy) and \sigma = (\sigma_x, \sigma_y)σ=(σx,σy). Initially, \muμ is set at the origin. After the fitness results are evaluated, we set \muμ to the best solution in the population, and sample the next generation of solutions around this new mean. This is how the algorithm behaves over 20 generations on the two problems mentioned earlier:In the visualisation above, the green dot indicates the mean of the distribution at each generation, the blue dots are the sampled solutions, and the red dot is the best solution found so far by our algorithm.This simple algorithm will generally only work for simple problems. Given its greedy nature, it throws away all but the best solution, and can be prone to be stuck at a local optimum for more complicated problems. It would be beneficial to sample the next generation from a probability distribution that represents a more diverse set of ideas, rather than just from the best solution from the current generation.Simple Genetic AlgorithmOne of the oldest black-box optimisation algorithms is the genetic algorithm. There are many variations with many degrees of sophistication, but I will illustrate the simplest version here.The idea is quite simple: keep only 10% of the best performing solutions in the current generation, and let the rest of the population die. In the next generation, to sample a new solution is to randomly select two solutions from the survivors of the previous generation, and recombine their parameters to form a new solution. This crossover recombination process uses a coin toss to determine which parent to take each parameter from. In the case of our 2D toy function, our new solution might inherit xx or yy from either parents with 50% chance. Gaussian noise with a fixed standard deviation will also be injected into each new solution after this recombination process.The figure above illustrates how the simple genetic algorithm works. The green dots represent members of the elite population from the previous generation, the blue dots are the offsprings to form the set of candidate solutions, and the red dot is the best solution.Genetic algorithms help diversity by keeping track of a diverse set of candidate solutions to reproduce the next generation. However, in practice, most of the solutions in the elite surviving population tend to converge to a local optimum over time. There are more sophisticated variations of GA out there, such as CoSyNe, ESP, and NEAT, where the idea is to cluster similar solutions in the population together into different species, to maintain better diversity over time.Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)A shortcoming of both the Simple ES and Simple GA is that our standard deviation noise parameter is fixed. There are times when we want to explore more and increase the standard deviation of our search space, and there are times when we are confident we are close to a good optima and just want to fine tune the solution. We basically want our search process to behave like this:Amazing isn’it it? The search process shown in the figure above is produced by Covariance-Matrix Adaptation Evolution Strategy (CMA-ES). CMA-ES an algorithm that can take the results of each generation, and adaptively increase or decrease the search space for the next generation. It will not only adapt for the mean $\mu$ and sigma $\sigma$ parameters, but will calculate the entire covariance matrix of the parameter space. At each generation, CMA-ES provides the parameters of a multi-variate normal distribution to sample solutions from. So how does it know how to increase or decrease the search space?Before we discuss its methodology, let’s review how to estimate a covariance matrix. This will be important to understand CMA-ES’s methodology later on. If we want to estimate the covariance matrix of our entire sampled population of size of $N$, we can do so using the set of equations below to calculate the maximum likelihood estimate of a covariance matrix $C$. We first calculate the means of each of the $x_i$ and $y_i$ in our population:$$\mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i,$$$$\mu_y = \frac{1}{N} \sum_{i=1}^{N}y_i.$$The terms of the 2x2 covariance matrix $C$ will be:$$\begin{align}\sigma_x^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2, \\\sigma_y^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(y_i - \mu_y)^2, \\\sigma_{xy} &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y).\end{align}$$Of course, these resulting mean estimates $\mu_x$ and $\mu_y$, and covariance terms $\sigma_x$, $\sigma_y$, $\sigma_{xy}$ will just be an estimate to the actual covariance matrix that we originally sampled from, and not particularly useful to us.CMA-ES modifies the above covariance calculation formula in a clever way to make it adapt well to an optimisation problem. I will go over how it does this step-by-step. Firstly, it focuses on the best $N_{best}$ solutions in the current generation. For simplicity let’s set $N_{best}$ to be the best 25% of solutions. After sorting the solutions based on fitness, we calculate the mean $\mu^{(g+1)}$ of the next generation $(g+1)$ as the average of only the best 25% of the solutions in current population $(g)$, i.e.:$$\begin{align}\mu_x^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}x_i, \\\mu_y^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}y_i.\end{align}$$Next, we use only the best 25% of the solutions to estimate the covariance matrix $C^{(g+1)}$ of the next generation, but the clever hack here is that it uses the current generation’s $\mu^{(g)}$, rather than the updated $\mu^{(g+1)}$ parameters that we had just calculated, in the calculation:$$\begin{align}\sigma_x^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})^2, \\\sigma_y^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(y_i - \mu_y^{(g)})^2, \\\sigma_{xy}^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})(y_i - \mu_y^{(g)}).\end{align}$$Armed with a set of $\mu_x$, $\mu_y$, $\sigma_x$, $\sigma_y$, and $\sigma_{xy}$ parameters for the next generation $(g+1)$, we can now sample the next generation of candidate solutions.Below is a set of figures to visually illustrate how it uses the results from the current generation $(g)$ to construct the solutions in the next generation $(g+1)$:Step 1Step 2Step 3Step 4Calculate the fitness score of each candidate solution in generation $(g)$.Isolates the best 25% of the population in generation $(g)$, in purple.Using only the best solutions, along with the mean $\mu^{(g)}$ of the current generation (the green dot), calculate the covariance matrix $C^{(g+1)}$ of the next generation.Sample a new set of candidate solutions using the updated mean $\mu^{(g+1)}$ and covariance matrix $C^{(g+1)}$.Let’s visualise the scheme one more time, on the entire search process on both problems:Because CMA-ES can adapt both its mean and covariance matrix using information from the best solutions, it can decide to cast a wider net when the best solutions are far away, or narrow the search space when the best solutions are close by. My description of the CMA-ES algorithm for a 2D toy problem is highly simplified to get the idea across. For more details, I suggest reading the CMA-ES Tutorial prepared by Nikolaus Hansen, the author of CMA-ES.This algorithm is one of the most popular gradient-free optimisation algorithms out there, and has been the algorithm of choice for many researchers and practitioners alike. The only real drawback is the performance if the number of model parameters we need to solve for is large, as the covariance calculation is $O(N^2)$, although recently there has been approximations to make it $O(N)$. CMA-ES is my algorithm of choice when the search space is less than a thousand parameters. I found it still usable up to ~ 10K parameters if I’m willing to be patient.Natural Evolution StrategiesImagine if you had built an artificial life simulator, and you sample a different neural network to control the behavior of each ant inside an ant colony. Using the Simple Evolution Strategy for this task will optimise for traits and behaviours that benefit individual ants, and with each successive generation, our population will be full of alpha ants who only care about their own well-being.Instead of using a rule that is based on the survival of the fittest ants, what if you take an alternative approach where you take the sum of all fitness values of the entire ant population, and optimise for this sum instead to maximise the well-being of the entire ant population over successive generations? Well, you would end up creating a Marxist utopia.A perceived weakness of the algorithms mentioned so far is that they discard the majority of the solutions and only keep the best solutions. Weak solutions contain information about what not to do, and this is valuable information to calculate a better estimate for the next generation.Many people who studied RL are familiar with the REINFORCE paper. In this 1992 paper, Williams outlined an approach to estimate the gradient of the expected rewards with respect to the model parameters of a policy neural network. This paper also proposed using REINFORCE as an Evolution Strategy, in Section 6 of the paper. This special case of REINFORCE-ES was expanded later on in Parameter-Exploring Policy Gradients (PEPG, 2009) and Natural Evolution Strategies (NES, 2014).In this approach, we want to use all of the information from each member of the population, good or bad, for estimating a gradient signal that can move the entire population to a better direction in the next generation. Since we are estimating a gradient, we can also use this gradient in a standard SGD update rule typically used for deep learning. We can even use this estimated gradient with Momentum SGD, RMSProp, or Adam if we want to.The idea is to maximise the expected value of the fitness score of a sampled solution. If the expected result is good enough, then the best performing member within a sampled population will be even better, so optimising for the expectation might be a sensible approach. Maximising the expected fitness score of a sampled solution is almost the same as maximising the total fitness score of the entire population.If $z$ is a solution vector sampled from a probability distribution function $\pi(z, \theta)$, we can define the expected value of the objective function $F$ as:$$J(\theta) = E_{\theta}[F(z)] = \int F(z) \; \pi(z, \theta) \; dz,$$where $\theta$ are the parameters of the probability distribution function. For example, if $\pi$ is a normal distribution, then $\theta$ would be \muμand $\sigma$. For our simple 2D toy problems, each ensemble $z$ is a 2D vector $(x, y)$.The NES paper contains a nice derivation of the gradient of $J(\theta)$ with respect to $\theta$. Using the same log-likelihood trick as in the REINFORCE algorithm allows us to calculate the gradient of $J(\theta)$:$$\nabla_{\theta} J(\theta) = E_{\theta}[ \; F(z) \; \nabla_{\theta} \log \pi(z, \theta) \; ].$$In a population size of $N$, where we have solutions $z^1, z^2, … z^N$, we can estimate this gradient as a summation:$$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).$$With this gradient $\nabla_{\theta} J(\theta)$, we can use a learning rate parameter \alphaα (such as 0.01) and start optimising the $\theta$ parameters of pdf $\pi$ so that our sampled solutions will likely get higher fitness scores on the objective function $F$. Using SGD (or Adam), we can update $\theta$ for the next generation:$$\theta \rightarrow \theta + \alpha \nabla_{\theta} J(\theta),$$and sample a new set of candidate solutions $z$ from this updated pdf, and continue until we arrive at a satisfactory solution.In Section 6 of the REINFORCE paper, Williams derived closed-form formulas of the gradient $\nabla_{\theta} \log \pi(z^i, \theta)$, for the special case where $ \pi(z, \theta)$ is a factored multi-variate normal distribution (i.e., the correlation parameters are zero). In this special case, $\theta$ are the $\mu$ and $\sigma$ vectors. Therefore, each element of a solution can be sampled from a univariate normal distribution $z_j \sim N(\mu_j, \sigma_j)$.The closed-form formulas for $\nabla_{\theta} \log N(z^i, \theta)$, for each individual element of vector $\theta$ on each solution $i$ in the population can be derived as:$$\nabla_{\mu_{j}} \log N(z^i, \mu, \sigma) = \frac{z_j^i - \mu_j}{\sigma_j},$$$$\nabla_{\sigma_{j}} \log N(z^i, \mu, \sigma) = \frac{(z_j^i - \mu_j)^2 - \sigma_j^2}{\sigma_j^3}.$$For clarity, I use the index of jj, to count across parameter space, and this is not to be confused with superscript $i$, used to count across each sampled member of the population. For our 2D problems, $z_1 = x, z_2 = y, \mu_1 = \mu_x, \mu_2 = \mu_y, \sigma_1 = \sigma_x, \sigma_2 = \sigma_y$ in this context.These two formulas can be plugged back into the approximate gradient formula to derive explicit update rules for \muμ and \sigmaσ. In the papers mentioned above, they derived more explicit update rules, incorporated a baseline, and introduced other tricks such as antithetic sampling in PEPG, which is what my implementation is based on. NES proposed incorporating the inverse of the Fisher Information Matrix into the gradient update rule. But the concept is basically the same as other ES algorithms, where we update the mean and standard deviation of a multi-variate normal distribution at each new generation, and sample a new set of solutions from the updated distribution. Below is a visualization of this algorithm in action, following the formulas described above:We see that this algorithm is able to dynamically change the $\sigma$’s to explore or fine tune the solution space as needed. Unlike CMA-ES, there is no correlation structure in our implementation, so we don’t get the diagonal ellipse samples, only the vertical or horizontal ones, although in principle we can derive update rules to incorporate the entire covariance matrix if we needed to, at the expense of computational efficiency.I like this algorithm because like CMA-ES, the $\sigma$’s can adapt so our search space can be expanded or narrowed over time. Because the correlation parameter is not used in this implementation, the efficiency of the algorithm is $O(N)$ so I use PEPG if the performance of CMA-ES becomes an issue. I usually use PEPG when the number of model parameters exceed several thousand.OpenAI Evolution StrategyIn OpenAI’s paper, they implement an evolution strategy that is a special case of the REINFORCE-ES algorithm outlined earlier. In particular, \sigmaσ is fixed to a constant number, and only the \muμ parameter is updated at each generation. Below is how this strategy looks like, with a constant \sigmaσ parameter:In addition to the simplification, this paper also proposed a modification of the update rule that is suitable for parallel computation across different worker machines. In their update rule, a large grid of random numbers have been pre-computed using a fixed seed. By doing this, each worker can reproduce the parameters of every other worker over time, and each worker needs only to communicate a single number, the final fitness result, to all of the other workers. This is important if we want to scale evolution strategies to thousands or even a million workers located on different machines, since while it may not be feasible to transmit an entire solution vector a million times at each generation update, it may be feasible to transmit only the final fitness results. In the paper, they showed that by using 1440 workers on Amazon EC2 they were able to solve the Mujoco Humanoid walking task in ~ 10 minutes.I think in principle, this parallel update rule should work with the original algorithm where they can also adapt $\sigma$, but perhaps in practice, they wanted to keep the number of moving parts to a minimum for large-scale parallel computing experiments. This inspiring paper also discussed many other practical aspects of deploying ES for RL-style tasks, and I highly recommend going through it to learn more.Fitness ShapingMost of the algorithms above are usually combined with a fitness shaping method, such as the rank-based fitness shaping method I will discuss here. Fitness shaping allows us to avoid outliers in the population from dominating the approximate gradient calculation mentioned earlier:$$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).$$If a particular $F(z^m)$ is much larger than other $F(z^i)$ in the population, then the gradient might become dominated by this outliers and increase the chance of the algorithm being stuck in a local optimum. To mitigate this, one can apply a rank transformation of the fitness. Rather than use the actual fitness function, we would rank the results and use an augmented fitness function which is proportional to the solution’s rank in the population. Below is a comparison of what the original set of fitness may look like, and what the ranked fitness looks like:What this means is supposed we have a population size of 101. We would evaluate each population to the actual fitness function, and then sort the solutions based by their fitness. We will assign an augmented fitness value of -0.50 to the worse performer, -0.49 to the second worse solution, …, 0.49 to the second best solution, and finally a fitness value of 0.50 to the best solution. This augmented set of fitness values will be used to calculate the gradient update, instead of the actual fitness values. In a way, it is a similar to just applying Batch Normalization to the results, but more direct. There are alternative methods for fitness shaping but they all basically give similar results in the end.I find fitness shaping to be very useful for RL tasks if the objective function is non-deterministic for a given policy network, which is often the cases on RL environments where maps are randomly generated and various opponents have random policies. It is less useful for optimising for well-behaved functions that are deterministic, and the use of fitness shaping can sometimes slow down the time it takes to find a good solution.MNISTAlthough ES might be a way to search for more novel solutions that are difficult for gradient-based methods to find, it still vastly underperforms gradient-based methods on many problems where we can calculate high quality gradients. For instance, only an idiot would attempt to use a genetic algorithm for image classification. But sometimes such people do exist in the world, and sometimes these explorations can be fruitful!Since all ML algorithms should be tested on MNIST, I also tried to apply these various ES algorithms to find weights for a small, simple 2-layer convnet used to classify MNIST, just to see where we stand compared to SGD. The convnet only has ~ 11k parameters so we can accommodate the slower CMA-ES algorithm. The code and the experiments are available here.Below are the results for various ES methods, using a population size of 101, over 300 epochs. We keep track of the model parameters that performed best on the entire training set at the end of each epoch, and evaluate this model once on the test set after 300 epochs. It is interesting how sometimes the test set’s accuracy is higher than the training set for the models that have lower scores.MethodTrain SetTest SetAdam (BackProp) Baseline99.898.9Simple GA82.182.4CMA-ES98.498.1OpenAI-ES96.096.2PEPG98.598.0We should take these results with a grain of salt, since they are based on a single run, rather than the average of 5-10 runs. The results based on a single-run seem to indicate that CMA-ES is the best at the MNIST task, but the PEPG algorithm is not that far off. Both of these algorithms achieved ~ 98% test accuracy, 1% lower than the SGD/ADAM baseline. Perhaps the ability to dynamically alter its covariance matrix, and standard deviation parameters over each generation allowed it to fine-tune its weights better than OpenAI’s simpler variation.Try It YourselfThere are probably open source implementations of all of the algorithms described in this article. The author of CMA-ES, Nikolaus Hansen, has been maintaining a numpy-based implementation of CMA-ES with lots of bells and whistles. His python implementation introduced me to the training loop interface described earlier. Since this interface is quite easy to use, I also implemented the other algorithms such as Simple Genetic Algorithm, PEPG, and OpenAI’s ES using the same interface, and put it in a small python file called es.py, and also wrapped the original CMA-ES library in this small library. This way, I can quickly compare different ES algorithms by just changing one line:import es#solver = es.SimpleGA(...)#solver = es.PEPG(...)#solver = es.OpenES(...)solver = es.CMAES(...)while True:solutions = solver.ask()fitness_list = np.zeros(solver.popsize)for i in range(solver.popsize):fitness_list[i] = evaluate(solutions[i])solver.tell(fitness_list)result = solver.result()if result[1] &gt; MY_REQUIRED_FITNESS:breakYou can look at es.py on GitHub and the IPython notebook examples using the various ES algorithms.In this IPython notebook that accompanies es.py, I show how to use the ES solvers in es.py to solve a 100-Dimensional version of the Rastrigin function with even more local optimum points. The 100-D version is somewhat more challenging than the trivial 2D version used to produce the visualizations in this article. Below is a comparison of the performance for various algorithms discussed:On this 100-D Rastrigin problem, none of the optimisers got to the global optimum solution, although CMA-ES comes close. CMA-ES blows everything else away. PEPG is in 2nd place, and OpenAI-ES / Genetic Algorithm falls behind. I had to use an annealing schedule to gradually lower \sigmaσ for OpenAI-ES to make it perform better for this task.Final solution that CMA-ES discovered for 100-D Rastrigin function.Global optimal solution is a 100-dimensional vector of exactly 10.References and Other LinksBelow are a few links to information related to evolutionary computing which I found useful or inspiring.Image Credits of Lemmings Jumping off a Cliff. Your results may vary when investing in ICOs.CMA-ES: Official Reference Implementation on GitHub, Tutorial, Original CMA-ES Paper from 2001, Overview SlidesSimple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning (REINFORCE), 1992.Parameter-Exploring Policy Gradients, 2009.Natural Evolution Strategies, 2014.Evolution Strategies as a Scalable Alternative to Reinforcement Learning, OpenAI, 2017.Risto Miikkulainen’s Slides on Neuroevolution.A Neuroevolution Approach to General Atari Game Playing, 2013.Kenneth Stanley’s Talk on Why Greatness Cannot Be Planned: The Myth of the Objective, 2015.Neuroevolution: A Different Kind of Deep Learning. The quest to evolve neural networks through evolutionary algorithms.Compressed Network Search Finds Complex Neural Controllers with a Million Weights.Karl Sims Evolved Virtual Creatures, 1994.Evolved Step Climbing Creatures.Super Mario World Agent Mario I/O, Mario Kart 64 Controller using) using NEAT Algorithm.Ingo Rechenberg, the inventor of Evolution Strategies.A Tutorial on Differential Evolution with Python.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Introduction to Genetic Algorithm]]></title>
      <url>%2F2018%2F01%2F23%2FIntroduction-to-Genetic-Algorithm%2F</url>
      <content type="text"><![CDATA[1. Intuition behind Genetic AlgorithmsLet’s start with the famous quote by Charles Darwin:It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line.Let us understand with a basic example:Let’s take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this.You select all the good people, and ask them to extend their generation by having their children.This repeats for a few generations.You will notice that now you have an entire population of good people.Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So let’s us quickly grasp some little concepts, so that we can draw a parallel line between them.2. Biological InspirationI am sure you would remember:Cells are the basic building block of all living things.Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.Traditionally, these chromosomes are represented in binary as strings of 0’s and 1’s.Source : linkA chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color.I wanted you to recall these basics concept of biology before going further. Let’s get back and understand what actually is a genetic algorithm?3. What is a Genetic Algorithm?Let’s get back to the example we discussed above and summarize what we did.Firstly, we defined our initial population as our countrymen.We defined a function to classify whether is a person is good or bad.Then we selected good people for mating to produce their off-springs.And finally, these off-springs replace the bad people from the population and this process repeats.This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent.So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results.The working of a genetic algorithm is also derived from biology, which is as shown in the image below.Source: linkSo, let us try to understand the steps one by one.4. Steps Involved in Genetic AlgorithmHere, to make things easier, let us understand it by the famous Knapsack problem.If you haven’t come across this problem, let me introduce my version of this problem.Let’s say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of 30 kg. Now you have different survival items, each having its own “Survival Points” (which are given for each item in the table). So, your objective is maximise the survival points.Here is the table giving details about each item.4.1 InitialisationTo solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes.We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped.This set of chromosome is considered as our initial population.4.2 Fitness FunctionLet us calculate fitness points for our first two chromosomes.For A1 chromosome [100110],Similarly for A2 chromosome [001110],So, for this problem, our chromosome will be considered as more fit when it contains more survival points.Therefore chromosome 1 is more fit than chromosome 2.4.3 SelectionNow, we can select fit chromosomes from our population which can mate and create their off-springs.General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity.Therefore, we generally use Roulette Wheel Selection method.Don’t be afraid of name, just take a look at the image below.I suppose we all have seen this, either in real or in movies. So, let’s build our roulette wheel.Consider a wheel, and let’s divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value.Based on these values, let us create our roulette wheel.So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated.Sometimes we mark two fixed point as shown in the figure below.So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method.4.4 CrossoverSo in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below.This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs.If you take two crossover point, then it will called as multi point crossover which is as shown below.4.5 MutationNow if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population.A simple method of mutation is shown in the image below.So the entire process is summarise as shown in the figure.Source : linkThe off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population.But the question is how we will get to know that we have reached our best possible solution?So basically there are different termination conditions, which are listed below:There is no improvement in the population for over x iterations.We have already predefined an absolute number of generation for our algorithm.When our fitness function has reached a predefined value.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]]]></title>
      <url>%2F2018%2F01%2F22%2FEvolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost%2F</url>
      <content type="text"><![CDATA[Source blog is here.We’ve discovered that evolution strategies (ES), an optimization technique that’s been known for decades, rivals the performance of standard reinforcement learning (RL)techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.In particular, ES is simpler to implement (there is no need for backpropagation), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer hyperparameters. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on finite differences along a few random directions at each step.Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the “AlexNet” paper showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the Deep Q-Learning paper showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.In what follows, we’ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.Reinforcement LearningLet’s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we’d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the player’s paddle (in green, on right) Up, Down, or neither.The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network’s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.Exploration by injecting noise in the actions. The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent’s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.Evolution StrategiesOn “Evolution”. Before we dive into the ES approach, it is important to note that despite the word “evolution”, ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.Black-box optimization. In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function f(w) with respect to the input vector w(the parameters / weights of the network), but we make no assumptions about the structure of f, except that we can evaluate it (hence “black box”).The ES algorithm. Intuitively, the optimization is a “guess and check” process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector w and generate a population of, say, 100 slightly different parameter vectors w1 ... w100 by jittering w with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you’ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we’re still doing RL (Policy Gradients, or REINFORCE specifically), where the agent’s actions are to emit entire parameter vectors using a gaussian policy.Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.Code sample. To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this longer version with more comments):1234567891011121314151617# simple example: minimize a quadratic around some solution pointimport numpy as npsolution = np.array([0.5, 0.1, -0.3])def f(w): return -np.sum((w - solution)**2)npop = 50 # population sizesigma = 0.1 # noise standard deviationalpha = 0.001 # learning ratew = np.random.randn(3) # initial guessfor i in range(300): N = np.random.randn(npop, 3) R = np.zeros(npop) for j in range(npop): w_try = w + sigma*N[j] R[j] = f(w_try) A = (R - np.mean(R)) / np.std(R) w = w + alpha/(npop*sigma) * np.dot(N.T, A)Injecting noise in the parameters. Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a “guess and check” on actions, while ES is a “guess and check” on parameters. Since we’re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.Tradeoffs between ES and RLES enjoys multiple advantages over RL algorithms (some of them are a little technical):No need for backpropagation. ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers).Highly parallelizable. ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization.Higher robustness. Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not “scale-free”, so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip.Structured exploration. Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration.Credit assignment over long time scales. By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available.Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma’s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.ES is competitive with RLWe compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we’ve seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the TRPO algorithm in this case):Data efficiency comparison. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn’t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).Wall clock comparison. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.Related WorkES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at Wikipedia, Scholarpedia, and Jürgen Schmidhuber’s review article (Section 6.6). The work that most closely informed our approach is Natural Evolution Strategies by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. virtual batch norm), and evaluating them on modern RL benchmarks.It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with HyperNetworks, “Large-Scale Evolution of Image Classifiers” and “Convolution by Evolution”.ConclusionOur work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.Note on supervised learning. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.Code release. Finally, if you’d like to try running ES yourself, we encourage you to dive into the full details by reading our paper or looking at our code on this Github repo.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Seq2Seq with Attention and Beam Search [Repost]]]></title>
      <url>%2F2018%2F01%2F21%2FSeq2Seq-with-Attention-and-Beam-Search-Repost%2F</url>
      <content type="text"><![CDATA[Source Post is hereSequence to Sequence basicsLet’s explain the sequence to sequence framework as we’ll rely on it for our model. Let’s start with the simplest version on the translation task.As an example, let’s translate how are you in French comment vas tu.Vanilla Seq2SeqThe Seq2Seq framework relies on the encoder-decoder paradigm. The encoder encodes the input sequence, while the decoder produces the target sequenceEncoderOur input sequence is how are you. Each word from the input sequence is associated to a vector $w \in \mathbb{R}^d$ (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w_0, w_1, w_2] \in \mathbb{R}^{d \times 3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation $e$. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$)Vanilla EncoderDecoderNow that we have a vector ee that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: ee as hidden state and a special start of sentence vector $w_{\text{sos}}$ as input. The LSTM computes the next hidden state $h_0 \in \mathbb{R}^{h}$. Then, we apply some function $g : \mathbb{R}^h \mapsto \mathbb{R}^V$ so that $s_0 := g(h_0) \in \mathbb{R}^V$ is a vector of the same size as the vocabulary.$$\begin{align}h_0 &amp;= \operatorname{LSTM}\left(e, w_{sos} \right)\\s_0 &amp;= g(h_0)\\p_0 &amp;= \operatorname{softmax}(s_0)\\i_0 &amp;= \operatorname{argmax}(p_0)\\\end{align}$$Then, apply a softmax to $s_0$ to normalize it into a vector of probabilities $p_0 \in \mathbb{R}^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word “comment” has the highest probability (and thus $i_0 = \operatorname{argmax}(p_0)$) corresponds to the index of “comment”). Get a corresponding vector and $w_{i_0} = w_{comment}$ repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.$$\begin{align}h_1 &amp;= \operatorname{LSTM}\left(h_0, w_{i_0} \right)\\s_1 &amp;= g(h_1)\\p_1 &amp;= \operatorname{softmax}(s_1)\\i_1 &amp;= \operatorname{argmax}(p_1)\end{align}$$The decoding stops when the predicted word is a special end of sentence token.Vanilla DecoderIntuitively, the hidden vector represents the “amount of meaning” that has not been decoded yet.The above method aims at modelling the distribution of the next word conditionned on the beginning of the sentence$$\mathbb{P}\left[ y_{t+1} | y_1, \dots, y_{t}, x_0, \dots, x_n \right]$$by writing$$\mathbb{P}\left[ y_{t+1} | y_t, h_{t}, e \right]$$Seq2Seq with AttentionThe previous model has been refined over the past few years and greatly benefited from what is known as attention. Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is explained by Bahdanau et al.. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM$$\begin{align}h_{t} &amp;= \operatorname{LSTM}\left(h_{t-1}, [w_{i_{t-1}}, c_t] \right)\\s_t &amp;= g(h_t)\\p_t &amp;= \operatorname{softmax}(s_t)\\i_t &amp;= \operatorname{argmax}(p_t)\end{align}$$The vector ctct is the attention (or context) vector. We compute a new context vector at each decoding step. First, with a function $f (h_{t-1}, e_{t’}) \mapsto \alpha_{t’} \in \mathbb{R}$, compute a score for each hidden state $e_{t^{\prime}}$ of the encoder. Then, normalize the sequence of using a softmax and compute $c_t$ as the weighted average of the $e_{t^{\prime}}$.$$\begin{align}\alpha_{t’} &amp;= f(h_{t-1}, e_{t’}) \in \mathbb{R} &amp; \text{for all } t’\\\bar{\alpha} &amp;= \operatorname{softmax} (\alpha)\\c_t &amp;= \sum_{t’=0}^n \bar{\alpha}_{t’} e_{t’}\end{align}$$Attention MechanismThe choice of the function ff varies, but is usually one of the following$$f(h_{t-1}, e_{t’}) =\begin{cases}h_{t-1}^T e_{t’} &amp; \text{dot}\\h_{t-1}^T W e_{t’} &amp; \text{general}\\v^T \tanh \left(W [h_{t-1}, e_{t’}]\right) &amp; \text{concat}\\\end{cases}$$It turns out that the attention weighs $\bar{\alpha}$ can be easily interpreted. When generating the word vas(corresponding to are in English), we expect $\bar{\alpha} _ {\text{are}}$ are to be close to $1$ while $\bar{\alpha} _ {\text{how}}$ and $\bar{\alpha} _ {\text{you}}$ to be close to $0$. Intuitively, the context vector $c$ will be roughly equal to the hidden vector of are and it will help to generate the French word vas.By putting the attention weights into a matrix (rows = input sequence, columns = output sequence), we would have access to the alignment between the words from the English and French sentences… (see page 6) There is still a lot of things to say about sequence to sequence models (for instance, it works better if the encoder processes the input sequence backwards…).TrainingWhat happens if the first time step is not sure about wether it should generate comment or vas (most likely case at the beginning of the training)? Then it would mess up the entire sequence, and the model will hardly learn anything…If we use the predicted token as input to the next step during training (as explained above), errors would accumulate and the model would rarely be exposed to the correct distribution of inputs, making training slow or impossible. To speedup things, one trick is to feed the actual output sequence (&lt;sos&gt; comment vas tu) into the decoder’s LSTM and predict the next token at every position (comment vas tu &lt;eos&gt;).TrainingThe decoder outputs vectors of probability over the vocabulary $p_i \in \mathbb{R}^V$ for each time step. Then, for a given target sequence $y_1, \dots, y_n$, we can compute its probability as the product of the probabilities of each token being produced at each relevant time step:$$\mathbb{P}\left(y_1, \dots, y_m \right) = \prod_{i=1}^m p_i [y_i]$$where $p_i [y_i]$ means that we extract the $y_i$-th entry of the probability vector $p_i$ from the $i$-th decoding step. In particular, we can compute the probability of the actual target sequence. A perfect system would give a probabilty of 1 to this target sequence, so we are going to train our network to maximize the probability of the target sequence, which is the same as minimizing$$\begin{align}-\log \mathbb{P} \left(y_1, \dots, y_m \right) &amp;= - \log \prod_{i=1}^m p_i [y_i]\\&amp;= - \sum_{i=1}^n \log p_i [y_i]\\\end{align}$$in our example, this is equal to$$\log p_1[\text{comment}] - \log p_2[\text{vas}] - \log p_3[\text{tu}] - \log p_4[\text{}]$$and you recognize the standard cross entropy: we actually are minimizing the cross entropy between the target distribution (all one-hot vectors) and the predicted distribution outputed by our model (our vectors $p_i$).DecodingThe main takeaway from the discussion above is that for the same model, we can define different behaviors. In particular, we defined a specific behavior that speeds up training.What about inference/testing time then? Is there an other way to decode a sentence?There indeed are 2 main ways of performing decoding at testing time (translating a sentence for which we don’t have a translation). The first of these methods is the one covered at the beginning of the article: greedy decoding. It is the most natural way and it consists in feeding to the next step the most likely word predicted at the previous step.Greedy Decoder - feeds the best token to the next stepBut didn’t we say that this behavior is likely to accumulate errors?Even after having trained the model, it can happen that the model makes a small error (and gives a small advantage to vas over comment for the first step of the decoding). This would mess up the entire decoding…There is a better way of performing decoding, called Beam Search. Instead of only predicting the token with the best score, we keep track of $k$ hypotheses (for example $k=5$, we refer to kk as the beam size). At each new time step, for these 5 hypotheses we have $V$ new possible tokens. It makes a total of $5V$ new hypotheses. Then, only keep the $5$ best ones, and so on… Formally, define $\mathcal{H}_t$ the set of hypotheses decoded at time step $t$.$$\mathcal{H}_ t := \{ (w^1_1, \dots, w^1_t), \dots, (w^k_1, \dots, w^k_t) \}$$For instance if $k=2$, one possible $\mathcal{H}_2$ would be$$\mathcal{H}_ 2 := \{ (\text{comment vas}), (\text{comment tu}) \}$$Now we consider all the possible candidates $\mathcal{C}_{t+1}$, produced from $\mathcal{H}_t$ by adding all possible new tokens$$\mathcal{C}_ {t+1} := \bigcup_{i=1}^k \{ (w^i_1, \dots, w^i_t, 1), \dots, (w^i_1, \dots, w^i_t, V) \}$$and keep the $k$ highest scores (probability of the sequence). If we keep our example$$\begin{align}\mathcal{C}_ 3 =&amp; \{ (\text{comment vas comment}), (\text{comment vas vas}), (\text{comment vas tu})\} \\\cup &amp; \{ (\text{comment tu comment}), \ \ (\text{comment tu vas}), \ \ (\text{comment tu tu}) \}\end{align}$$and for instance we can imagine that the 2 best ones would be$$\mathcal{H}_ 3 := \{ (\text{comment vas tu}), (\text{comment tu vas}) \}$$Once every hypothesis reached the &lt;eos&gt; token, we return the hypothesis with the highest score.If we use beam search, a small error at the first step might be rectified at the next step, as we keep the gold hypthesis in the beam!ConclusionIn this article we covered the seq2seq concepts. We showed that training is different than decoding. We covered two methods for decoding: greedy and beam search. While beam search generally achieves better results, it is not perfect and still suffers from exposure bias. During training, the model is never exposed to its errors! It also suffers from Loss-Evaluation Mismatch. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…Now, let’s apply Seq2Seq for LaTeX generation from images!Producing LaTeX code from an imageApproachPrevious part covered the concepts of sequence-to-sequence applied to Machine Translation. The same framework can be applied to our LaTeX generation problem. The input sequence would just be replaced by an image, preprocessed with some convolutional model adapted to OCR (in a sense, if we unfold the pixels of an image into a sequence, this is exactly the same problem). This idea proved to be efficient for image captioning (see the reference paper Show, Attend and Tell). Building on some great work from the Harvard NLP group, my teammate Romain and I chose to follow a similar approach.Keep the seq2seq framework but replace the encoder by a convolutional network over the image!Good Tensorflow implementations of such models were hard to find. Together with this post, I am releasing the code and hope some will find it useful. You can use it to train your own image captioning model or adapt it for a more advanced use. The code does not rely on the Tensorflow Seq2Seq library as it was not entirely ready at the time of the project and I also wanted more flexibility (but adopts a similar interface).DataTo train our model, we’ll need labeled examples: images of formulas along with the LaTeX code used to generate the images. A good source of LaTeX code is arXiv, that has thousands of articles under the .tex format. After applying some heuristics to find equations in the .tex files, keeping only the ones that actually compile, the Harvard NLP group extracted $\sim 100,000$ formulas.Wait… Don’t you have a problem as different LaTeX codes can give the same image?Good point: (x^2 + 1) and \left( x^{2} + 1 \right) indeed give the same output. That’s why Harvard’s paper found out that normalizing the data using a parser (KaTeX) improved performance. It forces adoption of some conventions, like writing x ^ { 2 } instead of x^2, etc. After normalization, they end up with a .txt file containing one formula per line that looks like1234\alpha + \beta\frac &#123; 1 &#125; &#123; 2 &#125;\frac &#123; \alpha &#125; &#123; \beta &#125;1 + 2From this file, we’ll produce images 0.png, 1.png, etc. and a matching file mapping the image files to the indices (=line numbers) of the formulas12340.png 01.png 12.png 23.png 3The reason why we use this format is that it is flexible and allows you to use the pre-built dataset from Harvard (You may need to use the preprocessing scripts as explained here). You’ll also need to have pdflatex and ImageMagick installed.We also build a vocabulary, to map LaTeX tokens to indices that will be given as input to our model. If we keep the same data as above, our vocabulary will look like+ 1 2 \alpha \beta \frac { }ModelOur model is going to rely on a variation of the Seq2Seq model, adapted to images. First, let’s define the input of our graph. Not surprisingly we get as input a batch of black-and-white images of shape $[H,W]$ and a batch of formulas (ids of the LaTeX tokens):123456# batch of images, shape = (batch size, height, width, 1)img = tf.placeholder(tf.uint8, shape=(None, None, None, 1), name='img')# batch of formulas, shape = (batch size, length of the formula)formula = tf.placeholder(tf.int32, shape=(None, None), name='formula')# for paddingformula_length = tf.placeholder(tf.int32, shape=(None, ), name='formula_length')A special note on the type of the image input. You may have noticed that we use tf.uint8. This is because our image is encoded in grey-levels (integers from 0 to 255 - and $2^8=256$). Even if we could give a tf.float32 Tensor as input to Tensorflow, this would be 4 times more expensive in terms of memory bandwith. As data starvation is one of the main bottlenecks of GPUs, this simple trick can save us some training time. For further improvement of the data pipeline, have a look at the new Tensorflow data pipeline.EncoderHigh-level idea Apply some convolutional network on top of the image an flatten the output into a sequence of vectors $[e_1, \cdots, e_n]$, each of those corresponding to a region of the input image. These vectors will correspond to the hidden vectors of the LSTM that we used for translation.Once our image is transformed into a sequence, we can use the seq2seq model!Convolutional Encoder - produces a sequence of vectorsWe need to extract features from our image, and for this, nothing has (yet) been proven more effective than convolutions. Here, there is nothing much to say except that we pick some architecture that has been proven to be effective for Optical Character Recognition (OCR), which stacks convolutional layers and max-pooling to produce a Tensor of shape $[H^{\prime},W^{\prime},512]$12345678910111213141516171819# casting the image back to float32 on the GPUimg = tf.cast(img, tf.float32) / 255.out = tf.layers.conv2d(img, 64, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, 2, 2, "SAME")out = tf.layers.conv2d(out, 128, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, 2, 2, "SAME")out = tf.layers.conv2d(out, 256, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.conv2d(out, 256, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, (2, 1), (2, 1), "SAME")out = tf.layers.conv2d(out, 512, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, (1, 2), (1, 2), "SAME")# encoder representation, shape = (batch size, height', width', 512)out = tf.layers.conv2d(out, 512, 3, 1, "VALID", activation=tf.nn.relu)Now that we have extracted some features from the image, let’s unfold the image to get a sequence so that we can use our sequence to sequence framework. We end up with a sequence of length $[H^{\prime},W^{\prime}]$12H, W = tf.shape(out)[1:2]seq = tf.reshape(out, shape=[-1, H*W, 512])Don’t you loose a lot of structural information by reshaping? I’m afraid that when performing attention over the image, my decoder won’t be able to understand the location of each feature vector in the original image!It turns out that the model manages to work despite this issue, but that’s not completely satisfying. In the case of translation, the hidden states of the LSTM contained some positional information that was computed by the LSTM (after all, LSTM are by essence sequential). Can we fix this issue?Positional Embeddings I decided to follow the idea from Attention is All you Need that adds positional embeddings to the image representation (out), and has the huge advantage of not adding any new trainable parameter to our model. The idea is that for each position of the image, we compute a vector of size $512$ such that its components are coscos or sinsin. More formally, the (2i)-th and (2i+1)-th entries of my positional embedding $v$ at position $p$ will be$$\begin{align}v_{2i} &amp;= \sin\left(p / f^{2i}\right)\\v_{2i+1} &amp;= \cos\left(p / f^{2i}\right)\\\end{align}$$where $f$ is some frequency parameter. Intuitively, because $\sin(a+b)$ and $\cos⁡(a+b)$ can be expressed in terms of $\sin⁡(b)$ , $\sin(a)$ , $\cos⁡(b)$ and $\cos⁡(a)$, there will be linear dependencies between the components of distant embeddings, authorizing the model to extract relative positioning information. Good news: the tensorflow code for this technique is available in the library tensor2tensor, so we just need to reuse the same function and transform our out with the following call1out = add_timing_signal_nd(out)DecoderNow that we have a sequence of vectors $[e_1, \dots, e_n]$ that represents our input image, let’s decode it! First, let’s explain what variant of the Seq2Seq framework we are going to use.First hidden vector of the decoder’s LSTM In the seq2seq framework, this is usually just the last hidden vector of the encoder’s LSTM. Here, we don’t have such a vector, so a good choice would be to learn to compute it with a matrix $W$ and a vector $b$$$h_0 = \tanh\left( W \cdot \left( \frac{1}{n} \sum_{i=1}^n e_i\right) + b \right)$$This can be done in Tensorflow with the following logic1234img_mean = tf.reduce_mean(seq, axis=1)W = tf.get_variable("W", shape=[512, 512])b = tf.get_variable("b", shape=[512])h = tf.tanh(tf.matmul(img_mean, W) + b)Attention Mechanism We first need to compute a score $\alpha_{t^{\prime}}$ for each vector $e_{t^{\prime}}$ of the sequence. We use the following method$$\begin{align}\alpha_{t’} &amp;= \beta^T \tanh\left( W_1 \cdot e_{t’} + W_2 \cdot h_{t} \right)\\\bar{\alpha} &amp;= \operatorname{softmax}\left(\alpha\right)\\c_t &amp;= \sum_{i=1}^n \bar{\alpha}_{t’} e_{t’}\\\end{align}$$This can be done in Tensorflow with the follwing code12345678910111213141516# over the image, shape = (batch size, n, 512)W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False)# over the hidden vector, shape = (batch size, 512)W2_h = tf.layers.dense(inputs=h, units=512, use_bias=False)# sums the two contributionsa = tf.tanh(W1_e + tf.expand_dims(W2_h, axis=1))beta = tf.get_variable("beta", shape=[512, 1], dtype=tf.float32)a_flat = tf.reshape(a, shape=[-1, 512])a_flat = tf.matmul(a_flat, beta)a = tf.reshape(a, shape=[-1, n])# compute weightsa = tf.nn.softmax(a)a = tf.expand_dims(a, axis=-1)c = tf.reduce_sum(a * seq, axis=1)Note that the line W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False) is common to every decoder time step, so we can just compute it once and for all. The dense layer with no bias is just a matrix multiplication.Now that we have our attention vector, let’s just add a small modification and compute an other vector $o_{t-1}$ (as in Luong, Pham and Manning) that we will use to make our final prediction and that we will feed as input to the LSTM at the next step. Here $w_{t−1}$ denotes the embedding of the token generated at the previous step.$o_t$ passes some information about the distribution from the previous time step as well as the confidence it had for the predicted token$$\begin{align}h_t &amp;= \operatorname{LSTM}\left( h_{t-1}, [w_{t-1}, o_{t-1}] \right)\\c_t &amp;= \operatorname{Attention}([e_1, \dots, e_n], h_t)\\o_{t} &amp;= \tanh\left(W_3 \cdot [h_{t}, c_t] \right)\\p_t &amp;= \operatorname{softmax}\left(W_4 \cdot o_{t} \right)\\\end{align}$$and now the code1234567# compute oW3_o = tf.layers.dense(inputs=tf.concat([h, c], axis=-1), units=512, use_bias=False)o = tf.tanh(W3_o)# compute the logits scores (before softmax)logits = tf.layers.dense(inputs=o, units=vocab_size, use_bias=False)# the softmax will be computed in the loss or somewhere elseIf I read carefully, I notice that for the first step of the decoding process, we need to compute an $o_0$ too, right?This is a good point, and we just use the same technique that we used to generate $h_0$ but with different weights.TrainingWe’ll need to create 2 different outputs in the Tensorflow graph: one for training (that uses the formulaand feeds the ground truth at each time step, see part I) and one for test time (that ignores everything about the actual formula and uses the prediction from the previous step).AttentionCellWe’ll need to encapsulate the reccurent logic into a custom cell that inherits RNNCell. Our custom cell will be able to call the LSTM cell (initialized in the __init__). It also has a special recurrent state that combines the LSTM state and the vector oo (as we need to pass it through). An elegant way is to define a namedtuple for this recurrent state:12345678910111213141516171819202122AttentionState = collections.namedtuple("AttentionState", ("lstm_state", "o"))class AttentionCell(RNNCell): def __init__(self): self.lstm_cell = LSTMCell(512) def __call__(self, inputs, cell_state): """ Args: inputs: shape = (batch_size, dim_embeddings) embeddings from previous time step cell_state: (AttentionState) state from previous time step """ lstm_state, o = cell_state # compute h h, new_lstm_state = self.lstm_cell(tf.concat([inputs, o], axis=-1), lstm_state) # apply previous logic c = ... new_o = ... logits = ... new_state = AttentionState(new_lstm_state, new_o) return logits, new_stateThen, to compute our output sequence, we just need to call the previous cell on the sequence of LaTeX tokens. We first produce the sequence of token embeddings to which we concatenate the special &lt;sos&gt; token. Then, we call dynamic_rnn.123456789101112131415# 1. get token embeddingsE = tf.get_variable("E", shape=[vocab_size, 80], dtype=tf.float32)# special &lt;sos&gt; tokenstart_token = tf.get_variable("start_token", dtype=tf.float32, shape=[80])tok_embeddings = tf.nn.embedding_lookup(E, formula)# 2. add the special &lt;sos&gt; token embedding at the beggining of every formulastart_token_ = tf.reshape(start_token, [1, 1, dim])start_tokens = tf.tile(start_token_, multiples=[batch_size, 1, 1])# remove the &lt;eos&gt; that won't be used because we reached the endtok_embeddings = tf.concat([start_tokens, tok_embeddings[:, :-1, :]], axis=1)# 3. decodeattn_cell = AttentionCell()seq_logits, _ = tf.nn.dynamic_rnn(attn_cell, tok_embeddings, initial_state=AttentionState(h_0, o_0))LossCode speaks for itself12345678910# compute - log(p_i[y_i]) for each time step, shape = (batch_size, formula length)losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seq_logits, labels=formula)# masking the lossesmask = tf.sequence_mask(formula_length)losses = tf.boolean_mask(losses, mask)# averaging the loss over the batchloss = tf.reduce_mean(losses)# building the train opoptimizer = tf.train.AdamOptimizer(learning_rate)train_op = optimizer.minimize(loss)and when iterating over the batches during training, train_op will be given to the tf.Session along with a feed_dict containing the data for the placeholders.Decoding in TensorflowLet’s have a look at the Tensorflow implementation of the Greedy Method before dealing with Beam SearchGreedy SearchWhile greedy decoding is easy to conceptualize, implementing it in Tensorflow is not straightforward, as you need to use the previous prediction and can’t use dynamic_rnn on the formula. There are basically 2 ways of approaching the problemModify our AttentionCell and AttentionState so that AttentionState also contains the embedding of the predicted word at the previous time step,123456789101112131415AttentionState = namedtuple("AttentionState", ("lstm_state", "o", "embedding"))class AttentionCell(RNNCell): def __call__(self, inputs, cell_state): lstm_state, o, embbeding = cell_state # compute h h, new_lstm_state = self.lstm_cell(tf.concat([embedding, o], axis=-1), lstm_state) # usual logic logits = ... # compute new embeddding new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32) new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids) new_state = AttentionState(new_lstm_state, new_o, new_embedding) return logits, new_stateThis technique has a few downsides. It doesn’t use inputs (which used to be the embedding of the gold token from the formula and thus we would have to call dynamic_rnn on a “fake” sequence). Also, how do you know when to stop decoding, once you’ve reached the &lt;eos&gt; token?Implement a variant of dynamic_rnn that would not run on a sequence but feed the prediction from the previous time step to the cell, while having a maximum number of decoding steps. This would involve delving deeper into Tensorflow, using tf.while_loop. That’s the method we’re going to use as it fixes all the problems of the first technique. We eventually want something that looks like12345attn_cell = AttentionCell(...)# wrap the attention cell for decodingdecoder_cell = GreedyDecoderCell(attn_cell)# call a special dynamic_decode primitivetest_outputs, _ = dynamic_decode(decoder_cell, max_length_formula+1)Much better isn’t it? Now let’s see what GreedyDecoderCell and dynamic_decode look like.Greedy Decoder CellWe first wrap the attention cell in a GreedyDecoderCell that takes care of the greedy logic for us, without having to modify the AttentionCell12345678910111213141516class DecoderOutput(collections.namedtuple("DecoderOutput", ("logits", "ids"))): passclass GreedyDecoderCell(object): def step(self, time, state, embedding, finished): # next step of attention cell logits, new_state = self._attention_cell.step(embedding, state) # get ids of words predicted and get embedding new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32) new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids) # create new state of decoder new_output = DecoderOutput(logits, new_ids) new_finished = tf.logical_or(finished, tf.equal(new_ids, self._end_token)) return (new_output, new_state, new_embedding, new_finished)Dynamic Decode primitiveWe need to implement a function dynamic_decode that will recursively call the above step function. We do this with a tf.while_loop that stops when all the hypotheses reached &lt;eos&gt; or time is greater than the max number of iterations.123456789101112131415161718192021def dynamic_decode(decoder_cell, maximum_iterations): # initialize variables (details on github) def condition(time, unused_outputs_ta, unused_state, unused_inputs, finished): return tf.logical_not(tf.reduce_all(finished)) def body(time, outputs_ta, state, inputs, finished): new_output, new_state, new_inputs, new_finished = decoder_cell.step( time, state, inputs, finished) # store the outputs in TensorArrays (details on github) new_finished = tf.logical_or(tf.greater_equal(time, maximum_iterations), new_finished) return (time + 1, outputs_ta, new_state, new_inputs, new_finished) with tf.variable_scope("rnn"): res = tf.while_loop( condition, body, loop_vars=[initial_time, initial_outputs_ta, initial_state, initial_inputs, initial_finished]) # return the final outputs (details on github)Some details using TensorArrays or nest.map_structure have been omitted for clarity but may be found on githubNotice that we place the tf.while_loop inside a scope named rnn. This is because dynamic_rnndoes the same thing and thus the weights of our LSTM are defined in that scope.Beam Search Decoder CellWe can follow the same approach as in the greedy method and use dynamic_decodeLet’s create a new wrapper for AttentionCell in the same way we did for GreedyDecoderCell. This time, the code is going to be more complicated and the following is just for intuition. Note that when selecting the top kk hypotheses from the set of candidates, we must know which “beginning” they used (=parent hypothesis).1234567891011121314151617181920212223class BeamSearchDecoderCell(object): # notice the same arguments as for GreedyDecoderCell def step(self, time, state, embedding, finished): # compute new logits logits, new_cell_state = self._attention_cell.step(embedding, state.cell_state) # compute log probs of the step (- log p(w) for all words w) # shape = [batch_size, beam_size, vocab_size] step_log_probs = tf.nn.log_softmax(new_logits) # compute scores for the (beam_size * vocabulary_size) new hypotheses log_probs = state.log_probs + step_log_probs # get top k hypotheses new_probs, indices = tf.nn.top_k(log_probs, self._beam_size) # get ids of next token along with the parent hypothesis new_ids = ... new_parents = ... # compute new embeddings, new_finished, new_cell state... new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)Look at github for the details. The main idea is that we add a beam dimension to every tensor, but when feeding it into AttentionCell we merge the beam dimension with the batch dimension. There is also some trickery involved to compute the parents and the new ids using modulos.ConclusionI hope that you learned something with this post, either about the technique or Tensorflow. While the model achieves impressive performance (at least on short formulas with roughly 85% of the LaTeX being reconstructed), it still raises some questions that I list here:How do we evaluate the performance of our model?. We can use standard metrics from Machine Translation like BLEU to evaluate how good the decoded LaTeX is compared to the reference. We can also choose to compile the predicted LaTeX sequence to get the image of the formula, and then compare this image to the orignal. As a formula is a sequence, computing the pixel-wise distance wouldn’t really make sense. A good idea is proposed by Harvard’s paper. First, slice the image vertically. Then, compare the edit distance between these slices…How to fix exposure bias? While beam search generally achieves better results, it is not perfect and still suffers from exposure bias. During training, the model is never exposed to its errors! It also suffers from Loss-Evaluation Mismatch. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…An Example of LaTeX generation - which one is the reference?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Understanding Convolutions]]></title>
      <url>%2F2018%2F01%2F18%2FUnderstanding-Convolutions%2F</url>
      <content type="text"><![CDATA[Lessons from a Dropped BallImagine we drop a ball from some height onto the ground, where it only has one dimension of motion. How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?Let’s break this down. After the first drop, it will land aa units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if it’s dropped from a different height.If we fix the result of the first drop so we know the ball went distance aa, for the ball to go a total distance cc, the distance traveled in the second drop is also fixed at bb, where $a+b=c$. So the probability of this happening is simply $f(a)⋅g(b)$.1Let’s think about this with a specific discrete example. We want the total distance $c$ to be 3. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$. The probability of this is $f(2)⋅g(1)$.However, this isn’t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any $a$ and $b$, as long as they add to 3.The probabilities are $f(1)⋅g(2)$ and $f(0)⋅g(3)$, respectively.In order to find the total likelihood of the ball reaching a total distance of $c$, we can’t consider only one possible way of reaching $c$. Instead, we consider all the possible ways of partitioning $c$ into two drops $a$ and $b$ and sum over the probability of each way.$$…\;\; f(0)\cdot g(3) ~+~ f(1)\cdot g(2) ~+~ f(2)\cdot g(1)\;\;…$$We already know that the probability for each case of $a+b=c$ is simply $f(a)⋅g(b)$. So, summing over every solution to $a+b=c$, we can denote the total likelihood as:$$\sum_{a+b=c} f(a) \cdot g(b)$$Turns out, we’re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:$$(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~$$If we substitute $b=c−a$, we get:$$(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)$$This is the standard definition2 of convolution.To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position aa with probability $f(a)$. If it lands at $a$, it has probability $g(c−a)$ of landing at a position $c$.To get the convolution, we consider all intermediate positions.Visualizing ConvolutionsThere’s a very nice trick that helps one think about convolutions more easily.First, an observation. Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$. Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(−x)$.If we know the ball lands at a position $c$ after the second drop, what is the probability that the previous position was $a$?So the probability that the previous position was $a$ is $g(−(a−c))=g(c−a)$.Now, consider the probability each intermediate position contributes to the ball finally landing at $c$. We know the probability of the first drop putting the ball into the intermediate position a is $f(a)$. We also know that the probability of it having been in $a$, if it lands at $c$ is $g(c−a)$.Summing over the $a$s, we get the convolution.The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$. This allows us to understand the convolution as a whole.For example, we can see that it peaks when the distributions align.And shrinks as the intersection between the distributions gets smaller.By using this trick in an animation, it really becomes possible to visually understand convolutions.Below, we’re able to visualize the convolution of two box functions:From WikipediaArmed with this perspective, a lot of things become more intuitive.Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.Higher Dimensional ConvolutionsConvolutions are an extremely general idea. We can also use them in a higher number of dimensions.Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.Convolution is the same as before:$$(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)$$Except, now $a$, $b$ and $c$ are vectors. To be more explicit,$$(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)$$Or in the standard definition:$$(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)$$Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”From the River Trail documentationThe kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value $\dfrac{1}{9}$ on each pixel in the box,Derived from the Gimp documentationWe can also detect edges by taking the values −1−1 and 11 on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.Derived from the Gimp documentationThe gimp documentation has many other examples.Convolutional Neural NetworksSo, how does convolution relate to convolutional neural networks?Consider a 1-dimensional convolutional layer with inputs $\{x_n\}$ and outputs $\{y_n\}$, like we discussed in the previous post:As we observed, we can describe the outputs in terms of the inputs:$$y_n = A(x_{n}, x_{n+1}, …)$$Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.Recall that a typical neuron in a neural network is described by:$$\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~…~ + b)$$Where $x_0$, $x_1$… are the inputs. The weights, $w_0$, $w_1$, … describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.3 Saying that multiple neurons are identical is the same thing as saying that the weights are the same.It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:$$y = \sigma(Wx + b)$$For example, we get:$$y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 …)$$$$y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 …)$$Each row of the matrix describes the weights connecting a neuron to its inputs.Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.Which corresponds to the equations:$$y_0 = \sigma(W_0x_0 + W_1x_1 -b)$$$$y_1 = \sigma(W_0x_1 + W_1x_2 -b)$$So while, normally, a weight matrix connects every input to every neuron with different weights:$$W = \left[\begin{array}{ccccc}W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; …\\W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; …\\W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; …\\W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; …\\… &amp; … &amp; … &amp; … &amp; …\\\end{array}\right]$$The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.$$W = \left[\begin{array}{ccccc}w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; …\\0 &amp; w_0 &amp; w_1 &amp; 0 &amp; …\\0 &amp; 0 &amp; w_0 &amp; w_1 &amp; …\\0 &amp; 0 &amp; 0 &amp; w_0 &amp; …\\… &amp; … &amp; … &amp; … &amp; …\\\end{array}\right]$$Multiplying by the above matrix is the same thing as convolving with $[…0, w_1, w_0, 0…]$. The function sliding to different positions corresponds to having neurons at those positions.What about two-dimensional convolutional layers?The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.We want the probability of the ball rolling aa units the first time and also rolling bb units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So $P(a,b)=P(a)∗P(b)=f(a)⋅g(b)$.↩The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.For example, convolution is a commutative operation. That is, $f∗g=g∗f$. Why?​$$\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)$$Convolution is also associative. That is, $(f∗g)∗h=f∗(g∗h)$. Why?​$$\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))$$↩​There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.↩]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PCA With Tensorflow]]></title>
      <url>%2F2018%2F01%2F17%2FPCA-With-Tensorflow%2F</url>
      <content type="text"><![CDATA[PCA (Principal Component Analysis) is probably the oldest trick in the book.PCA is well studied and there are numerous ways to get to the same solution, we will talk about two of them here, Eigen decomposition and Singular Value Decomposition (SVD) and then we will implement the SVD way in TensorFlow.From now on, X will be our data matrix, of shape (n, p) where n is the number of examples, and p are the dimensions.So given X, both methods will try to find, in their own way, a way to manipulate and decompose X in a manner that later on we could multiply the decomposed results to represent maximum information in less dimensions. I know I know, sounds horrible but I will spare you most of the math but keep the parts that contribute to the understanding of the method pros and cons.So Eigen decomposition and SVD are both ways to decompose matrices, lets see how they help us in PCA and how they are connected.Take a glance at the flow chart below and I will explain right after.Figure 1 PCA workflowSo why should you care about this? Well there is something very fundamental about the two procedures that tells us a lot about PCA.As you can see both methods are pure linear algebra, that basically tells us that using PCA is looking at the real data, from a different angle — this is unique to PCA since the other methods start with random representation of lower dimensional data and try to get it to behave like the high dimensional data.Some other notable things are that all operations are linear and with SVD are super-super fast.Also given the same data PCA will always give the same answer (which is not true about the other two methods).Notice how in SVD we choose the r (r is the number of dimensions we want to reduce to) left most values of Σ to lower dimensionality?Well there is something special about Σ .Σ is a diagonal matrix, there are p (number of dimensions) diagonal values (called singular values) and their magnitude indicates how significant they are to preserving the information.So we can choose to reduce dimensionality, to the number of dimensions that will preserve approx. given amount of percentage of the data and I will demonstrate that in the code (e.g. gives us the ability to reduce dimensionality with a constraint of losing a max of 15% of the data).As you will see, coding this in TensorFlow is pretty simple — what we are are going to code is a class that has fit method and a reduce method which we will supply the dimensions to.CODE (PCA)Lets see how the fit method looks like, given self.X contains the data and self.dtype=tf.float321234567891011121314def fit(self): self.graph = tf.Graph() with self.graph.as_default(): self.X = tf.placeholder(self.dtype, shape=self.data.shape) # Perform SVD singular_values, u, _ = tf.svd(self.X) # Create sigma matrix sigma = tf.diag(singular_values) with tf.Session(graph=self.graph) as session: self.u, self.singular_values, self.sigma = session.run([u, singular_values, sigma], feed_dict=&#123;self.X: self.data&#125;)So the goal of fit is to create our Σ and U for later use.We’ll start with the line tf.svd which gives us the singular values, which are the diagonal values of what was denoted as Σ in Figure 1, and the matrices U and V.Then tf.diag is TensorFlow’s way of converting a 1D vector, to a diagonal matrix, which in our case will result in Σ.At the end of the fit call we will have the singular values, Σ and U.Now lets lets implement reduce.123456789101112131415161718192021def reduce(self, n_dimensions=None, keep_info=None): if keep_info: # Normalize singular values normalized_singular_values = self.singular_values / sum(self.singular_values) # Create the aggregated ladder of kept information per dimension ladder = np.cumsum(normalized_singular_values) # Get the first index which is above the given information threshold index = next(idx for idx, value in enumerate(ladder) if value &gt;= keep_info) + 1 n_dimensions = index with self.graph.as_default(): # Cut out the relevant part from sigma sigma = tf.slice(self.sigma, [0, 0], [self.data.shape[1], n_dimensions]) # PCA pca = tf.matmul(self.u, sigma) with tf.Session(graph=self.graph) as session: return session.run(pca, feed_dict=&#123;self.X: self.data&#125;)So as you can see reduce gets either keep_info or n_dimensions (I didn’t implement the input check where only one must be supplied).If we supply n_dimensions it will simply reduce to that number, but if we supply keep_info which should be a float between 0 and 1, we will preserve that much information from the original data (0.9 — preserve 90% of the data).In the first ‘if’, we normalize and check how many singular values are needed, basically figuring out n_dimensions out of keep_info.In the graph, we just slice the Σ (sigma) matrix for as much data as we need and perform the matrix multiplication.So lets try it out on the iris dataset, which is (150, 4) dataset of 3 species of iris flowers.123456789101112from sklearn import datasetsimport matplotlib.pyplot as pltimport seaborn as snstf_pca = TF_PCA(iris_dataset.data, iris_dataset.target)tf_pca.fit()pca = tf_pca.reduce(keep_info=0.9) # Results in 2 dimensionscolor_mapping = &#123;0: sns.xkcd_rgb['bright purple'], 1: sns.xkcd_rgb['lime'], 2: sns.xkcd_rgb['ochre']&#125;colors = list(map(lambda x: color_mapping[x], tf_pca.target))plt.scatter(pca[:, 0], pca[:, 1], c=colors)Figure 2 Iris dataset PCA 2 dimensional plotNot so bad huh?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Word Embedding - Approximating the Softmax [Repost]]]></title>
      <url>%2F2018%2F01%2F16%2FWord-Embedding-Approximating-the-Softmax-Repost%2F</url>
      <content type="text"><![CDATA[This is the second post in a series on word embeddings and representation learning. In the previous post, we gave an overview of word embedding models and introduced the classic neural language model by Bengio et al. (2003), the C&amp;W model by Collobert and Weston (2008), and the word2vec model by Mikolov et al. (2013). We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) (Jean et al. [10]) and language modelling (Jozefowicz et al. [6]).In this post, we will thus focus on giving an overview of various approximations to the softmax layer that have been proposed over the last years, some of which have so far only been employed in the context of language modelling or MT. We will postpone the discussion of additional hyperparameters to the subsequent post.Let us know partially re-introduce the previous post’s notation both for consistency and to facilitate comparison as well as introduce some new notation: We assume a training corpus containing a sequence of $T$ training words $w_1,w_2,w_3, \cdots ,w_T$ that belong to a vocabulary $V$ whose size is $|V|$. Our models generally consider a context $c$ of $n$ words. We associate every word with an input embedding $v_w$ (the eponymous word embedding in the Embedding Layer) with $d$ dimensions and an output embedding $v_{w^{\prime}}$ (the representation of the word in the weight matrix of the softmax layer). We finally optimize an objective function $J_{\theta}$ with regard to our model parameters $\theta$.Recall that the softmax calculates the probability of a word $w$ given its context $c$ and can be computed using the following equation:$$p(w|c) = \frac{\exp(h^{\text{T}} v_{w^{\prime}})}{\sum_{w_i \in V} \exp(h^{\text{T}}v_{w_i}^{\prime})}$$where $h$ is the output vector of the penultimate network layer. Note that we use $c$ for the context as mentioned above and drop the index $t$ of the target word $w_t$ for simplicity. Computing the softmax is expensive as the inner product between $h$ and the output embedding of every word $w_i$ in the vocabulary $V$ needs to be computed as part of the sum in the denominator in order to obtain the normalized probability of the target word $w$ given its context $c$.In the following we will discuss different strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.Softmax-based ApproachesHierarchical SoftmaxHierarchical softmax (H-Softmax) is an approximation inspired by binary trees that was proposed by Morin and Bengio [3]. H-Softmax essentially replaces the flat softmax layer with a hierarchical layer that has the words as leaves, as can be seen in Figure 1.This allows us to decompose calculating the probability of one word into a sequence of probability calculations, which saves us from having to calculate the expensive normalization over all words. Replacing a softmax layer with H-Softmax can yield speedups for word prediction tasks of at least $50 \times $ and is thus critical for low-latency tasks such as real-time communication in Google’s new messenger app Allo.Figure 1: Hierarchical softmax (Quora)We can think of the regular softmax as a tree of depth 11, with each word in $V$ as a leaf node. Computing the softmax probability of one word then requires normalizing over the probabilities of all $|V|$ leaves. If we instead structure the softmax as a binary tree, with the words as leaf nodes, then we only need to follow the path to the leaf node of that word, without having to consider any of the other nodes.Since a balanced binary tree has a depth of $\log_2(|V|)$ we only need to evaluate at most $\log_2(|V|)$ nodes to obtain the final probability of a word. Note that this probability is already normalized, as the probabilities of all leaves in a binary tree sum to 11 and thus form a probability distribution. To informally verify this, we can reason that at a tree’s root node (Node 0) in Figure 1), the probabilities of branching decisions must sum to 11. At each subsequent node, the probability mass is then split among its children, until it eventually ends up at the leaf nodes, i.e. the words. Since no probability is lost along the way and since all words are leaves, the probabilities of all words must necessarily sum to 11 and hence the hierarchical softmax defines a normalized probability distribution over all words in $V$.To get a bit more concrete, as we go through the tree, we have to be able to calculate the probability of taking the left or right branch at every junction. For this reason, we assign a representation to every node. In contrast to the regular softmax, we thus no longer have output embeddings $v^{\prime}_w$ for every word $w$ – instead, we have embeddings $v^{\prime}_n$ for every node $n$. As we have $|V|−1$ nodes and each one possesses a unique representation, the number of parameters of H-Softmax is almost the same as for the regular softmax. We can now calculate the probability of going right (or left) at a given node $n$ given the context $c$ the following way:$$p(\text{right}|n,c) = \sigma(h^{\text{T}}v^{\prime}_n).$$This is almost the same as the computations in the regular softmax; now instead of computing the dot product between $h$ and the output word embedding $v^{\prime}_w$, we compute the dot product between $h$ and the embedding $v^{\prime}_w$ of each node in the tree; additionally, instead of computing a probability distribution over the entire vocabulary words, we output just one probability, the probability of going right at node $n$ in this case, with the sigmoid function. Conversely, the probability of turning left is simply $1−p(\text{right} | n,c)$.Figure 2: Hierarchical softmax computations (Hugo Lachorelle’s Youtube lectures)The probability of a word ww given its context cc is then simply the product of the probabilities of taking right and left turns respectively that lead to its leaf node. To illustrate this, given the context “the”, “dog”, “and”, “the”, the probability of the word “cat” in Figure 2 can be computed as the product of the probability of turning left at node 1, turning right at node 2, and turning right at node 5. Hugo Lachorelle gives a more detailed account in his excellent lecture video. Rong [7] also does a good job of explaining these concepts and also derives the derivatives of H-Softmax.Obviously, the structure of the tree is of significance. Intuitively, we should be able to achieve better performance, if we make it easier for the model to learn the binary predictors at every node, e.g. by enabling it to assign similar probabilities to similar paths. Based on this idea, Morin and Bengio use the synsets in WordNet as clusters for the tree. However, they still report inferior performance to the regular softmax. Mnih and Hinton [8] learn the tree structure with a clustering algorithm that recursively partitions the words in two clusters and allows them to achieve the same performance as the regular softmax at a fraction of the computation.Notably, we are only able to obtain this speed-up during training, when we know the word we want to predict (and consequently its path) in advance. During testing, when we need to find the most likely prediction, we still need to calculate the probability of all words, although narrowing down the choices in advance helps here.In practice, instead of using “right” and “left” in order to designate nodes, we can index every node with a bit vector that corresponds to the path it takes to reach that node. In Figure 2, if we assume a 0 bit for turning left and a 1 bit for turning right, we can thus represent the path to “cat” as 011.Recall that the path length in a balanced binary tree is $\log_2|V|$. If we set $|V|=10000$, this amounts to an average path length of about $13.3$. Analogously, we can represent every word by the bit vector of its path that is on average $13.3$ bits long. In information theory, this is referred to as an information content of $13.3$ bits per word.A note on the information content of wordsRecall that the information content $I(w)$ of a word $w$ is the negative logarithm of its probability $p(w)$:$$I(w) = − \log_2p(w)$$The entropy $H$ of all words in a corpus is then the expectation of the information content of all words in the vocabulary:$$H= \sum_{i \in V} p(w_i) I(w_i)$$We can also conceive of the entropy of a data source as the average number of bits needed to encode it. For a fair coin flip, we need $1$ bit per flip, whereas we need $0$ bits for a data source that always emits the same symbol. For a balanced binary tree, where we treat every word equally, the word entropy $H$ equals the information content $I(w)$ of every word $w$, as each word has the same probability. The average word entropy $H$ in a balanced binary tree with $|V|=10000$ thus coincides with its average path length:$$H = − \sum_{i \in V}\frac{1}{10000} \log_2 \frac{⁡1}{10000} = 13.3.$$We saw before that the structure of the tree is important. Notably, we can leverage the tree structure not only to gain better performance, but also to speed up computation: If we manage to encode more information into the tree, we can get away with taking shorter paths for less informative words. Morin and Bengio point out that leveraging word probabilities should work even better; as some words are more likely to occur than others, they can be encoded using less information. They note that the word entropy of their corpus (with $|V|=10,000$) is about $9.16$.Thus, by taking into account frequencies, we can reduce the average number of bits per word in the corpus from $13.3$ to $9.16$ in this case, which amounts to a speed-up of 31%. A Huffman tree, which is used by Mikolov et al. [1] for their hierarchical softmax, generates such a coding by assigning fewer bits to more common symbols. For instance, “the”, the most common word in the English language, would be assigned the shortest bit code in the tree, the second most frequent word would be assigned the second-shortest bit code, and so on. While we still need the same number of codes to designate all words, when we predict the words in a corpus, short codes appear now a lot more often, and we consequently need fewer bits to represent each word on average.A coding such as Huffman coding is also known as entropy encoding, as the length of each codeword is approximately proportional to the entropy of each symbol as we have observed. Shannon [5] establishes in his experiments that the lower bound on the information rate in English is between $0.6$ to $1.3$ bits per character; given an average word length of $4.5$, this amounts to $2.7$ - $5.85$ bits per word.To tie this back to language modelling (which we already talked about in the previous post): perplexity, the evaluation measure of language modelling, is $2^H$ where $H$ is the entropy. A unigram entropy of $9.16$ thus entails a still very high perplexity of $2^{9.16}=572.0$. We can render this value more tangible by observing that a model with a perplexity of $572$ is as confused by the data as if it had to choose among $572$ possibilities for each word uniformly and independently.To put this into context: The state-of-the-art language model by Jozefowicz et al. (2016) achieves a perplexity of $24.2$ per word on the 1B Word Benchmark. Such a model would thus require an average of around 4.604.60 bits to encode each word, as $2^{4.60}=24.2$, which is incredibly close to the experimental lower bounds documented by Shannon. If and how we could use such a model to construct a better hierarchical softmax layer is still left to be explored.Differentiated SoftmaxChen et al. [9] introduce a variation on the traditional softmax layer, the Differentiated Softmax (D-Softmax). D-Softmax is based on the intuition that not all words require the same number of parameters: Many occurrences of frequent words allow us to fit many parameters to them, while extremely rare words might only allow to fit a few.In order to do this, instead of the dense matrix of the regular softmax layer of size $d×|V|$ containing the output word embeddings $v^{\prime}_w \in \mathbb{R}^d$, they use a sparse matrix. They then arrange $v′w$ in blocks sorted by frequency, with the embeddings in each block being of a certain dimensionality $d_k$. The number of blocks and their embedding sizes are hyperparameters that can be tuned.Figure 3: Differentiated softmax (Chen et al. (2015))In Figure 3, embeddings in partition $A$ are of dimensionality $d_A$ (these are embeddings of frequent words, as they are allocated more parameters), while embeddings in partitions $B$ and $C$ have $d_B$ and $d_C$ dimensions respectively. Note that all areas not part of any partition, i.e. the non-shaded areas in Figure 1, are set to $0$.The output of the previous hidden layer $h$ is treated as a concatenation of features corresponding to each partition of the dimensionality of that partition, e.g. $h$ in Figure 3 is made up of partitions of size $d_A$, $d_B$, and $d_B$ respectively. Instead of computing the matrix-vector product between the entire output embedding matrix and $h$ as in the regular softmax, D-Softmax then computes the product of each partition and its corresponding section in $h$.As many words will only require comparatively few parameters, the complexity of computing the softmax is reduced, which speeds up training. In contrast to H-Softmax, this speed-up persists during testing. Chen et al. (2015) observe that D-Softmax is the fastest method when testing, while being one of the most accurate. However, as it assigns fewer parameters to rare words, D-Softmax does a worse job at modelling them.CNN-SoftmaxAnother modification to the traditional softmax layer is inspired by recent work by Kim et al. [13] who produce input word embeddings $v_w$ via a character-level CNN. Jozefowicz et al. (2016) in turn suggest to do the same thing for the output word embeddings $v^{\prime}_w$ via a character-level CNN – and refer to this as CNN-Softmax. Note that if we have a CNN at the input and at the output as in Figure 4, the CNN generating the output word embeddings $v^{\prime}_w$ is necessarily different from the CNN generating the input word embeddings $v_w$, just as the input and output word embedding matrices would be different.Figure 4: CNN-Softmax (Jozefowicz et al. (2016))While this still requires computing the regular softmax normalization, this approach drastically reduces the number of parameters of the model: Instead of storing an embedding matrix of $d \times |V|$, we now only need to keep track of the parameters of the CNN. During testing, the output word embeddings $v^{\prime}_w$ can be pre-computed, so that there is no loss in performance.However, as characters are represented in a continuous space and as the resulting model tends to learn a smooth function mapping characters to a word embedding, character-based models often find it difficult to differentiate between similarly spelled words with different meanings. To mitigate this, the authors add a correction factor that is learned per word, which significantly reduces the performance gap between regular and CNN-softmax. By adjusting the dimensionality of the correction term, the authors are able to trade-off model size versus performance.The authors also note that instead of using a CNN-softmax, the output of the previous layer hh can be fed to a character-level LSTM, which predicts the output word one character at a time. Instead of a softmax over words, a softmax outputting a probability distribution over characters would thus be used at every time step. They, however, fail to achieve competitive performance with this layer. Ling et al. [14] use a similar layer for machine translation and achieve competitive results.Sampling-based ApproachesWhile the approaches discussed so far still maintain the overall structure of the softmax, sampling-based approaches on the other hand completely do away with the softmax layer. They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. However, sampling-based approaches are only useful at training time – during inference, the full softmax still needs to be computed to obtain a normalised probability.In order to gain some intuitions about the softmax denominator’s impact on the loss, we will derive the gradient of our loss function $J_{\theta}$ w.r.t. the parameters of our model $\theta$.During training, we aim to minimize the cross-entropy loss of our model for every word $w$ in the training set. This is simply the negative logarithm of the output of our softmax. If you are unsure of this connection, have a look at Karpathy’s explanation to gain some more intuitions about the connection between softmax and cross-entropy. The loss of our model is then the following:$$J_{\theta} = − \log \frac{\exp(h^{\text{T}} v^{\prime}_w)}{\sum_{w_i \in V} \exp(h^{\text{T}} v^{\prime}_{w_i})}.$$Note that in practice $J_{\theta}$ would be the average of all negative log-probabilities over the whole corpus. To facilitate the derivation, we decompose $J_{\theta}$ into a sum as $\log \frac{x}{y} = \log x − \log y$:$$J_\theta = - \: h^\top v^{\prime}_{w} + \text{log} \sum_{w_i \in V} \text{exp}(h^\top v’_{w_i})$$For brevity and to conform with the notation of Bengio and Senécal [4, 15] (note that in the first paper, they compute the gradient of the positive logarithm), we replace the dot product $h^\top v’_{w}$ with $- \mathcal{E}(w)$. Our loss then looks like the following:$$J_\theta = \: \mathcal{E}(w) + \text{log} \sum_{w_i \in V} \text{exp}( - \mathcal{E}(w_i))$$For back-propagation, we can now compute the gradient $\nabla$i of $J_{\theta}$ w.r.t. our model’s parameters $\theta$:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \nabla_\theta \text{log} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))$$As the gradient of $\log x$ is $\dfrac{1}{x}$, an application of the chain rule yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)$$We can now move the gradient inside the sum:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \nabla_\theta \: \text{exp}(- \mathcal{E}(w_i))$$As the gradient of $\exp(x)​$ is just $\exp(x)​$, another application of the chain rule yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)) \nabla_\theta (- \mathcal{E}(w_i))$$We can rewrite this as:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} \dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta (- \mathcal{E}(w_i))$$Note that $\dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))}$ is just the softmax probability $P(w_i)$ of $w_i$ (we omit the dependence on the context cc here for brevity). Replacing it yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} P(w_i) \nabla_\theta (- \mathcal{E}(w_i))$$Finally, repositioning the negative coefficient in front of the sum yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) - \sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i)$$Bengio and Senécal (2003) note that the gradient essentially has two parts: a positive reinforcement for the target word $w$ (the first term in the above equation) and a negative reinforcement for all other words $w_i$, which is weighted by their probability (the second term). As we can see, this negative reinforcement is just the expectation $\mathbb{E}_{w_i \sim P}$ of the gradient of $\mathcal{E}$ for all words $w_i$ in $V$:$$\sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i) = \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)]$$The crux of most sampling-based approach now is to approximate this negative reinforcement in some way to make it easier to compute, since we don’t want to sum over the probabilities for all words in $V$.Importance SamplingWe can approximate the expected value $E$ of any probability distribution using the Monte Carlo method, i.e. by taking the mean of random samples of the probability distribution. If we knew the network’s distribution, i.e. $P(w)$, we could thus directly sample mm words $w_1, \cdots ,w_m$ from it and approximate the above expectation with:$$\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{m} \sum\limits^m_{i=1} \nabla_\theta \mathcal{E}(w_i)$$However, in order to sample from the probability distribution $P$, we need to compute $P$, which is just what we wanted to avoid in the first place. We therefore have find some other distribution $Q$ (we call this the proposal distribution), from which it is cheap to sample and which can be used as the basis of Monte-Carlo sampling. Preferably, $Q$ should also be similar to $P$, since we want our approximated expectation to be as accurate as possible. A straightforward choice in the case of language modelling is to simply use the unigram distribution of the training set for $Q$.This is essentially what classical Importance Sampling (IS) does: It uses Monte-Carlo sampling to approximate a target distribution $P$ via a proposal distribution $Q$. However, this still requires computing $P(w)$ for every word ww that is sampled. To avoid this, Bengio and Senécal (2003) use a biased estimator that was first proposed by Liu [16]. This estimator can be used when $P(w)$ is computed as a product, which is the case here, since every division can be transformed into a multiplication.Essentially, instead of weighting the gradient $\nabla_\theta \mathcal{E}(w_i)$ with the expensive to compute probability $P_{w_i}$, we weight it with a factor that leverages the proposal distribution $Q$. For biased IS, this factor is $\dfrac{1}{R}r(w_i)$ where $r(w) = \dfrac{\text{exp}(- \mathcal{E}(w))}{Q(w)}$ and $R = \sum^m_{j=1} r(w_j)$.Note that we use $r$ and $R$ instead of $w$ and $W$ as in Bengio and Senécal (2003, 2008) to avoid name clashes. As we can see, we still compute the numerator of the softmax, but replace the normalisation in the denominator with the proposal distribution $Q$. Our biased estimator that approximates the expectation thus looks like the following:$$\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{R} \sum\limits^m_{i=1} r(w_i) \nabla_\theta \: \mathcal{E}(w_i)$$Note that the fewer samples we use, the worse is our approximation. We additionally need to adjust our sample size during training, as the network’s distribution $P$ might diverge from the unigram distribution $Q$ during training, which leads to divergence of the model, if the sample size that is used is too small. Consequently, Bengio and Senécal introduce a measure to calculate the effective sample size in order to protect against possible divergence. Finally, the authors report a speed-up factor of $19$ over the regular softmax for this method.Adaptive Importance SamplingBengio and Senécal (2008) note that for Importance Sampling, substituting more complex distributions, e.g. bigram and trigram distributions, later in training to combat the divergence of the unigram distribution $Q$ from the model’s true distribution $P$ does not help, as n-gram distributions seem to be quite different from the distribution of trained neural language models. As an alternative, they propose an n-gram distribution that is adapted during training to follow the target distribution $P$ more closely. To this end, they interpolate a bigram distribution and a unigram distribution according to some mixture function, whose parameters they train with SGD for different frequency bins to minimize the Kullback-Leibler divergence between the distribution $Q$ and the target distribution $P$. For experiments, they report a speed-up factor of about $100$.Target SamplingJean et al. (2015) propose to use Adaptive Importance Sampling for machine translation. In order to make the method more suitable for processing on a GPU with limited memory, they limit the number of target words that need to be sampled from. They do this by partitioning the training set and including only a fixed number of sample words in every partition, which form a subset $V^{\prime}$ of the vocabulary.This essentially means that a separate proposal distribution $Q_i$ can be used for every partition ii of the training set, which assigns equal probability to all words included in the vocabulary subset $V’_i$ and zero probability to all other words.Noise Contrastive EstimationNoise Contrastive Estimation (NCE) (Gutmann and Hyvärinen) [17] is proposed by Mnih and Teh [18] as a more stable sampling method than Importance Sampling (IS), as we have seen that IS poses the risk of having the proposal distribution $Q$ diverge from the distribution $P$ that should be optimized. In contrast to the former, NCE does not try to estimate the probability of a word directly. Instead, it uses an auxiliary loss that also optimises the goal of maximizing the probability of correct words.Recall the pairwise-ranking criterion of Collobert and Weston (2008) that ranks positive windows higher than “corrupted” windows, which we discussed in the previous post. NCE does a similar thing: We train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.Figure 4: Noise Contrastive Estimation (Stephan Gouws’ PhD dissertation [24])For every word $w_i$ given its context $c_i$ of $n$ previous words $w_{t-1} , \cdots , w_{t-n+1}$ in the training set, we thus generate $k$ noise samples $w~ik$ from a noise distribution $Q$. As in IS, we can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their context $c_i$ as true ($y=1$) and all noise samples $\tilde{w}_{ik}$ as false ($y=0$).We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the positive log-likelihood as some papers do):$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \mathbb{E}_{\tilde{w}_{ik} \sim Q} [ \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]]$$Instead of computing the expectation $\mathbb{E}_{\tilde{w}_{ik} \sim Q}$ of our noise samples, which would still require summing over all words in $V$ to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \sum_{j=1}^k \dfrac{1}{k} \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]$$which reduces to:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + \: \sum_{j=1}^k \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]$$By generating $k$ noise samples for every genuine word $w_i$ given its context $c$, we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set $P_{\text{train}}$ and depend on their context $c$, whereas noise samples come from the noise distribution $Q$. We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:$$P(y, w \: | \: c) = \dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)$$Given this mixture, we can now calculate the probability that a sample came from the training $P_{\text{train}}$ distribution as a conditional probability of $y$ given $w$ and $c$:$$P(y=1\:|\:w,c)= \dfrac{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)}{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)}$$which can be simplified to:$$P(y=1\:|\:w,c)= \dfrac{P_{\text{train}}(w \: | \: c)}{P_{\text{train}}(w \: | \: c) + k \: Q(w)}$$As we don’t know $P_{\text{train}}$ (which is what we would like to calculate), we replace $P_{\text{train}}$ with the probability of our model $P$:$$P(y=1\:|\:w,c)= \dfrac{P(w \: | \: c)}{P(w \: | \: c) + k \: Q(w)}$$The probability of predicting a noise sample ($y=0$) is then simply $P(y=0\:|\:w,c) = 1 - P(y=1\:|\:w,c)$. Note that computing $P(w|c)$, i.e. the probability of a word $w$ given its context $c$ is essentially the definition of our softmax:$$P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{\sum_{w_i \in V} \text{exp}({h^\top v’_{w_i}})}$$For notational brevity and unambiguity, let us designate the denominator of the softmax with $Z(c)$, since the denominator only depends on $h$, which is generated from $c$ (assuming a fixed $V$). The softmax then looks like this:$$P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{Z(c)}$$Having to compute $P(w|c)$ means that – again – we need to compute $Z(c)$, which requires us to sum over the probabilities of all words in $V$. In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn.Mnih and Teh (2012) and Vaswani et al. [20] actually keep $Z(c)$ fixed at $1$, which they report does not affect the model’s performance. This assumption has the nice side-effect of reducing the model’s parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, Zoph et al. [19] find that even when learned, $Z(c)$ is close to $1$ and has low variance.If we thus set $Z(c)$ to $1$ in the above softmax equation, we are left with the following probability of word $w$ given a context $c$:$$P(w \: | \: c) = \text{exp}({h^\top v’_{w}})$$We can now insert this term in the above equation to compute $P(y=1|w,c)$:$$P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}$$Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{\text{exp}({h^\top v’_{w_i}}) + k \: Q(w_i)} + \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{\text{exp}({h^\top v’_{\tilde{w}_{ij}}})}{\text{exp}({h^\top v’_{\tilde{w}_{ij}}}) + k \: Q(\tilde{w}_{ij})})]$$Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples $k$, the NCE derivative tends towards the gradient of the softmax function. Mnih and Teh (2012) report that $25$ noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about $45$. For more information on NCE, Chris Dyer has published some excellent notes [21].One caveat of NCE is that as typically different noise samples are sampled for every training word ww, the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.Similarity between NCE and ISJozefowicz et al. (2016) show that NCE and IS are not only similar as both are sampling-based approaches, but are strongly connected. While NCE uses a binary classification task, they show that IS can be described similarly using a surrogate loss function: Instead of performing binary classification with a logistic loss function like NCE, IS then optimises a multi-class classification problem with a softmax and cross-entropy loss function. They observe that as IS performs multi-class classification, it may be a better choice for language modelling, as the loss leads to tied updates between the data and noise samples rather than independent updates as with NCE. Indeed, Jozefowicz et al. (2016) use IS for language modelling and obtain state-of-the-art performance (as mentioned above) on the 1B Word benchmark.Negative SamplingNegative Sampling (NEG), the objective that has been popularised by Mikolov et al. (2013), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples kk increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word $w$ comes from the empirical training distribution $P_{\text{train}}$ given a context $c$ as follows:$$P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}$$The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, $kQ(w)$ to $1$, which leaves us with:$$P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + 1}$$$kQ(w)=1$ is exactly then true, when $k=|V|$ and $Q$ is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set $kQ(w)=1$ and not to some other constant can be seen by rewriting the equation, as $P(y=1|w,c)$ can be transformed into the sigmoid function:$$P(y=1\:|\:w,c)= \dfrac{1}{1 + \text{exp}({-h^\top v’_{w}})}$$If we now insert this back into the logistic regression loss from before, we get:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{1}{1 + \text{exp}({-h^\top v’_{\tilde{w}_{ij}}})}]$$By simplifying slightly, we obtain:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (\dfrac{1}{1 + \text{exp}({h^\top v’_{\tilde{w}_{ij}}})}]$$Setting $\sigma(x) = \dfrac{1}{1 + \text{exp}({-x})}$ finally yields the NEG loss:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \sigma(h^\top v’_{w_i}) + \: \sum_{j=1}^k \:\text{log} \: \sigma(-h^\top v’_{\tilde{w}_{ij}})]$$To conform with the notation of Mikolov et al. (2013), $h$ must be replaced with $v_{w_{I}}$, v′wivwi′ with v′wOvwO′ and vw~ijvw~ij with v′wivwi′. Also, in contrast to Mikolov’s NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation Ew~ik∼QEw~ik∼Q with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at Goldberg and Levy’s notes [22].We have seen that NEG is only equivalent to NCE when $k=|V|$ and $Q$ is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.Self-NormalisationEven though the self-normalisation technique proposed by Devlin et al. 23 is not a sampling-based approach, it provides further intuitions on self-normalisation of language models, which we briefly touched upon. We previously mentioned in passing that by setting the denominator $Z(c)$ of the NCE loss to $1$, the model essentially self-normalises. This is a useful property as it allows us to skip computing the expensive normalisation in $Z(c)$.Recall that our loss function $J_{\theta}$ minimises the negative log-likelihood of all words $w_i$ in our training data:$$J_\theta = - \sum\limits_i [\text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{Z(c)}]$$We can decompose the softmax into a sum as we did before:$$J_\theta \: P(w \: | \: c) = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c)]$$If we are able to constrain our model so that it sets $Z(c)=1$ or similarly $\log Z(c)=0$, then we can avoid computing the normalisation in $Z(c)$ altogether. Devlin et al. (2014) thus propose to add a squared error penalty term to the loss function that encourages the model to keep $\log Z(c)$ as close as possible to $0$:$$J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \:(\text{log}(Z(c)) - 0)^2]$$which can be rewritten as:$$J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \: \text{log}^2 Z(c)]$$where αα allows us to trade-off between model accuracy and mean self-normalisation. By doing this, we can essentially guarantee that $Z(c)$ will be as close to $1$ as we want. At decoding time in their MT system, Devlin et al. (2014) then set the denominator of the softmax to $1$ and only use the numerator for computing P(w|c)P(w|c) together with their penalty term:$$J_\theta = - \sum\limits_i [h^\top v’_{w_i} - \alpha \: \text{log}^2 Z(c)]$$They report that self-normalisation achieves a speed-up factor of about $15$, while only resulting in a small degradation of BLEU scores compared to a regular non-self-normalizing neural language model.Infrequent NormalisationAndreas and Klein [11] suggest that it should even be sufficient to only normalise a fraction of the training examples and still obtain approximate self-normalising behaviour. They thus propose Infrequent Normalisation (IN), which down-samples the penalty term, making this a sampling-based approach.Let us first decompose the sum of the previous loss $J_{\theta}$ into two separate sums:$$J_\theta = - \sum\limits_i h^\top v’_{w_i} + \alpha \sum\limits_i \text{log}^2 Z(c)$$We can now down-sample the second term by only computing the normalisation for a subset $C$ of words $w_j$ and thus of contexts $c_j$ (as $Z(c)$ only depends on the context $c$) in the training data:$$J_\theta = - \sum\limits_i h^\top v’_{w_i} + \dfrac{\alpha}{\gamma} \sum\limits_{c_j \in C} \text{log}^2 Z(c_j)$$where $\gamma$ controls the size of the subset $C$. Andreas and Klein (2015) suggest that IF combines the strengths of NCE and self-normalisation as it does not require computing the normalisation for all training examples (which NCE avoids entirely), but like self-normalisation allows trading-off between the accuracy of the model and how well normalisation is approximated. They observe a speed-up factor of $10$ when normalising only a tenth of the training set, with no noticeable performance penalty.Other ApproachesSo far, we have focused exclusively on approximating or even entirely avoiding the computation of the softmax denominator $Z(c)$, as it is the most expensive term in the computation. We have thus not paid particular attention to $h^\top v’_{w}$, i.e. the dot-product between the penultimate layer representation hh and output word embedding $v^{\prime}_w$. Vijayanarasimhan et al. [12] propose fast locality-sensitive hashing to approximate $h^\top v^{\prime}_{w}$. However, while this technique accelerates the model at test time, during training, these speed-ups virtually vanish as embeddings must be re-indexed and the batch size increases.Which Approach to Choose?Having reviewed the most popular softmax-based and sampling-based approaches, we have shown that there are plenty of alternatives to the good ol’ softmax and almost all of them promise a significant speed-up and equivalent or at most marginally deteriorated performance. This naturally poses the question which approach is the best for a particular task.ApproachSpeed-upfactorDuringtraining?Duringtesting?Performance(small vocab)Performance(large vocab)Proportion ofparametersSoftmax1x--very goodvery poor100%Hierarchical Softmax25x (50-100x)X-very poorvery good100%Differentiated Softmax2xXXvery goodvery good&lt; 100%CNN-Softmax-X--bad - good30%Importance Sampling(19x)X---100%AdaptiveImportance Sampling(100x)X---100%Target Sampling2xX-goodbad100%Noise ContrastiveEstimation8x (45x)X-very badvery bad100%Negative Sampling(50-100x)X---100%Self-Normalisation(15x)X---100%InfrequentNormalisation6x (10x)X-very goodgood100%Table 1: Comparison of approaches to approximate the softmax for language modelling.We compare the performance of the approaches we discussed in this post for language modelling in Table 1. Speed-up factors and performance are based on the experiments by Chen et al. (2015), while we show speed-up factors reported by the authors of the original papers in brackets. The third and fourth columns indicate if the speed-up is achieved during training and testing respectively. Note that divergence of speed-up factors might be due to unoptimised implementations or the fact that the original authors might not have had access to GPUs, which benefit the regular softmax more than some of the other approaches. Performance for approaches where no comparison is available should largely be analogous to similar approaches, i.e. Self-Normalisation should achieve similar performance as Infrequent Normalisation and Importance Sampling and Adaptive Importance Sampling should achieve similar performance as Target Sampling. The performance of CNN-Softmax is as reported by Jozefowicz et al. (2016) and ranges from bad to good depending on the size of the correction. Of all approaches, only CNN-Softmax achieves a substantial reduction in parameters as the other approaches still require storing output embeddings. Differentiated Softmax reduces parameters by being able to store a sparse weight matrix.As it always is, there is no clear winner that beats all other approaches on all datasets or tasks. For language modelling, the regular softmax still achieves very good performance on small vocabulary datasets, such as the Penn Treebank, and even performs well on medium datasets, such as Gigaword, but does very poorly on large vocabulary datasets, e.g. the 1B Word Benchmark. Target Sampling, Hierarchical Softmax, and Infrequent Normalisation in turn do better with large vocabularies.Differentiated Softmax generally does well for both small and large vocabularies and is the only approach that ensures a speed-up at test time. Interestingly, Hierarchical Softmax (HS) performs very poorly with small vocabularies. However, of all methods, HS is the fastest and processes most training examples in a given time frame. While NCE performs well with large vocabularies, it is generally worse than the other methods. Negative Sampling does not work well for language modelling, but it is generally superior for learning word representations, as attested by word2vec’s success. Note that all results should be taken with a grain of salt: Chen et al. (2015) report having difficulties using Noise Contrastive Estimation in practice; Kim et al. (2016) use Hierarchical Softmax to achieve state-of-the-art with a small vocabulary, while Importance Sampling is used by the state-of-the-art language model by Jozefowicz et al. (2016) on a dataset with a large vocabulary.Finally, if you are looking to actually use the described methods, TensorFlow has implementations for a few sampling-based approaches and also explains the differences between some of them here.ConclusionThis overview of different methods to approximate the softmax attempted to provide you with intuitions that can not only be applied to improve and speed-up learning word representations, but are also relevant for language modelling and machine translation. As we have seen, most of these approaches are closely related and are driven by one uniting factor: the necessity to approximate the expensive normalisation in the denominator of the softmax. With these approaches in mind, I hope you feel now better equipped to train and understand your models and that you might even feel ready to work on learning better word representations yourself.As we have seen, learning word representations is a vast field and many factors are relevant for success. In the previous blog post, we looked at the architectures of popular models and in this blog post, we investigated more closely a key component, the softmax layer. In the next one, we will introduce GloVe, a method that relies on matrix factorisation rather than language modelling, and turn our attention to other hyperparameters that are essential for successfully learning word embeddings.As always, let me know about any mistakes I made and approaches I missed in the comments below.CitationIf you found this blog post helpful, please consider citing it as:Sebastian Ruder. On word embeddings - Part 2: Approximating the Softmax. http://ruder.io/word-embeddings-softmax, 2016.Other blog posts on word embeddingsIf you want to learn more about word embeddings, these other blog posts on word embeddings are also available:On word embeddings - Part 1On word embeddings - Part 3: The secret ingredients of word2vecUnofficial Part 4: A survey of cross-lingual embedding modelsUnofficial Part 5: Word embeddings in 2017 - Trends and future directionsTranslationsThis blog post has been translated into the following languages:ChineseReferencesMikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. Bengio, Y., &amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. http://doi.org/10.1002/j.1538-7305.1951.tb01366.x Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from http://arxiv.org/abs/1411.2738 Mnih, A., &amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from http://arxiv.org/abs/1512.04906 Jean, S., Cho, K., Memisevic, R., &amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from http://www.aclweb.org/anthology/P15-1001 Andreas, J., &amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. Vijayanarasimhan, S., Shlens, J., Monga, R., &amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from http://arxiv.org/abs/1412.7479 Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from http://arxiv.org/abs/1511.04586 Bengio, Y., &amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. http://doi.org/10.1109/TNN.2007.912312 Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. http://doi.org/10.1017/CBO9781107415324.004 Gutmann, M., &amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf Mnih, A., &amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML’12), 1751–1758. Zoph, B., Vaswani, A., May, J., &amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. Vaswani, A., Zhao, Y., Fossum, V., &amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from http://arxiv.org/abs/1410.8251 Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722 Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL’2014, 1370–1380. Gouws, S. (2016). Training neural word embeddings for transfer learning and translation (Doctoral dissertation, Stellenbosch: Stellenbosch University). Credit for the cover image goes to Stephan Gouws who included the image in his PhD dissertation and in the Tensorflow word2vec tutorial.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[DropConnect Implementation in Python and TensorFlow [Repost]]]></title>
      <url>%2F2018%2F01%2F15%2FDropConnect-Implementation-in-Python-and-TensorFlow-Repost%2F</url>
      <content type="text"><![CDATA[Source post is here.I wouldn’t expect DropConnect to appear in TensorFlow, Keras, or Theano since, as far as I know, it’s used pretty rarely and doesn’t seem as well-studied or demonstrably more useful than its cousin, Dropout. However, there don’t seem to be any implementations out there, so I’ll provide a few ways of doing so.For the briefest of refreshers, DropConnect (Wan et al.) regularizes networks like Dropout. Instead of dropping neurons, DropConnect regularizes by randomly dropping a subset of weights. A binary mask drawn from a Bernoulli distribution is applied to the original weight matrix (we’re just setting some connections to 0 with a certain probability):where a is an activation function, v is input matrix, W is weight matrix, is Hadamard (element-wise multiplication), and M is the binary mask drawn from a Bernoulli distribution with probability p.Pure Python:1234567891011121314151617import operatorimport numpy as np def mask_size_helper(args): # multiply n dimensions to get array size return reduce(operator.mul, args) def create_dropconnect_mask(dc_keep_prob, dimensions): # get binary mask of size=*dimensions from binomial dist. with dc_keep_prob = prob of drawing a 1 mask_vector = np.random.binomial(1, dc_keep_prob, mask_size_helper(dimensions)) # reshape mask to correct dimensions (we could just broadcast, but that's messy) mask_array = mask_vector.reshape(dimensions) return mask_array def dropconnect(W, dc_keep_prob): dimensions = W.shape return W * create_dropconnect_mask(dc_keep_prob, dimensions)TensorFlow (unnecessarily hard way):12345def dropconnect(W, p): M_vector = tf.multinomial(tf.log([[1-p, p]]), np.prod(W_shape)) M = tf.reshape(M_vector, W_shape) M = tf.cast(M, tf.float32) return M * WTensorFlow (easy way / recommended):12def dropconnect(W, p): return tf.nn.dropout(W, keep_prob=p) * pYes, sadly after a good amount of time spent searching for existing implementations and then creating my own, I took a look at the dropout source code and found that plain old dropout does the job so long as you remember to scale the weight matrix back down by keep_prob. After realizing that a connection weight matrix used for DropConnect is compatible input for the layer of neurons used in dropout, the only actual implementation difference between Dropout and DropConnect on TensorFlow is whether or not the weights in the masked matrix get scaled up (to preserve the expected sum).I find DropConnect interesting, not so much as a regularization method but for some novel extensions that I’d like to try. I’ve played around with using keep_prob in our new DropConnect function as a trainable variable in the graph so that, if you incorporate keep_prob into the loss function in a way that creates interesting gradients, you can punish your network for the amount of connections it makes between neurons.More interesting would be to see if we can induce modularity in the network by persisting dropped connections. That is, instead of randomly dropping an entirely new subset of connections at each training example, connections would drop and stay dropped perhaps as a result of the input data class or the connection’s contribution to deeper layers. For another post…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Short Video Title Classification]]></title>
      <url>%2F2017%2F12%2F06%2FShort-Video-Title-Classification-Problem%2F</url>
      <content type="text"><![CDATA[本文档是Github项目的流程解释文档，具体实现请移步。本项目解决的是视频短标题的多分类问题，目前涉及到33个类，所采用的算法包括TextCNN，TextRNN，TextRCNN以及HAN。目前效果最好的是TextCNN算法。项目流程大体框架如下：数据预处理数据预处理部分主要涉及到的文件有：ordered_set.pypreprocess.py大致流程如下：数据加载初始的文件包括三个：all_video_info.txt 该文件是后两个数据的合并，作为数据预处理算法输入all_video_info_month_day.txt（这里的month和day由具体数值替换）这类文件包含多个，只使用最新的，是正式的标题数据， 包括已标记的以及未标记的add_signed_video_info.txt 该文件是从其他数据库中选取的经人工标注的数据，只含有已标记的标题所有文件的格式都是一样的，每一行代表一个样本，分为四列，中间用制表符间隔。其中第一列代表视频URL；第二列为该视频类别是否经过算法修改，最开始全都为0；第三列为视频标签；第四列为视频标题。视频标签的映射表如下：在数据加载部分，我们将数据分为有标记数据以及无标记数据，有标记数据将用来训练以及测试分类器，然后用训练好的分类器预测无标记数据的标签。分类的依据首先是根据视频标签是否为0，如果为0，代表视频是未标记的。其次，已标记的数据中有些类别是会对算法造成干扰，这里我们也将其去掉。具体代码参照preprocess.py文件中的load_data方法。去除特殊符号由于视频标题中存在一些表情等特殊符号，在这个阶段将其去掉。具体代码参照preprocess.py文件中的remove_emoji方法。分词本项目采用结巴分词作为分词器。具体代码参照preprocess.py文件中的cut方法。去停止词本项目采用了data/stopword.dic文件中的停止词表，值得注意的是，句子去停止词前后去停止词后，单词的相对顺序保持不变。这里我们采用了有序集合（具体实现在ordered_set.py文件中）实现。经过这一步之后，句子中重复的非停止词将只会取一次。但是由于视频标题较短，出现重复词的概率非常小，因此不会有太大影响。具体代码参照preprocess.py文件中的remove_stop_words方法。建立词典将所有视频标题经过分词后的单词汇总起来建立一个词典，供后续句子建模使用。具体代码参照preprocess.py文件中的vocab_build方法。句子建模将分词后的视频标题中的每个词替换为其在词典中的序号，这样每个标题将会转换为由一串数组构成的向量。具体代码参照preprocess.py文件中的word2index方法。训练之前提到过，本文一共运用了四种深度学习模型，采用tensorflow框架，训练过程中涉及到的文件分为两类：模型文件， 包括textcnn.py, textrnn.py, textrcnn.py以及han.py训练文件，包括train_cnn.py, train_rnn.py, train_rcnn.py以及train_han.py模型文件定义了具体的模型，本篇文档将不会具体地讲解实现代码，只会从理论层面介绍模型。训练文件包含了算法的训练过程，由于不同算法的训练流程一致，这里单挑TextCNN讲解。下面开始介绍模型，如果只关注实现可以跳过到训练部分。模型词向量分布式表示（Distributed Representation）是Hinton 在1986年提出的，基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征 $k^n$个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在 A Neural Probabilistic Language Model 的网络结构：这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model）采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型：词的分布式表示即词向量（word embedding）是训练语言模型的一个附加产物，即图中的Matrix C。尽管Hinton 86年就提出了词的分布式表示，Bengio 03年便提出了NNLM，词向量真正火起来是google Mikolov 13年发表的两篇word2vec的文章 Efficient Estimation of Word Representations in Vector Space和Distributed Representations of Words and Phrases and their Compositionality，更重要的是发布了简单好用的word2vec工具包，在语义维度上得到了很好的验证，极大的推进了文本分析的进程。下图是文中提出的CBOW 和 Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文词预测当前词，Skip-Gram则相反。除此之外，提出了Hierarchical Softmax 和 Negative Sample两个方法，很好的解决了计算有效性，事实上这两个方法都没有严格的理论证明，有些trick之处，非常的实用主义。详细的过程不再阐述了，有兴趣深入理解word2vec的，推荐读读这篇很不错的paper: word2vec Parameter Learning Explained。额外多提一点，实际上word2vec学习的向量和真正语义还有差距，更多学到的是具备相似上下文的词，比如“good” “bad”相似度也很高，反而是文本分类任务输入有监督的语义能够学到更好的语义表示。至此，文本的表示通过词向量的表示方式，把文本数据从高纬度高稀疏的神经网络难处理的方式，变成了类似图像、语音的的连续稠密数据。深度学习算法本身有很强的数据迁移性，很多之前在图像领域很适用的深度学习算法比如CNN等也可以很好的迁移到文本领域了，深度学习文本分类模型TextCNN本篇文章的题图选用的就是14年这篇文章提出的TextCNN的结构（见下图）。卷积神经网络CNN Convolutional Neural Network最初在图像领域取得了巨大成功，CNN原理就不讲了，核心点在于可以捕捉局部相关性，具体到文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。TextCNN的详细过程原理图见下：TextCNN详细过程：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。特征：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。通道（Channels）：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。一维卷积（conv-1d）：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。Pooling层：利用CNN解决文本分类问题的文章还是很多的，比如这篇 A Convolutional Neural Network for Modelling Sentences 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：1“ 我觉得这个地方景色还不错，但是人也实在太多了 ”虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。TextRNN尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，Recurrent Neural Network for Text Classification with Multi-Task Learning文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。TextRCNN (TextCNN + TextRNN)我们参考的是中科院15年发表在AAAI上的这篇文章 Recurrent Convolutional Neural Networks for Text Classification 的结构：利用前向和后向RNN得到每个词的前向和后向上下文的表示：这样词的表示就变成词向量和前向后向上下文向量concat起来的形式了，即：最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1就可以了，不再需要更大 filter_size 获得更大视野，这里词的表示也可以只用双向RNN输出。HAN (TextRNN + Attention)CNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。Attention机制介绍：详细介绍Attention恐怕需要一小篇文章的篇幅，感兴趣的可参考14年这篇paper NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE。以机器翻译为例简单介绍下，下图中$x_t$是源语言的一个词，$y_t$是目标语言的一个词，机器翻译的任务就是给定源序列得到目标序列。翻译$y_t$的过程产生取决于上一个词 $y_{t-1}$ 和源语言的词的表示 $h_{j}$($x_{j}$) 的 bi-RNN 模型的表示），而每个词所占的权重是不一样的。比如源语言是中文 “我 / 是 / 中国人” 目标语言 “i / am / Chinese”，翻译出“Chinese”时候显然取决于“中国人”，而与“我 / 是”基本无关。下图公式, $\alpha _{ij}$则是翻译英文第$i$个词时，中文第$j$个词的贡献，也就是注意力。显然在翻译“Chinese”时，“中国人”的注意力值非常大。Attention的核心point是在翻译每个目标词（或 预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。TextRNN + Attention 模型：我们参考了这篇文章 Hierarchical Attention Networks for Document Classification，下图是模型的网络结构图，它一方面用层次化的结构保留了文档的结构，另一方面在word-level和sentence-level。标题场景只需要 word-level 这一层的 Attention 即可。加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。训练现在来详细讲解训练过程，涉及到的文件train_cnn.py, utils.py, textcnn.py注意到train_cnn.py文件最后：123if __name__ == '__main__': os.environ["CUDA_VISIBLE_DEVICES"] = '1' tf.app.run()其中第一行是指定只用一个GPU。第二行是tensorflow的一个运行框架，run会运行文件内的main方法，并且传入文件最开始设定的参数：123456789101112131415161718192021222324252627282930# configurationFLAGS = tf.app.flags.FLAGStf.app.flags.DEFINE_integer("num_classes", 33, "number of label")tf.app.flags.DEFINE_float("learning_rate", 0.01, "learning rate")tf.app.flags.DEFINE_integer( "batch_size", 64, "Batch size for training/evaluating.")tf.app.flags.DEFINE_integer( "decay_steps", 1000, "how many steps before decay learning rate.")tf.app.flags.DEFINE_float( "decay_rate", 0.95, "Rate of decay for learning rate.")tf.app.flags.DEFINE_string( "ckpt_dir", "text_cnn_title_desc_checkpoint/", "checkpoint location for the model")tf.app.flags.DEFINE_integer( "sentence_len", 15, "max sentence length")tf.app.flags.DEFINE_integer("embed_size", 64, "embedding size")tf.app.flags.DEFINE_boolean( "is_training", True, "is traning.true:tranining,false:testing/inference")tf.app.flags.DEFINE_integer( "num_epochs", 30, "number of epochs to run.")tf.app.flags.DEFINE_integer( "validate_every", 1, "Validate every validate_every epochs.")tf.app.flags.DEFINE_boolean( "use_embedding", True, "whether to use embedding or not.")tf.app.flags.DEFINE_integer( "num_filters", 256, "number of filters")tf.app.flags.DEFINE_boolean( "multi_label_flag", False, "use multi label or single label.")tf.app.flags.DEFINE_boolean( "just_train", False, "whether use all data to train or not.")第一个参数代表参数名（调用这个参数的方法：FLAGS.name），第二个参数是默认值，第三个参数是描述。值得说明的是这里有一个just_train参数，它代表是否将测试集放入训练集一起训练，一般在用模型最终确定之后。所以运行python train_cnn.py就是启动训练过程，同时可以传入参数，方法为python train_cnn.py --name value, 这里的name就是文件定义的参数名，value就是你要设定的值。如果不传入参数，则参数为默认值。下面我们来看一下main函数，流程如下：​ 数据加载这个过程主要是调用train_test_loader方法切分训练集与测试集。12X_train, X_val, y_train, y_val, n_classes = train_test_loader(FLAGS.just_train)词典加载加载数据预处理过程中建立的词典。目的是用来从预训练的词向量词典中拿出对应的词向量。1234with open('data/vocab.dic', 'rb') as f: vocab = pickle.load(f)vocab_size = len(vocab) + 1print('size of vocabulary: &#123;&#125;'.format(vocab_size))这里将词典的长度加一是为了给一个特殊词“空”加入位置，“空”的作用是填充短标题，让所有标题长度一样。Padding这个阶段就是将所有标题长度变成一致，短了就填充，长了就截断。标题长度是一个参数，可以设置。123456# padding sentences X_train = pad_sequences(X_train, maxlen=FLAGS.sentence_len, value=float(vocab_size - 1)) if not FLAGS.just_train: X_val = pad_sequences( X_val, maxlen=FLAGS.sentence_len, value=float(vocab_size - 1))模型实例化12345textcnn = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training, multi_label_flag=False)如果有之前训练到一半的模型，那我们就加载那个模型的参数，继续训练，否则进行参数初始化12345678910111213# Initialize save saver = tf.train.Saver() if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'): print('restoring variables from checkpoint') saver.restore( sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir)) else: print('Initializing Variables') sess.run(tf.global_variables_initializer()) if FLAGS.use_embedding: assign_pretrained_word_embedding( sess, vocab, vocab_size, textcnn)模型训练模型训练过程中包括两个循环，第一个是大循环，表示遍历所有训练数据多少遍。第二个是mini-batch循环，小循环走过一遍代表遍历了所有训练数据一遍。123456for epoch in range(curr_epoch, total_epochs): loss, acc, counter = .0, .0, 0 for start, end in zip( range(0, number_of_training_data, batch_size), range(batch_size, number_of_training_data, batch_size)):下面就是将训练数据喂到模型中:12feed_dict = &#123;textcnn.input_x: X_train[start:end], textcnn.dropout_keep_prob: 0.5&#125;第二个参数是模型相关的dropout参数，用于减少过拟合，范围是(0, 1]，基本不用改变。123curr_loss, curr_acc, _ = sess.run( [textcnn.loss_val, textcnn.accuracy, textcnn.train_op], feed_dict)这一步就是得到这一小部分训练数据对应的准确率以及loss。然后每经过validate_every个大循环的训练，在测试集上看看模型性能。如果性能比上一次更好，就保存模型，否则就退出，因为算法开始发散了。模型训练完毕检查性能之后，如果模型可行，下一步就将所有数据用于训练，也即运行以下命令python train_cnn.py --just_train True。这个过程会迭代固定的20个大循环。训练完毕之后，下面的预测过程将使用这个模型。预测预测涉及到的文件predict_cnn.py以及utils.py预测的流程和训练差不多，只不过不再进行多次对数据集的遍历，只进行对未标记数据进行一次遍历，拿到结果之后，由于算法输出的结果是[0, 32]这样一个序号，我们需要转化为中文标签。具体参照代码，不再赘述。引用【1】https://zhuanlan.zhihu.com/p/25928551【2】https://github.com/brightmart/text_classification]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[BFG Repo-Cleaner]]></title>
      <url>%2F2017%2F11%2F29%2FBFG-Repo-Cleaner%2F</url>
      <content type="text"><![CDATA[An alternative to git-filter-branchThe BFG is a simpler, faster alternative to git-filter-branch for cleansing bad data out of your Git repository history:Removing Crazy Big FilesRemoving Passwords, Credentials &amp; other Private dataThe git-filter-branch command is enormously powerful and can do things that the BFG can’t - but the BFG is much better for the tasks above, because:Faster : 10 - 720x fasterSimpler : The BFG isn’t particularily clever, but is focused on making the above tasks easyBeautiful : If you need to, you can use the beautiful Scala language to customise the BFG. Which has got to be better than Bash scripting at least some of the time.UsageFirst clone a fresh copy of your repo, using the --mirror flag:1$ git clone --mirror git://example.com/some-big-repo.gitThis is a bare repo, which means your normal files won’t be visible, but it is a full copy of the Git database of your repository, and at this point you should make a backup of it to ensure you don’t lose anything.Now you can run the BFG to clean your repository up:1$ java -jar bfg.jar --strip-blobs-bigger-than 100M some-big-repo.gitThe BFG will update your commits and all branches and tags so they are clean, but it doesn’t physically delete the unwanted stuff. Examine the repo to make sure your history has been updated, and then use the standard git gc command to strip out the unwanted dirty data, which Git will now recognise as surplus to requirements:12$ cd some-big-repo.git$ git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressiveFinally, once you’re happy with the updated state of your repo, push it back up (note that because your clone command used the –mirror flag, this push will update *all* refs on your remote server):1$ git pushAt this point, you’re ready for everyone to ditch their old copies of the repo and do fresh clones of the nice, new pristine data. It’s best to delete all old clones, as they’ll have dirty history that you don’t want to risk pushing back into your newly cleaned repo.ExamplesIn all these examples bfg is an alias for java -jar bfg.jar.Delete all files named ‘id_rsa’ or ‘id_dsa’ :1$ bfg --delete-files id_&#123;dsa,rsa&#125; my-repo.gitRemove all blobs bigger than 50 megabytes :1$ bfg --strip-blobs-bigger-than 50M my-repo.gitReplace all passwords listed in a file (prefix lines ‘regex:’ or ‘glob:’ if required) with ***REMOVED***wherever they occur in your repository :1$ bfg --replace-text passwords.txt my-repo.gitRemove all folders or files named ‘.git’ - a reserved filename in Git. These often become a problemwhen migrating to Git from other source-control systems like Mercurial :1$ bfg --delete-folders .git --delete-files .git --no-blob-protection my-repo.gitFor further command-line options, you can run the BFG without any arguments, which will output text like this.Your current files are sacred…The BFG treats you like a reformed alcoholic: you’ve made some mistakes in the past, but now you’ve cleaned up your act. Thus the BFG assumes that your latest commit is a good one, with none of the dirty files you want removing from your history still in it. This assumption by the BFG protects your work, and gives you peace of mind knowing that the BFG is only changing your repo history, not meddling with the current files of your project.By default the HEAD branch is protected, and while its history will be cleaned, the very latest commit (the ‘tip’) is a protected commit and its file-hierarchy won’t be changed at all.If you want to protect the tips of several branches or tags (not just HEAD), just name them for the BFG:1$ bfg --strip-biggest-blobs 100 --protect-blobs-from master,maint,next repo.gitNote:Cleaning Git repos is about completely eradicating bad stuff from history. If something ‘bad’ (like a 10MB file, when you’re specifying --strip-blobs-bigger-than 5M) is in a protected commit, it won’t be deleted - it’ll persist in your repository, even if the BFG deletes if from earlier commits. If you want the BFG to delete something you need to make sure your current commits are clean.Note that although the files in those protected commits won’t be changed, when those commits follow on from earlier dirty commits, their commit ids will change, to reflect the changed history - only the SHA-1 id of the filesystem-tree will remain the same.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Understanding LSTM Networks [repost]]]></title>
      <url>%2F2017%2F11%2F28%2FUnderstanding-LSTM-Networks-repost%2F</url>
      <content type="text"><![CDATA[source post is here.Recurrent Neural NetworksHumans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.Recurrent Neural Networks have loops.In the above diagram, a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:An unrolled recurrent neural network.This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing.Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.The Problem of Long-Term DependenciesOne of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult.Thankfully, LSTMs don’t have this problem!LSTM NetworksLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.The repeating module in a standard RNN contains a single layer.LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.The repeating module in an LSTM contains four interacting layers.Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using.In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.The Core Idea Behind LSTMsThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”An LSTM has three of these gates, to protect and control the cell state.Step-by-Step LSTM Walk ThroughThe first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ct−1Ct−1. A 11represents “completely keep this” while a 00 represents “completely get rid of this.”Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C~tC~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.It’s now time to update the old cell state, Ct−1Ct−1, into the new cell state CtCt. The previous steps already decided what to do, we just need to actually do it.We multiply the old state by ftft, forgetting the things we decided to forget earlier. Then we add it∗C~tit∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanhtanh (to push the values to be between −1−1 and 11) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.Variants on Long Short Term MemoryWhat I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014).Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. Jozefowicz, et al. (2015)tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.ConclusionEarlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al.(2015) do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models – such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer &amp; Osendorfer (2015) – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A Simple Multi-Class Classification Task: Keras and Scikit-Learn]]></title>
      <url>%2F2017%2F11%2F21%2FA-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn%2F</url>
      <content type="text"><![CDATA[1. Problem DescriptionIn this tutorial, we will use the standard machine learning problem called the iris flowers dataset.This dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling.The iris flower dataset is a well-studied problem and a such we can expect to achieve a model accuracy in the range of 95% to 97%. This provides a good target to aim for when developing our models.You can download the iris flowers dataset from the UCI Machine Learning repository and place it in your current working directory with the filename “iris.csv“.Need help with Deep Learning in Python?Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).Click to sign-up now and also get a free PDF Ebook version of the course.Start Your FREE Mini-Course Now!2. Import Classes and FunctionsWe can begin by importing all of the classes and functions we will need in this tutorial.This includes both the functionality we require from Keras, but also data loading from pandasas well as data preparation and model evaluation from scikit-learn.12345678910import numpyimport pandasfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifierfrom keras.utils import np_utilsfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import KFoldfrom sklearn.preprocessing import LabelEncoderfrom sklearn.pipeline import Pipeline3. Initialize Random Number GeneratorNext, we need to initialize the random number generator to a constant value (7).This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced.123# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)4. Load The DatasetThe dataset can be loaded directly. Because the output variable contains strings, it is easiest to load the data using pandas. We can then split the attributes (columns) into input variables (X) and output variables (Y).12345# load datasetdataframe = pandas.read_csv("iris.csv", header=None)dataset = dataframe.valuesX = dataset[:,0:4].astype(float)Y = dataset[:,4]5. Encode The Output VariableThe output variable contains three different string values.When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.This is called one hot encoding or creating dummy variables from a categorical variable.For example, in this problem three class values are Iris-setosa, Iris-versicolor and Iris-virginica. If we had the observations:123Iris-setosaIris-versicolorIris-virginicaWe can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:1234Iris-setosa, Iris-versicolor, Iris-virginica1, 0, 00, 1, 00, 0, 1We can do this by first encoding the strings consistently to integers using the scikit-learn class LabelEncoder. Then convert the vector of integers to a one hot encoding using the Keras function to_categorical().123456# encode class values as integersencoder = LabelEncoder()encoder.fit(Y)encoded_Y = encoder.transform(Y)# convert integers to dummy variables (i.e. one hot encoded)dummy_y = np_utils.to_categorical(encoded_Y)6. Define The Neural Network ModelThe Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.Below is a function that will create a baseline neural network for the iris classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.The network topology of this simple one-layer neural network can be summarized as:14 inputs -&gt; [8 hidden nodes] -&gt; 3 outputsNote that we use a “softmax” activation function in the output layer. This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.Finally, the network uses the efficient Adam gradient descent optimization algorithm with a logarithmic loss function, which is called “categorical_crossentropy” in Keras.123456789# define baseline modeldef baseline_model(): # create model model = Sequential() model.add(Dense(8, input_dim=4, activation='relu')) model.add(Dense(3, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return modelWe can now create our KerasClassifier for use in scikit-learn.We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.12estimator = KerasClassifier( build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)7. Evaluate The Model with k-Fold Cross ValidationWe can now evaluate the neural network model on our training data.The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.1kfold = KFold(n_splits=10, shuffle=True, random_state=seed)Now we can evaluate our model (estimator) on our dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).Evaluating the model only takes approximately 10 seconds and returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset.12results = cross_val_score(estimator, X, dummy_y, cv=kfold)print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))The results are summarized as both the mean and standard deviation of the model accuracy on the dataset. This is a reasonable estimation of the performance of the model on unseen data. It is also within the realm of known top results for this problem.1Accuracy: 97.33% (4.42%)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The Right Way to Oversample in Predictive Modeling]]></title>
      <url>%2F2017%2F11%2F21%2FThe-Right-Way-to-Oversample-in-Predictive-Modeling%2F</url>
      <content type="text"><![CDATA[The Source Blog: https://beckernick.github.io/oversampling-modeling/Imbalanced datasets spring up everywhere. Amazon wants to classify fake reviews, banks want to predict fraudulent credit card charges, and, as of this November, Facebook researchers are probably wondering if they can predict which news articles are fake.In each of these cases, only a small fraction of observations are actually positives. I’d guess that only 1 in 10,000 credit card charges are fraudulent, at most. Recently, oversampling the minority class observations has become a common approach to improve the quality of predictive modeling. By oversampling, models are sometimes better able to learn patterns that differentiate classes.However, this post isn’t about how this can improve modeling. Instead, it’s about how the *timing* of oversampling can affect the generalization ability of a model. Since one of the primary goals of model validation is to estimate how it will perform on unseen data, oversampling correctly is critical.Preparing the DataI’m going to try to predict whether someone will default on or a creditor will have to charge off a loan, using data from Lending Club. I’ll start by importing some modules and loading the data.123456import numpy as npimport pandas as pdfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import recall_scorefrom imblearn.over_sampling import SMOTE12loans = pd.read_csv('../lending-club-data.csv.zip')loans.iloc[0]There’s a lot of cool person and loan-specific information in this dataset. The target variable is bad_loans, which is 1 if the loan was charged off or the lessee defaulted, and 0 otherwise. I know this dataset should be imbalanced (most loans are paid off), but how imbalanced is it?1loans.bad_loans.value_counts()1230 994571 23150Name: bad_loans, dtype: int64Charge offs occurred or people defaulted on about 19% of loans, so there’s some imbalance in the data but it’s not terrible. I’ll remove a few observations with missing values for a payment-to-income ratio and then pick a handful of features to use in a random forest model.1loans = loans[~loans.payment_inc_ratio.isnull()]123model_variables = ['grade', 'home_ownership','emp_length_num', 'sub_grade','short_emp', 'dti', 'term', 'purpose', 'int_rate', 'last_delinq_none', 'last_major_derog_none', 'revol_util', 'total_rec_late_fee', 'payment_inc_ratio', 'bad_loans']loans_data_relevent = loans[model_variables]Next, I need to one-hot encode the categorical features as binary variables to use them in sklearn’s random forest classifier.1loans_relevant_enconded = pd.get_dummies(loans_data_relevent)Creating the Training and Test SetsWith the data prepared, I can create a training dataset and a test dataset. I’ll use the training dataset to build and validate the model, and treat the test dataset as the unseen new data I’d see if the model were in production.1234training_features, test_features, \training_target, test_target, = train_test_split(loans_relevant_enconded.drop(['bad_loans'], axis=1), loans_relevant_enconded['bad_loans'], test_size = .1, random_state=12)The Wrong Way to OversampleWith my training data created, I’ll upsample the bad loans using the SMOTE algorithm (Synthetic Minority Oversampling Technique). At a high level, SMOTE creates synthetic observations of the minority class (bad loans) by:Finding the k-nearest-neighbors for minority class observations (finding similar observations)Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.After upsampling to a class ratio of 1.0, I should have a balanced dataset. There’s no need (and often it’s not smart) to balance the classes, but it magnifies the issue caused by incorrectly timed oversampling.123sm = SMOTE(random_state=12, ratio = 1.0)x_res, y_res = sm.fit_sample(training_features, training_target)print training_target.value_counts(), np.bincount(y_res)1230 894931 20849Name: bad_loans, dtype: int64 [89493 89493]After upsampling, I’ll split the data into separate training and validation sets and build a random forest model to classify the bad loans.1234x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res, y_res, test_size = .1, random_state=12)123clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)clf_rf.fit(x_train_res, y_train_res)clf_rf.score(x_val_res, y_val_res)10.8846862953237610888% accuracy looks good, but I’m not just interested in accuracy. I also want to know how well I can specifically classify bad loans, since they’re more important. In statistics, this is called recall, and it’s the number of correctly predicted “positives” divided by the total number of “positives”.1recall_score(y_val_res, clf_rf.predict(x_val_res))10.8119209733229154681% recall. That means the model correctly identified 81% of the total bad loans. That’s pretty great. But is this actually representative of how the model will perform? To find out, I’ll calculate the accuracy and recall for the model on the test dataset I created initially.12print clf_rf.score(test_features, test_target)print recall_score(test_target, clf_rf.predict(test_features))120.8019737378680.129943502825Only 80% accuracy and 13% recall on the test data. That’s a huge difference!What Happened?By oversampling before splitting into training and validation datasets, I “bleed” information from the validation set into the training of the model.To see how this works, think about the case of simple oversampling (where I just duplicate observations). If I upsample a dataset before splitting it into a train and validation set, I could end up with the same observation in both datasets. As a result, a complex enough model will be able to perfectly predict the value for those observations when predicting on the validation set, inflating the accuracy and recall.When upsampling using SMOTE, I don’t create duplicate observations. However, because the SMOTE algorithm uses the nearest neighbors of observations to create synthetic data, it still bleeds information. If the nearest neighbors of minority class observations in the training set end up in the validation set, their information is partially captured by the synthetic data in the training set. Since I’m splitting the data randomly, we’d expect to have this happen. As a result, the model will be better able to predict validation set values than completely new data.The Right Way to OversampleOkay, so I’ve gone through the wrong way to oversample. Now I’ll go through the right way: oversampling on only the training data.1234x_train, x_val, y_train, y_val = \ train_test_split(training_features, training_target, test_size = .1, random_state=12)12sm = SMOTE(random_state=12, ratio = 1.0)x_train_res, y_train_res = sm.fit_sample(x_train, y_train)By oversampling only on the training data, none of the information in the validation data is being used to create synthetic observations. So these results should be generalizable. Let’s see if that’s true.12clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)clf_rf.fit(x_train_res, y_train_res)123456print 'Validation Results'print clf_rf.score(x_val, y_val)print recall_score(y_val, clf_rf.predict(x_val))print '\nTest Results'print clf_rf.score(test_features, test_target)print recall_score(test_target, clf_rf.predict(test_features))1234567Validation Results0.8003624830090.138195777351Test Results0.8032786885250.142546718818The validation results closely match the unseen test data results, which is exactly what I would want to see after putting a model into production.ConclusionOversampling is a well-known way to potentially improve models trained on imbalanced data. But it’s important to remember that oversampling incorrectly can lead to thinking a model will generalize better than it actually does. Random forests are great because the model architecture reduces overfitting (see Brieman 2001 for a proof), but poor sampling practices can still lead to false conclusions about the quality of a model.When the model is in production, it’s predicting on unseen data. The main point of model validation is to estimate how the model will generalize to new data. If the decision to put a model into production is based on how it performs on a validation set, it’s critical that oversampling is done correctly.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Note of the DenseNet (contains TensorFlow and PyTorch Implementation)]]></title>
      <url>%2F2017%2F11%2F20%2FNote-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation%2F</url>
      <content type="text"><![CDATA[The blog source:https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504.I have added the PyTorch implementation fromhttps://github.com/gpleiss/efficient_densenet_pytorch.DenseNet(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to ResNet but has some fundamental differences.With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:Error rates on various datasets(from source paper)And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:Why dense net differs from another convolution networks.What difficulties I’ve met during the implementation of DenseNet in tensorflow.If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the second chapter or check the source code on GitHub. If you not familiar with any topics but want to get some knowledge — I highly advise you CS231n Stanford classes.Compare DenseNet with other Convolution NetworksUsually, ConvNets work such way:We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:source: &lt;http://cs231n.github.io/convolutional-networks/&gt;ResNet architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.The output from Lᵢ layer will be (28, 28, 12).But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.Block of convolution layers with results concatenatedAfter a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).Transition layerNow we can again pass the image through the block with N convolutions.With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.Full DenseNet example with 3 blocks from source paperNotes about implementationIn the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:1234for block in range(required_blocks): output = build_block(output) if block != (required_blocks — 1): output = transition_layer(output)For weights initialization authors proposed use MRSA initialization(as perthis paper). In tensorflow this initialization can be easy implemented withvariance scaling initializer.In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:1batch norm -&gt; relu -&gt; conv 1x1 -&gt; dropout -&gt; batch norm -&gt; relu -&gt; conv 3x3 -&gt; dropout -&gt; output.Despite two conv filters, only last output will be concatenated to the main pool of features.Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).Where theta — some reduction values, in the range (0, 1).In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.1234567# without this line next slice assignment will silently fail!# at least in numpy 1.12.0images = images.astype(‘float64’)for i in range(channels): images[:, :, :, i] = ( (images[:, :, :, i] — self.images_means[i]) / self.images_stds[i])ConclusionDenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.Appendix: PyTorch Implementation (naive version ~100 lines)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# This implementation is based on the DenseNet-BC implementation in torchvision# https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.pyimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom collections import OrderedDictclass _DenseLayer(nn.Sequential): def __init__(self, num_input_features, growth_rate, bn_size, drop_rate): super(_DenseLayer, self).__init__() self.add_module('norm.1', nn.BatchNorm2d(num_input_features)), self.add_module('relu.1', nn.ReLU(inplace=True)), self.add_module('conv.1', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)), self.add_module('norm.2', nn.BatchNorm2d(bn_size * growth_rate)), self.add_module('relu.2', nn.ReLU(inplace=True)), self.add_module('conv.2', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)), self.drop_rate = drop_rate def forward(self, x): new_features = super(_DenseLayer, self).forward(x) if self.drop_rate &gt; 0: new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) return torch.cat([x, new_features], 1)class _Transition(nn.Sequential): def __init__(self, num_input_features, num_output_features): super(_Transition, self).__init__() self.add_module('norm', nn.BatchNorm2d(num_input_features)) self.add_module('relu', nn.ReLU(inplace=True)) self.add_module('conv', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)) self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))class _DenseBlock(nn.Sequential): def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate): super(_DenseBlock, self).__init__() for i in range(num_layers): layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate) self.add_module('denselayer%d' % (i + 1), layer)class DenseNet(nn.Module): r"""Densenet-BC model class, based on `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;` Args: growth_rate (int) - how many filters to add each layer (`k` in paper) block_config (list of 3 or 4 ints) - how many layers in each pooling block num_init_features (int) - the number of filters to learn in the first convolution layer bn_size (int) - multiplicative factor for number of bottle neck layers (i.e. bn_size * k features in the bottleneck layer) drop_rate (float) - dropout rate after each dense layer num_classes (int) - number of classification classes """ def __init__(self, growth_rate=12, block_config=(16, 16, 16), compression=0.5, num_init_features=24, bn_size=4, drop_rate=0, avgpool_size=8, num_classes=10): super(DenseNet, self).__init__() assert 0 &lt; compression &lt;= 1, 'compression of densenet should be between 0 and 1' self.avgpool_size = avgpool_size # First convolution self.features = nn.Sequential(OrderedDict([ ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)), ])) # Each denseblock num_features = num_init_features for i, num_layers in enumerate(block_config): block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate) self.features.add_module('denseblock%d' % (i + 1), block) num_features = num_features + num_layers * growth_rate if i != len(block_config) - 1: trans = _Transition(num_input_features=num_features, num_output_features=int(num_features * compression)) self.features.add_module('transition%d' % (i + 1), trans) num_features = int(num_features * compression) # Final batch norm self.features.add_module('norm_final', nn.BatchNorm2d(num_features)) # Linear layer self.classifier = nn.Linear(num_features, num_classes) def forward(self, x): features = self.features(x) out = F.relu(features, inplace=True) out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view( features.size(0), -1) out = self.classifier(out) return out]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Word2Vec: The Skip-Gram Model]]></title>
      <url>%2F2017%2F11%2F14%2FWord2Vec-The-Skip-Gram-Model%2F</url>
      <content type="text"><![CDATA[This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here I’m diving into the skip gram neural network model.The ModelThe skip-gram neural network model is actually surprisingly simple in its most basic form; I think it’s the all the little tweaks and enhancements that start to clutter the explanation.Let’s start with a high-level insight about where we’re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer–it’s a trick for learning good image features without having labeled training data.The Fake TaskSo now we need to talk about this “fake” task that we’re going to build the neural network to perform, and then we’ll come back later to how this indirectly gives us those word vectors that we are really after.We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.Model DetailsSo how is this all represented?First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.Here’s the architecture of our neural network.There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vectorrepresenting the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).The Hidden LayerFor our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).If you look at the rows of this weight matrix, these are actually what will be our word vectors!So the end goal of all of this is really just to learn this hidden layer weight matrix – the output layer we’ll just toss when we’re done!Let’s get back, though, to working through the definition of this model that we’re going to train.Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the “1”. Here’s a small example to give you a visual.This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word.The Output LayerThe 1 x 300 word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression here, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function exp(x) to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes.Here’s an illustration of calculating the output of the output neuron for the word “car”.Note that neural network does not know anything about the offset of the output word relative to the input word. It does not learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let’s say that in our training corpus, every single occurrence of the word ‘York’ is preceded by the word ‘New’. That is, at least according to the training data, there is a 100% probability that ‘New’ will be in the vicinity of ‘York’. However, if we take the 10 words in the vicinity of ‘York’ and randomly pick one of them, the probability of it being ‘New’ is not 100%; you may have picked one of the other words in the vicinity.IntuitionOk, are you ready for an exciting bit of insight into this network?If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.More Math DetailsFor each word $t=1\cdots T$, predict surrounding words in a window of “radius” $m$ of every word.Objective function:Maximize the probability of any context word given the current center word:The Skip-Gram Algorithm:Gradients]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Prioritized Experience Replay]]></title>
      <url>%2F2017%2F10%2F30%2FPrioritized-Experience-Replay%2F</url>
      <content type="text"><![CDATA[Prioritized Experience ReplayOne of the possible improvements already acknowledged in the original research2 lays in the way experience is used. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay3(PER) is one strategy that tries to leverage this fact by changing the sampling distribution.The main idea is that we prefer transitions that does not fit well to our current estimate of the Q function, because these are the transitions that we can learn most from. This reflects a simple intuition from our real world – if we encounter a situation that really differs from our expectation, we think about it over and over and change our model until it fits.We can define an error of a sample $S = (s, a, r, s’)$ as a distance between the $Q(s, a)$ and its target $T(S)$:$$error = |Q(s, a) - T(S)|$$For DDQN described above, $T$ it would be:$$T(S) = r + \gamma \tilde{Q}(s’, argmax_a Q(s’, a))$$We will store this error in the agent’s memory along with every sample and update it with each learning step.One of the possible approaches to PER is proportional prioritization. The error is first converted to priority using this formula:$$p = (error + \epsilon)^\alpha$$Epsilon $\epsilon$ is a small positive constant that ensures that no transition has zero priority.Alpha, $0 \leq \alpha \leq 1$, controls the difference between high and low error. It determines how much prioritization is used. With $\alpha$ we would get the uniform case.Priority is translated to probability of being chosen for replay. A sample $i$ has a probability of being picked during the experience replay determined by a formula:$$P_i = \frac{p_i}{\sum_k p_k}$$The algorithm is simple – during each learning step we will get a batch of samples with this probability distribution and train our network on it. We only need an effective way of storing these priorities and sampling from them.Initialization and new transitionsThe original paper says that new transitions come without a known error3, but I argue that with definition given above, we can compute it with a simple forward pass as it arrives. It’s also effective, because high value transitions are discovered immediately.Another thing is the initialization. Remember that before the learning itself, we fill the memory using random agent. But this agent does not use any neural network, so how could we estimate any error? We can use a fact that untrained neural network is likely to return a value around zero for every input. In this case the error formula becomes very simple:$$error = |Q(s, a) - T(S)| = |Q(s, a) - r - \gamma \tilde{Q}(s’, argmax_a Q(s’, a))| = | r |$$The error in this case is simply the reward experienced in a given sample. Indeed, the transitions where the agent experienced any reward intuitively seem to be very promising.Efficient implementationSo how do we store the experience and effectively sample from it?A naive implementation would be to have all samples in an array sorted according to their priorities. A random number s, $0 \leq s \leq \sum_k p_k$, would be picked and the array would be walked left to right, summing up a priority of the current element until the sum is exceeded and that element is chosen. This will select a sample with the desired probability distribution.But this would have a terrible efficiency: $O(n log n)$ for insertion and update and O$(n) $for sampling.A first important observation is that we don’t have to actually store the array sorted. Unsorted array would do just as well. Elements with higher priorities are still picked with higher probability.This releases the need for sorting, improving the algorithm to O(1) for insertion and update.But the O(n) for sampling is still too high. We can further improve our algorithm by using a different data structure. We can store our samples in unsorted sum tree – a binary tree data structure where the parent’s value is the sum of its children. The samples themselves are stored in the leaf nodes.Update of a leaf node involves propagating a value difference up the tree, obtaining O(log n). Sampling follows the thought process of the array case, but achieves O(log n). For a value s, $0 \leq s \leq \sum_k p_k$, we use the following algorithm (pseudo code):12345def retrieve(n, s): if n is leaf_node: return n if n.left.val &gt;= s: return retrieve(n.left, s) else: return retrieve(n.right, s - n.left.val)Following picture illustrates sampling from a tree with s = 24:With this effective implementation we can use large memory sizes, with hundreds of thousands or millions of samples.For known capacity, this sum tree data structure can be backed by an array. Its reference implementation containing 50 lines of code is available on GitHub.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu Server 16.04 Install Gnome and remote connect from Windows VNCViewer]]></title>
      <url>%2F2017%2F10%2F26%2FUbuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer%2F</url>
      <content type="text"><![CDATA[第一步：装 Gnome 环境首先按照如下命令安装 Gnome 环境。12345sudo apt-get updatesudo apt-get upgradesudo apt-get install gnomesudo apt-get install ubuntu-gnome-desktopsudo apt-get install gnome-shell第二步：安装 Gnome 界面管理工具安装 Gnome 桌面环境的配置工具。可以使用该工作对 Linux 进行很多配置，包括外观，工作台的数量等。后续安装的主题和图标都可以通过这个工具的 _外观（Appearance）_ 进行调整。1sudo apt-get gnome-tweak-tool第三步：安装 Dash to Dock 工具条安装 Gnome 桌面环境下的 Dock 工具条，可提供 mac os 下dock类似的使用体验。在任意浏览器打开 Gnome extensions.找到 _Dash to Dock_ 扩展栏，点开右面的 _[ON OFF]_ 选项。点击旁边的 _工具_ 选项，可进一步配置更多选项。第四步：安装 _ARC_ 扁平化主题和图标12345sudo add-apt-repository ppa:noobslab/themessudo add-apt-repository ppa:noobslab/iconssudo apt-get updatesudo apt-get install arc-themesudo apt-get install arc-icons第五步：选装 _Flat Plat_ 扁平化主题另一个扁平化主题。123curl -sL https://github.com/nana-4/Flat-Plat/archive/v20170323.tar.gz | tar xzcd Flat-Plat-20170323/sudo ./install.sh第六步：安装vncserver12345sudo apt-get install vnc4serversudo apt-get install gnome-panel gnome-settings-daemon metacity nautilus gnome-terminalcd ~/.vncmv xstartup xstartup.bakvim xstartup使用以下配置文件：1234567891011121314151617#!/bin/shexport XKL_XMODMAP_DISABLE=1unset SESSION_MANAGERunset DBUS_SESSION_BUS_ADDRESS[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresourcesxsetroot -solid greyvncconfig -iconic &amp;gnome-session &amp;gnome-panel &amp;gnome-settings-daemon &amp;metacity &amp;nautilus &amp;gnome-terminal &amp;第七步：启动 vncserver12# ：1可以更改vncserver -geometry 1920x1080 -alwaysshared :1第八步：在 Windows 上安装 VNCViewer启动只要 输入 ip:1 即可]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Union Find]]></title>
      <url>%2F2017%2F10%2F23%2FUnion-Find%2F</url>
      <content type="text"><![CDATA[五分钟搞懂并查集转自：lasersshttp://blog.csdn.net/dellaserss/article/details/7724401/并查集是我暑假从高手那里学到的一招，觉得真是太精妙的设计了。来看一个实例，杭电1232畅通工程。首先在地图上给你若干个城镇，这些城镇都可以看作点，然后告诉你哪些对城镇之间是有道路直接相连的。最后要解决的是整幅图的连通性问题。比如随意给你两个点，让你判断它们是否连通，或者问你整幅图一共有几个连通分支，也就是被分成了几个互相独立的块。像畅通工程这题，问还需要修几条路，实质就是求有几个连通分支。如果是1个连通分支，说明整幅图上的点都连起来了，不用再修路了；如果是2个连通分支，则只要再修1条路，从两个分支中各选一个点，把它们连起来，那么所有的点都是连起来的了；如果是3个连通分支，则只要再修两条路……以下面这组数据输入数据来说明4 2 1 3 4 3第一行告诉你，一共有4个点，2条路。下面两行告诉你，1、3之间有条路，4、3之间有条路。那么整幅图就被分成了1-3-4和2两部分。只要再加一条路，把2和其他任意一个点连起来，畅通工程就实现了，那么这个这组数据的输出结果就是1。好了，现在编程实现这个功能吧，城镇有几百个，路有不知道多少条，而且可能有回路。这可如何是好？我以前也不会呀，自从用了并查集之后，嗨，效果还真好！我们全家都用它！并查集由一个整数型的数组和两个函数构成。数组pre[]记录了每个点的前导点是什么，函数find是查找，join是合并。12345678910111213141516171819202122232425int pre[1000 ];int find(int x) //查找根节点&#123; int r=x; while ( pre[r ] != r ) //返回根节点 r r=pre[r ]; int i=x , j ; while( i != r ) //路径压缩 &#123; j = pre[ i ]; // 在改变上级之前用临时变量 j 记录下他的值 pre[ i ]= r ; //把上级改为根节点 i=j; &#125; return r ; &#125; //判断x y是否连通，如果已经连通，就不用管了 //如果不连通，就把它们所在的连通分支合并起, void join(int x,int y) &#123; int fx=find(x),fy=find(y); if(fx!=fy) pre[fx ]=fy; &#125;为了解释并查集的原理，我将举一个更有爱的例子。 话说江湖上散落着各式各样的大侠，有上千个之多。他们没有什么正当职业，整天背着剑在外面走来走去，碰到和自己不是一路人的，就免不了要打一架。但大侠们有一个优点就是讲义气，绝对不打自己的朋友。而且他们信奉“朋友的朋友就是我的朋友”，只要是能通过朋友关系串联起来的，不管拐了多少个弯，都认为是自己人。这样一来，江湖上就形成了一个一个的群落，通过两两之间的朋友关系串联起来。而不在同一个群落的人，无论如何都无法通过朋友关系连起来，于是就可以放心往死了打。但是两个原本互不相识的人，如何判断是否属于一个朋友圈呢？我们可以在每个朋友圈内推举出一个比较有名望的人，作为该圈子的代表人物，这样，每个圈子就可以这样命名“齐达内朋友之队”“罗纳尔多朋友之队”……两人只要互相对一下自己的队长是不是同一个人，就可以确定敌友关系了。但是还有问题啊，大侠们只知道自己直接的朋友是谁，很多人压根就不认识队长，要判断自己的队长是谁，只能漫无目的的通过朋友的朋友关系问下去：“你是不是队长？你是不是队长？”这样一来，队长面子上挂不住了，而且效率太低，还有可能陷入无限循环中。于是队长下令，重新组队。队内所有人实行分等级制度，形成树状结构，我队长就是根节点，下面分别是二级队员、三级队员。每个人只要记住自己的上级是谁就行了。遇到判断敌友的时候，只要一层层向上问，直到最高层，就可以在短时间内确定队长是谁了。由于我们关心的只是两个人之间是否连通，至于他们是如何连通的，以及每个圈子内部的结构是怎样的，甚至队长是谁，并不重要。所以我们可以放任队长随意重新组队，只要不搞错敌友关系就好了。于是，门派产生了。下面我们来看并查集的实现。 int pre[1000]; 这个数组，记录了每个大侠的上级是谁。大侠们从1或者0开始编号（依据题意而定），pre[15]=3就表示15号大侠的上级是3号大侠。如果一个人的上级就是他自己，那说明他就是掌门人了，查找到此为止。也有孤家寡人自成一派的，比如欧阳锋，那么他的上级就是他自己。每个人都只认自己的上级。比如胡青牛同学只知道自己的上级是杨左使。张无忌是谁？不认识！要想知道自己的掌门是谁，只能一级级查上去。 find这个函数就是找掌门用的，意义再清楚不过了（路径压缩算法先不论，后面再说）。1234567int find(int x) //查找我（x）的掌门&#123; int r=x; //委托 r 去找掌门 while (pre[r ]!=r) //如果r的上级不是r自己（也就是说找到的大侠他不是掌门 = =） r=pre[r] ; // r 就接着找他的上级，直到找到掌门为止。 return r ; //掌门驾到~~~&#125;再来看看join函数，就是在两个点之间连一条线，这样一来，原先它们所在的两个板块的所有点就都可以互通了。这在图上很好办，画条线就行了。但我们现在是用并查集来描述武林中的状况的，一共只有一个pre[]数组，该如何实现呢？还是举江湖的例子，假设现在武林中的形势如图所示。虚竹小和尚与周芷若MM是我非常喜欢的两个人物，他们的终极boss分别是玄慈方丈和灭绝师太，那明显就是两个阵营了。我不希望他们互相打架，就对他俩说：“你们两位拉拉勾，做好朋友吧。”他们看在我的面子上，同意了。这一同意可非同小可，整个少林和峨眉派的人就不能打架了。这么重大的变化，可如何实现呀，要改动多少地方？其实非常简单，我对玄慈方丈说：“大师，麻烦你把你的上级改为灭绝师太吧。这样一来，两派原先的所有人员的终极boss都是师太，那还打个球啊！反正我们关心的只是连通性，门派内部的结构不要紧的。”玄慈一听肯定火大了：“我靠，凭什么是我变成她手下呀，怎么不反过来？我抗议！”抗议无效，上天安排的，最大。反正谁加入谁效果是一样的，我就随手指定了一个。这段函数的意思很明白了吧？1234567void join(int x,int y) //我想让虚竹和周芷若做朋友 &#123; int fx=find(x),fy=find(y); //虚竹的老大是玄慈，芷若MM的老大是灭绝 if(fx!=fy) //玄慈和灭绝显然不是同一个人 pre[fx]=fy; //方丈只好委委屈屈地当了师太的手下啦 &#125;再来看看路径压缩算法。建立门派的过程是用join函数两个人两个人地连接起来的，谁当谁的手下完全随机。最后的树状结构会变成什么胎唇样，我也完全无法预计，一字排开也有可能。这样查找的效率就会比较低下。最理想的情况就是所有人的直接上级都是掌门，一共就两级结构，只要找一次就找到掌门了。哪怕不能完全做到，也最好尽量接近。这样就产生了路径压缩算法。设想这样一个场景：两个互不相识的大侠碰面了，想知道能不能揍。 于是赶紧打电话问自己的上级：“你是不是掌门？” 上级说：“我不是呀，我的上级是谁谁谁，你问问他看看。” 一路问下去，原来两人的最终boss都是东厂曹公公。 “哎呀呀，原来是记己人，西礼西礼，在下三营六组白面葫芦娃!” “幸会幸会，在下九营十八组仙子狗尾巴花！” 两人高高兴兴地手拉手喝酒去了。 “等等等等，两位同学请留步，还有事情没完成呢！”我叫住他俩。 “哦，对了，还要做路径压缩。”两人醒悟。白面葫芦娃打电话给他的上级六组长：“组长啊，我查过了，其习偶们的掌门是曹公公。不如偶们一起及接拜在曹公公手下吧，省得级别太低，以后查找掌门麻环。” “唔，有道理。” 白面葫芦娃接着打电话给刚才拜访过的三营长……仙子狗尾巴花也做了同样的事情。这样，查询中所有涉及到的人物都聚集在曹公公的直接领导下。每次查询都做了优化处理，所以整个门派树的层数都会维持在比较低的水平上。路径压缩的代码，看得懂很好，看不懂也没关系，直接抄上用就行了。总之它所实现的功能就是这么个意思。下面给出杭电1232畅通工程的解题代码，仅供大家参考，使用并查集来解决问题。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include&lt;iostream using namespace std; int pre[1050]; bool t[1050]; //t 用于标记独立块的根结点 int Find(int x) &#123; int r=x; while(r!=pre[r]) r=pre[r]; int i=x,j; while(pre[i]!=r) &#123; j=pre[i]; pre[i]=r; i=j; &#125; return r; &#125; void mix(int x,int y) &#123; int fx=Find(x),fy=Find(y); if(fx!=fy) &#123; pre[fy]=fx; &#125; &#125; int main() &#123; int N,M,a,b,i,j,ans; while(scanf(&quot;%d%d&quot;,&amp;N,&amp;M)&amp;&amp;N) &#123; for(i=1;i&lt;=N;i++) //初始化 pre[i]=i; for(i=1;i&lt;=M;i++) //吸收并整理数据 &#123; scanf(&quot;%d%d&quot;,&amp;a,&amp;b); mix(a,b); &#125; memset(t,0,sizeof(t)); for(i=1;i&lt;=N;i++) //标记根结点 &#123; t[Find(i)]=1; &#125; for(ans=0,i=1;i&lt;=N;i++) if(t[i]) ans++; printf(&quot;%d\n&quot;,ans-1); &#125; return 0; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[37 Reasons why your Neural Network is not working [Repost]]]></title>
      <url>%2F2017%2F07%2F28%2F37-Reasons-why-your-Neural-Network-is-not-working%2F</url>
      <content type="text"><![CDATA[The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then came the predictions: all zeroes, all background, nothing detected. “What did I do wrong?” — I asked my computer, who didn’t answer.Where do you start checking if your model is outputting garbage (for example predicting the mean of all outputs, or it has really poor accuracy)?A network might not be training for a number of reasons. Over the course of many debugging sessions, I would often find myself doing the same checks. I’ve compiled my experience along with the best ideas around in this handy list. I hope they would be of use to you, too.Table of Contents0. How to use this guide?I. Dataset issuesII. Data Normalization/Augmentation issuesIII. Implementation issuesIV. Training issues0. How to use this guide?A lot of things can go wrong. But some of them are more likely to be broken than others. I usually start with this short list as an emergency first response:Start with a simple model that is known to work for this type of data (for example, VGG for images). Use a standard loss if possible.Turn off all bells and whistles, e.g. regularization and data augmentation.If finetuning a model, double check the preprocessing, for it should be the same as the original model’s training.Verify that the input data is correct.Start with a really small dataset (2–20 samples). Overfit on it and gradually add more data.Start gradually adding back all the pieces that were omitted: augmentation/regularization, custom loss functions, try more complex models.If the steps above don’t do it, start going down the following big list and verify things one by one.I. Dataset issuesSource: http://dilbert.com/strip/2014-05-071. Check your input dataCheck if the input data you are feeding the network makes sense. For example, I’ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.2. Try random inputTry passing random numbers instead of actual data and see if the error behaves the same way. If it does, it’s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.3. Check the data loaderYour data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.4. Make sure input is connected to outputCheck if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.5. Is the relationship between input and output too random?Maybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isn’t an universal way to detect this as it depends on the nature of the data.6. Is there too much noise in the dataset?This happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldn’t learn. Check a bunch of input samples manually and see if labels seem off.The cutoff point is up for debate, as this paper got above 50% accuracy on MNIST using 50% corrupted labels.7. Shuffle the datasetIf your dataset hasn’t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.8. Reduce class imbalanceAre there a 1000 class A images for every class B image? Then you might need to balance your loss function or try other class imbalance approaches.9. Do you have enough training examples?If you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, people say you need a 1000 images per class or more.10. Make sure your batches don’t contain a single labelThis can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.11. Reduce batch sizeThis paper points out that having a very large batch can reduce the generalization ability of the model.Addition 1. Use standard dataset (e.g. mnist, cifar10)Thanks to @hengcherkeng for this one:When testing new network architecture or writing a new piece of code, use the standard datasets first, instead of your own data. This is because there are many reference results for these datasets and they are proved to be ‘solvable’. There will be no issues of label noise, train/test distribution difference , too much difficulty in dataset, etc.II. Data Normalization/Augmentation12. Standardize the featuresDid you standardize your input to have zero mean and unit variance?13. Do you have too much data augmentation?Augmentation has a regularizing effect. Too much of this combined with other forms of regularization (weight L2, dropout, etc.) can cause the net to underfit.14. Check the preprocessing of your pretrained modelIf you are using a pretrained model, make sure you are using the same normalization and preprocessing as the model was when training. For example, should an image pixel be in the range [0, 1], [-1, 1] or [0, 255]?15. Check the preprocessing for train/validation/test setCS231n points out a common pitfall:“… any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation/test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. “Also, check for different preprocessing in each sample or batch.III. Implementation issuesCredit: https://xkcd.com/1838/16. Try solving a simpler version of the problemThis will help with finding where the issue is. For example, if the target output is an object class and coordinates, try limiting the prediction to object class only.17. Look for correct loss “at chance”Again from the excellent CS231n: Initialize with small parameters, without regularization. For example, if we have 10 classes, at chance means we will get the correct class 10% of the time, and the Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.After this, try increasing the regularization strength which should increase the loss.18. Check your loss functionIf you implemented your own loss function, check it for bugs and add unit tests. Often, my loss would be slightly incorrect and hurt the performance of the network in a subtle way.19. Verify loss inputIf you are using a loss function provided by your framework, make sure you are passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss and CrossEntropyLoss as the former requires a softmax input and the latter doesn’t.20. Adjust loss weightsIf your loss is composed of several smaller loss functions, make sure their magnitude relative to each is correct. This might involve testing different combinations of loss weights.21. Monitor other metricsSometimes the loss is not the best predictor of whether your network is training properly. If you can, use other metrics like accuracy.22. Test any custom layersDid you implement any of the layers in the network yourself? Check and double-check to make sure they are working as intended.23. Check for “frozen” layers or variablesCheck if you unintentionally disabled gradient updates for some layers/variables that should be learnable.24. Increase network sizeMaybe the expressive power of your network is not enough to capture the target function. Try adding more layers or more hidden units in fully connected layers.25. Check for hidden dimension errorsIf your input looks like (k, H, W) = (64, 64, 64) it’s easy to miss errors related to wrong dimensions. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network.26. Explore Gradient checkingIf you implemented Gradient Descent by hand, gradient checking makes sure that your backpropagation works like it should. More info: 1 2 3.IV. Training issuesCredit: http://carlvondrick.com/ihog/27. Solve for a really small datasetOverfit a small subset of the data and make sure it works. For example, train with just 1 or 2 examples and see if your network can learn to differentiate these. Move on to more samples per class.28. Check weights initializationIf unsure, use Xavier or He initialization. Also, your initialization might be leading you to a bad local minimum, so try a different initialization and see if it helps.29. Change your hyperparametersMaybe you using a particularly bad set of hyperparameters. If feasible, try a grid search.30. Reduce regularizationToo much regularization can cause the network to underfit badly. Reduce regularization such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent “Practical Deep Learning for coders” course, Jeremy Howard advises getting rid of underfitting first. This means you overfit the training data sufficiently, and only then addressing overfitting.31. Give it timeMaybe your network needs more time to train before it starts making meaningful predictions. If your loss is steadily decreasing, let it train some more.32. Switch from Train to Test modeSome frameworks have layers like Batch Norm, Dropout, and other layers behave differently during training and testing. Switching to the appropriate mode might help your network to predict properly.33. Visualize the trainingMonitor the activations, weights, and updates of each layer. Make sure their magnitudes match. For example, the magnitude of the updates to the parameters (weights and biases) should be 1-e3.Consider a visualization library like Tensorboard and Crayon. In a pinch, you can also print weights/biases/activations.Be on the lookout for layer activations with a mean much larger than 0. Try Batch Norm or ELUs.Deeplearning4j points out what to expect in histograms of weights and biases:“For weights, these histograms should have an approximately Gaussian (normal) distribution, after some time. For biases, these histograms will generally start at 0, and will usually end up being approximately Gaussian(One exception to this is for LSTM). Keep an eye out for parameters that are diverging to +/- infinity. Keep an eye out for biases that become very large. This can sometimes occur in the output layer for classification if the distribution of classes is very imbalanced.”Check layer updates, they should have a Gaussian distribution.34. Try a different optimizerYour choice of optimizer shouldn’t prevent your network from training unless you have selected particularly bad hyperparameters. However, the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time. The paper which describes the algorithm you are using should specify the optimizer. If not, I tend to use Adam or plain SGD with momentum.Check this excellent post by Sebastian Ruder to learn more about gradient descent optimizers.35. Exploding / Vanishing gradientsCheck layer updates, as very large values can indicate exploding gradients. Gradient clipping may help.Check layer activations. From Deeplearning4j comes a great guideline: “A good standard deviation for the activations is on the order of 0.5 to 2.0. Significantly outside of this range may indicate vanishing or exploding activations.”36. Increase/Decrease Learning RateA low learning rate will cause your model to converge very slowly.A high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution.Play around with your current learning rate by multiplying it by 0.1 or 10.37. Overcoming NaNsGetting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from what I hear). Some approaches to fix it:Decrease the learning rate, especially if you are getting NaNs in the first 100 iterations.NaNs can arise from division by zero or natural log of zero or negative number.Russell Stewart has great pointers on how to deal with NaNs.Try evaluating your network layer by layer and see where the NaNs appear.Resources:http://cs231n.github.io/neural-networks-3/http://russellsstewart.com/notes/0.htmlhttps://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-classhttps://deeplearning4j.org/visualizationhttps://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phasehttp://book.caltech.edu/bookforum/showthread.php?t=4113https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithmOrigin post is here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PyTorch Tutorial (fork from official website)]]></title>
      <url>%2F2017%2F07%2F24%2FPyTorch-Totorial-fork-from-official-website%2F</url>
      <content type="text"><![CDATA[Tensor tutorialAutograd_tutorialNeural_networks_tutorialCIFAR10_tutorial]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning Models Implemented By Tensorflow]]></title>
      <url>%2F2017%2F07%2F24%2FMachine-Learning-Models-Implemented-By-Tensorflow%2F</url>
      <content type="text"><![CDATA[Project: https://github.com/ewanlee/finch/tree/masterThere are these algorithms in the tensorflow-models:Linear regressionLogistic regressionSVMAutoencoder (MLP based and CNN based)NMFGANConditional GANDCGANCNNRNN (for classification and for regression)Highway network (MLP based)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sklearn-based feature engineering]]></title>
      <url>%2F2017%2F07%2F20%2Fsklearn-based-feature-engineering%2F</url>
      <content type="text"><![CDATA[feature engineeringpipeline]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WGAN-GP [Repost]]]></title>
      <url>%2F2017%2F07%2F18%2FWGAN-GP%2F</url>
      <content type="text"><![CDATA[WGAN存在着训练困难、收敛速度慢等问题。WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题，认为关键在于原设计中Lipschitz限制的施加方式不对：I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way of how weight clipping works. It’s just a terrible way of enforcing a Lipschitz constraint, and better ways are out there. I feel like apologizing for being too lazy and sticking to what could be done in one line of torch code.A simple alternative (less than 5 lines of code) has been found by Montréal students. It works on quite a few settings (inc 100 layer resnets) with default hyperparameters. Arxiv coming this or next week, stay tuned.并在新论文中提出了相应的改进方案：论文：[1704.00028] Improved Training of Wasserstein GANsTensorflow实现：brianherman/improved_wgan_training (Python 3)igul222/improved_wgan_training (Python 2)首先回顾一下WGAN的关键部分——Lipschitz限制是什么。WGAN中，判别器D和生成器G的loss函数分别是$$\begin{align}L(D) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] \\L(G) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)]\end{align}$$公式1表示判别器希望尽可能拉高真样本的分数，拉低假样本的分数，公式2表示生成器希望尽可能拉高假样本的分数。Lipschitz限制则体现为，在整个样本空间$\mathcal{X}$上，要求判别器函数$D(x)$梯度的$L_p$ norm大于一个有限的常数K：$$| \nabla_x D(x) |_p \leq K, \forall x \in \mathcal{X}$$直观上解释，就是当输入的样本稍微变化后，判别器给出的分数不能发生太过剧烈的变化。在原来的论文中，这个限制具体是通过weight clipping的方式实现的：每当更新完一次判别器的参数之后，就检查判别器的所有参数的绝对值有没有超过一个阈值，比如0.01，有的话就把这些参数clip回 [-0.01, 0.01] 范围内。通过在训练过程中保证判别器的所有参数有界，就保证了判别器不能对两个略微不同的样本给出天差地别的分数值，从而间接实现了Lipschitz限制。然而weight clipping的实现方式存在两个严重问题：第一，如公式1所言，判别器loss希望尽可能拉大真假样本的分数差，然而weight clipping独立地限制每一个网络参数的取值范围，在这种情况下我们可以想象，最优的策略就是尽可能让所有参数走极端，要么取最大值（如0.01）要么取最小值（如-0.01）！为了验证这一点，作者统计了经过充分训练的判别器中所有网络参数的数值分布，发现真的集中在最大和最小两个极端上：这样带来的结果就是，判别器会非常倾向于学习一个简单的映射函数（想想看，几乎所有参数都是正负0.01，都已经可以直接视为一个二值神经网络**了，太简单了）。而作为一个深层神经网络来说，这实在是对自身强大拟合能力的巨大浪费！判别器没能充分利用自身的模型能力，经过它回传给生成器的梯度也会跟着变差。在正式介绍gradient penalty之前，我们可以先看看在它的指导下，同样充分训练判别器之后，参数的数值分布就合理得多了，判别器也能够充分利用自身模型的拟合能力：第二个问题，weight clipping会导致很容易一不小心就梯度消失或者梯度爆炸。原因是判别器是一个多层网络，如果我们把clipping threshold设得稍微小了一点，每经过一层网络，梯度就变小一点点，多层之后就会指数衰减；反之，如果设得稍微大了一点，每经过一层网络，梯度变大一点点，多层之后就会指数爆炸。只有设得不大不小，才能让生成器获得恰到好处的回传梯度，然而在实际应用中这个平衡区域可能很狭窄，就会给调参工作带来麻烦。相比之下，gradient penalty就可以让梯度在后向传播的过程中保持平稳。论文通过下图体现了这一点，其中横轴代表判别器从低到高第几层，纵轴代表梯度回传到这一层之后的尺度大小（注意纵轴是对数刻度），c是clipping threshold：说了这么多，gradient penalty到底是什么？前面提到，Lipschitz限制是要求判别器的梯度不超过K，那我们何不直接设置一个额外的loss项来体现这一点呢？比如说：$$ReLU[| \nabla_x D(x) |_p - K]$$不过，既然判别器希望尽可能拉大真假样本的分数差距，那自然是希望梯度越大越好，变化幅度越大越好，所以判别器在充分训练之后，其梯度norm其实就会是在K附近。知道了这一点，我们可以把上面的loss改成要求梯度norm离K越近越好，效果是类似的：$$[| \nabla_x D(x) |_p - K]^2$$究竟是公式4好还是公式5好，我看不出来，可能需要实验验证，反正论文作者选的是公式5。接着我们简单地把K定为1，再跟WGAN原来的判别器loss加权合并，就得到新的判别器loss：$$L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \mathcal{X}} [| \nabla_x D(x) |_p - 1]^2$$这就是所谓的gradient penalty了吗？还没完。公式6有两个问题，首先是loss函数中存在梯度项，那么优化这个loss岂不是要算梯度的梯度？一些读者可能对此存在疑惑，不过这属于实现上的问题，放到后面说。其次，3个loss项都是期望的形式，落到实现上肯定得变成采样的形式。前面两个期望的采样我们都熟悉，第一个期望是从真样本集里面采，第二个期望是从生成器的噪声输入分布采样后，再由生成器映射到样本空间。可是第三个分布要求我们在整个样本空间$\mathcal{X}$上采样，这完全不科学！由于所谓的维度灾难问题，如果要通过采样的方式在图片或自然语言这样的高维样本空间中估计期望值，所需样本量是指数级的，实际上没法做到。所以，论文作者就非常机智地提出，我们其实没必要在整个样本空间上施加Lipschitz限制，只要重点抓住生成样本集中区域、真实样本集中区域以及夹在它们中间的区域就行了。具体来说，我们先随机采一对真假样本，还有一个0-1的随机数：$$x_r \sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0, 1]$$然后在$x_r$和$x_g$的连线上随机插值采样：$$\hat{x} = \epsilon x_r + (1 - \epsilon) x_g$$把按照上述流程采样得到的$\hat{x}$所满足的分布记为$P_{\hat{x}}$, 就得到最终版本的判别器loss：$$L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \hat{x}} [| \nabla_x D(x) |_p - 1]^2$$这就是新论文所采用的gradient penalty方法，相应的新WGAN模型简称为WGAN-GP。我们可以做一个对比：weight clipping是对样本空间全局生效，但因为是间接限制判别器的梯度norm，会导致一不小心就梯度消失或者梯度爆炸；gradient penalty只对真假样本集中区域、及其中间的过渡地带生效，但因为是直接把判别器的梯度norm限制在1附近，所以梯度可控性非常强，容易调整到合适的尺度大小。论文还讲了一些使用gradient penalty时需要注意的配套事项，这里只提一点：由于我们是对每个样本独立地施加梯度惩罚，所以判别器的模型架构中不能使用Batch Normalization，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择其他normalization方法，如Layer Normalization、Weight Normalization和Instance Normalization，这些方法就不会引入样本之间的依赖。论文推荐的是Layer Normalization。实验表明，gradient penalty能够显著提高训练速度，解决了原始WGAN收敛缓慢的问题：虽然还是比不过DCGAN，但是因为WGAN不存在平衡判别器与生成器的问题，所以会比DCGAN更稳定，还是很有优势的。不过，作者凭什么能这么说？因为下面的实验体现出，在各种不同的网络架构下，其他GAN变种能不能训练好，可以说是一件相当看人品的事情，但是WGAN-GP全都能够训练好，尤其是最下面一行所对应的101层残差神经网络：剩下的实验结果中，比较厉害的是第一次成功做到了“纯粹的”的文本GAN训练！我们知道在图像上训练GAN是不需要额外的有监督信息的，但是之前就没有人能够像训练图像GAN一样训练好一个文本GAN，要么依赖于预训练一个语言模型，要么就是利用已有的有监督ground truth提供指导信息。而现在WGAN-GP终于在无需任何有监督信息的情况下，生成出下图所示的英文字符序列：它是怎么做到的呢？我认为关键之处是对样本形式的更改。以前我们一般会把文本这样的离散序列样本表示为sequence of index，但是它把文本表示成sequence of probability vector。对于生成样本来说，我们可以取网络softmax层输出的词典概率分布向量，作为序列中每一个位置的内容；而对于真实样本来说，每个probability vector实际上就蜕化为我们熟悉的onehot vector。但是如果按照传统GAN的思路来分析，这不是作死吗？一边是hard onehot vector，另一边是soft probability vector，判别器一下子就能够区分它们，生成器还怎么学习？没关系，对于WGAN来说，真假样本好不好区分并不是问题，WGAN只是拉近两个分布之间的Wasserstein距离，就算是一边是hard onehot另一边是soft probability也可以拉近，在训练过程中，概率向量中的有些项可能会慢慢变成0.8、0.9到接近1，整个向量也会接近onehot，最后我们要真正输出sequence of index形式的样本时，只需要对这些概率向量取argmax得到最大概率的index就行了。新的样本表示形式+WGAN的分布拉近能力是一个“黄金组合”，但除此之外，还有其他因素帮助论文作者跑出上图的效果，包括：文本粒度为英文字符，而非英文单词，所以字典大小才二三十，大大减小了搜索空间文本长度也才32生成器用的不是常见的LSTM架构，而是多层反卷积网络，输入一个高斯噪声向量，直接一次性转换出所有32个字符最后说回gradient penalty的实现问题。loss中本身包含梯度，优化loss就需要求梯度的梯度，这个功能并不是现在所有深度学习框架的标配功能，不过好在Tensorflow就有提供这个接口—tf.gradients。开头链接的GitHub源码中就是这么写的：12# interpolates就是随机插值采样得到的图像，gradients就是loss中的梯度惩罚项gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]完整的loss是这样实现的：12345678910111213141516171819202122232425gen_cost = -tf.reduce_mean(disc_fake)disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)alpha = tf.random_uniform( shape=[BATCH_SIZE,1], minval=0., maxval=1.)differences = fake_data - real_datainterpolates = real_data + (alpha*differences)gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))gradient_penalty = tf.reduce_mean((slopes-1.)**2)disc_cost += LAMBDA*gradient_penaltygen_train_op = tf.train.AdamOptimizer( learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(gen_cost, var_list=gen_params)disc_train_op = tf.train.AdamOptimizer( learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(disc_cost, var_list=disc_params)对于我这样的PyTorch党就非常不幸了，高阶梯度的功能还在开发，感兴趣的PyTorch党可以订阅这个GitHub的pull request：Autograd refactor，如果它被merged了话就可以在最新版中使用高阶梯度的功能实现gradient penalty了。但是除了等待我们就没有别的办法了吗？其实可能是有的，我想到了一种近似方法来实现gradient penalty，只需要把微分换成差分：$$L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x_1 \sim \hat{x}, x_2 \sim \hat{x}} [ \frac{|D(x_1) - D(x_2)|}{| x_1 - x_2 |_p} - 1]^2$$也就是说，我们仍然是在分布 $P_{\hat{x}}$ 上随机采样，但是一次采两个，然后要求它们的连线斜率要接近1，这样理论上也可以起到跟公式9一样的效果，我自己在MNIST+MLP上简单验证过有作用，PyTorch党甚至Tensorflow党都可以尝试用一下。作者：郑华滨链接：https://www.zhihu.com/question/52602529/answer/158727900来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Policy Gradient Methods]]></title>
      <url>%2F2017%2F07%2F10%2FPolicy-Gradient-Methods%2F</url>
      <content type="text"><![CDATA[Until now almost all the methods have learned the values of actions and then selected actions based on their estimated action values; their policies would not even exist without the action-value estimates. In this post we consider methods that instead learn a parameterized policy that can select actions without consulting a value function. A value function may still be used to learn the policy parameter, but is not required for action selection. We use the notation $\boldsymbol{\theta} \in \mathbb{R}^d$ for the policy’s parameter vector. Thus we write $\pi(a|s, \boldsymbol{\theta}) = \text{Pr}(A_t=a | S_t=s, \boldsymbol{\theta}_t=\boldsymbol{\theta})$ for the probability that action $a$ is taken at time $t$ given that the agent is in state $s$ at time $t$ with parameter $\boldsymbol{\theta}$. If a method uses a learned value function as well, then the value function’s weight vector is denoted $\mathbf{w} \in \mathbb{R}^m$, as in $\hat{v}(s, \mathbf{w})$.In this chapter we consider methods for learning the policy parameter based on the gradient of some performance measure $J(\boldsymbol{\theta})$ with respect to the policy parameter. These methods seek to maximize performance, so their updates approximate gradient ascent in $J$ :$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J(\boldsymbol{\theta}_t)}.$$All methods that follow this general schema we call policy gradient methods, whether or not they also learn an approximate value function. Methods that learn approximations to both policy and value functions are often called actor–critic methods, where ‘actor’ is a reference to the learned policy, and ‘critic’ refers to the learned value function, usually a state-value function.Policy ApproximationThe most preferred actions in each state are given the highest probability of being selected, for example, according to an exponential softmax distribution:$$\pi(a|s, \boldsymbol{\theta}) = \frac{\exp(h(s, a, \boldsymbol{\theta}))}{\sum_b \exp(h(s, b, \boldsymbol{\theta}))}.$$For example, they might be computed by a deep neural network, where $\boldsymbol{\theta}$ is the vector of all the connection weights of the network. Or the preferences could simply be linear in features,$$h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\top} \mathbf{x}(s, a).$$The Policy Gradient TheoremWe deﬁne the performance measure as the value of the start state of the episode. We can simplify the notation without losing any meaningful generality by assuming that every episode starts in some particular (non-random) state s0. Then, in the episodic case we deﬁne performance as$$J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}(s_0),$$where $ v_{\pi_{\boldsymbol{\theta}}}$ is the true value function for $\pi_{\boldsymbol{\theta}}$, the policy determined by $\boldsymbol{\theta}$.The policy gradient theorem is that$$\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}),$$where $\mu_{\pi}(s)$ we mentioned in earlier.REINFORCE: Monte Carlo Policy Gradient$$\begin{align}\nabla J(\boldsymbol{\theta}) &amp;= \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}) \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a q_{\pi}(S_t, a) \nabla_{\boldsymbol{\theta}} \pi(a | S_t, \boldsymbol{\theta}) \Bigg] \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a \pi(a|S_t, \boldsymbol{\theta}) q_{\pi}(S_t, a) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})} \Bigg] \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t q_{\pi}(S_t, A_t) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \text{(replacing a by the sample } A_t \sim \pi \;) \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \;\;\; \;\;\; \;\;\; \;\; (\text{because } \mathbb{E}_{\pi}[G_t|S_t,A_t]=q_{\pi}(S_t,A_t)).\end{align}$$So we get$$\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.$$This is shown explicitly in the boxed pseudocode below.Notice that $\nabla \log x = \frac{\nabla x}{x}$.REINFORCE with BaselineThe policy gradient theorem can be generalized to include a comparison of the action value to an arbitrary baseline $b(s)$:$$\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a \big(q_{\pi}(s, a) - b(s)\big) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}).$$The baseline can be any function, even a random variable, as long as it does not vary with $a$; the equation remains true, because the subtracted quantity is zero:$$\sum_a b(s) \nabla_{\boldsymbol{\theta}} \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} \sum_a \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} 1 = 0 \;\;\;\; \forall s \in \mathcal{S}.$$The update rule that we end up with is a new version of REINFORCE that includes a general baseline:$$\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \big(G_t-b(S_t) \big) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.$$One natural choice for the baseline is an estimate of the state value, $\hat{v}(S_t, \mathbf{w})$, where $\mathbf{w} \in \mathbb{R}^m$ is a weight vector learned by one of the methods presented in previous posts. A complete pseudocode algorithm for REINFROCE with baseline is given in the box (use Monte Carlo method for learning the policy parameter and state-value weights).Actor-Critic MethodsAlthough the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor-critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating a state from the estimated values of subsequent states), but only as a baseline for the state being updated. Inorder to gain these advantages in the case of policy gradient methods we use actor-critic methods with a true bootstrapping critic.One-step actor-critic methods replace the full return of REINFORCE with the one-step return (and use a learned state-value function as the baseline) as follow:$$\begin{align}\boldsymbol{\theta}_{t+1} &amp;\doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( G_{t:t+1} - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \delta_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})}.\end{align}$$The natural state-value-function learning method to pair with this is semi-gradient TD(0). Pseudocode for the complete algorithm is given in the box below. Note that it is now a fully online, incremental algorithm, with states, actions, and rewards processed as they occur and then never revisited.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout]]></title>
      <url>%2F2017%2F07%2F09%2FUsing-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout%2F</url>
      <content type="text"><![CDATA[In previous blog, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Atari 3600).Here, we will use the OpenAI gym toolkit to construct out environment. Detail implementation is as follows:1env = gym.envs.make("Breakout-v0")And then we look some demos:1234567891011121314print("Action space size: &#123;&#125;".format(env.action_space.n))# print(env.get_action_meanings())observation = env.reset()print("Observation space shape: &#123;&#125;".format(observation.shape))plt.figure()plt.imshow(env.render(mode='rgb_array'))[env.step(2) for x in range(1)]plt.figure()plt.imshow(env.render(mode='rgb_array'))env.render(close=True)For deep learning purpose, we need to crop the image to a square image:12# Check out what a cropped image looks likeplt.imshow(observation[34:-16,:,:])Not bad !Ok, now let us to use the Tensorflow to develop the DQN algorithm first.First of all, we need to reference some packages and initialize the environment.1234567891011121314151617181920%matplotlib inlineimport gymfrom gym.wrappers import Monitorimport itertoolsimport numpy as npimport osimport randomimport sysimport tensorflow as tfif "../" not in sys.path: sys.path.append("../")from lib import plottingfrom collections import deque, namedtupleenv = gym.envs.make("Breakout-v0")# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actionsVALID_ACTIONS = [0, 1, 2, 3]As mentioned above, we need to crop the image and preprocess the input before feed the raw image into the algorithm. So we define a StateProcessor class to do this.123456789101112131415161718192021222324class StateProcessor(): """ Processes a raw Atari images. Resizes it and converts it to grayscale. """ def __init__(self): # Build the Tensorflow graph with tf.variable_scope("state_processor"): self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8) self.output = tf.image.rgb_to_grayscale(self.input_state) self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160) self.output = tf.image.resize_images( self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) self.output = tf.squeeze(self.output) def process(self, sess, state): """ Args: sess: A Tensorflow session object state: A [210, 160, 3] Atari RGB State Returns: A processed [84, 84, 1] state representing grayscale values. """ return sess.run(self.output, &#123; self.input_state: state &#125;)We first convert the image to gray image and then resize it to 84 by 84 (the size DQN paper used). Then, we construct the neural network to estimate the value function. The structure of the network as the same as the DQN paper.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102class Estimator(): """Q-Value Estimator neural network. This network is used for both the Q-Network and the Target Network. """ def __init__(self, scope="estimator", summaries_dir=None): self.scope = scope # Writes Tensorboard summaries to disk self.summary_writer = None with tf.variable_scope(scope): # Build the graph self._build_model() if summaries_dir: summary_dir = os.path.join(summaries_dir, "summaries_&#123;&#125;".format(scope)) if not os.path.exists(summary_dir): os.makedirs(summary_dir) self.summary_writer = tf.summary.FileWriter(summary_dir) def _build_model(self): """ Builds the Tensorflow graph. """ # Placeholders for our input # Our input are 4 RGB frames of shape 160, 160 each self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name="X") # The TD target value self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name="y") # Integer id of which action was selected self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name="actions") X = tf.to_float(self.X_pl) / 255.0 batch_size = tf.shape(self.X_pl)[0] # Three convolutional layers conv1 = tf.contrib.layers.conv2d( X, 32, 8, 4, activation_fn=tf.nn.relu) conv2 = tf.contrib.layers.conv2d( conv1, 64, 4, 2, activation_fn=tf.nn.relu) conv3 = tf.contrib.layers.conv2d( conv2, 64, 3, 1, activation_fn=tf.nn.relu) # Fully connected layers flattened = tf.contrib.layers.flatten(conv3) fc1 = tf.contrib.layers.fully_connected(flattened, 512) self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS)) # Get the predictions for the chosen actions only gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices) # Calcualte the loss self.losses = tf.squared_difference(self.y_pl, self.action_predictions) self.loss = tf.reduce_mean(self.losses) # Optimizer Parameters from original paper self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6) self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step()) # Summaries for Tensorboard self.summaries = tf.summary.merge([ tf.summary.scalar("loss", self.loss), tf.summary.histogram("loss_hist", self.losses), tf.summary.histogram("q_values_hist", self.predictions), tf.summary.scalar("max_q_value", tf.reduce_max(self.predictions)) ]) def predict(self, sess, s): """ Predicts action values. Args: sess: Tensorflow session s: State input of shape [batch_size, 4, 160, 160, 3] Returns: Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated action values. """ return sess.run(self.predictions, &#123; self.X_pl: s &#125;) def update(self, sess, s, a, y): """ Updates the estimator towards the given targets. Args: sess: Tensorflow session object s: State input of shape [batch_size, 4, 160, 160, 3] a: Chosen actions of shape [batch_size] y: Targets of shape [batch_size] Returns: The calculated loss on the batch. """ feed_dict = &#123; self.X_pl: s, self.y_pl: y, self.actions_pl: a &#125; summaries, global_step, _, loss = sess.run( [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss], feed_dict) if self.summary_writer: self.summary_writer.add_summary(summaries, global_step) return lossAs mentioned in DQN paper, there are two network that share the same parameters in DQN algorithm. We need to copy the parameters to the target network on each $t$ steps.1234567891011121314151617181920def copy_model_parameters(sess, estimator1, estimator2): """ Copies the model parameters of one estimator to another. Args: sess: Tensorflow session instance estimator1: Estimator to copy the paramters from estimator2: Estimator to copy the parameters to """ e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)] e1_params = sorted(e1_params, key=lambda v: v.name) e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)] e2_params = sorted(e2_params, key=lambda v: v.name) update_ops = [] for e1_v, e2_v in zip(e1_params, e2_params): op = e2_v.assign(e1_v) update_ops.append(op) sess.run(update_ops)We also need a policy to take an action.1234567891011121314151617181920def make_epsilon_greedy_policy(estimator, nA): """ Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon. Args: estimator: An estimator that returns q values for a given state nA: Number of actions in the environment. Returns: A function that takes the (sess, observation, epsilon) as an argument and returns the probabilities for each action in the form of a numpy array of length nA. """ def policy_fn(sess, observation, epsilon): A = np.ones(nA, dtype=float) * epsilon / nA q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0] best_action = np.argmax(q_values) A[best_action] += (1.0 - epsilon) return A return policy_fnNow let us to develop the DQN algorithm (we skip the details here because we explained it earlier).123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187def deep_q_learning(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size=500000, replay_memory_init_size=50000, update_target_estimator_every=10000, discount_factor=0.99, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay_steps=500000, batch_size=32, record_video_every=50): """ Q-Learning algorithm for fff-policy TD control using Function Approximation. Finds the optimal greedy policy while following an epsilon-greedy policy. Args: sess: Tensorflow Session object env: OpenAI environment q_estimator: Estimator object used for the q values target_estimator: Estimator object used for the targets state_processor: A StateProcessor object num_episodes: Number of episodes to run for experiment_dir: Directory to save Tensorflow summaries in replay_memory_size: Size of the replay memory replay_memory_init_size: Number of random experiences to sampel when initializing the reply memory. update_target_estimator_every: Copy parameters from the Q estimator to the target estimator every N steps discount_factor: Lambda time discount factor epsilon_start: Chance to sample a random action when taking an action. Epsilon is decayed over time and this is the start value epsilon_end: The final minimum value of epsilon after decaying is done epsilon_decay_steps: Number of steps to decay epsilon over batch_size: Size of batches to sample from the replay memory record_video_every: Record a video every N episodes Returns: An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"]) # The replay memory replay_memory = [] # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) # Create directories for checkpoints and summaries checkpoint_dir = os.path.join(experiment_dir, "checkpoints") checkpoint_path = os.path.join(checkpoint_dir, "model") monitor_path = os.path.join(experiment_dir, "monitor") if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) if not os.path.exists(monitor_path): os.makedirs(monitor_path) saver = tf.train.Saver() # Load a previous checkpoint if we find one latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir) if latest_checkpoint: print("Loading model checkpoint &#123;&#125;...\n".format(latest_checkpoint)) saver.restore(sess, latest_checkpoint) # Get the current time step total_t = sess.run(tf.contrib.framework.get_global_step()) # The epsilon decay schedule epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps) # The policy we're following policy = make_epsilon_greedy_policy( q_estimator, len(VALID_ACTIONS)) # Populate the replay memory with initial experience print("Populating replay memory...") state = env.reset() state = state_processor.process(sess, state) state = np.stack([state] * 4, axis=2) for i in range(replay_memory_init_size): action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)]) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) next_state, reward, done, _ = env.step(VALID_ACTIONS[action]) next_state = state_processor.process(sess, next_state) next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2) replay_memory.append(Transition(state, action, reward, next_state, done)) if done: state = env.reset() state = state_processor.process(sess, state) state = np.stack([state] * 4, axis=2) else: state = next_state # Record videos # Add env Monitor wrapper env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True) for i_episode in range(num_episodes): # Save the current checkpoint saver.save(tf.get_default_session(), checkpoint_path) # Reset the environment state = env.reset() state = state_processor.process(sess, state) state = np.stack([state] * 4, axis=2) loss = None # One step in the environment for t in itertools.count(): # Epsilon for this time step epsilon = epsilons[min(total_t, epsilon_decay_steps-1)] # Add epsilon to Tensorboard episode_summary = tf.Summary() episode_summary.value.add(simple_value=epsilon, tag="epsilon") q_estimator.summary_writer.add_summary(episode_summary, total_t) # Maybe update the target estimator if total_t % update_target_estimator_every == 0: copy_model_parameters(sess, q_estimator, target_estimator) print("\nCopied model parameters to target network.") # Print out which step we're on, useful for debugging. print("\rStep &#123;&#125; (&#123;&#125;) @ Episode &#123;&#125;/&#123;&#125;, loss: &#123;&#125;".format( t, total_t, i_episode + 1, num_episodes, loss), end="") sys.stdout.flush() # Take a step action_probs = policy(sess, state, epsilon) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) next_state, reward, done, _ = env.step(VALID_ACTIONS[action]) next_state = state_processor.process(sess, next_state) next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2) # If our replay memory is full, pop the first element if len(replay_memory) == replay_memory_size: replay_memory.pop(0) # Save transition to replay memory replay_memory.append(Transition(state, action, reward, next_state, done)) # Update statistics stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t # Sample a minibatch from the replay memory samples = random.sample(replay_memory, batch_size) states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples)) # Calculate q values and targets q_values_next = target_estimator.predict(sess, next_states_batch) targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1) # Perform gradient descent update states_batch = np.array(states_batch) loss = q_estimator.update(sess, states_batch, action_batch, targets_batch) if done: break state = next_state total_t += 1 # Add summaries to tensorboard episode_summary = tf.Summary() episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name="episode_reward", tag="episode_reward") episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name="episode_length", tag="episode_length") q_estimator.summary_writer.add_summary(episode_summary, total_t) q_estimator.summary_writer.flush() yield total_t, plotting.EpisodeStats( episode_lengths=stats.episode_lengths[:i_episode+1], episode_rewards=stats.episode_rewards[:i_episode+1]) return statsFinally, run it.1234567891011121314151617181920212223242526272829303132333435tf.reset_default_graph()# Where we save our checkpoints and graphsexperiment_dir = os.path.abspath("./experiments/&#123;&#125;".format(env.spec.id))# Create a glboal step variableglobal_step = tf.Variable(0, name='global_step', trainable=False) # Create estimatorsq_estimator = Estimator(scope="q", summaries_dir=experiment_dir)target_estimator = Estimator(scope="target_q")# State processorstate_processor = StateProcessor()# Run it!with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for t, stats in deep_q_learning(sess, env, q_estimator=q_estimator, target_estimator=target_estimator, state_processor=state_processor, experiment_dir=experiment_dir, num_episodes=10000, replay_memory_size=500000, replay_memory_init_size=50000, update_target_estimator_every=10000, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay_steps=500000, discount_factor=0.99, batch_size=32): print("\nEpisode Reward: &#123;&#125;".format(stats.episode_rewards[-1]))Next, we will develop the Double-DQN algorithm. This algorithm only has a little changes.In DQN q_learning method,123# Calculate q values and targetsq_values_next = target_estimator.predict(sess, next_states_batch)targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)we just change these codes to,123456# Calculate q values and targets# This is where Double Q-Learning comes in!q_values_next = q_estimator.predict(sess, next_states_batch)best_actions = np.argmax(q_values_next, axis=1)q_values_next_target = target_estimator.predict(sess, next_states_batch)targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), best_actions]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Summary of Papers]]></title>
      <url>%2F2017%2F07%2F08%2FSummary-of-the-papers%2F</url>
      <content type="text"><![CDATA[CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep LearningDataSetChestX-ray14 dataset Wang et al. 2017ArchDenseNet (121 layers)Batch NormalizationThe weights of the network are randomly initializedTrained end-to-end using Adam with standard pa- rameters (β1 = 0.9 and β2 = 0.999)Batch size = 16Oversample the minority (positive) class Buda et al., 2017Use an initial learning rate of 0.01 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.Model InterpretationTo interpret the network predictions, we also produce heatmaps to visualize the areas of the image most in- dicative of the disease using class activation mappings (CAMs) Zhou et al., 2016ResultsRelated WordDiabetic retinopathy detection Gulshan et al., 2016Skin cancer classification Esteva et al., 2017Arrhythmia detection Rajpurkar et al., 2017Hemorrhage identificatio Grewal et al., 2017Pulmonary tuberculosis classification Lakhani &amp; Sun- daram, 2017Lung nodule detection Huang et al., 2017, Islam et al. 2017studied the performance of various convolutional architectures on different ab- normalities using the publicly available OpenI dataset Demner-Fushman et al., 2015Yao et al. 2017 exploited statistical dependencies between la- bels in order make more accurate predictions, outper- forming Wang et al. 2017 on 13 of 14 classes.###Maximum Entropy Deep Inverse Reinforcement Learning [2016]Maximum Entropy Inverse Reinforcement Learning Brian [2008][Repost]this is a summary of Ziebart et al’s 2008 paper: Maximum Entropy Inverse Reinforcement Learning **. I found this is a good way for me to distill the essence of the paper. Since the Maxent algorithm is mostly cited by the later papers in IRL/imitation learning, I would like to look into details of this algorithm. Code is available at github. Hope this post can also help others who are interested in IRL.MotivationsThe paper points out the limitations of the previous algorithms including LPIRL (Ng &amp; Russell 2000), structured maximum margin prediction (MMP, Ratliff, Bagnell &amp; Zinkevich 2006, Apprenticeship Learning vis IRL (Abbeel &amp; Ng 2004) about feature counts and IRLBoth IRL and the matching of feature counts are ambiguous.Each policy can be optimal for many reward functions.many policies lead to the same feature counts.The ambiguity of suboptimality is unresolved.NotationsAn MDP is a tuple $(S,A,P_{sa},\gamma,R)$$S$ is a finite set of $N$ states.$A={a_1,..,a_k}$ is a set of $k$ actions.$P_{sa}(s^{\prime})$ is the state transition probability of landing at state $s^{\prime}$: $P(s,a,s^{\prime})$ upon taking the action aa at state $s$.$\gamma \in [0,1)$ is the discount factor.$R: S \rightarrow \mathbf{R}$ is the reward function.Maxent$\zeta:\{(s, a)\}$ is a trajectory.$\mathbf{f_s} \in \mathbf{R}^k$ is the feature vector of the state $s$.$\theta \in \mathbf{R}^k$ reward function parameters.$P(\zeta)$ probability of the trajectory $\zeta$ to occur$P(s)$ the probatility of visiting state $s$ (state visitation frequency), $P(\zeta) = \prod_{s\in\zeta} P(s)$.AssumptionsThe reward of a trajectory is expressed as a linearly combination with feature counts$$R(\zeta) = \theta ^T \mathbf{f}_{\zeta} = \sum_{s\in \zeta} \theta ^T \mathbf{f}_s$$Principle of maximum entropy (Jaynes 1957): probability of a trajectory demonstrated by the expert is exponentially higher for higher rewards than lower rewards,$$P(\zeta) \propto e^{R(\zeta)}$$AlgorithmThe Maxent algorithm learns from demonstrated expert trajectories with the objective being maximizing the likelihood of the demonstrated trajectories,$$\begin{align}\theta^{\star} &amp;= \text{argmax}_{\theta} L(\theta) \\&amp;= \text{argmax}_{\theta} \frac{1}{M}\text{log} P(\{\zeta\} | \theta)\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\text{log} \prod_{\zeta} P(\zeta|\theta)\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \text{log} P(\zeta|\theta)\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \text{log} \frac{e^{R(\zeta)}}{Z}\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} R(\zeta) - \text{log} Z\\\end{align}$$Where $M$ is the number of trajectories, $Z$ is the normalization term,$$Z = \sum_{\zeta} e^{R(\zeta)}$$Then,$$\theta^{\star} = \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} R(\zeta) - \text{log} \sum_{\zeta} e^{R(\zeta)}\\\theta^{\star} = \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \theta ^T \mathbf{f}_{\zeta} - \text{log} \sum_{\zeta} e^{\theta ^T \mathbf{f}_{\zeta}}\\$$And the objective is convex! (with the second term being log-sum-exp). We go ahead to differentiate the objective to find the gradients:$$\begin{align}\nabla_{\theta} L &amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \frac{1}{\sum_\zeta e^{R(\zeta)}} \sum_{\zeta} (e^{R(\zeta)\frac{d R(\zeta)}{d\theta}})\\&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \frac{1}{\sum_\zeta e^{R(\zeta)}} \sum_{\zeta} (e^{R(\zeta)}\frac{d R(\zeta)}{d\theta})\\&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \sum_{\zeta}\frac{e^{R(\zeta)}}{\sum_\zeta e^{R(\zeta)}} \mathbf{f}_{\zeta}\\&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \sum_{\zeta} P(\zeta | \theta) \mathbf{f}_{\zeta}\\\end{align}$$Since the trajectories {ζ}{ζ} are consist of states,$$\nabla_{\theta} L = \frac{1}{M}\sum_s \mathbf{f}_{s} - \sum_{s} P(s | \theta) \mathbf{f}_{s}$$Where $\mathbf{f}_s$ is the feature vector for the state $s$. And$$P(s|\theta)$$is the state visitation frequency for state $s$.So far the main body of the algorithm is described. The only thing left is to compute the state visitation frequency (SVF) vector. To do so, we can use the following dynamic programming algorithm (for convienience we use $P(s)$ to denote SVF on state $s$).We use μt(s)μt(s) to denote the prob of visiting $s$ at $t$ (obviously,$$P(s) = \sum_t \mu_t(s)$$solve the MDP using value iteration with the intermediate recovered rewards to get current optimal policy $\{\pi(a,s)\}$.compute $\mu_1(s)$ using sampled trajectoriesusing DP to solve for the rest given optimal policy $\{\pi(a,s)\}$ and the transition dynamics $\{P_{sa}(s’)\}$For $t = 1,..,T$$$\mu_{t+1} (s) = \sum_{a}\sum_{s’} \mu_{t}(s’)\pi(a,s’)P_{sa}(s’)$$And finally.$$P(s) = \sum_t \mu_t(s)$$One key things to note is that, the algorithm solves MDP in each iteration of training.If the transition dynamics $\{P_{sa}(s’)\}$ is unknown, we can actually using Monte Carlo to estimate the SVF with the trajectories. This is much more easier, so the details are omitted. Plugging the SVF back to the gradients, we can use iterative gradient descent to solve for the parameters $\theta$.SummaryAs a final summary of the algorithm, here is the slide from UC Berkeley’s CS 294, Deep Reinforcement Learning course,Strengths and LimitationsStrengthsscales to neural network costs (overcome the drawbacks of linear costs)efficient enough for real robotsLimitationsrequires repeatedly solving the MDPassumes known dynamicsReferencesZiebart et al’s 2008 paper: Maximum Entropy Inverse Reinforcement Learning **UCB’s CS 294 DRL course, lecture on IRLCode Details123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=True): """compute the expected states visition frequency p(s| theta, T) using dynamic programming inputs: P_a NxNxN_ACTIONS matrix - transition dynamics gamma float - discount factor trajs list of list of Steps - collected from expert policy Nx1 vector (or NxN_ACTIONS if deterministic=False) - policy returns: p Nx1 vector - state visitation frequencies """ N_STATES, _, N_ACTIONS = np.shape(P_a) T = len(trajs[0]) # mu[s, t] is the prob of visiting state s at time t mu = np.zeros([N_STATES, T]) for traj in trajs: mu[traj[0].cur_state, 0] += 1 mu[:,0] = mu[:,0]/len(trajs) for s in range(N_STATES): for t in range(T-1): if deterministic: mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] for pre_s in range(N_STATES)]) else: mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] for a1 in range(N_ACTIONS)]) for pre_s in range(N_STATES)]) p = np.sum(mu, 1) return pdef maxent_irl(feat_map, P_a, gamma, trajs, lr, n_iters): """ Maximum Entropy Inverse Reinforcement Learning (Maxent IRL) inputs: feat_map NxD matrix - the features for each state P_a NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of landing at state s1 when taking action a at state s0 gamma float - RL discount factor trajs a list of demonstrations lr float - learning rate n_iters int - number of optimization steps returns rewards Nx1 vector - recoverred state rewards """ N_STATES, _, N_ACTIONS = np.shape(P_a) # init parameters theta = np.random.uniform(size=(feat_map.shape[1],)) # calc feature expectations feat_exp = np.zeros([feat_map.shape[1]]) for episode in trajs: for step in episode: feat_exp += feat_map[step.cur_state,:] feat_exp = feat_exp/len(trajs) # training for iteration in range(n_iters): if iteration % (n_iters/20) == 0: print 'iteration: &#123;&#125;/&#123;&#125;'.format(iteration, n_iters) # compute reward function rewards = np.dot(feat_map, theta) # compute policy _, policy = value_iteration.value_iteration(P_a, rewards, gamma, error=0.01, deterministic=False) # compute state visition frequences svf = compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=False) # compute gradients grad = feat_exp - feat_map.T.dot(svf) # update params theta += lr * grad rewards = np.dot(feat_map, theta) # return sigmoid(normalize(rewards)) return normalize(rewards)Apprenticeship Learning via Inverse Reinforcement Learning [ICML 2004]PreliminariesThe reward function can be expressed as a linear combination of known features.The value of a policy $\pi$:$$\begin{align}E_{s_0 \sim D}[V^{\pi}(s_0)] &amp;= E[\sum_{t=0}^{\infty}\gamma^t R(s_t) | \pi] \\&amp;= E[\sum_{t=0}^{\infty}\gamma^t w \cdot \phi(s_t)| \pi] \\&amp;= w \cdot E[\sum_{t=0}^{\infty}\gamma^t \phi(s_t)| \pi],\end{align}$$where vector of features $\phi: S \rightarrow [0, 1]^k$ or $\phi: S \times A \ \rightarrow [0, 1]^k$, $w \in \mathbb{R}^k$. In order to ensure that the rewards are bounded by 1, we also assume $| w |_1 \leq 1$.The feature exceptions:$$\mu(\pi) = E[\sum_{t=0}^{\infty}\gamma^t \phi(s_t)| \pi] \in \mathbb{R}^k.$$So the value of a policy may be rewritten to$$E_{s_0 \sim D}[V^{\pi}(s_0)] = w \cdot \mu(\pi)$$We assume the expert policy is $\pi_E$ and we need to estimate the expert’s feature expectations $\mu_E = \mu(\pi_E)$. Specifically, given a set of $m$ trajectories $\{s_0^{(i)}, s_1^{(i)}, \cdots, \}_{i=1}^{m}$ generated by the expert, we denote the empirical estimate for $\mu_E$ by$$\hat{\mu}_E = \frac{1}{m}\sum_{i=1}^m\sum_{t=0}^m \gamma^t \phi(s_t^{(i)}).$$Furthermore, we could construct a new policy by linear combination some other policies. More specifically, we have$$\mu(\pi_3) = \lambda \mu(\pi_1) + (1 - \lambda) \mu(\pi_2) .$$Note that the randomization step selecting between $\pi_1$ and $\pi_2$ occurs only once at the start of a trajectory, and not on every step taken in the MDP.AlgorithmWe want to find a policy whose performance is close to that of the expert, that is, for any $w \in \mathbb{R}^k (|w|_1 \leq 1)$,$$|w^T\mu(\tilde{\pi}) - w^T\mu_E| \leq |w|_2 |w^T\mu(\tilde{\pi}) - w^T\mu_E|_2 \leq 1 \cdot \epsilon \leq \epsilon.$$So the problem is reduced to finding a policy $\tilde{\pi}$ that induces feature expectations $\mu(\tilde{\pi})$ close to $\mu_E$. We find such a policy as follows:Randomly pick some policy $\pi^{(0)}$. compute (or approximate via Monte Carlo) $\mu^{(0)} = \mu(\pi^{(0)})$, and set $i=1$.Solve optimization problem (Solver as same as SVM) (*):If $t^{(i)} \leq \epsilon$, then terminal.Using the RL algorithm, compute the optimal policy $\pi^{(i)}$ for the MDP using reward $R = (w^{(i)})^T\phi$.Compute (or estimate) $\mu^{(i)} = \mu(\pi^{(i)})$.Set $i = i + 1$, and go back to step $2$.Finally, we can find the point closest to $\mu_E$ in the convex closure of $\mu^{(0)}, \cdots, \mu^{(n)}$ by solving the following QP:$$\min| \mu_E - \mu |_2, \; \text{s.t.} \; \mu = \sum_i \lambda_i \mu^{(i)}, \lambda_i \geq 0, \sum_i \lambda_i = 1.$$(*)$$\begin{align}\max_{t, w} &amp;\;t \\s.t. &amp;\; w^T\mu_E \geq w^T\mu^{(j)} + t, \; j = 0, \cdots, i-1 \\&amp;\; |w |_2 \leq 1.\end{align}$$Note that, step 2 could replace by a simpler way that no QP solver is needed:Set $\bar{\mu}^{(i-1)} = \bar{\mu}^{(i-2)} + \frac{(\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T(\mu_E - \bar{\mu}^{(i-2)})}{(\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T(\mu^{(i-1)} - \bar{\mu}^{(i-2)})} (\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T$This computes the orthogonal projection of $\mu_E$ onto the line through $\bar{\mu}^{(i-2)}$ and $\mu^{(i-1)}$.Set $w^{(i)} = \mu_E - \bar{\mu}^{(i-1)}$Set $t^{(i)} = | \mu_E - \bar{\mu}^{(i-1)} |_2$In the first iteration, we also set $w^{(1)} = \mu_E - \mu^{(0)}$ and $\bar{\mu}^{(0)} = \mu^{(0)}$.Deep Reinforcement Learning with Double Q-learningThe idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having introduce additional networks. We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value. In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN. Its update is the same as for DQN, but replacing the target $Y_t^{DQN}$ with$$Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\mathbf{-}}).$$In comparison to Double Q-learning$$Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\boldsymbol{\prime}}),$$the weights of the second network $\boldsymbol{\theta_t^{\prime}}$ are replaced with the weights of the target network $\boldsymbol{\theta_t^{-}}$ for the evaluation of the current greedy policy. The update to the target stays unchanged from DQN, and remains a periodic copy of the online network.Prioritized Experience Replaywhere $p_i &gt; 0$ is the priority of transition $i$. The exponent $\alpha$ determines how much prioritization is used, with $\alpha=0$ corresponding to the uniform case.The first we consider is the direct, proportional prioritization where $p_i = |\delta_i| + \epsilon$, where $\delta_i$ is the TD-error of transition $i$ and $\epsilon$ is a small positive constant that prevent the edge-case of transitions not being revisited once their error is zero. The second variant is an indirect, rand-based prioritization where $p_i = \frac{1}{\text{rank}(i)}$, where $\text{rank}_i$ is the rank of transition $i$ when the replay memory is sorted according to $|\delta_i|$Dueling Network Architectures for Deep Reinforcement LearningLet us consider the dueling network shown in above, where we make one stream of fully-connected layers output a scalar $V(s;\theta,\beta)$, and the other stream output an $\mathcal{A}$-dimensional vector $A(s, a; \theta, \alpha)$. Here, $\theta$ denotes the parameters of the convolutional layers, while $\alpha$ and $\beta$ are the parameters of the two streams of fully-connected layers. Using the definition of advantage, we might be tempted to construct the aggregating module as follows:$$Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s, a; \theta, \alpha),$$Note that this expression applies to all $(s, a)$ instances; that is, to express equation above in matrix form we need to replicate the scalar, $V(s;\theta,\beta)$, $|\mathcal{A}|$ times.Equation above is unidentifiable in the sense that given $Q$ we cannot recover V and A uniquely. To see this, add a constant to $V(s;\theta,\beta)$ and subtract the same constant from $A(s, a; \theta, \alpha)$. This constant cancels out resulting in the same $Q$ value. This lack of identifiability is mirrored by poor practical performance when this equation is used directly.To address this issue of identifiability, we can replace the equation above to this one:$$Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \Big( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A(s, a^{\prime}; \theta, \alpha) \Big).$$Learning Tetris Using the Noisy Cross-Entropy Methodthe paper works on solving Tetris with modified cross entropy method. original CE method in reinforcement learning usually results in early convergence.Cross entropy method in reinforcement learningfirst we start with a random uniform distribution F_0drawn from F_0 and get N samples θ_0, θ_1, …choose the top K samples that get the highest scores and use these selected sample(θ_0, θ_1, …) update distribution and get F_1keypointsadd noise to the cross-entropy method to prevent early convergeif we decrease the noise, which is only depend on time steps, the performance can even be better.noise 👉 prevent early convergenotecan view the noise apply to std as ensure enough explorationDoubly Robust Off-policy Value Evaluation for Reinforcement LearningWe study the off-policy value evaluation problem, where one aims to estimate the value of a policy with data collected by another policy.There are roughly two classes of approaches to off-policy value evaluation. The first is to fit an MDP model from data via regression, and evaluate the policy against the model; The second class of approaches are based on the idea of importance sampling (IS), which corrects the mismatch between the distributions induces by the target policy ang by the behavior policy.Importance Sampling EstimatorsThe basic IS EstimatorThe IS estimator provides an unbiased estimate of $\pi_1$’s value by averaging the following function of each trajectory $(s_1, a_1, r_1, \cdots, s_{H+1})$ in the data: define the per-step importance ratio as $\rho_t := \pi_1(a_t|s_t)/\pi_0(a_t|s_t)$, and the cumulative importance ratio $\rho_{1:t}:=\prod_{t^{\prime}=1}^{t}\rho_{t^{\prime}}$; the basic (trajectory-wise) IS estimator, and an improved step-wise version are given as follows:$$V_{\text{IS}} := \rho_{1:H} \cdot (\sum_{t=1}^{H} \gamma^{t-1} r_t),$$$$V_{\text{step-IS}} := \sum_{t=1}^{H} \gamma^{t-1}\rho_{1:t}r_t.$$Given a dataset $D$, the IS estimator is simply the average estimate over the trajectories, namely $\frac{1}{|D|}\sum_{i=1}V_{\text{IS}}^{(i)}$, where $|D|$ is the number of trajectories in $D$ and $V_{\text{IS}}^{(i)}$ is IS applied to the $i$-th trajectory.The weighted importance sampling (WIS)Define $w_t = \sum_{i=1}^{|D|}\rho_{1:t}^{(i)}/|D|$ as the average cumulative importance ratio at horizon $t$ in a dataset $D$, then for each trajectory in $D$, the estimates given by trajectory-wise and step-wise WIS are respectively$$V_{\text{WIS}} = \frac{\rho_{1:H}}{w_{H}}(\sum_{t=1}^H\gamma^{t-1}r_t),$$$$V_{\text{step-WIS}} = \sum_{t=1}^H\gamma^{t-1}\frac{\rho_{1:t}}{w_t}r_t.$$The doubly robust estimator for contextual bandits$$V_{\text{DR}} := \widehat{V}(s) + \rho(r - \widehat{R}(s, a)),$$where $\rho := \frac{\pi_1(a|s)}{\pi_0(a|s)}$ and $\widehat{V}(s) := \sum_a \pi_1(a|s)\widehat{R}(s, a)$. If $\widehat{R}(s, a)$ is a good estimator of $r$, the magnitude of $r - \widehat{R}(s, a)$ can be much smaller than of $r$. Consequently, the variance of $\rho(r - \widehat{R}(s, a))$ tends to be smaller than that of $\rho r$, implying that DR often has a lower variance that IS (Dud´ık et al., 2011).DR Estimator for the Sequential SettingA key observation is that Eqn.(6) can be written in a recursive form. Define $V_{\text{step-IS}}^0 := 0$, and for $t=1, \cdots, H$,$$V_{\text{step-IS}}^{H+1-t} := \rho_t (r_t + \gamma V_{\text{step-IS}}^{H-t}).$$We can apply the bandit DR estimator at each horizon, and obtain the following unbiased estimator: define $V_{\text{DR}}^0 := 0$, and$$V_{\text{DR}}^{H+1-t} := \widehat{V}(s_t) + \rho_t (r_t + \gamma V_{\text{DR}}^{H-t} - \widehat{Q}(s_t, a_t)).$$The DR estimator of the policy value is then $V_{\text{DR}} := V_{\text{DR}}^H$.One modification of DR that further reduces the variance in state transitions is:$$V_{\text{DR-v2}}^{H+1-t} := \widehat{V}(s_t) + \rho_t (r_t + \gamma V_{\text{DR-v2}}^{H-t} - \widehat{Q}(s_t, a_t) - \gamma \widehat{V}(s_{t+1})).$$Continuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning ApproachModelState Autoencoder + Deep Q-Network + Dueling Network + Double Q Learning + Prioritized Experience ReplayNote: loss function have changed:$$\mathcal{L}(\theta) = \mathbb{E}[(Q_{\text{double-target}} - Q(s, a;\theta))^2] - \lambda \cdot \max(|Q(s, a;\theta) - R_{\max}|, 0),$$where $R_{\max} = \pm15, Q_{\text{double-target}}=r+\gamma Q(s^{\prime}, \arg\max_{a^{\prime}}Q(s^{\prime}, a^{\prime};\theta);\theta)$. $\theta$ are the weights used to parameterize the main network, and $\theta^{\prime}$ are the weights used to parameterize the target network.FeaturePreprocessingData were aggregated into windows of 4 hours, with the mean or sum being recorded (as appropriate) when several data points were present in one window. Variables with excessive missingness were removed, and any remaining missing values were imputed with k-nearest neighbors, yielding a 47 × 1 feature vector for each patient at each timestep. Values exceeding clinical limits were capped, and capped data were normalized per-feature to zero mean and unit variance.Feature listThe physiological features used in our model are presented below.Demographics/StaticShock Index, Elixhauser, SIRS, Gender, Re-admission, GCS - Glasgow Coma Scale, SOFA - Sequential Organ Failure Assessment, AgeLab ValuesAlbumin, Arterial pH, Calcium, Glucose, Hemoglobin, Magnesium, PTT - Partial Thromboplastin Time, Potassium, SGPT - Serum Glutamic-Pyruvic Transaminase, Arterial Blood Gas, BUN - Blood Urea Nitrogen, Chloride, Bicarbonate, INR -International Normalized Ratio, Sodium, Arterial Lactate, CO2, Creatinine, Ionised Calcium, PT - Prothrombin Time, Platelets Count, SGOT - Serum Glutamic-Oxaloacetic Transaminase, Total bilirubin, White Blood Cell CountVital SignsDiastolic Blood Pressure, Systolic Blood Pressure, Mean Blood Pressure, PaCO2, PaO2, FiO2, PaO/FiO2 ratio, Respiratory Rate, Temperature (Celsius), Weight (kg), Heart Rate, SpO2Intake and Output EventsFluid Output - 4 hourly period, Total Fluid Output, Mechanical VentilationEvaluationDiscounted Returns vs. MortalityWe bin Q-values obtained via SARSA (baseline) on the test set into discrete buckets, and for each, if it is part of a trajectory where a patient died, we assign it a label of 1; if the patient survived, we assign a label of 0.State RepresentationDoubly Robust off-policy value evaluationAction differencesAction vs. MortalityA Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care UnitsModelPreprocessing using Gaussian ProcessesDenoting the observations of the vital signs by $v$ and the measurement time $t$, we model$$v = f(t) + \varepsilon,$$where $\varepsilon$ vector represents i.i.d Gaussian noise, and $f(t)$ is the latent noise-free function we would like to estimate. We put a GP prior on the latent function $f(t)$:$$f(t) \sim \mathcal{GP}(m(t), \mathcal{K}(t, t^{\prime})),$$where $m(t)$ is the mean function and $\mathcal{K}(t, t^{\prime})$ is the covariance function or kernel. In this work, we use a multi-output GP to account for temporal correlations between physiological signals during interpolation. We adapt the framework in Cheng et al. [2017] to impute the physiological signals jointly by estimating covariance structures between them, excluding the sparse prior settings (please read the paper for more details).Note: Just for continuous features.Reward FunctionFitted Q-iteration with sampling (Omitted)EvaluationsFeatures importanceThe outcome of differenceWe divide the 664 test admissions into six groups according to the fraction ofFQI policy actions that differ from the hospital’s policy: $\Delta_0$ comprises admissions in which the true and recommended policies agree perfectly, while those in $\Delta_5$ show the greatest deviation. Plotting the distribution of the number of reintubations and the mean accumulated reward over patient admissions respectively, for all patients in each set.Deep Reinforcement Learning, Decision Making and Control (ICML 2017 Tutorial)Model-Free RLpolicy gradientsREINFORCE algorithm:sample ${\tau^i}$ from $\pi_\theta(s_t|a_t)$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i))(\sum_t r(s_t^i, a_t^i))$$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$Reduce variance$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i))(\sum_{t^{\prime}=1}^T r(s_{t^{\prime}}^i, a_{t^{\prime}}^i))$“reward to go”Baselinesone baseline: average reward.$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i\nabla_\theta \log \pi_\theta(\tau) [r(\tau) - b]$$b = \frac{1}{N} \sum_{i=1}^N r(\tau)$Control variates (see also: Gu et al. 2016 (Q-Prop))$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)) \Big (\hat{Q}_t^i - b(s_{t^{\prime}}^i, a_{t^{\prime}}^i) \Big ) + \frac{1}{N} \sum_i \sum_t \nabla_\theta E_{a \sim \pi_\theta(a_t | s_t^i)}[b(s_t^i, a_t)]$covatriant/natural policy gradientnatural gradient: pick $\alpha$$\theta \leftarrow \theta + \alpha \mathbf{F}^{-1}\nabla_\theta J(\theta)$$\mathbf{F} = E_{\pi_{\theta}}[\log\pi_{\theta}(a|s)\log\pi_{\theta}(a|s)^T]$trust region policy optimization: pick $\epsilon$$\theta^{\prime} \leftarrow \arg\max_{\theta^{\prime}} (\theta^{\prime} - \theta)^T \nabla_\theta J(\theta) \; \text{s.t.} (\theta^{\prime} - \theta)^T\mathbf{F}(\theta^{\prime} - \theta) \leq \epsilon$Policy gradients suggested readings• Classic papers​ • Williams (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces REINFORCE algorithm​ • Baxter &amp; Bartlett (2001). Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient (not the first paper on this! see actor-critic section later)​ • Peters &amp; Schaal (2008). Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and natural gradient• Deep reinforcement learning policy gradient papers​ • L. &amp; Koltun (2013). Guided policy search: deep RL with importance sampled policy gradient (unrelated to later discussion of guided policy search)​ • Schulman, L., Moritz, Jordan, Abbeel (2015). Trust region policy optimization: deep RL with natural policy gradient and adaptive step size​ • Schulman, Wolski, Dhariwal, Radford, Klimov (2017). Proximal policy optimization algorithms: deep RL with importance sampled policy gradientActor-Critic algorithmsValue function fittingbatch actor-critic algorithm:sample $\{s_i, a_i\}$ from $\pi_\theta(a|s)$fit $\hat{V_\phi}(s)$ to sampled reward sums (*)evaluate $\hat{A^{\theta}}(s_i, a_i) = r(s_i, a_i) + \hat{V_\phi}(s_i^{\prime}) - \hat{V_\phi}(s_i)$$\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta(a^i|s^i) \hat{A}(s_i, a_i) $$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$(*) $y_{i,t} \approx \sum_{t=1}^T r(s_t^i, a_{t}^i)$$\mathcal{L}(\phi) = \frac{1}{2}\sum_i | \hat{V_\phi}(s_i) - y_i|^2$Discount factors$y_{i,t} \approx r(s_t^i, a_{t}^i) + \gamma \hat{V_\phi}(s_{t+1}^i)$ (0.99 works well)online actor-critic algorithm:take action $\mathbf{a} \sim \pi_\theta(a|s)$, get $(s, a, s^{\prime}, r)$update $\hat{V_\phi^{\pi}}(s)$ using target $r + \gamma \hat{V_\phi^{\pi}}(s^{\prime})$evaluate $\hat{A^{\theta}}(s_i, a_i) = r(s_i, a_i) + \hat{V_\phi}(s_i^{\prime}) - \hat{V_\phi}(s_i)$$\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta(a^i|s^i) \hat{A}(s_i, a_i) $$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$Step 2 and 4 works best with a batch.We can design better estimators (for both batch and online). See Schulman, Moritz, L. Jordan, Abbeel ‘16: Generalized advantage estimation.We can use single network for actor and critic.Actor-critic suggested readings• Classic papers​ • Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods forreinforcement learning with function approximation: actor-critic algorithms withvalue function approximation• Deep reinforcement learning actor-critic papers​ • Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu (2016).Asynchronous methods for deep reinforcement learning: A3C – parallel onlineactor-critic​ • Schulman, Moritz, L., Jordan, Abbeel (2016). High-dimensional continuouscontrol using generalized advantage estimation: batch-mode actor-critic withblended Monte Carlo and function approximator returns​ • Gu, Lillicrap, Ghahramani, Turner, L. (2017). Q-Prop: sample-efficient policy-gradient with an off-policy critic: policy gradient with Q-function control variateValue FunctionsQ-Learningtake some action $a_i$ and observe $(s_i, a_i, s^{\prime}_i, r_i)$, add it to $\mathcal{R}$sample mini-batch $(s_j, a_j, s^{\prime}_j, r_j)$ from $\mathcal{R}$ uniformlycompute $y_j = r_j + \gamma \max_{a^{\prime}_j} Q_{\phi^{\prime}} (s_j^{\prime},a_j^{\prime})$ using target network $Q_{\phi^{\prime}} $$\phi \leftarrow \phi - \alpha \sum_j \frac{dQ_{\phi}}{d\phi} (Q_\phi(s_j, a_j) - y_j)$update $\phi^{\prime}$: copy $\phi$ every $N$ steps, or Polyak average $\phi^{\prime} \leftarrow \tau\phi^{\prime} + (1-\tau)\phi$Q-Learning with continuous actionsOption 1: use function class that is easy to optimize (Gu, Lillicrap, Sutskever, L., ICML 2016)$Q(s, a | \theta^Q) = A(s, a|\theta^A) + V(x|\theta^V)$$A(s, a|\theta^A) = -\frac{1}{2}(s-\mu(x|\theta^{\mu}))^TP(s|\theta^P)(s-\mu(x|\theta^{\mu}))$Option 2: learn an approximate maximizer DDPG (Lillicrap et al., ICLR 2016)$\mu_\theta(s) = a, \theta \leftarrow \arg\max_\theta Q_{\phi}(s, \mu_{\theta}(s))$$y_j = r_j + \gamma \max_{a^{\prime}_j} Q_{\phi^{\prime}}(s_j^{\prime}, \mu_{\theta(s_j^{\prime})})$Q-learning suggested readings• Classic papers​ • Watkins. (1989). Learning from delayed rewards: introduces Q-learning​ • Riedmiller. (2005). Neural fitted Q-iteration: batch-mode Q-learning with neuralnetworks• Deep reinforcement learning Q-learning papers​ • Lange, Riedmiller. (2010). Deep auto-encoder neural networks in reinforcementlearning: early image-based Q-learning method using autoencoders to constructembeddings​ • Mnih et al. (2013). Human-level control through deep reinforcement learning: Q-learning with convolutional networks for playing Atari.​ • Van Hasselt, Guez, Silver. (2015). Deep reinforcement learning with double Q-learning: a very effective trick to improve performance of deep Q-learning.​ • Lillicrap et al. (2016). Continuous control with deep reinforcement learning: continuous Q-learning with actor network for approximate maximization.​ • Gu, Lillicrap, Stuskever, L. (2016). Continuous deep Q-learning with model-basedacceleration: continuous Q-learning with action-quadratic value functions.​ • Wang, Schaul, Hessel, van Hasselt, Lanctot, de Freitas (2016). Dueling networkarchitectures for deep reinforcement learning: separates value and advantage estimation in Q-function.Soft optimality (WIG)RL inference in a graphical modelsoft Q-learning$\phi \leftarrow \phi + \alpha \nabla_\phi Q_\phi (s, a) (r(s, a) + \gamma V(s^{\prime}) - Q_\phi(s, a))$target value: $V(s^{\prime}) = \text{soft}\max_{a^{\prime}} Q_\phi(s^{\prime}, a^{\prime}) = \log \int \exp(Q_\phi(s^{\prime}, a^{\prime})da^{\prime}$$\pi(a|s) = \exp(Q_\phi(s,a)-V(s)) = \exp(A(s, a))$policy gradient with soft optimality (WIG)Soft optimality suggested readings• Todorov. (2006). Linearly solvable Markov decision problems: one framework forreasoning about soft optimality.• Todorov. (2008). General duality between optimal control and estimation: primer on the equivalence between inference and control.• Kappen. (2009). Optimal control as a graphical model inference problem: frames control as an inference problem in a graphical model.• Ziebart. (2010). Modeling interaction via the principle of maximal causal entropy:connection between soft optimality and maximum entropy modeling.• Rawlik, Toussaint, Vijaykumar. (2013). On stochastic optimal control and reinforcement learning by approximate inference: temporal difference style algorithm with soft optimality.• Haarnoja, Tang, Abbeel, L. (2017). Reinforcement learning with deep energy basedmodels: soft Q-learning algorithm, deep RL with continuous actions and soft optimality• Nachum, Norouzi, Xu, Schuurmans. (2017). Bridging the gap between value and policy based reinforcement learning.• Schulman, Abbeel, Chen. (2017). Equivalence between policy gradients and soft Q-learning.Inverse RLMaximum Entropy Inverse RL (Ziebart et al. ’08)Initialize $\psi$, gather demonstrations $\mathcal{D}$Solve for optimal policy $\pi(a|s)$ w.r.t. reward $r_\psi$Solve for state visitation frequencies $p(s|\psi)$Compute gradient $\nabla_\psi \mathcal{L} = -\frac{1}{|\mathcal{D}|}\sum_{\tau_d \in \mathcal{D}} \frac{dr_\psi}{d\psi}(\tau_d)\sum_{s} \frac{dr_\psi}{d\psi}(s)$Update $\psi$ with one gradient step using $\nabla_\psi\mathcal{L}$guided cost learning &amp; generative adversarial imitation algorithm (Finn et al. ICML ’16, Ho &amp; Ermon NIPS ’16)Suggested Reading on Inverse RL Classic PapersAbbeel &amp; Ng ICML ’04. Apprenticeship Learning via Inverse Reinforcement Learning. Good introduction to inverse reinforcement learningZiebart et al. AAAI ’08. Maximum Entropy Inverse Reinforcement Learning.Introduction of probabilistic method for inverse reinforcement learningModern PapersWulfmeier et al. arXiv ’16. Deep Maximum Entropy Inverse Reinforcement Learning. MaxEnt IRL using deep reward functionsFinn et al. ICML ’16. Guided Cost Learning. Sampling-based method for MaxEnt IRLthat handles unknown dynamics and deep reward functionsHo &amp; Ermon NIPS ’16. Generative Adversarial Imitation Learning. IRL methodbuilding on Abbeel &amp; Ng ’04 using generative adversarial networksFurther Reading on Inverse RLMaxEnt-based IRL: Ziebart et al. AAAI ’08, Wulfmeier et al. arXiv ’16, Finn et al. ICML ‘16Adversarial IRL: Ho &amp; Ermon NIPS ’16, Finn, Christiano et al. arXiv ’16, Baram et al. ICML ’17Handling multimodality: Li et al. arXiv ’17, Hausman et al. arXiv ’17, Wang, Merel et al. ‘17Handling domain shift: Stadie et al. ICLR ‘17Model-bases RL (WIS)Guided Policy Search (GPS)Suggested Reading on Model-based RLTassa et al. IROS ’12. Synthesis and Stabilization of Complex Behaviors. Goodintroduction to MPC with a known modelLevine, Finn et al. JMLR ’16. End-to-End Learning of Deep Visuomotor Policies.Thorough paper on guided policy search for learning real robotic vision-based skillsHeess et al. NIPS ’15. Stochastic Value Gradients. Backdrop through dynamics to assist model-free learnerWatter et al. NIPS ’15. Embed-to-Control, Learn latent space and use model-baed RL in learned latent space to reach image of goalFinn &amp; Levine ICRA ’17. Deep Visual Foresight for Planning Robot Motion. Plan using learned action-conditioned video prediction modelFurther Reading on Model-based RLUse known model: Tassa et al. IROS ’12, Tan et al. TOG ’14, Mordatch et al. TOG ‘14Guided policy search: Levine, Finn et al. JMLR ’16, Mordatch et al. RSS ’14, NIPS ‘15Backprop through model: Deisenroth et al. ICML ’11, Heess et al. NIPS ’15, Mishra et al. ICML ’17, Degrave et al. ’17, Henaﬀ et al. ‘17MBRL in latent space: Watter et al. NIPS ’15, Finn et al. ICRA ‘16MPC with deep models: Lenz et al. RSS ’15, Finn &amp; Levine ICRA ‘17Combining Model-Based &amp; Model-Free:use roll-outs from model as experience: Sutton ’90, Gu et al. ICML ‘16use model as baseline: Chebotar et al. ICML ‘17use model for exploration: Stadie et al. arXiv ’15, Oh et al. NIPS ’16model-free policy with planning capabilities: Tamar et al. NIPS ’16, Pascanu et al. ‘17model-based look-ahead: Guo et al. NIPS ’14, Silver et al. Nature ‘16]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Using Keras and Deep Q-Network to Play FlappyBird (Repost)]]></title>
      <url>%2F2017%2F07%2F07%2FUsing-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost%2F</url>
      <content type="text"><![CDATA[200 lines of python code to demonstrate DQN with KerasOverviewThis project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.This article is intended to target newcomers who are interested in Reinforcement Learning.Installation Dependencies:(Update : 13 March 2017, code and weight file has been updated to support latest version of tensorflow and keras)Python 2.7Keras 1.0pygamescikit-imageHow to Run?CPU only/TensorFlow123git clone https://github.com/yanpanlau/Keras-FlappyBird.gitcd Keras-FlappyBirdpython qlearn.py -m &quot;Run&quot;GPU version (Theano)123git clone https://github.com/yanpanlau/Keras-FlappyBird.gitcd Keras-FlappyBirdTHEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.2 python qlearn.py -m &quot;Run&quot;lib.cnmem=0.2 means you assign only 20% of the GPU’s memory to the program.If you want to train the network from beginning, delete “model.h5” and run qlearn.py -m “Train”What is Deep Q-Network?Deep Q-Network is a learning algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games.The following post is a must-read for those who are interested in deep reinforcement learning.Demystifying Deep Reinforcement LearningCode Explanation (in details)Let’s go though the example in qlearn.py, line by line. If you familiar with Keras and DQN, you can skip this sessionThe code simply does the following:The code receives the Game Screen Input in the form of a pixel arrayThe code does some image pre-processingThe processed image will be fed into a neural network (Convolution Neural Network), and the network will then decide the best action to execute (Flap or Not Flap)The network will be trained millions of times, via an algorithm called Q-learning, to maximize the future expected reward.Game Screen InputFirst of all, the FlappyBird is already written in Python via pygame, so here is the code snippet to access the FlappyBird API12import wrapped_flappy_bird as gamex_t1_colored, r_t, terminal = game_state.frame_step(a_t)The idea is quite simple, the input is a_t (0 represent don’t flap, 1 represent flap), the API will give you the next frame x_t1_colored, the reward (0.1 if alive, +1 if pass the pipe, -1 if die) and terminal is a boolean flag indicates whether the game is FINISHED or NOT. We also followed DeepMind suggestion to clip the reward between [-1,+1] to improve the stability. I have not yet get a chance to test out different reward functions but it would be an interesting exercise to see how the performance is changed with different reward functions.Interesting readers can modify the reward function in game/wrapped_flappy_bird.py”, under the function **def frame_step(self, input_actions)Image pre-processingIn order to make the code train faster, it is vital to do some image processing. Here are the key elements:I first convert the color image into grayscaleI crop down the image size into 80x80 pixelI stack 4 frames together before I feed into neural network.Why do I need to stack 4 frames together? This is one way for the model to be able to infer the velocity information of the bird.123456x_t1 = skimage.color.rgb2gray(x_t1_colored)x_t1 = skimage.transform.resize(x_t1,(80,80))x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))x_t1 = x_t1.reshape(1, 1, x_t1.shape[0], x_t1.shape[1])s_t1 = np.append(x_t1, s_t[:, :3, :, :], axis=1)x_t1 is a single frame with shape (1x1x80x80) and s_t1 is the stacked frame with shape (1x4x80x80). You might ask, why the input dimension is (1x4x80x80) but not (4x80x80)? Well, it is a requirement in Keras so let’s stick with it.Note: Some readers may ask what is axis=1? It means that when I stack the frames, I want to stack on the “2nd” dimension. i.e. I am stacking under (1x4x80x80), the 2nd index.Convolution Neural NetworkNow, we can input the pre-processed screen into the neural network, which is a convolution neural network:123456789101112131415161718def buildmodel(): print("Now we build the model") model = Sequential() model.add(Convolution2D(32, 8, 8, subsample=(4,4),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same',input_shape=(img_channels,img_rows,img_cols))) model.add(Activation('relu')) model.add(Convolution2D(64, 4, 4, subsample=(2,2),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same')) model.add(Activation('relu')) model.add(Convolution2D(64, 3, 3, subsample=(1,1),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same')) model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(512, init=lambda shape, name: normal(shape, scale=0.01, name=name))) model.add(Activation('relu')) model.add(Dense(2,init=lambda shape, name: normal(shape, scale=0.01, name=name))) adam = Adam(lr=1e-6) model.compile(loss='mse',optimizer=adam) print("We finish building the model") return modelThe exact architecture is following : The input to the neural network consists of an 4x80x80 images. The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. The final hidden layer is fully-connected consisted of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.So wait, what is convolution? The easiest way to understand a convolution is by thinking of it as a sliding window function apply to a matrix. The following gif file should help to understand.You might ask what’s the purpose the convolution? It actually help computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is appliedFor more details about Convolution in Neural Network, please read Understanding Convolution Neural Networks for NLPNoteKeras makes it very easy to build convolution neural network. However, there are few things I would like to highlightA) It is important to choose a right initialization method. I choose normal distribution with $\sigma=0.01$1init=lambda shape, name: normal(shape, scale=0.01, name=name)B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. Alert: If your input dimension is 80x80x4 (Tensorflow setting) you need to set dim_ordering = tf (tf means tensorflow, th means theano)C) In Keras, subsample=(2,2) means you down sample the image size from (80x80) to (40x40). In ML literature it is often called “stride”D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is 1-e6.Interested readers who want to learn more various learning algoithms please read belowAn overview of gradient descent optimization algorithmsDQNFinally, we can using the Q-learning algorithm to train the neural network.So, what is Q-learning? In Q-learning the most important thing is the Q-function : Q(s, a) representing the maximum discounted future reward when we perform action a in state s. Q(s, a) gives you an estimation of how good to choose an action a in state s.REPEAT : Q(s, a) representing the maximum discounted future reward when we perform action a in state sYou might ask 1) Why Q-function is useful? 2) How can I get the Q-function?Let me give you an analogy of the Q-function: Suppose you are playing a difficult RPG game and you don’t know how to play it well. If you have bought a strategy guide, which is an instruction book that contain hints or complete solutions to a specific video game, then playing that video game is easy. You just follow the guidiance from the strategy book. Here, Q-function is similar to a strategy guide. Suppose you are in state s and you need to decide whether you take action a or b. If you have this magical Q-function, the answers become really simple – pick the action with highest Q-value!$${\pi(s) = {argmax}_{a} Q(s,a)}$$Here, $\pi$ represents the policy, which you will often see in the ML literature.How do we get the Q-function? That’s where Q-learning is coming from. Let me quickly derive here:Define total future reward from time t onward$$R_t = r_t + r_{t+1} + r_{t+2} … + r_n$$But because our environment is stochastic, we can never be sure, the more future we go, the more uncertain. Therefore, it is common to use discount future reward instead$$R_t = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} … + \gamma^{n-t} r_n$$which, can be written as$$R_t = r_t + \gamma \ast R_{t+1}$$Recall the definition of Q-function (maximum discounted future reward if we choose action a in state s)$$Q(s_t, a_t) = max R_{t+1}$$therefore, we can rewrite the Q-function as below$$Q(s, a) = r + \gamma \ast max_{a^{‘}} Q(s^{\prime}, a^{\prime})$$In plain English, it means maximum future reward for this state and action (s,a) is the immediate reward r plus maximum future reward for the next state $s^{\prime}$ , action $a^{\prime}$We could now use an iterative method to solve for the Q-function. Given a transition $(s, a, r, s^{\prime})$ , we are going to convert this episode into training set for the network. i.e. We want $r + \gamma max_a Q (s,a)$ to be equal to $Q(s,a)$. You can think of finding a Q-value is a regession task now, I have a estimator $r + \gamma max_a Q (s,a)$ and a predictor $Q(s,a)$, I can define the mean squared error (MSE), or the loss function, as below:Define a loss function$$L = [r + \gamma max_{a^{\prime}} Q (s^{\prime},a^{\prime}) - Q(s,a)]^{2}$$Given a transition $(s, a, r, s^{\prime})$, how can I optimize my Q-function such that it returns the smallest mean squared error loss? If L getting smaller, we know the Q-function is getting converged into the optimal value, which is our “strategy book”.Now, you might ask, where is the role of the neural network? This is where the DEEP Q-Learning comes in. You recall that $Q(s,a)$, is a stategy book, which contains millions or trillions of states and actions if you display as a table. The idea of the DQN is that I use the neural network to COMPRESS this Q-table, using some parameters \thetaθ (We called it weight in Neural Network). So instead of handling a large table, I just need to worry the weights of the neural network. By smartly tuning the weight parameters, I can find the optimal Q-function via the various Neural Network training algorithm.$$Q(s,a) = f_{\theta}(s)$$where $f$ is our neural network with input $s$ and weight parameters $\theta$Here is the code below to demonstrate how it works123456789101112131415161718192021222324252627282930if t &gt; OBSERVE: #sample a minibatch to train on minibatch = random.sample(D, BATCH) inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3])) #32, 80, 80, 4 targets = np.zeros((inputs.shape[0], ACTIONS)) #32, 2 #Now we do the experience replay for i in range(0, len(minibatch)): state_t = minibatch[i][0] action_t = minibatch[i][1] #This is action index reward_t = minibatch[i][2] state_t1 = minibatch[i][3] terminal = minibatch[i][4] # if terminated, only equals reward inputs[i:i + 1] = state_t #I saved down s_t targets[i] = model.predict(state_t) # Hitting each buttom probability Q_sa = model.predict(state_t1) if terminal: targets[i, action_t] = reward_t else: targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa) loss += model.train_on_batch(inputs, targets) s_t = s_t1 t = t + 1Experience ReplayIf you examine the code above, there is a comment called “Experience Replay”. Let me explain what it does: It was found that approximation of Q-value using non-linear functions like neural network is not very stable. The most important trick to solve this problem is called experience replay. During the gameplay all the episode $(s, a, r, s^{‘})$ are stored in replay memory D. (I use Python function deque() to store it). When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability.Exploration vs. ExploitationThere is another issue in the reinforcement learning algorithm which called Exploration vs. Exploitation. How much of an agent’s time should be spent exploiting its existing known-good policy, and how much time should be focused on exploring new, possibility better, actions? We often encounter similar situations in real life too. For example, we face on which restaurant to go to on Saturday night. We all have a set of restaurants that we prefer, based on our policy/strategy book $Q(s,a)$. If we stick to our normal preference, there is a strong probability that we’ll pick a good restaurant. However, sometimes, we occasionally like to try new restaurants to see if they are better. The RL agents face the same problem. In order to maximize future reward, they need to balance the amount of time that they follow their current policy (this is called being “greedy”), and the time they spend exploring new possibilities that might be better. A popular approach is called $\epsilon$ greedy approach. Under this approach, the policy tells the agent to try a random action some percentage of the time, as defined by the variable $\epsilon$ (epsilon), which is a number between 0 and 1. The strategy will help the RL agent to occasionally try something new and see if we can achieve ultimate strategy.123456789if random.random() &lt;= epsilon: print("----------Random Action----------") action_index = random.randrange(ACTIONS) a_t[action_index] = 1 else: q = model.predict(s_t) #input a stack of 4 images, get the prediction max_Q = np.argmax(q) action_index = max_Q a_t[max_Q] = 1I think that’s it. I hope this blog will help you to understand how DQN works.FAQMy training is very slowYou might need a GPU to accelerate the calculation. I used a TITAN X and train for at least 1 million frames to make it workFuture works and thoughtsCurrent DQN depends on large experience replay. Is it possible to replace it or even remove it?How can one decide on the optimal Convolution Neural Network?Training is very slow, how to speed it up/to make the model converge faster?What does the Neural Network actually learn? Is the knowledge transferable?I believe the questions are still not resolved and it’s an active research area in Machine Learning.Reference[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. Nature, 529-33, 2015.DisclaimerThis work is highly based on the following repos:https://github.com/yenchenlin/DeepLearningFlappyBirdhttp://edersantana.github.io/articles/keras_rl/AcknowledgementI must thank to @hardmaru to encourage me to write this blog. I also thank to @fchollet to help me on the weight initialization in Keras and @edersantana his post on Keras and reinforcement learning which really help me to understand it.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Demystifying Deep Reinforcement Learning (Repost)]]></title>
      <url>%2F2017%2F07%2F07%2FDemystifying-Deep-Reinforcement-Learning%2F</url>
      <content type="text"><![CDATA[Two years ago, a small company in London called DeepMind uploaded their pioneering paper “Playing Atari with Deep Reinforcement Learning” to Arxiv. In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!It has been hailed since then as the first step towards general artificial intelligence – an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess. No wonder DeepMind was immediately bought by Google and has been on the forefront of deep learning research ever since. In February 2015 their paper “Human-level control through deep reinforcement learning” was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.Still, while deep models for supervised and unsupervised learning have seen widespread adoption in the community, deep reinforcement learning has remained a bit of a mystery. In this blog post I will be trying to demystify this technique and understand the rationale behind it. The intended audience is someone who already has background in machine learning and possibly in neural networks, but hasn’t had time to delve into reinforcement learning yet.The roadmap ahead:What are the main challenges in reinforcement learning? We will cover the credit assignment problem and the exploration-exploitation dilemma here.How to formalize reinforcement learning in mathematical terms? We will define Markov Decision Process and use it for reasoning about reinforcement learning.How do we form long-term strategies? We define “discounted future reward”, that forms the main basis for the algorithms in the next sections.How can we estimate or approximate the future reward? Simple table-based Q-learning algorithm is defined and explained here.What if our state space is too big? Here we see how Q-table can be replaced with a (deep) neural network.What do we need to make it actually work? Experience replay technique will be discussed here, that stabilizes the learning with neural networks.Are we done yet? Finally we will consider some simple solutions to the exploration-exploitation problem.Reinforcement LearningConsider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases – you get a reward.Figure 1: Atari Breakout game. Image credit: DeepMind.Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be three actions: left, right or fire (to launch the ball). It would make sense to treat it as a classification problem – for each game screen you have to decide, whether you should move left, right or press fire. Sounds straightforward? Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but that’s not really how we learn. We don’t need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.This is the task reinforcement learning tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels – the rewards. Based only on those rewards the agent has to learn to behave in the environment.While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did just before getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the credit assignment problem – i.e., which of the preceding actions was responsible for getting the reward and to what extent.Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the explore-exploit dilemma – should you exploit the known working strategy or explore other, possibly better strategies.Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work – these are all examples of rewards. Credit assignment problems and exploration-exploitation dilemmas come up every day both in business and in relationships. That’s why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.Markov Decision ProcessNow the question is, how do you formalize a reinforcement learning problem, so that you can reason about it? The most common method is to represent it as a Markov decision process.Suppose you are an agent, situated in an environment (e.g. Breakout game). The environment is in a certain state(e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain actions in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a reward (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called policy. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).Figure 2: Left: reinforcement learning problem. Right: Markov decision process.The set of states and actions, together with rules for transitioning from one state to another, make up a Markov decision process. One episode of this process (e.g. one game) forms a finite sequence of states, actions and rewards:Here si represents the state, ai is the action and ri+1 is the reward after performing the action. The episode ends with terminal state sn (e.g. “game over” screen). A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.Discounted Future RewardTo perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?Given one run of the Markov decision process, we can easily calculate the total reward for one episode:Given that, the total future reward from time point t onward can be expressed as:But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use discounted future reward instead:Here γ is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step t can be expressed in terms of the same thing at time step t+1:If we set the discount factor γ=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like γ=0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor γ=1.A good strategy for an agent would be to always choose an action that maximizes the (discounted) future reward.Q-learningIn Q-learning we define a function Q(s, a) representing the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on.The way to think about Q(s, a) is that it is “the best possible score at the end of the game after performing action ain state s“. It is called Q-function, because it represents the “quality” of a certain action in a given state.This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really can’t. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: “Q(s, a) exists, Q(s, a) exists, …”. Feel it?If you’re still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action a or b. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple – pick the action with the highest Q-value!Here π represents the policy, the rule how we choose an action in each state.OK, how do we get that Q-function then? Let’s focus on just one transition &lt;s, a, r, s’&gt;. Just like with discounted future rewards in the previous section, we can express the Q-value of state s and action a in terms of the Q-value of the next state s’.This is called the Bellman equation. If you think about it, it is quite logical – maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.The main idea in Q-learning is that we can iteratively approximate the Q-function using the Bellman equation. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q-learning algorithm is as simple as the following[1]:α in the algorithm is a learning rate that controls how much of the difference between previous Q-value and newly proposed Q-value is taken into account. In particular, when α=1, then two Q[s,a] cancel and the update is exactly the same as the Bellman equation.The maxa’ Q[s’,a’] that we use to update Q[s,a] is only an approximation and in early stages of learning it may be completely wrong. However the approximation get more and more accurate with every iteration and it has been shown, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.Deep Q NetworkThe state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. This intuitive representation however is game specific. Could we come up with something more universal, that would be suitable for all the games? The obvious choice is screen pixels – they implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.If we apply the same preprocessing to game screens as in the DeepMind paper – take the four last screen images, resize them to 84×84 and convert to grayscale with 256 gray levels – we would have 25684x84x4 ≈ 1067970 possible game states. This means 1067970 rows in our imaginary Q-table – more than the number of atoms in the known universe! One could argue that many pixel combinations (and therefore states) never occur – we could possibly represent it as a sparse table containing only visited states. Even so, most of the states are very rarely visited and it would take a lifetime of the universe for the Q-table to converge. Ideally, we would also like to have a good guess for Q-values for states we have never seen before.This is the point where deep learning steps in. Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q-value update or pick the action with the highest Q-value, we only have to do one forward pass through the network and have all Q-values for all actions available immediately.Figure 3: Left: Naive formulation of deep Q-network. Right: More optimized architecture of deep Q-network, used in DeepMind paper.The network architecture that DeepMind used is as follows:This is a classical convolutional neural network with three convolutional layers, followed by two fully connected layers. People familiar with object recognition networks may notice that there are no pooling layers. But if you really think about it, pooling layers buy you translation invariance – the network becomes insensitive to the location of an object in the image. That makes perfectly sense for a classification task like ImageNet, but for games the location of the ball is crucial in determining the potential reward and we wouldn’t want to discard this information!Input to the network are four 84×84 grayscale game screens. Outputs of the network are Q-values for each possible action (18 in Atari). Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.Given a transition &lt; s, a, r, s’ &gt;, the Q-table update rule in the previous algorithm must be replaced with the following:Do a feedforward pass for the current state s to get predicted Q-values for all actions.Do a feedforward pass for the next state s’ and calculate maximum overall network outputs max a’ Q(s’, a’).Set Q-value target for action to r + γmax a’ Q(s’, a’) (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.Update the weights using backpropagation.Experience ReplayBy now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable. There is a whole bag of tricks that you have to use to actually make it converge. And it takes a long time, almost a week on a single GPU.The most important trick is experience replay. During gameplay all the experiences &lt; s, a, r, s’ &gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.Exploration-ExploitationQ-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward. But we haven’t touched the exploration-exploitation dilemma yet…Firstly observe, that when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds.A simple and effective fix for the above problem is ε-greedy exploration – with probability ε choose a random action, otherwise go with the “greedy” action with the highest Q-value. In their system DeepMind actually decreases ε over time from 1 to 0.1 – in the beginning the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate.Deep Q-learning AlgorithmThis gives us the final deep Q-learning algorithm with experience replay:There are many more tricks that DeepMind used to actually make it work – like target network, error clipping, reward clipping etc, but these are out of scope for this introduction.The most amazing part of this algorithm is that it learns anything at all. Just think about it – because our Q-function is initialized randomly, it initially outputs complete garbage. And we are using this garbage (the maximum Q-value of the next state) as targets for the network, only occasionally folding in a tiny reward. That sounds insane, how could it learn anything meaningful at all? The fact is, that it does.Final notesMany improvements to deep Q-learning have been proposed since its first introduction – Double Q-learning, Prioritized Experience Replay, Dueling Network Architecture and extension to continuous action space to name a few. For latest advancements check out the NIPS 2015 deep reinforcement learning workshop and ICLR 2016(search for “reinforcement” in title). But beware, that deep Q-learning has been patented by Google.It is often said, that artificial intelligence is something we haven’t figured out yet. Once we know how it works, it doesn’t seem intelligent any more. But deep Q-networks still continue to amaze me. Watching them figure out a new game is like observing an animal in the wild – a rewarding experience by itself.CreditsThanks to Ardi Tampuu, Tanel Pärnamaa, Jaan Aru, Ilya Kuzovkin, Arjun Bansal and Urs Köster for comments and suggestions on the drafts of this post.LinksDavid Silver’s lecture about deep reinforcement learningSlightly awkward but accessible illustration of Q-learningUC Berkley’s course on deep reinforcement learningDavid Silver’s reinforcement learning courseNando de Freitas’ course on machine learning (two lectures about reinforcement learning in the end)Andrej Karpathy’s course on convolutional neural networks[1] Algorithm adapted from http://artint.info/html/ArtInt_265.htmlThis blog was first published at: http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/This is the part 1 of my series on deep reinforcement learning. Tune in next week for “Deep Reinforcement Learning with Neon” for an actual implementation with Neon deep learning toolkit.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[On-policy Control with Approximation]]></title>
      <url>%2F2017%2F07%2F06%2FOn-policy-Control-with-Approximation%2F</url>
      <content type="text"><![CDATA[In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a finite-dimensional weight vector.Episodic Semi-gradient ControlThe general gradient-descent update for action-value prediction is$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).$$For example, the update for the one-step Sarsa method is$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t)- \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).$$We call this method episode semi-gradient one-step sarsa.To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters.Example: Mountain-Car TaskConsider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the below.The diﬃculty is that gravity is stronger than the car’s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to ﬁrst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great diﬃculties with tasks of this kind unless explicitly aided by a human designer.The reward in this problem is −1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (−1), and zero throttle (0). The car moves according to a simpliﬁed physics. Its position, $x_t$, and velocity, $\dot{x}_t$, are updated by$$\begin{align}x_{t+1} &amp;\doteq bound \big[x_t + \dot{x}_{t+1} \big] \\\dot{x}_{t+1} &amp;\doteq bound \big[\dot{x}_t + 0.001 A_t - 0.0025 \cos(3x_t) \big],\end{align}$$where the bound operation enforces $-1.2 \leq x_{t+1} \leq 0.5 \;\; \text{and} \; -0.07 \leq \dot{x}_{t+1} \leq 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position $x_t \in [−0.6, −0.4)$ and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings.First of all, we define the environment of this problem:123456789101112131415# all possible actionsACTION_REVERSE = -1ACTION_ZERO = 0ACTION_FORWARD = 1# order is importantACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]# bound for position and velocityPOSITION_MIN = -1.2POSITION_MAX = 0.5VELOCITY_MIN = -0.07VELOCITY_MAX = 0.07# use optimistic initial value, so it's ok to set epsilon to 0EPSILON = 0After take an action, we transition to a new state and get a reward:1234567891011# take an @action at @position and @velocity# @return: new position, new velocity, reward (always -1)def takeAction(position, velocity, action): newVelocity = velocity + 0.001 * action - 0.0025 * np.cos(3 * position) newVelocity = min(max(VELOCITY_MIN, newVelocity), VELOCITY_MAX) newPosition = position + newVelocity newPosition = min(max(POSITION_MIN, newPosition), POSITION_MAX) reward = -1.0 if newPosition == POSITION_MIN: newVelocity = 0.0 return newPosition, newVelocity, rewardThe $\varepsilon$-greedy policy:12345678# get action at @position and @velocity based on epsilon greedy policy and @valueFunctiondef getAction(position, velocity, valueFunction): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(ACTIONS) values = [] for action in ACTIONS: values.append(valueFunction.value(position, velocity, action)) return np.argmax(values) - 1We need map out continuous state to discrete state:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# wrapper class for state action value functionclass ValueFunction: # In this example I use the tiling software instead of implementing standard tiling by myself # One important thing is that tiling is only a map from (state, action) to a series of indices # It doesn't matter whether the indices have meaning, only if this map satisfy some property # View the following webpage for more information # http://incompleteideas.net/sutton/tiles/tiles3.html # @maxSize: the maximum # of indices def __init__(self, stepSize, numOfTilings=8, maxSize=2048): self.maxSize = maxSize self.numOfTilings = numOfTilings # divide step size equally to each tiling self.stepSize = stepSize / numOfTilings self.hashTable = IHT(maxSize) # weight for each tile self.weights = np.zeros(maxSize) # position and velocity needs scaling to satisfy the tile software self.positionScale = self.numOfTilings / (POSITION_MAX - POSITION_MIN) self.velocityScale = self.numOfTilings / (VELOCITY_MAX - VELOCITY_MIN) # get indices of active tiles for given state and action def getActiveTiles(self, position, velocity, action): # I think positionScale * (position - position_min) would be a good normalization. # However positionScale * position_min is a constant, so it's ok to ignore it. activeTiles = tiles(self.hashTable, self.numOfTilings, [self.positionScale * position, self.velocityScale * velocity], [action]) return activeTiles # estimate the value of given state and action def value(self, position, velocity, action): if position == POSITION_MAX: return 0.0 activeTiles = self.getActiveTiles(position, velocity, action) return np.sum(self.weights[activeTiles]) # learn with given state, action and target def learn(self, position, velocity, action, target): activeTiles = self.getActiveTiles(position, velocity, action) estimation = np.sum(self.weights[activeTiles]) delta = self.stepSize * (target - estimation) for activeTile in activeTiles: self.weights[activeTile] += delta # get # of steps to reach the goal under current state value function def costToGo(self, position, velocity): costs = [] for action in ACTIONS: costs.append(self.value(position, velocity, action)) return -np.max(costs)Because the one-step semi-gradient sarsa is a special case of n-step, so we develop the n-step algorithm$$G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \; n \geq 1,0 \leq t \leq T-n.$$The n-step equation is$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ G_{t:t+n}- \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}), \;\;\; 0 \leq t \leq T.$$Complete pseudocode is given in the box below.So the code is as follows:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# semi-gradient n-step Sarsa# @valueFunction: state value function to learn# @n: # of stepsdef semiGradientNStepSarsa(valueFunction, n=1): # start at a random position around the bottom of the valley currentPosition = np.random.uniform(-0.6, -0.4) # initial velocity is 0 currentVelocity = 0.0 # get initial action currentAction = getAction(currentPosition, currentVelocity, valueFunction) # track previous position, velocity, action and reward positions = [currentPosition] velocities = [currentVelocity] actions = [currentAction] rewards = [0.0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # take current action and go to the new state newPostion, newVelocity, reward = takeAction(currentPosition, currentVelocity, currentAction) # choose new action newAction = getAction(newPostion, newVelocity, valueFunction) # track new state and action positions.append(newPostion) velocities.append(newVelocity) actions.append(newAction) rewards.append(reward) if newPostion == POSITION_MAX: T = time # get the time of the state to update updateTime = time - n if updateTime &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(updateTime + 1, min(T, updateTime + n) + 1): returns += rewards[t] # add estimated state action value to the return if updateTime + n &lt;= T: returns += valueFunction.value(positions[updateTime + n], velocities[updateTime + n], actions[updateTime + n]) # update the state value function if positions[updateTime] != POSITION_MAX: valueFunction.learn(positions[updateTime], velocities[updateTime], actions[updateTime], returns) if updateTime == T - 1: break currentPosition = newPostion currentVelocity = newVelocity currentAction = newAction return timeNext, we use the method mentioned earlier to solve this problem:12345678910episodes = 9000targetEpisodes = [1-1, 12-1, 104-1, 1000-1, episodes - 1]numOfTilings = 8alpha = 0.3valueFunction = ValueFunction(alpha, numOfTilings)for episode in range(0, episodes): print('episode:', episode) semiGradientNStepSarsa(valueFunction) if episode in targetEpisodes: prettyPrint(valueFunction, 'Episode: ' + str(episode + 1))Result is as follows:The result shows what typically happens while learning to solve this task with this form of function approximation. Shown is the negative of the value function (the cost-to-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, $\varepsilon$, was 0. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found.Next, let us test the performance of various step size (learning rate).12345678910111213141516171819202122232425runs = 10episodes = 500numOfTilings = 8alphas = [0.1, 0.2, 0.5]steps = np.zeros((len(alphas), episodes))for run in range(0, runs): valueFunctions = [ValueFunction(alpha, numOfTilings) for alpha in alphas] for index in range(0, len(valueFunctions)): for episode in range(0, episodes): print('run:', run, 'alpha:', alphas[index], 'episode:', episode) step = semiGradientNStepSarsa(valueFunctions[index]) steps[index, episode] += stepsteps /= runsglobal figureIndexplt.figure(figureIndex)figureIndex += 1for i in range(0, len(alphas)): plt.plot(steps[i], label='alpha = '+str(alphas[i])+'/'+str(numOfTilings))plt.xlabel('Episode')plt.ylabel('Steps per episode')plt.yscale('log')plt.legend()The result is as follows:And then, let us test the performance of one-step sarsa and multi-step sarsa on the Mountain Car task:12345678910111213141516171819202122232425runs = 10episodes = 500numOfTilings = 8alphas = [0.5, 0.3]nSteps = [1, 8]steps = np.zeros((len(alphas), episodes))for run in range(0, runs): valueFunctions = [ValueFunction(alpha, numOfTilings) for alpha in alphas] for index in range(0, len(valueFunctions)): for episode in range(0, episodes): print('run:', run, 'steps:', nSteps[index], 'episode:', episode) step = semiGradientNStepSarsa(valueFunctions[index], nSteps[index]) steps[index, episode] += stepsteps /= runsglobal figureIndexplt.figure(figureIndex)figureIndex += 1for i in range(0, len(alphas)): plt.plot(steps[i], label='n = '+str(nSteps[i]))plt.xlabel('Episode')plt.ylabel('Steps per episode')plt.yscale('log')plt.legend()The result is as follows:Finally, let me shows the results of a more detailed study of the eﬀect of the parameters $\alpha$ and $n$ on the rate of learning on this task.123456789101112131415161718192021222324252627282930313233alphas = np.arange(0.25, 1.75, 0.25)nSteps = np.power(2, np.arange(0, 5))episodes = 50runs = 5truncateStep = 300steps = np.zeros((len(nSteps), len(alphas)))for run in range(0, runs): for nStepIndex, nStep in zip(range(0, len(nSteps)), nSteps): for alphaIndex, alpha in zip(range(0, len(alphas)), alphas): if (nStep == 8 and alpha &gt; 1) or \ (nStep == 16 and alpha &gt; 0.75): # In these cases it won't converge, so ignore them steps[nStepIndex, alphaIndex] += truncateStep * episodes continue valueFunction = ValueFunction(alpha) for episode in range(0, episodes): print('run:', run, 'steps:', nStep, 'alpha:', alpha, 'episode:', episode) step = semiGradientNStepSarsa(valueFunction, nStep) steps[nStepIndex, alphaIndex] += step# average over independent runs and episodessteps /= runs * episodes# truncate high values for better displaysteps[steps &gt; truncateStep] = truncateStepglobal figureIndexplt.figure(figureIndex)figureIndex += 1for i in range(0, len(nSteps)): plt.plot(alphas, steps[i, :], label='n = '+str(nSteps[i]))plt.xlabel('alpha * number of tilings(8)')plt.ylabel('Steps per episode')plt.legend()The result is as follows:UpdateUse OpenAI gymNow, let us use the OpenAI gym toolkit to simply our Mountain Car task. As we know, we need to define the environment of task before develop the detail algorithm. It’s easy to make mistakes with some detail. So it is fantastic if we do not need to care about that. The gym toolkit help us to define the environment. Such as the Mountain Car task, there is a build-in model in the gym toolkit. We just need to write one row code:1env = gym.envs.make("MountainCar-v0")That is amazing!We also can test the environment very convenience and get a pretty good user graphic:12345678910111213env.reset()plt.figure()plt.imshow(env.render(mode='rgb_array'))# for x in range(10000):# env.step(0)# plt.figure()# plt.imshow(env.render(mode='rgb_array')) [env.step(0) for x in range(10000)]plt.figure()plt.imshow(env.render(mode='rgb_array'))env.render(close=True)These codes will return the result as follows:Bravo~Now, let us solve the task by to use the Q-Learning algorithm. Meanwhile, we will use the RBF kernel to construct the features and use the SGDRegressor gradient-descent method of scikit-learn package to update $\mathbf{w}$.First of all, we need to normalized our features to accelerate the convergence speed and use the RBF kernel to reconstruct our feature space. The both methods (Normalization and Reconstruction) need to use some training set to train a model. Then we use this model to transform our raw data.123456789101112131415# Feature Preprocessing: Normalize to zero mean and unit variance# We use a few samples from the observation space to do thisobservation_examples = np.array([env.observation_space.sample() for x in range(10000)])scaler = sklearn.preprocessing.StandardScaler()scaler.fit(observation_examples)# Used to converte a state to a featurizes represenation.# We use RBF kernels with different variances to cover different parts of the spacefeaturizer = sklearn.pipeline.FeatureUnion([ ("rbf1", RBFSampler(gamma=5.0, n_components=100)), ("rbf2", RBFSampler(gamma=2.0, n_components=100)), ("rbf3", RBFSampler(gamma=1.0, n_components=100)), ("rbf4", RBFSampler(gamma=0.5, n_components=100)) ])featurizer.fit(scaler.transform(observation_examples))Next, we define a class named Estimator to simply the gradient descent process:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Estimator(): """ Value Function approximator. """ def __init__(self): # We create a separate model for each action in the environment's # action space. Alternatively we could somehow encode the action # into the features, but this way it's easier to code up. self.models = [] for _ in range(env.action_space.n): model = SGDRegressor(learning_rate="constant") # We need to call partial_fit once to initialize the model # or we get a NotFittedError when trying to make a prediction # This is quite hacky. model.partial_fit([self.featurize_state(env.reset())], [0]) self.models.append(model) def featurize_state(self, state): """ Returns the featurized representation for a state. """ scaled = scaler.transform([state]) featurized = featurizer.transform(scaled) return featurized[0] def predict(self, s, a=None): """ Makes value function predictions. Args: s: state to make a prediction for a: (Optional) action to make a prediction for Returns If an action a is given this returns a single number as the prediction. If no action is given this returns a vector or predictions for all actions in the environment where pred[i] is the prediction for action i. """ features = self.featurize_state(s) if not a: return np.array([m.predict([features])[0] for m in self.models]) else: return self.models[a].predict([features])[0] def update(self, s, a, y): """ Updates the estimator parameters for a given state and action towards the target y. """ features = self.featurize_state(s) self.models[a].partial_fit([features], [y])We also need a $\varepsilon$-greedy policy to select action:123456789101112131415161718192021def make_epsilon_greedy_policy(estimator, epsilon, nA): """ Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon. Args: estimator: An estimator that returns q values for a given state epsilon: The probability to select a random action . float between 0 and 1. nA: Number of actions in the environment. Returns: A function that takes the observation as an argument and returns the probabilities for each action in the form of a numpy array of length nA. """ def policy_fn(observation): A = np.ones(nA, dtype=float) * epsilon / nA q_values = estimator.predict(observation) best_action = np.argmax(q_values) A[best_action] += (1.0 - epsilon) return A return policy_fnThen we develop the Q-Learning method:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980def q_learning(env, estimator, num_episodes, discount_factor=1.0, epsilon=0.1, epsilon_decay=1.0): """ Q-Learning algorithm for fff-policy TD control using Function Approximation. Finds the optimal greedy policy while following an epsilon-greedy policy. Args: env: OpenAI environment. estimator: Action-Value function estimator num_episodes: Number of episodes to run for. discount_factor: Lambda time discount factor. epsilon: Chance the sample a random action. Float betwen 0 and 1. epsilon_decay: Each episode, epsilon is decayed by this factor Returns: An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) for i_episode in range(num_episodes): # The policy we're following policy = make_epsilon_greedy_policy( estimator, epsilon * epsilon_decay**i_episode, env.action_space.n) # Print out which episode we're on, useful for debugging. # Also print reward for last episode last_reward = stats.episode_rewards[i_episode - 1] sys.stdout.flush() # Reset the environment and pick the first action state = env.reset() # Only used for SARSA, not Q-Learning next_action = None # One step in the environment for t in itertools.count(): # Choose an action to take # If we're using SARSA we already decided in the previous step if next_action is None: action_probs = policy(state) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) else: action = next_action # Take a step next_state, reward, done, _ = env.step(action) # Update statistics stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t # TD Update q_values_next = estimator.predict(next_state) # Use this code for Q-Learning # Q-Value TD Target td_target = reward + discount_factor * np.max(q_values_next) # Use this code for SARSA TD Target for on policy-training: # next_action_probs = policy(next_state) # next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs) # td_target = reward + discount_factor * q_values_next[next_action] # Update the function approximator using our target estimator.update(state, action, td_target) print("\rStep &#123;&#125; @ Episode &#123;&#125;/&#123;&#125; (&#123;&#125;)".format(t, i_episode + 1, num_episodes, last_reward), end="") if done: break state = next_state return statsRun this method:12345estimator = Estimator()# Note: For the Mountain Car we don't actually need an epsilon &gt; 0.0# because our initial estimate for all states is too "optimistic" which leads# to the exploration of all states.stats = q_learning(env, estimator, 100, epsilon=0.0)The result is as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[On-policy Prediction with Approximation]]></title>
      <url>%2F2017%2F07%2F05%2FOn-policy-Prediction-with-Approximation%2F</url>
      <content type="text"><![CDATA[The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector $\mathbf{w} \in \mathbb{R}^d$. We will write $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$ for the approximated value of state $s$ given weight vector $\mathbf{w}$. For example, $\hat{v}$ might be a linear function in features of the state, with $\mathbf{w}$ the vector of feature weights. Consequently, when a single state is updated, the change generalizes from that state to affect the values of many other states.The prediction Objective (MSVE)In the tabular case a continuous measure of prediction quality was not necessary because the learned value function could come to equal the true value function exactly. But with genuine approximation, an update at one state aﬀects many others, and it is not possible to get all states exactly correct. By assumption we have far more states than weights, so making one state’s estimate more accurate invariably means making others’ less accurate.By the error in a state $s$ we mean the square of the diﬀerence between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_{\pi}(s)$. Weighting this over the state space by the distribution $\mu$, we obtain a natural objective function, the Mean Squared Value Error, or MSVE:$$\text{MSVE}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) \Big[v_{\pi}(s) - \hat{v}(s, \mathbf{w}) \Big]^2.$$The square root of this measure, the root MSVE or RMSVE, gives a rough measure of how much the approximate values diﬀer from the true values and is often used in plots. Typically one chooses $\mu(s)$ to be the fraction of time spent in $s$ under the target policy $\pi$. This is called the on-policy distribution.Stochastic-gradient MethodsWe assume that states appear in examples with the same distribution, µ, over which we are trying to minimize the MSVE. A good strategy in this case is to try to minimize error on the observed examples. Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example by a small amount in the direction that would most reduce the error on that example:$$\begin{align}\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big]^2 \\&amp;= \mathbf{w}_t + \alpha \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).\end{align}$$And$$\nabla f(\mathbf{w}) \doteq \Big( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \cdots, \frac{\partial f(\mathbf{w})}{\partial w_d}\Big)^{\top}.$$Although $v_{\pi}(S_t)$ is unknown, but we can approximate it by substituting $U_t$ (the $t$th training example) in place of $v_{\pi}(S_t)$. This yields the following general SGD method for state-value prediction:$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).$$If $U_t$ is an unbiased estimate, that is, if $\mathbb{E}[U_t]=v_{\pi}(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic approximation conditions for decreasing $\alpha$.For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy $\pi$. Because the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t \doteq G_t$ is by deﬁnition an unbiased estimate of $v_{\pi}(S_t)$. Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to ﬁnd a locally optimal solution. Pseudocode for a complete algorithmis shown in the box.Example: State Aggregation on the 1000-state Random WalkState aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its group’s component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD in which the gradient, $\nabla \hat{v}(S_t, \mathbf{w}_t)$, is 1 for $S_t$’s group’s component and 0 for the other components.Consider a 1000-state version of the random walk task. The states are numbered from 1 to 1000, left to right, and all episodes begin near the center, in state 500. State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. Of course, if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it. In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). As usual, termination on the left produces a reward of −1, and termination on the right produces a reward of +1. All other transitions have a reward of zero.Now, let us solve this problem by gradient Monte Carlo algorithm. First of all, we define the environment of this problem.12345678910111213141516171819202122# # of states except for terminal statesN_STATES = 1000# true state values, just a promising guesstrueStateValues = np.arange(-1001, 1003, 2) / 1001.0# all statesstates = np.arange(1, N_STATES + 1)# start from a central stateSTART_STATE = 500# terminal statesEND_STATES = [0, N_STATES + 1]# possible actionsACTION_LEFT = -1ACTION_RIGHT = 1ACTIONS = [ACTION_LEFT, ACTION_RIGHT]# maximum stride for an actionSTEP_RANGE = 100We need a true value of each state, thus use the dynamic programming to get these value:12345678910111213141516171819# Dynamic programming to find the true state values, based on the promising guess above# Assume all rewards are 0, given that we have already given value -1 and 1 to terminal stateswhile True: oldTrueStateValues = np.copy(trueStateValues) for state in states: trueStateValues[state] = 0 for action in ACTIONS: for step in range(1, STEP_RANGE + 1): step *= action newState = state + step newState = max(min(newState, N_STATES + 1), 0) # asynchronous update for faster convergence trueStateValues[state] += 1.0 / (2 * STEP_RANGE) * trueStateValues[newState] error = np.sum(np.abs(oldTrueStateValues - trueStateValues)) print(error) if error &lt; 1e-2: break# correct the state value for terminal states to 0trueStateValues[0] = trueStateValues[-1] = 0The policy of episodes generation:12345678910111213# take an @action at @state, return new state and reward for this transitiondef takeAction(state, action): step = np.random.randint(1, STEP_RANGE + 1) step *= action state += step state = max(min(state, N_STATES + 1), 0) if state == 0: reward = -1 elif state == N_STATES + 1: reward = 1 else: reward = 0 return state, rewardThe reward after take an action:12345# get an action, following random policydef getAction(): if np.random.binomial(1, 0.5) == 1: return 1 return -1And we have a special value function:1234567891011121314151617181920212223# a wrapper class for aggregation value functionclass ValueFunction: # @numOfGroups: # of aggregations def __init__(self, numOfGroups): self.numOfGroups = numOfGroups self.groupSize = N_STATES // numOfGroups # thetas self.params = np.zeros(numOfGroups) # get the value of @state def value(self, state): if state in END_STATES: return 0 groupIndex = (state - 1) // self.groupSize return self.params[groupIndex] # update parameters # @delta: step size * (target - old estimation) # @state: state of current sample def update(self, delta, state): groupIndex = (state - 1) // self.groupSize self.params[groupIndex] += deltaAnd the gradient MC algorithm:12345678910111213141516171819202122# gradient Monte Carlo algorithm# @valueFunction: an instance of class ValueFunction# @alpha: step size# @distribution: array to store the distribution statisticsdef gradientMonteCarlo(valueFunction, alpha, distribution=None): currentState = START_STATE trajectory = [currentState] # We assume gamma = 1, so return is just the same as the latest reward reward = 0.0 while currentState not in END_STATES: action = getAction() newState, reward = takeAction(currentState, action) trajectory.append(newState) currentState = newState # Gradient update for each state in this trajectory for state in trajectory[:-1]: delta = alpha * (reward - valueFunction.value(state)) valueFunction.update(delta, state) if distribution is not None: distribution[state] += 1Finally. let us solve this problem:12345678910111213141516171819202122232425nEpisodes = int(1e5)alpha = 2e-5# we have 10 aggregations in this example, each has 100 statesvalueFunction = ValueFunction(10)distribution = np.zeros(N_STATES + 2)for episode in range(0, nEpisodes): print('episode:', episode) gradientMonteCarlo(valueFunction, alpha, distribution)distribution /= np.sum(distribution)stateValues = [valueFunction.value(i) for i in states]plt.figure(0)plt.plot(states, stateValues, label='Approximate MC value')plt.plot(states, trueStateValues[1: -1], label='True value')plt.xlabel('State')plt.ylabel('Value')plt.legend()plt.figure(1)plt.plot(states, distribution[1: -1], label='State distribution')plt.xlabel('State')plt.ylabel('Distribution')plt.legend()Results are as follows:Semi-gradient MethodsBootstrapping target such as n-step returns $G_{t:t+n}$ or the DP target $\sum_{a, s^{\prime}, r} \pi(a|S_t)p(s^{\prime}, r|S_t, a)[r + \gamma \hat{v}(s^{\prime}, \mathbf{w}_t)]$ all depend on the current value of the weight vector $\mathbf{w}_t$, which implies that they will be biased and that they will not produce a true gradient=descent method. We call them semi-gradient methods.Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case. Moreover, they oﬀer important advantages which makes them often clearly preferred. One reason for this is that they are typically signiﬁcantly faster to learn. Another is that they enable learning to be continual and online, without waiting for the end of an episode. This enables them to be used on continuing problems and provides computational advantages. A prototypical semi-gradient method is semi-gradient TD(0), which uses $U_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})$ as its target. Complete pseudocode for this method is given in the box below.Linear MethodsOne of the most important special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot, \mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$. Corresponding to every state $s$, there is a real-valued vector of features $\mathbf{x}(s) \doteq (x_1(s),x_2(s),\cdots,x_d(s))^{\top}$, with the same number of components as $\mathbf{w}$. However the features are constructed, the approximate state-value function is given by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:$$\hat{v}(s, \mathbf{w}) \doteq \mathbf{w^{\top}x}(s) \doteq \sum_{i=1}^d w_i x_i(s).$$The individual functions $x_i:\mathcal{S} \rightarrow \mathbb{E}$ are called basis functions. The gradient of the approximate value function with respect to $\mathbf{w}$ in this case is$$\nabla \hat{v} (s, \mathbf{w}) = \mathbf{x}(s).$$The update at each time $t$ is$$\begin{align}\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t + \alpha \Big( R_{t+1} + \gamma \mathbf{w}_t^{\top}\mathbf{x}_{t+1} - \mathbf{w}_t^{\top}\mathbf{x}_{t}\Big)\mathbf{x}_t \\&amp;= \mathbf{w}_t + \alpha \Big( R_{t+1}\mathbf{x}_t - \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \mathbf{w}_t\Big),\end{align}$$where here we have used the notational shorthand $\mathbf{x}_t = \mathbf{x}(S_t)$. If the system converges, it must converge to the weight vector $\mathbf{w}_{TD}$ at which$$\mathbf{w}_{TD} \doteq \mathbf{A^{-1}b},$$where$$\mathbf{b} \doteq \mathbb{E}[R_{t+1}\mathbf{x}_t] \in \mathbb{R}^d \;\; \text{and} \;\; \mathbf{A} \doteq \mathbb{E}\big[ \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \big] \in \mathbb{R}^d \times \mathbb{R}^d.$$This quantity is called the TD fixedpoint. At this point we have:$$\text{MSVE}(\mathbf{w}_{TD}) \leq \frac{1}{1 - \gamma} \min_{\mathbf{w}} \text{MSVE}(\mathbf{w}).$$Now we use the state aggregation example again, but use the semi-gradient TD method.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# semi-gradient n-step TD algorithm# @valueFunction: an instance of class ValueFunction# @n: # of steps# @alpha: step sizedef semiGradientTemporalDifference(valueFunction, n, alpha): # initial starting state currentState = START_STATE # arrays to store states and rewards for an episode # space isn't a major consideration, so I didn't use the mod trick states = [currentState] rewards = [0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # choose an action randomly action = getAction() newState, reward = takeAction(currentState, action) # store new state and new reward states.append(newState) rewards.append(reward) if newState in END_STATES: T = time # get the time of the state to update updateTime = time - n if updateTime &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(updateTime + 1, min(T, updateTime + n) + 1): returns += rewards[t] # add state value to the return if updateTime + n &lt;= T: returns += valueFunction.value(states[updateTime + n]) stateToUpdate = states[updateTime] # update the value function if not stateToUpdate in END_STATES: delta = alpha * (returns - valueFunction.value(stateToUpdate)) valueFunction.update(delta, stateToUpdate) if updateTime == T - 1: break currentState = newState1234567891011121314nEpisodes = int(1e5) alpha = 2e-4 valueFunction = ValueFunction(10) for episode in range(0, nEpisodes): print('episode:', episode) semiGradientTemporalDifference(valueFunction, 1, alpha) stateValues = [valueFunction.value(i) for i in states] plt.figure(2) plt.plot(states, stateValues, label='Approximate TD value') plt.plot(states, trueStateValues[1: -1], label='True value') plt.xlabel('State') plt.ylabel('Value') plt.legend()Results are as follows:We also could use the n-step semi-gradient TD method. To obtain such quantitatively similar results we switched the state aggregation to 20 groups of 50 states each. The 20 groups are then quantitatively close to the 19 states of the tabular problem.The semi-gradient n-step TD algorithm we used in this example is the natural extension of the tabular n-step TD algorithm. The key equation is$$\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \Big[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1})\Big] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T,$$where$$G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T-n.$$Pseudocode for the complete algorithm is given in the box below.Now let us show the performance of different value of n:1234567891011121314151617181920212223242526272829303132333435363738# truncate value for better displaytruncateValue = 0.55# all possible stepssteps = np.power(2, np.arange(0, 10))# all possible alphasalphas = np.arange(0, 1.1, 0.1)# each run has 10 episodesepisodes = 10# perform 100 independent runsruns = 100# track the errors for each (step, alpha) combinationerrors = np.zeros((len(steps), len(alphas)))for run in range(0, runs): for stepInd, step in zip(range(len(steps)), steps): for alphaInd, alpha in zip(range(len(alphas)), alphas): print('run:', run, 'step:', step, 'alpha:', alpha) # we have 20 aggregations in this example valueFunction = ValueFunction(20) for ep in range(0, episodes): semiGradientTemporalDifference(valueFunction, step, alpha) # calculate the RMS error currentStateValues = np.asarray([valueFunction.value(i) for i in states]) errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - trueStateValues[1: -1], 2)) / N_STATES)# take averageerrors /= episodes * runs# truncate the errorerrors[errors &gt; truncateValue] = truncateValueplt.figure(3)for i in range(0, len(steps)): plt.plot(alphas, errors[i, :], label='n = ' + str(steps[i]))plt.xlabel('alpha')plt.ylabel('RMS error')plt.legend()The results are as follows:Feature Construction for Linear MethodsChoosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.PolynomialsFourier BasisKonidaris et al. (2011) found that when using Fourier cosine basis functions with a learning algorithm such as semi-gradient TD(0), or semi-gradient Sarsa, it is helpful to use a diﬀerent step-size parameter for each basis function. If $\alpha$ is the basic step-size parameter, they suggest setting the step-size parameter for basis function $x_i$ to $a_i=\alpha/\sqrt{(c_1^i)^2 + \cdots + (c_d^i)^2}$ (except when each $c_j^i=0$, in which case $\alpha_i=\alpha$).Now, let us we compare the Fourier and polynomial bases on the 1000-state random walk example. In general, we do not recommend using the polynomial basis for online learning.123456789101112131415161718192021222324252627282930313233# a wrapper class for polynomial / Fourier -based value functionPOLYNOMIAL_BASES = 0FOURIER_BASES = 1class BasesValueFunction: # @order: # of bases, each function also has one more constant parameter (called bias in machine learning) # @type: polynomial bases or Fourier bases def __init__(self, order, type): self.order = order self.weights = np.zeros(order + 1) # set up bases function self.bases = [] if type == POLYNOMIAL_BASES: for i in range(0, order + 1): self.bases.append(lambda s, i=i: pow(s, i)) elif type == FOURIER_BASES: for i in range(0, order + 1): self.bases.append(lambda s, i=i: np.cos(i * np.pi * s)) # get the value of @state def value(self, state): # map the state space into [0, 1] state /= float(N_STATES) # get the feature vector feature = np.asarray([func(state) for func in self.bases]) return np.dot(self.weights, feature) def update(self, delta, state): # map the state space into [0, 1] state /= float(N_STATES) # get derivative value derivativeValue = np.asarray([func(state) for func in self.bases]) self.weights += delta * derivativeValueThe function upper is used to construction the features of states (map states to features).Next, we will compare different super-parameters’ (order) performance:1234567891011121314151617181920212223242526272829303132333435363738runs = 1episodes = 5000# # of basesorders = [5, 10, 20]alphas = [1e-4, 5e-5]labels = [['polynomial basis'] * 3, ['fourier basis'] * 3]# track errors for each episodeerrors = np.zeros((len(alphas), len(orders), episodes))for run in range(0, runs): for i in range(0, len(orders)): valueFunctions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)] for j in range(0, len(valueFunctions)): for episode in range(0, episodes): print('run:', run, 'order:', orders[i], labels[j][i], 'episode:', episode) # gradient Monte Carlo algorithm gradientMonteCarlo(valueFunctions[j], alphas[j]) # get state values under current value function stateValues = [valueFunctions[j].value(state) for state in states] # get the root-mean-squared error errors[j, i, episode] += np.sqrt(np.mean(np.power(trueStateValues[1: -1] - stateValues, 2)))# average over independent runserrors /= runsplt.figure(5)for i in range(0, len(alphas)): for j in range(0, len(orders)): plt.plot(errors[i, j, :], label=labels[i][j]+' order = ' + str(orders[j]))plt.xlabel('Episodes')plt.ylabel('RMSVE')plt.legend()Results:TODO: TILE CODING]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[n-step TD]]></title>
      <url>%2F2017%2F07%2F04%2Fn-step-TD%2F</url>
      <content type="text"><![CDATA[In this post, we unify the Monte Carlo methods and the one-step TD methods. We first consider the prediction problem.n-step TD PredictionMonte Carlo methods preform a backup for each state based on the entire sequence of observed rewards from that state until the end of the episode. One-step TD methods is based on just on next reward. So n-step TD methods perform a backup based on an intermediate number of rewards: more than one, but less than all of them until termination.More formally, consider the backup applied to state $S_t$ as a result of the state-reward sequence, $S_t, R_{t+1},S_{t+1}, R_{t+2}, \cdots, R_T, S_T$ (omitting the actions for simplicity). We know that in Monte Carlo backups the estimate of $v_{\pi}(S_t)$ updated in the direction of the complete return:$$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T,$$where $T$ is the last time step of the episode. Let us call this quantity the target of the backup. Whereas in Monte Carlo backups the target is the return, in one-step backups the target is the first reward plus the discounted estimated value of the next state, which we call the one-step return:$$G_{t:t+1} \doteq R_{t+1} + \gamma V_t (S_{t+1}),$$where $V_t : \mathcal{S} \rightarrow \mathbb{R}$ here is an estimate at time $t$ of $v_{\pi}$. The subscripts on $G_{t:t+1}$ indicate that it is truncated return for time t using rewards up until time $t+1$. In the one-step return, $\gamma V_t (S_{t+1})$ takes the place of the other terms $ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$ of the full return. Our point now is that this idea makes just as much sense after two steps as it does after one. The target for a two-step backup is the two-step return:$$G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1} (S_{t+2}),$$where now $\gamma^2 V_{t+1}(S_{t+2})$ corrects for the absence of the terms $\gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$. Similarly, the target for an arbitrary n-step backup is the n-step return:$$G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}),$$for all $n,t$ such that $n \ge 1$ and $0 \leq t \leq T-n$. If $t+n \ge T$, then all the missing terms are taken as zero, and the n-step return defined to be equal to the ordinary full return.No real algorithm can use the n-step return until after it has seen $R_{t+n}$ and computed $V_{t+n-1}$. The first time these are available is $t+n$. The natural algorithm state-value learning algorithm for using n-step returns is thus$$V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)], \;\;\;\;\;\; 0 \leq t \leq T$$while the values of all other states remain unchanged. Note that no changes at all are made during the first $n-1$ steps of each episode. Complete pseudocode is given in the box below.The worst error of the expected n-step return is guaranteed to be less than or equal to $\gamma^n$ times the worst error under $V_{t+n-1}$:$$\max_s \left |\mathbb{E}[G_{t:t+1}|S_t=s] - v_{\pi}(s) \right | \leq \gamma^n \max_s |V_{t+n-1}(s) - v_{\pi}(s)|,$$for all $n \geq 1$. This is called the error reduction property of n-step returns. The n-step TD methods thus form a family of sound methods, with one-step TD methods and Monte Carlo methods as extreme members.Example: n-step TD Methods on the Random WalkNow we have a larger MDP (19 non-terminal states). First of all we need to define the new environment:1234567891011121314151617181920212223# all statesN_STATES = 19# discountGAMMA = 1# initial state valuesstateValues = np.zeros(N_STATES + 2)# all states but terminal statesstates = np.arange(1, N_STATES + 1)# start from the middle stateSTART_STATE = 10# two terminal states# an action leading to the left terminal state has reward -1# an action leading to the right terminal state has reward 1END_STATES = [0, N_STATES + 1]# true state value from bellman equationrealStateValues = np.arange(-20, 22, 2) / 20.0realStateValues[0] = realStateValues[-1] = 0And then develop the n-step TD algorithm:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# n-steps TD method# @stateValues: values for each state, will be updated# @n: # of steps# @alpha: # step sizedef temporalDifference(stateValues, n, alpha): # initial starting state currentState = START_STATE # arrays to store states and rewards for an episode # space isn't a major consideration, so I didn't use the mod trick states = [currentState] rewards = [0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # choose an action randomly if np.random.binomial(1, 0.5) == 1: newState = currentState + 1 else: newState = currentState - 1 if newState == 0: reward = -1 elif newState == 20: reward = 1 else: reward = 0 # store new state and new reward states.append(newState) rewards.append(reward) if newState in END_STATES: T = time # get the time of the state to update updateTime = time - n if updateTime &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(updateTime + 1, min(T, updateTime + n) + 1): returns += pow(GAMMA, t - updateTime - 1) * rewards[t] # add state value to the return if updateTime + n &lt;= T: returns += pow(GAMMA, n) * stateValues[states[(updateTime + n)]] stateToUpdate = states[updateTime] # update the state value if not stateToUpdate in END_STATES: stateValues[stateToUpdate] += alpha * (returns - stateValues[stateToUpdate]) if updateTime == T - 1: break currentState = newStateNow, let us test the performance under different $n$ values and $\alpha$ values:123456789101112131415161718192021222324252627282930313233343536# truncate value for better displaytruncateValue = 0.55# all possible stepssteps = np.power(2, np.arange(0, 10))# all possible alphasalphas = np.arange(0, 1.1, 0.1)# each run has 10 episodesepisodes = 10# perform 100 independent runsruns = 100# track the errors for each (step, alpha) combinationerrors = np.zeros((len(steps), len(alphas)))for run in range(0, runs): for stepInd, step in zip(range(len(steps)), steps): for alphaInd, alpha in zip(range(len(alphas)), alphas): print('run:', run, 'step:', step, 'alpha:', alpha) currentStateValues = np.copy(stateValues) for ep in range(0, episodes): temporalDifference(currentStateValues, step, alpha) # calculate the RMS error errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - realStateValues, 2)) / N_STATES)# take averageerrors /= episodes * runs# truncate the errorerrors[errors &gt; truncateValue] = truncateValueplt.figure()for i in range(0, len(steps)): plt.plot(alphas, errors[i, :], label='n = ' + str(steps[i]))plt.xlabel('alpha')plt.ylabel('RMS error')plt.legend()Results are as follows:TODO: N-STEP SARSATODO: N-STEP OFF-POLICY ALGORITHM]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Temporal-Difference Learning]]></title>
      <url>%2F2017%2F07%2F02%2FTemporal-Difference-Learning%2F</url>
      <content type="text"><![CDATA[If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome.TD(0)Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environment is$$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)],$$where $G_t$ is the actual return following time $t$. Let us call this method $constant\text{-}\alpha \ MC$. Notice that, if we are in a stationary environment (like earlier. For some reason, don’t use incremental implementation), the $\alpha$ is equals to $\frac{1}{N(S_t)}$. whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update$$V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]$$immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \gamma V(S_{t+1})$. This TD method is called $TD(0)$, or one-step TD. The box below specifies TD(0) completely in procedural form.TD(0)’s backup diagram is as follows:Because the TD(0) bases its update in part on an existing estimate, we say that it is a bootstrapping method, like DP. We know that$$\begin{align}v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [G_t \ | \ S_t=s] \\&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} \ | \ S_t=s] \\&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_t=s].\end{align}$$Roughly speaking, Monte Carlo methods use an estimate of (3) as a target, whereas DP methods use an estimate of (5) as a target, The Monte Carlo target is an estimate because the expected value in (3) is not known; a sample return is used in place of the real expected return. The DP target is an estimate not because of the excepted value, which are assumed to be completely provided by a model of the environment (the environment is known for the DP methods), but because $v_{\pi}(S_{t+1})$ is not known and the current estimate, $V(S_{t+1})$, is used instead. The TD target is an estimate for both reasons.Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$. This quantity, called the TD error, arises in various forms throughout reinforcement learning:$$\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t).$$Notice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and the next reward, it is not actually available until one time step later. Also note that if the array $V$ does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:$$\begin{align}G_t - V(S_t) &amp;= R_{t+1} + \gamma G(S_{t+1}) - V(S_t) + \gamma V(S_{t+1} ) - \gamma V(S_{t+1}) \\&amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(G_t-V(S_T)) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 -0) \\&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.\end{align}$$This identity is not exact if $V$ is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.Example: Random walkIn this example we empirically compare the prediction abilities of TD(0) and constant-$\alpha$ MC applied to the small Markov reward process shown in the upper part of the figure below. All episodes start in the center state, C, and the proceed either left or right by one state on each step, with equal probability. This behavior can be thought of as due to the combined effect of a fixed policy and an environment’s state-transition probabilities, but we do not care which; we are concerned only with predicting returns however they are generated. Episodes terminates on the right, a reward of +1 occurs; all other reward are zero. For example, a typical episode might consist of the following state-and-reward sequence: C, 0, B, 0, C, 0, D, 0, E, 1. Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state. Thus, the true value of the center state is $v_{\pi}(\text{C}) = 0.5$. The true values of all the states, A through E, are $\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}$, and $\frac{5}{6}$. In all cases the approximate value function was initialized to the intermediate value $V(s)=0.5$, for all $s$.Now, let us develop the codes to solve problem.The first, we initialize some truth.1234567891011121314151617# 0 is the left terminal state# 6 is the right terminal state# 1 ... 5 represents A ... Estates = np.zeros(7)states[1:6] = 0.5# For convenience, we assume all rewards are 0# and the left terminal state has value 0, the right terminal state has value 1# This trick has been used in Gambler's Problemstates[6] = 1# set up true state valuestrueValue = np.zeros(7)trueValue[1:6] = np.arange(1, 6) / 6.0trueValue[6] = 1ACTION_LEFT = 0ACTION_RIGHT = 1The below box is the TD(0) algorithm:1234567891011121314151617181920def temporalDifference(states, alpha=0.1, batch=False): state = 3 trajectory = [state] rewards = [0] while True: oldState = state if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 # Assume all rewards are 0 reward = 0 trajectory.append(state) # TD update if not batch: states[oldState] += alpha * (reward + states[state] - states[oldState]) if state == 6 or state == 0: break rewards.append(reward) return trajectory, rewardsAnd below box is the constant-$\alpha$ Monte Carlo algorithm:1234567891011121314151617181920212223def monteCarlo(states, alpha=0.1, batch=False): state = 3 trajectory = [3] # if end up with left terminal state, all returns are 0 # if end up with right terminal state, all returns are 1 returns = 0 while True: if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 trajectory.append(state) if state == 6: returns = 1.0 break elif state == 0: returns = 0.0 break if not batch: for state_ in trajectory[:-1]: # MC update states[state_] += alpha * (returns - states[state_]) return trajectory, [returns] * (len(trajectory) - 1)First of all, let us test the performance of the TD(0) algorithm:1234567891011121314def stateValue(): episodes = [0, 1, 10, 100] currentStates = np.copy(states) plt.figure(1) axisX = np.arange(0, 7) for i in range(0, episodes[-1] + 1): if i in episodes: plt.plot(axisX, currentStates, label=str(i) + ' episodes') temporalDifference(currentStates) plt.plot(axisX, trueValue, label='true values') plt.xlabel('state') plt.legend() stateValue()Results are as follows:And then let us show the RMS error of the TD(0) algorithm and constant-$\alpha$ Monte Carlo algorithm, for various $\alpha$ values:12345678910111213141516171819202122232425262728293031def RMSError(): # I'm lazy here, so do not let same alpha value appear in both arrays # For example, if in TD you want to use alpha = 0.2, then in MC you can use alpha = 0.201 TDAlpha = [0.15, 0.1, 0.05] MCAlpha = [0.01, 0.02, 0.03, 0.04] episodes = 100 + 1 runs = 100 plt.figure(2) axisX = np.arange(0, episodes) for alpha in TDAlpha + MCAlpha: totalErrors = np.zeros(episodes) if alpha in TDAlpha: method = 'TD' else: method = 'MC' for run in range(0, runs): errors = [] currentStates = np.copy(states) for i in range(0, episodes): errors.append(np.sqrt(np.sum(np.power(trueValue - currentStates, 2)) / 5.0)) if method == 'TD': temporalDifference(currentStates, alpha=alpha) else: monteCarlo(currentStates, alpha=alpha) totalErrors += np.asarray(errors) totalErrors /= runs plt.plot(axisX, totalErrors, label=method + ', alpha=' + str(alpha)) plt.xlabel('episodes') plt.legend() RMSError()Results are as follows:We can see, the TD method was consistently better than the MC method on this task.Now, suppose that there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning method is to present the experience repeatedly until the method converges upon an answer. We call this batch updating.Example: Random walk under batch updatingAfter each new episodes, all episodes seen so far were treated as a batch. They were repeatedly presented to the algorithm.12345678910111213141516171819202122232425262728293031323334353637def batchUpdating(method, episodes, alpha=0.001): # perform 100 independent runs runs = 100 totalErrors = np.zeros(episodes - 1) for run in range(0, runs): currentStates = np.copy(states) errors = [] # track shown trajectories and reward/return sequences trajectories = [] rewards = [] for ep in range(1, episodes): print('Run:', run, 'Episode:', ep) if method == 'TD': trajectory_, rewards_ = temporalDifference(currentStates, batch=True) else: trajectory_, rewards_ = monteCarlo(currentStates, batch=True) trajectories.append(trajectory_) rewards.append(rewards_) while True: # keep feeding our algorithm with trajectories seen so far until state value function converges updates = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + currentStates[trajectory_[i + 1]] - currentStates[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # perform batch updating currentStates += updates # calculate rms error errors.append(np.sqrt(np.sum(np.power(currentStates - trueValue, 2)) / 5.0)) totalErrors += np.asarray(errors) totalErrors /= runs return totalErrorsNotice that the core codes:12345678910111213141516while True: # keep feeding our algorithm with trajectories seen so far until state # value function converges updates = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + \ currentStates[trajectory_[i + 1]] - currentStates[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # perform batch updating currentStates += updatesEither TD methods or MC methods, the target is to minimize the TD error (or MC error, I say).The result is as follows:Under batch training, constant-$\alpha$ MC converges to value, $V(s)$, that are sample averages of the actual returns experienced after visiting each state $s$. These are optimal estimate in the sense that they minimize the mean-squared error from the actual returns in the training set. In this sense it is surprising that the batch TD method was able to perform better according to the root mean-squared error measure shown in the top figure. How is it that batch TD was able to perform better than this optimal methods? Consider the example in below box:Example illustrates a general difference between the estimates founds by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the certainty-equivalence estimate.Sarsa$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\right]$$This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state-action pair to the next. This quintuple gives rise to the name Sarsa for the algorithm. The backup diagram for Sarsa is as shown to the bottom.The general form of the Sarsa control algorithm is given in the box below.Example: Windy GridworldThe figure below is a standard grid-world, with start and goal states, but with one diﬀerence: there is a crosswind upward through the middle of the grid. The actions are the standard four—up, down,right, and left—but in the middle region the resultant next states are shifted upward by a “wind,” the strength of which varies from column to column. The strength of the wind is given below each column, in number of cells shifted upward. For example, if you are one cell to the right of the goal, then the action left takes you to the cell just above the goal. Let us treat this as an undiscounted episodic task, with constant rewards of −1 until the goal state is reached.To demonstrate the problem clearly, we use the OpenAI gym toolkit to develop the algorithm.First of all, we need to define a environment (the windy grid world):12345# represents every action as a integerUP = 0RIGHT = 1DOWN = 2LEFT = 3The environment is a class that inherit the gym default class discrete.DiscreteEnv (shows that the states are discrete):1class WindyGridworldEnv(discrete.DiscreteEnv)First we need to construct our world:1234567891011121314151617181920212223242526272829def __init__(self): self.shape = (7, 10) # the number of all states nS = np.prod(self.shape) # the number of all actions nA = 4 # Wind strength winds = np.zeros(self.shape) winds[:,[3,4,5,8]] = 1 winds[:,[6,7]] = 2 # Calculate transition probabilities # P is the transition matrix P = &#123;&#125; for s in range(nS): position = np.unravel_index(s, self.shape) P[s] = &#123; a : [] for a in range(nA) &#125; P[s][UP] = self._calculate_transition_prob(position, [-1, 0], winds) P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1], winds) P[s][DOWN] = self._calculate_transition_prob(position, [1, 0], winds) P[s][LEFT] = self._calculate_transition_prob(position, [0, -1], winds) # We always start in state (3, 0) isd = np.zeros(nS) isd[np.ravel_multi_index((3,0), self.shape)] = 1.0 super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)This is natural, uh? Notice that there is a method called _calculate_transition_prob:123456def _calculate_transition_prob(self, current, delta, winds): new_position = np.array(current) + np.array(delta) + np.array([-1, 0]) * winds[tuple(current)] new_position = self._limit_coordinates(new_position).astype(int) new_state = np.ravel_multi_index(tuple(new_position), self.shape) is_done = tuple(new_position) == (3, 7) return [(1.0, new_state, -1.0, is_done)]and _limit_corrdinates method:123456def _limit_coordinates(self, coord): coord[0] = min(coord[0], self.shape[0] - 1) coord[0] = max(coord[0], 0) coord[1] = min(coord[1], self.shape[1] - 1) coord[1] = max(coord[1], 0) return coordIt is worth to mention that the default gym environment class has some useful parameters: nS, nA, P and is_done. nS is the total number of states and nA is the total number of actions (here assume all states only could take the same fixed actions). P is the state transition matrix, the default environment class has a step method (accept a parameter action) that could generates episode automatically according the P and is_done that represents whether a state is terminal state or not.Finally, we define a output method for pretty show the result:123456789101112131415161718192021222324def _render(self, mode='human', close=False): if close: return outfile = StringIO() if mode == 'ansi' else sys.stdout for s in range(self.nS): position = np.unravel_index(s, self.shape) # print(self.s) if self.s == s: output = " x " elif position == (3,7): output = " T " else: output = " o " if position[1] == 0: output = output.lstrip() if position[1] == self.shape[1] - 1: output = output.rstrip() output += "\n" outfile.write(output) outfile.write("\n")Then, let us test our model：12345678910111213141516171819202122env = WindyGridworldEnv()print(env.reset())env.render()print(env.step(1))env.render()print(env.step(1))env.render()print(env.step(1))env.render()print(env.step(2))env.render()print(env.step(1))env.render()print(env.step(1))env.render()The results are as follows:Each state transition, the step method return a tuple (next_state, reward, is_done, some_extra_info).Next, we define the episodes generation policy:def make_epsilon_greedy_policy(Q, epsilon, nA):1234567891011121314151617181920"""Creates an epsilon-greedy policy based on a given Q-function and epsilon.Args: Q: A dictionary that maps from state -&gt; action-values. Each value is a numpy array of length nA (see below) epsilon: The probability to select a random action . float between 0 and 1. nA: Number of actions in the environment.Returns: A function that takes the observation as an argument and returns the probabilities for each action in the form of a numpy array of length nA."""def policy_fn(observation): A = np.ones(nA, dtype=float) * epsilon / nA best_action = np.argmax(Q[observation]) A[best_action] += (1.0 - epsilon) return Areturn policy_fnNow, let us implement the sarsa algorithm:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1): """ SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy. Args: env: OpenAI environment. num_episodes: Number of episodes to run for. discount_factor: Lambda time discount factor. alpha: TD learning rate. epsilon: Chance the sample a random action. Float betwen 0 and 1. Returns: A tuple (Q, stats). Q is the optimal action-value function, a dictionary mapping state -&gt; action values. stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ # The final action-value function. # A nested dictionary that maps state -&gt; (action -&gt; action-value). Q = defaultdict(lambda: np.zeros(env.action_space.n)) # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) # The policy we're following policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n) for i_episode in range(num_episodes): # Print out which episode we're on, useful for debugging. if (i_episode + 1) % 100 == 0: print("\rEpisode &#123;&#125;/&#123;&#125;.".format(i_episode + 1, num_episodes), end="") sys.stdout.flush() # Implement this! state = env.reset() action_probs = policy(state) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) for t in itertools.count(): next_state, reward, is_done, _ = env.step(action) next_action_probs = policy(next_state) stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs) Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action]) if is_done: break state = next_state action = next_action return Q, statsFor understand easily, we put the pesudo-code here again:The results (with $\varepsilon=0.1,\ \alpha=0.5$) are as follows:The increasing slope (bottom figure) of the graph shows that the goal is reached more and more quickly over time. Note that Monte Carlo methods cannot easily be used on this task because termination is not guaranteed for all policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Step-by-step learning methods such as Sarsa do not have this problem because they quickly learn during the episode that suchpolicies are poor, and switch to something else.Q-learningOne of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\right]$$The algorithm is shown in procedural form in the box below:And below is the backup diagram:Example: Cliff WalkingThis grid world example compares Sarsa and Q-learning, highlighting the difference between on-policy (Sarsa) and off-policy (Q-learning) methods. Consider the grid world shown in the figure below:The same as earlier, we define the environment first. But the new environment just changes a little, so we just paste the code here.Let us test the environment first:12345678910111213141516env = CliffWalkingEnv()print(env.reset())env.render()print(env.step(0))env.render()print(env.step(1))env.render()print(env.step(1))env.render()print(env.step(2))env.render()Not bad.Then, let us develop the Q-learning algorithm (the episodes generation policy is not change):12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1): """ Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy while following an epsilon-greedy policy Args: env: OpenAI environment. num_episodes: Number of episodes to run for. discount_factor: Lambda time discount factor. alpha: TD learning rate. epsilon: Chance the sample a random action. Float betwen 0 and 1. Returns: A tuple (Q, episode_lengths). Q is the optimal action-value function, a dictionary mapping state -&gt; action values. stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ # The final action-value function. # A nested dictionary that maps state -&gt; (action -&gt; action-value). Q = defaultdict(lambda: np.zeros(env.action_space.n)) # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) # The policy we're following policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n) for i_episode in range(num_episodes): # Print out which episode we're on, useful for debugging. if (i_episode + 1) % 100 == 0: print("\rEpisode &#123;&#125;/&#123;&#125;.".format(i_episode + 1, num_episodes), end="") sys.stdout.flush() # Reset the environment and pick the first action state = env.reset() # One step in the environment # total_reward = 0.0 for t in itertools.count(): # Take a step action_probs = policy(state) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) next_state, reward, done, _ = env.step(action) # Update statistics stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t # TD Update best_next_action = np.argmax(Q[next_state]) td_target = reward + discount_factor * Q[next_state][best_next_action] td_delta = td_target - Q[state][action] Q[state][action] += alpha * td_delta if done: break state = next_state return Q, statsResults ($\varepsilon=0.1$) are as follows:For compare convenience, we put the result of Sarsa here again:We can see, for average, After an initial transient, Q-learning learns values for the optimal policy, that which travels right along the edge of the cliﬀ. Unfortunately, this results in its occasionally falling oﬀ the cliﬀ because of the ε-greedy action selection. Sarsa, on the other hand, takes the action selection into account and learns the longer but safer path through the upper part of thegrid. Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of Sarsa, which learns the roundabout policy. Of course, if ε were gradually reduced, then both methods would asymptotically converge to the optimal policy.Expected SarsaConsider the learning algorithm that is just like Q-learning except that instead of the maximum over next state-action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule$$\begin{align}Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \mathbb{E}[Q(S_{t+1}, A_{t+1} \ | \ S_{t+1})] - Q(S_t, A_t) \right ] \\&amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t) \right ],\end{align}$$but that otherwise follows the schema of Q-learning. Its backup diagram is shown below:For compare the results on the cliff-walking task with Excepted Sarsa with Sarsa and Q-learning, we develop another codes (here we are not use the OpenAI gym toolkit).The first we define some truth of the environment:1234567891011121314151617181920212223242526272829303132# world heightWORLD_HEIGHT = 4# world widthWORLD_WIDTH = 12# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.5# gamma for Q-Learning and Expected SarsaGAMMA = 1# all possible actionsACTION_UP = 0ACTION_DOWN = 1ACTION_LEFT = 2ACTION_RIGHT = 3actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]# initial state action pair valuesstateActionValues = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))startState = [3, 0]goalState = [3, 11]# reward for each action in each stateactionRewards = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))actionRewards[:, :, :] = -1.0actionRewards[2, 1:11, ACTION_DOWN] = -100.0actionRewards[3, 0, ACTION_RIGHT] = -100.0And then we define the state transitions:123456789101112131415# set up destinations for each action in each stateactionDestination = []for i in range(0, WORLD_HEIGHT): actionDestination.append([]) for j in range(0, WORLD_WIDTH): destinaion = dict() destinaion[ACTION_UP] = [max(i - 1, 0), j] destinaion[ACTION_LEFT] = [i, max(j - 1, 0)] destinaion[ACTION_RIGHT] = [i, min(j + 1, WORLD_WIDTH - 1)] if i == 2 and 1 &lt;= j &lt;= 10: destinaion[ACTION_DOWN] = startState else: destinaion[ACTION_DOWN] = [min(i + 1, WORLD_HEIGHT - 1), j] actionDestination[-1].append(destinaion)actionDestination[3][0][ACTION_RIGHT] = startStateWe also need a policy to generate the next action according to the current state:123456# choose an action based on epsilon greedy algorithmdef chooseAction(state, stateActionValues): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(actions) else: return np.argmax(stateActionValues[state[0], state[1], :])The stateActionValues just is the Q.Then, let us develop the Sarsa (and Excepted Sarsa) algorithm:123456789101112131415161718192021222324252627282930313233# an episode with Sarsa# @stateActionValues: values for state action pair, will be updated# @expected: if True, will use expected Sarsa algorithm# @stepSize: step size for updating# @return: total rewards within this episodedef sarsa(stateActionValues, expected=False, stepSize=ALPHA): currentState = startState currentAction = chooseAction(currentState, stateActionValues) rewards = 0.0 while currentState != goalState: newState = actionDestination[currentState[0]][currentState[1]][currentAction] newAction = chooseAction(newState, stateActionValues) reward = actionRewards[currentState[0], currentState[1], currentAction] rewards += reward if not expected: valueTarget = stateActionValues[newState[0], newState[1], newAction] else: # calculate the expected value of new state valueTarget = 0.0 actions_list = stateActionValues[newState[0], newState[1], :] bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist() for action in actions: if action in bestActions: valueTarget += ((1.0 - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[0], newState[1], action] else: valueTarget += EPSILON / len(actions) * stateActionValues[newState[0], newState[1], action] valueTarget *= GAMMA # Sarsa update stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward + valueTarget - stateActionValues[currentState[0], currentState[1], currentAction]) currentState = newState currentAction = newAction return rewardsBecause we develop the Sarsa algorithm earlier, so we just concentrate on the Excepted Sarsa algorithm here:123456789# calculate the expected value of new statevalueTarget = 0.0actions_list = stateActionValues[newState[0], newState[1], :]bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()for action in actions: if action in bestActions: valueTarget += ((1.0 - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[0], newState[1], action] else: valueTarget += EPSILON / len(actions) * stateActionValues[newState[0], newState[1], action]By the way, let us develop the Q-learning algorithm again:12345678910111213141516171819# an episode with Q-Learning# @stateActionValues: values for state action pair, will be updated# @expected: if True, will use expected Sarsa algorithm# @stepSize: step size for updating# @return: total rewards within this episodedef qLearning(stateActionValues, stepSize=ALPHA): currentState = startState rewards = 0.0 while currentState != goalState: currentAction = chooseAction(currentState, stateActionValues) reward = actionRewards[currentState[0], currentState[1], currentAction] rewards += reward newState = actionDestination[currentState[0]][currentState[1]][currentAction] # Q-Learning update stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * ( reward + GAMMA * np.max(stateActionValues[newState[0], newState[1], :]) - stateActionValues[currentState[0], currentState[1], currentAction]) currentState = newState return rewardsNow we can see the optimal policy in each state of both algorithm (we are not mentioned earlier):1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# print optimal policydef printOptimalPolicy(stateActionValues): optimalPolicy = [] for i in range(0, WORLD_HEIGHT): optimalPolicy.append([]) for j in range(0, WORLD_WIDTH): if [i, j] == goalState: optimalPolicy[-1].append('G') continue bestAction = np.argmax(stateActionValues[i, j, :]) if bestAction == ACTION_UP: optimalPolicy[-1].append('U') elif bestAction == ACTION_DOWN: optimalPolicy[-1].append('D') elif bestAction == ACTION_LEFT: optimalPolicy[-1].append('L') elif bestAction == ACTION_RIGHT: optimalPolicy[-1].append('R') for row in optimalPolicy: print(row)# averaging the reward sums from 10 successive episodesaverageRange = 10# episodes of each runnEpisodes = 500# perform 20 independent runsruns = 20rewardsSarsa = np.zeros(nEpisodes)rewardsQLearning = np.zeros(nEpisodes)for run in range(0, runs): stateActionValuesSarsa = np.copy(stateActionValues) stateActionValuesQLearning = np.copy(stateActionValues) for i in range(0, nEpisodes): # cut off the value by -100 to draw the figure more elegantly rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), -100) rewardsQLearning[i] += max(qLearning(stateActionValuesQLearning), -100)# averaging over independt runsrewardsSarsa /= runsrewardsQLearning /= runs# averaging over successive episodessmoothedRewardsSarsa = np.copy(rewardsSarsa)smoothedRewardsQLearning = np.copy(rewardsQLearning)for i in range(averageRange, nEpisodes): smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + 1]) smoothedRewardsQLearning[i] = np.mean(rewardsQLearning[i - averageRange: i + 1])# display optimal policyprint('Sarsa Optimal Policy:')printOptimalPolicy(stateActionValuesSarsa)print('Q-Learning Optimal Policy:')printOptimalPolicy(stateActionValuesQLearning)The results are as follows (emits the results of the changes of reward):Now let us compare the three algorithms:123456789101112131415161718192021222324252627282930313233343536373839404142stepSizes = np.arange(0.1, 1.1, 0.1) nEpisodes = 1000 runs = 10 ASY_SARSA = 0 ASY_EXPECTED_SARSA = 1 ASY_QLEARNING = 2 INT_SARSA = 3 INT_EXPECTED_SARSA = 4 INT_QLEARNING = 5 methods = range(0, 6) performace = np.zeros((6, len(stepSizes))) for run in range(0, runs): for ind, stepSize in zip(range(0, len(stepSizes)), stepSizes): stateActionValuesSarsa = np.copy(stateActionValues) stateActionValuesExpectedSarsa = np.copy(stateActionValues) stateActionValuesQLearning = np.copy(stateActionValues) for ep in range(0, nEpisodes): print('run:', run, 'step size:', stepSize, 'episode:', ep) sarsaReward = sarsa(stateActionValuesSarsa, expected=False, stepSize=stepSize) expectedSarsaReward = sarsa(stateActionValuesExpectedSarsa, expected=True, stepSize=stepSize) qLearningReward = qLearning(stateActionValuesQLearning, stepSize=stepSize) performace[ASY_SARSA, ind] += sarsaReward performace[ASY_EXPECTED_SARSA, ind] += expectedSarsaReward performace[ASY_QLEARNING, ind] += qLearningReward if ep &lt; 100: performace[INT_SARSA, ind] += sarsaReward performace[INT_EXPECTED_SARSA, ind] += expectedSarsaReward performace[INT_QLEARNING, ind] += qLearningReward performace[:3, :] /= nEpisodes * runs performace[3:, :] /= runs * 100 labels = ['Asymptotic Sarsa', 'Asymptotic Expected Sarsa', 'Asymptotic Q-Learning', 'Interim Sarsa', 'Interim Expected Sarsa', 'Interim Q-Learning'] plt.figure(2) for method, label in zip(methods, labels): plt.plot(stepSizes, performace[method, :], label=label) plt.xlabel('alpha') plt.ylabel('reward per episode') plt.legend()The results are as follows:As an on-policy method, Expected Sarsa retains the signiﬁcant advantage of Sarsa over Q-learning on this problem. In addition, Expected Sarsa shows a signiﬁcant improvement over Sarsa over a wide range of values for the step-size parameter α. In cliﬀ walking the state transitions are all deterministic and all randomness comes from the policy. In such cases, Expected Sarsa can safely set α = 1 without suﬀering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of α, at which short-term performance is poor. In this and other examples there is a consistent empirical advantage of Expected Sarsa over Sarsa. Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms.Double Q-learningAll the control algorithms that we have discussed so far involve maximization in the construction of their target policies. For example, in Q-learning the target policy is the greedy policy given the current action values, which is deﬁned with a max, and in Sarsa the policy is often ε-greedy, which also involves a maximization operation. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a signiﬁcant positive bias. To see why, consider a single state s where there are many actions a whose true values, $q(s, a)$ are all zero but whose estimated values, $Q(s, a)$, are uncertain and thus distributed some above and some below zero. The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias. We call this maximizationbias.Example: Maximization BiasWe have a small MDP:the expected return for any trajectory starting with left (from B) is −0.1, and thus taking left in state A is always a mistake. Nevertheless, our control methods may favor left because of maximization bias making B appear to have a positive value. The results (paste later) shows that Q-learning with ε-greedy action selection initially learns to strongly favor the left action on this example. Even at asymptote, Q-learning takes the left action about 5% more often than is optimal at our parameter settings (ε = 0.1, α = 0.1, and γ = 1).We could use the Double Q-learning algorithm to avoid this problem. One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates.Of course there are also doubled versions of Sarsa and Expected Sarsa.Now let us develop the both algorithms and compare their performance on the earlier example. First we define the problem environment:123456789101112131415161718192021222324252627282930313233343536# state ASTATE_A = 0# state BSTATE_B = 1# use one terminal stateSTATE_TERMINAL = 2# starts from state ASTATE_START = STATE_A# possible actions in AACTION_A_RIGHT = 0ACTION_A_LEFT = 1# possible actions in B, maybe 10 actionsactionsOfB = range(0, 10)# all possible actionsstateActions = [[ACTION_A_RIGHT, ACTION_A_LEFT], actionsOfB]# state action pair values, if a state is a terminal state, then the value is always 0stateActionValues = [np.zeros(2), np.zeros(len(actionsOfB)), np.zeros(1)]# set up destination for each state and each actionactionDestination = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(actionsOfB)]# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.1# discount for max valueGAMMA = 1.0And we need a policy to take an action:123456# choose an action based on epsilon greedy algorithmdef chooseAction(state, stateActionValues): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(stateActions[state]) else: return argmax(stateActionValues[state])After take an action, we get the reward:12345# take @action in @state, return the rewarddef takeAction(state, action): if state == STATE_A: return 0 return np.random.normal(-0.1, 1)Next, we develop the Double Q-learning algorithm:12345678910111213141516171819202122232425262728293031323334# if there are two state action pair value array, use double Q-Learning# otherwise use normal Q-Learningdef qLearning(stateActionValues, stateActionValues2=None): currentState = STATE_START # track the # of action left in state A leftCount = 0 while currentState != STATE_TERMINAL: if stateActionValues2 is None: currentAction = chooseAction(currentState, stateActionValues) else: # derive a action form Q1 and Q2 currentAction = chooseAction(currentState, [item1 + item2 for item1, item2 in zip(stateActionValues, stateActionValues2)]) if currentState == STATE_A and currentAction == ACTION_A_LEFT: leftCount += 1 reward = takeAction(currentState, currentAction) newState = actionDestination[currentState][currentAction] if stateActionValues2 is None: currentStateActionValues = stateActionValues targetValue = np.max(currentStateActionValues[newState]) else: if np.random.binomial(1, 0.5) == 1: currentStateActionValues = stateActionValues anotherStateActionValues = stateActionValues2 else: currentStateActionValues = stateActionValues2 anotherStateActionValues = stateActionValues bestAction = argmax(currentStateActionValues[newState]) targetValue = anotherStateActionValues[newState][bestAction] # Q-Learning update currentStateActionValues[currentState][currentAction] += ALPHA * ( reward + GAMMA * targetValue - currentStateActionValues[currentState][currentAction]) currentState = newState return leftCountAnd now, let us solve the example problem:12345678910111213141516171819202122232425262728# each independent run has 300 episodes episodes = 300 leftCountsQ = np.zeros(episodes) leftCountsDoubleQ = np.zeros(episodes) runs = 1000 for run in range(0, runs): print('run:', run) stateActionValuesQ = [np.copy(item) for item in stateActionValues] stateActionValuesDoubleQ1 = [np.copy(item) for item in stateActionValues] stateActionValuesDoubleQ2 = [np.copy(item) for item in stateActionValues] leftCountsQ_ = [0] leftCountsDoubleQ_ = [0] for ep in range(0, episodes): leftCountsQ_.append(leftCountsQ_[-1] + qLearning(stateActionValuesQ)) leftCountsDoubleQ_.append(leftCountsDoubleQ_[-1] + qLearning(stateActionValuesDoubleQ1, stateActionValuesDoubleQ2)) del leftCountsQ_[0] del leftCountsDoubleQ_[0] leftCountsQ += np.asarray(leftCountsQ_, dtype='float') / np.arange(1, episodes + 1) leftCountsDoubleQ += np.asarray(leftCountsDoubleQ_, dtype='float') / np.arange(1, episodes + 1) leftCountsQ /= runs leftCountsDoubleQ /= runs plt.figure() plt.plot(leftCountsQ, label='Q-Learning') plt.plot(leftCountsDoubleQ, label='Double Q-Learning') plt.plot(np.ones(episodes) * 0.05, label='Optimal') plt.xlabel('episodes') plt.ylabel('% left actions from A') plt.legend()Ok, results are as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning Resources]]></title>
      <url>%2F2017%2F06%2F30%2FReinforcement-Learning-Resources%2F</url>
      <content type="text"><![CDATA[BooksSutton’s book has new update (draft, version 2017) !Algorithms for Reinforcement Learning (Morgan)PapersReinforcement LearningDeep Reinforcement Learning with Double Q-learningSummaryProjectPrioritized Experience ReplaySummaryDueling Network Architectures for Deep Reinforcement LearningSummaryProjectLearning Tetris Using the Noisy Cross-Entropy MethodSummaryProjectDoubly Robust Off-policy Value Evaluation for Reinforcement LearningSummaryContinuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning ApproachSummaryA Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care UnitsSummaryDeep Reinforcement Learning, Decision Making and Control (ICML 2017 Tutorial)SummaryMaximum Entropy Deep Inverse Reinforcement LearningSummaryMaximum Entropy Inverse Reinforcement LearningSummaryApprenticeship Learning via Inverse Reinforcement LearningSummaryDeep LearningCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep LearningSummary​ProjectsUsing Keras and Deep Q-Network to Play FlappyBirdBlogsDemystifying Deep Reinforcement Learning]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A simple AI car]]></title>
      <url>%2F2017%2F06%2F27%2FA-simple-AI-car%2F</url>
      <content type="text"><![CDATA[I. 定义项目概述项目地址：https://github.com/ewanlee/rl_car最近，自动驾驶汽车十分火热。但是，自动驾驶问题是一个机器学习集大成的问题，十分的复杂。因此，我们希望可以设计出一个简单的学习环境能够对自动驾驶问题进行模拟，并且不需要GPU （主要是太贵）。我们的学习环境借鉴了Matt Harvey’s virtual car[1] 的环境设置。运用了 TensorFlow， Python 2.7 以及 PyGame 5.0. 本项目中运用了深度Q强化学习算法，但是为了符合我们上面提到的要求，我们去掉了该算法中 “深度” 的部分。代码设计的一些思想借鉴了 songotrek’s Q学习算法的TensorFlow实现 [2].问题描述图片来源于[1]我们所要解决的问题就是设计一个算法使得模拟小车能够自动行驶。上图就是我们实验用的环境。可以看出，它足够简单，但是足够进行一些强化学习算法的验证。最小的圆圈是我们模拟的小车，它拥有三个声纳感应器 （三条白色的虚线）。三个较大的圆圈代表障碍物，它会随着时间的变化缓慢移动。左上角的圆圈代表一只在环境中游走（速度相比于障碍物要快很多）的猫。圆圈上的缺口表示朝向。我们所要解决的问题就是希望小车可以尽可能长时间的运动，但不会撞到障碍物或者猫。环境需求Anaconda Python Distribution 2.7 [3]TensorFlow for Anaconda [4]PyGame [5]，用于展示图形界面PyMunk [6]，为了模拟游戏中的物理环境Numpy [7]Scipy [8]实验运行的环境为 Ubuntu 16.04 LTS 虚拟机， 虚拟机为VMware Workstation 12.5.2 build-4638234。虚拟机运行在Windows 10 Pro上。性能度量我们的baseline是一个随机（行为随机选择）小车，最后的评价指标是我们定义的指标score，代表小车存活的时间（在游戏中代表小车存活的frame）。并且，score是进行1000次实验的平均值。优化目标我们使用的是 Deep Q Learning [9] 论文中定义的 QMax 值。QMax 值指的是在一定时间范围内，对于所有的训练样本，Q 函数（使用神经网络进行拟合）输出的最大的 Q-value。随着agent（模拟小车）不断进行学习，它将采取更加优秀的策略，因此存活时间会更长，那么 Q-value (在我们的实验中便是score) 会越大。如果我们的优化目标是增大 Q-value 的上界，也便相应的增大了 Q-value 值。学习过程监测我们使用的是Tensorflow自带的TensorBoard来监测QMax以及最大score的变化情况（希望整体趋势是逐渐增大的）下面是运行过程中的截图：下面是各网络参数的分布变化情况：分析数据由于强化学习任务的数据集一般都是实验中产生的，因此不需要收集数据。在每一次迭代过程中，模拟环境提供以下数据（自己设计的）：s1, s2, s3 三个声纳传感器的数值，范围是[0, 40]，整数值，代表三个方向上障碍物的距离。范围确定为这样的原因是，为了检测障碍物，声纳传感器从源头开始，逐渐往外探测，每向外探测一次，距离就加1（可以看成虚线的点数，即虚线是由多少个点组成的）。x 代表x轴的位置，范围是[0, 1]y 代表y轴的位置，范围是[0, 1]theta 代表小车的方向，弧度表示，范围是[0, 2$\pi$]小车能够采取的动作如下：0，代表直走1， 往左转0.2弧度2， 往右转0.2弧度小车每进行一次动作会使得状态发生变化，并且有以下返回值：Reward，一个[-100, 10]之间的整数，负数代表动作产生的结果不好，正数则相反Termianl，布尔型数据，代表小车是否存活（是否撞到障碍物）我们和原始模型[1]不同的是，输了$s_1, s_2, s_3$三个特征之外，额外增加了$x, y, theta$三个特征。因为我们希望小车能够尽可能往地图中间运行，远离墙壁。并且当它们靠近障碍物时，能够选择更加合理的方向躲避。值得说明的一点是，小车如何检测是否撞到障碍物的问题。实验中使用的方法是检测声纳传感器的数值，如果数值是1（而不是0）就认为小车撞上了障碍物，并给出一个-100的reward。此时实验将重新开始，小车位置的选择是根据物理定律模拟的，即根据碰撞的角度给小车一个反向的速度，并且小车的朝向随机变化。这样模拟出一种碰撞后的混乱状态。算法下面介绍Deep Q-learning算法。以上的实验环境可以形式化的表述，如上图所示。我们拥有一个agent（小车），在时间$t$时必须要选择一个动作$a_t$。Agent采取动作与环境进行交互，使得在时间$t+1$时状态变为$s_{t+1}$。同时agent接收到环境给它的一个反馈$r_t$。这样agent就根据$(s_t, a_t, s_{t+1}, r_t)$来决定采取的动作$a_{t+1}$是什么。整个问题就是不断重复上述过程直到到达某个结束条件。机器学习领域将这个问题称为强化学习问题。每一个动作通过reward被 “强化”，使得agent不断接近我们期望它到达的状态。但是在强化学习中存在一个reward延迟的问题，也就是说，某一个action的回报可能不是即时的，需要很多时间步之后才能确定。举个例子，下棋的过程中需要布局，但是这个布局并不会马上给你带来好处，需要在以后的某个特定时间，你的对上掉入了你很久前设置的陷阱里，这时候才给你带来好处。所以，我们需要采用一种方式来对这个问题进行建模。我们定义一个价值函数$Q(s_t, a_t)$，它表示在状态$s_t$是采取$a_t$这个动作带来的 “价值”，而不是reward，reward是即时的，但是价值是若干时间步带来的reward的某种综合考量，更具实际意义。那么接下来的问题就是价值函数应当如何定义。最直观的想法就是，我们可以把强化学习问题定义为一个动态规划的问题。这里我直接列出公式，也就是非常著名的贝尔曼方程（Bellman equation）：可以看到，解决强化学习问题是一个不断迭代的过程，那么如何初始化Q非常重要。但实际上，如果迭代次数趋紧无穷大时，Q的初始值对于最终的结果并没有影响，因此一般来说只要初始化为均值为0的高斯白噪音。对于小规模的强化学习问题，由于状态的Q值随着迭代次数的增加会不断更新，那么我们需要一个地方来存储这些值。传统的强化学习算法一般采用一张表格（数组或字典）来存储这些值。但是随着问题规模的增大，状态会显著增加。对于我们的问题，状态空间更是无限的，因为状态是由浮点数组成的。这样我们就不可能把这些状态对应的Q值都存储下来。我们采用一个如下所示的神经网络来代替这些表格，即找出状态和Q值之间的一个映射。这里值得说明的是，网络输出的是所有动作对应的Q值，这是Deep Q-learning算法的一个创新点。在我们的实验中，输入维度是6维（$s_1, s_2, s_3, x, y, theta$），输出是3维（对应三个动作0， 1， 2）。我们采用白噪音来初始化网络。具体来说，权重采用标准高斯噪音，偏差初始化为0.01。至于训练过程，Deep Q-learning算法采用了一个trick。该算法采用了两个完全相同的网络，其中一个用来训练，另一个则用来预测。这样还可以防止过拟合。用于训练网络的训练集并不是agent当前的四元组$(s_t, a_t, s_{t+1}, r_t)$， 而是从最近四元组历史（之前某一个时间窗口中的所有四元组）中随机采样出的一个minibatch。我们通过这些训练样本来更新训练网络的参数，经过一定时间的训练之后，把训练网络的参数复制给预测网络，用预测网络来继续产生训练样本，供训练网络使用。整个算法就是不断重复上述过程直至收敛。具体算法的伪代码如下所示：12345678910111213141516Initialize replay memory D to size NInitialize action-value function Q with random weightsfor episode = 1, M do Initialize state s_1 for t = 1, T do With probability ϵ select random action a_t otherwise select a_t=argmax_a Q(s_t,a; θ_i) Execute action a_t in emulator and observe r_t and s_(t+1) Store transition (s_t,a_t,r_t,s_(t+1)) in D Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+1)) from D Set y_j:= r_j for terminal s_(j+1) r_j+γ*max_(a^') Q(s_(j+1),a'; θ_i) for non-terminal s_(j+1) Perform a gradient step on (y_j-Q(s_j,a_j; θ_i))^2 with respect to θ end forend forBenchmark我们希望算法能够比随机选择更好。下面是进行1000次实验随机算法的结果：方法数据预处理我们在实验之前进行了数据的标准化，使得所有数据都处于0到1之间，这样可以避免梯度爆炸等现象的发生。$x， y$ 这两个特征没有进行标准化，因为已经符合要求。$theta$通过除以$2\pi$进行标准化。在没有进行标准化之前，我们在实验中发现，$theta$的值会达到$10^3$这个数量级，使得网络发生了bias shift现象。$s_1, s_2, s_3$通过除以40来进行标准化。我们同样试着能够将reward也进行标准化，将其范围缩小到[-1, 1]。因为DQN论文中同样使用了这种方法，使得该算法应用在不同的Atari游戏上时不用对算法进行参数的调整。但是，我们在网络训练的前一百万步并没有发现性能有明显的提升。因为reward的值更大的话，学习将会更容易，这样reward信号会更加明显，不会被淹没在网络的高斯噪声中。所以我们希望reward能够大一点，但是多大比较合适又是一个问题。我们所借鉴的算法[1]，将这个reward的最小值设置成了-500（小车撞上了障碍物），但我们实验发现这个值设置的过小（下面将会解释），所以最后的范围调整为[-100, 10] （通过裁剪）。我们把这个过程称之为reward正则化。Reward 正则化在网络训练（反向传播）的过程中，我们希望最小化代价函数。我们的代价函数选为训练网络输出的Q值与训练样本的Q值之间的MSE。在试验过程中，我们发现，对于$s_1, s_2, s_3$值都比较大的状态，其reward都会落在[0, 40]的范围内，并且均值为20。但是网络刚开始训练时，输出值为均值为0的高斯噪声。也就是说初始的loss处于[400-1600]的范围内（由于最后的loss需要除以样本的数量，所以loss等于一个样本的loss）。现在我们假定网络处于一个最优点附近，这时候小车突然撞上了某个障碍物，那么唯一的可能就是猫出现在了小车后面。这时候就会引入一个250000的loss（如果将reward的最小值设置为-500）。但是网络初始时的loss都只处于[400, 1600]的范围内，这个loss是初始loss的100倍。这么大的loss所引入的梯度将会使得网络走一段非常大的距离，这就很可能直接跳过了局部最优点。不断如此的话，网络就会震荡的非常厉害。让我们用数学的观点来解释这个问题。当reward的负值设置的过大，将会使得原始问题空间距离最优空间有一个非常大的偏差，很难通过梯度下降靠近。这个大的偏差在问题空间创造了一些非常陡峭的cliff。就像我们爬山一样，好不容易爬到了山顶附近，一不小心就掉下了悬崖，那么我们只能一步一步非常慢的爬上来，花很久的时间才能到达刚才的位置。如果一不小心又掉下去了，那么又要重新爬。因此，减小reward的范围十分重要，这样可以减小cliff的坡度，使得网络训练更快更容易。但是又不能太小，以免被噪声淹没。最后我们选定了[-100, 10]这个范围。模型迭代过程我们最开始直接采用现成的模型，是一个两层的神经网络（不包括输入层），效果已经不错了，但是小车总是撞上障碍物。因此我们做了一些改变：类似DQN，我们使用了最近四次的state，将其映射为一个input，这使得我们的QMax值提高到了120我们继续进行改变，从使用最近四次改为最近16次，使得我们的QMax值提高到了140我们尝试了使用一个更小的网络进行学习（2层，每层32维），并且只使用一个state进行输入，但是结果比随机算法更差。继续尝试使用grid search选择模型，还是两层网络，每一层的维数从32到512，训练迭代次数为200, 000，但是最后的QMax值还是不能超过140。我们尝试了更小的时间窗口，更大的minibatch，网络训练时震荡的十分厉害我们尝试在小车的背面增加一个声纳传感器，发i按网络训练速度变快了，但是最后的QMax值还是不能达到更高。这些尝试说明应当是两层网络的特征表达能力不够，我们尝试使用更深的网络。最后使用的网络有8层（不算输入输出层），输入层和输出层各有32维，中间6层为64维。最后取得了很好的效果，QMax达到了之前的10倍。同时，我们在每一层网络后都加入了一个20%的dropout层（除了输入层以及输出层之前），激活函数选用的ReLU函数。实验结果算法的训练过程如下所示：1234567891011121314151617181920212223242526272829303132In(4): ai.cycle()t= 11000[654.53412, 322.84866, 86.578796, 1414.0239]Games played 539Epoch Max score 144Epoch Mean score 30.3580705009t= 21000[474.16202, 251.2959, 79.489487, 1243.3118]Games played 774Epoch Max score 223Epoch Mean score 42.6255319149t= 31000[388.32297, 202.05305, 79.290771, 1086.0581]Games played 1020Epoch Max score 153Epoch Mean score 40.5081300813t= 41000[470.96552, 234.70471, 129.87579, 1320.3688]Games played 1281Epoch Max score 251Epoch Mean score 38.3908045977t= 51000[549.32666, 203.20442, 176.22263, 1079.8307]Games played 1546Epoch Max score 226Epoch Mean score 37.7773584906t= 61000[610.16583, 232.79211, 224.97626, 1264.9712]Games played 1759Epoch Max score 484Epoch Mean score 46.5774647887...实验结果：可以看出，我们的算法性能完全超越了随机算法。下面是我们训练大概250,000次后的结果：关于随机算法以及Q-learning算法的动画展示可以参照项目地址。但是我们发现小车还是会撞到障碍物，这经常发生在小车碰撞之后的恢复过程中。这时候小车可能到达地图的角落，充满障碍物。但是因为小车只有三个传感器，即使在背面加上还是太少了，所以信息捕捉不够。这是模型需要改进的地方。我们可以事先在小车中存储一个类似于地图的数据。另外，由于小车一直是匀速行驶，如果加入加速，减速等过程，应当会使得性能更好。但是由于时间原因，我们并没有进一步改进。进一步工作本次实验仅仅是在二维环境中进行的。但是严格来说并不是复杂环境的最佳简化。三维环境更加贴近现实情况，例如我们可以设计一个飞行的环境模拟。相关链接[1]. https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.58wi2s7ct[2]. https://github.com/songrotek/DQN-Atari-Tensorflow/blob/master/BrainDQN_Nature.py[3]. https://www.continuum.io/why-anaconda[4]. https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#anaconda-installation[5]. http://www.pygame.org/wiki/GettingStarted[6]. http://www.pymunk.org/en/latest/[7]. http://www.numpy.org/[8]. http://www.scipy.org/[9]. https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Store Management System]]></title>
      <url>%2F2017%2F06%2F27%2FStore-Management-System%2F</url>
      <content type="text"><![CDATA[SMSSMS (Store Management System), 一个简单的网店管理系统。源码：https://github.com/ewanlee/sms这是一个用于展示微服务的 proof-of-concept 应用，运用了Spring Boot, Spring Cloud 以及 Docker部署。核心服务SHOP 分成了三个核心微服务，它们都是独立开发的，采用了Spring MVC架构：Order service进行订单的添加，删除，以及显示MethodPathDescriptionUser authenticatedAvailable from UIGET/返回订单列表无有GET/form增加订单，并进行用户选择无有POST/line增加一条订单到数据库无无GET/{id}显示某一条订单的详情无有POST/增加订单行为无有DELETE/{id}删除订单无有Customer service进行用户的添加，删除，以及显示MethodPathDescriptionUser authenticatedAvailable from UIGET/list返回用户列表无有GET/{id}返回指定id的用户详情无有GET/form返回增加用户界面无有POST/form增加用户无无PUT/{id}增加用户行为无有DELETE/{id}删除用户无有Catalog service进行商品的添加，删除，以及显示MethodPathDescriptionUser authenticatedAvailable from UIGET/list返回商品列表无有GET/{id}返回指定id的商品详情无有GET/form返回增加商品界面无有POST/form增加商品无无PUT/{id}增加商品行为无有DELETE/{id}删除商品无有TEXT_HTML_VALUE/searchForm返回搜索界面无有TEXT_HTML_VALUE/searchByName返回搜索结果无有注意每个微服务都有自己的数据库，因此互相之间没有直接访问数据库的接口这里的数据库使用的是spring框架自带的数据库服务到服务的通信非常简单，通过暴露的接口即可架构服务分布式系统中有一些通用的模式，Spring Cloud框架都有提供，在本项目中仅仅运用了一小部分：API 网关可以看到，有三个核心服务，它将外部API暴露给客户端。在一个现实世界的系统中，核心服务的数量可以非常快速地增长，并且整个系统的复杂性更是急剧增加。实际上，一个复杂的网页可能需要渲染数百个服务。理论上，客户端可以直接向每个微服务器发出请求。但是显然，这将面临很大的挑战以及限制。比如必须要知道所有端点的地址。通常一个更好的方法是使用API网关。它是系统中的单个入口点，用于通过将请求路由到适当的后端服务或通过调用多个后端服务并聚合结果来处理请求。此外，它还可以用于认证，压力测试，服务迁移，静态响应处理，主动流量管理等Netflix开辟了这样一个优势服务，现在使用Spring Cloud，我们可以通过一个@EnableZuulProxy注释来实现。在这个项目中使用了Zuul存储静态内容（ui应用程序），并将请求路由到适当的微服务器。Zuul使用服务发现机制来定位服务实例以及断路器和负载平衡器，如下所述。服务发现另外一个众所周知的架构模式便是服务发现机制。它可以进行服务实例网络位置的动态检测。当应用需要扩展、容错或者升级的时候就可以自动为服务实例分配地址。服务发现机制的核心是注册阶段。本项目使用了 Netflix Eureka。 Eureka是一个客户端的发现模式，因为很多网络应用都需要客户端自己去确定特定服务的地址（使用注册服务器）并且进行请求的负载均衡。使用Spring Boot时，只要在pom文件中加入spring-cloud-starter-eureka-server依赖并且使用@EnableEurekaServer注解即可使用该服务。负载均衡、断路器以及Http客户端Netflix还提供了另外一些十分好用的工具。RibbonRibbon 是一个客户端的负载均衡器。相比传统的均衡器，你可以之间链接到相关服务。Ribbon已经和Spring Cloud以及服务发现机制集成在了一起。 Eureka Client 提供了一个可用服务器的动态列表供 Ribbon 进行服务器之间的均衡。HystrixHystrix 是断路器模式的具体实现，其可以调节网络访问依赖中经常出现的延迟以及错误。其主要目的是为了阻断在分布式环境中大量微服务极易出现的级联错误，使得系统尽快重新上线。Hystrix还提供了一个监控页面 （下面将会看到）。运行前期准备：网络安装 Docker 以及 Docker compose运行命令：cd microservice-demo/执行mvn clean packagecd ../docker/执行docker-compose build以及docker-compose up重要端口：http://127.0.0.1:8080 - 网关http://127.0.0.1:8761 - Eureka Dashboard注意：应用启动之后如果遇到 Whitelabel Error Page 错误请刷新页面UIIndexCustomer ServiceCatalog ServiceOrder ServiceEukera ServiceHystrix Dashboard]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Learning to act by predicting the future]]></title>
      <url>%2F2017%2F06%2F14%2FLearning-to-act-by-predicting-the-future%2F</url>
      <content type="text"><![CDATA[论文 Learning to act by predicting the future这篇论文提出的 DFP (Direct Future Prediction) 赢得了2016年 Virtual Doom AI Competition 的 “Full Deathmatch” 环节的比赛。Virtual Doom 是一个对战性的第一人称射击型游戏，根据玩家击杀数判定胜负。为了体现出模型的泛化能力， 训练过程中使用的地图不在比赛过程中出现。DFP的性能超出了第二名（Deep LSTM Q-Network）50%，并且其模型以及训练数据更加简洁，表现出了DFP模型的优越性。机器学习问题可以分为监督学习问题，无监督学习问题以及强化学习问题。监督学习主要是学习一个输入到输出的映射函数，无监督学习更加关注如何挖掘数据本身的隐含结构，强化学习是一个面向目标的策略学习问题。因此采用强化学习的方法使得机器人在Deathmatch游戏中表现良好十分合适。因为这是一个直接面向目标的问题 （在游戏中取得最大的击杀数）。所以 DQN 以及 A3C 这样的算法应运而生，并且取得了巨大的成功。但是这篇论文提出了一个不同的观点。它引用了Jordan &amp; Rumelhart (1992) 这篇论文中提出的一个观点：对于一个可以与环境进行交互的学习问题，如果环境提供的反馈是稀疏的标量 （例如，对于一个五子棋问题，反馈只在最后胜负已分时给出，并且只是一个类似+1，-1的标量反馈），采用传统的强化学习算法会十分有效；但是如果环境给出的反馈是一个即时密集的多维度反馈 （在短时间内具有很大的信息比特率），监督学习算法更具优势。由于监督学习方面的研究已经非常成熟，最近十分火热的深度学习更是在很多方面都取得了很好的结果，因此，如果我们能够把强化学习问题在某种程度上转化为一个监督学习问题，可以使得问题的求解大大简化。那么现在的问题是，我们要如何设计模型，从而可以得到一个监督信号呢？可以想到，我们唯一拥有的数据就是机器人通过与环境的交互得到的状态转移 （对于游戏来说就是玩家在游戏中采取不同的行为得到的环境的反馈，例如，玩家使用一个血包可以是的生命值回复；向左转可以使得画面发生变化等等）。我们可以对这些数据进行特殊的设计，从而能够满足我们的要求。具体来说，我们不再简单使用传统强化学习问题中单一的状态 （例如游戏中的一帧画面）与对应的回报。我们把单一的状态拆分开来，对于原始的图像，声音等信息原样保留，形成一个 ”感觉输入流 (sensory input stream)“ ，很明显它是一个高维的变量；另外，我们从这些原始的信息中提取出能够代表我们学习目标的测度 （例如健康度，剩余弹药数以及击杀数等），形成一个 ”测度流 (measurement stream)“ ，它是一个低维的变量 （因为只包含几个重要的变量）。注意，这里的stream不是代表了好几个时间步，而是代表它是多个测度的一个集合。这样做有什么好处呢？一个传统的强化学习问题，其训练对象就是最大化一个关于reward的函数。一般reward都是人为给定的 （还是拿五子棋举例，最后玩家赢了，回报就是一个正数， 反之就是负数），但是这就使得学习问题的方差变得很大，训练过程十分不稳定，收敛速度慢，甚至可能不收敛。因此，我们希望reward的值不要过于随机化，能够通过某些监督信号来减少其方差。这里就可以体现出我们之前进行状态分解的优势。我们可以将reward表示成 measurement stream 的函数，由于measurement是agent与真实环境进行交互时得到的，属于一种监督信号，这很好的满足了我们的需求。所以最后我们的训练对象由最大化一个关于reward的函数变成了最大化一个关于measurement stream的函数。而这个measurement stream可以认为是传统强化学习问题中的reward。模型现在我们正式地定义DFP模型。在每一个时间步$t$，agent接收一个观察 （转移到一个状态）$O_t$, 根据这个观察 （状态）的某些固有属性从可行的动作集合中选取一个动作执行。$O_t$详细定义如下：$$\mathbf{o}_t = (\mathbf{s}_t, \mathbf{m}_t)$$整个状态转移过程中，我们希望最大化目标，前面提到了，它是关于measurement stream的函数：$$\mathbf{f} = (\mathbf{m}_{t+\tau_1}-\mathbf{m}_t, \cdots, \mathbf{m}_{t+\tau_n}-\mathbf{m}_t)$$$\tau_1, \cdots, \tau_n$ 代表与当前时间步$t$的一个偏差。至于为什么不直接最大化measurement stream而是最大化一个差值，我认为作者可能是有如下考虑：借鉴了n-step Q-learning 的做法。由于模型是为了预测当前时间步$t$的measurement stream，因此优化对象中应该包含当前的measurement stream。最后，$$\mathbf{Goal} \; = \; u(\mathbf{f};\mathbf{g})$$一般线性函数即可满足我们的需求，即$$u(\mathbf{f};\mathbf{g}) = \mathbf{g}^{\text{T}}\mathbf{f}$$注意到现在我们的问题变成了一个监督学习的问题。为了训练模型，我们需要预测目标，然后再与真实的目标比较，通过最小化误差来进行学习。那么我们现在定义这个预测过程。注意到，由于目标只是measurement stream的函数，而且参数一般都是确定的，不需要进行学习。因此，我们的预测对象是measurement stream而不是目标。下面我们定义一个预测器F:$$\mathbf{p}_t^a = F(\mathbf{o}_t, a, \mathbf{g};\theta)$$注意，这里的$\text{g}$和(4)中是不一样的，它代表目标。$p_t^a$代表在$t$时间步下，执行行为$a$所得到的reward，也即measurement stream。当训练完成的时候，我们就要用这个预测器F进行决策，策略定义如下：$$a_t = {\arg\max}_{a \in \mathcal{A}} \mathbf{g}^{\text{T}}F(\mathbf{o}_t, a, \mathbf{g};\theta)$$注意到，模型实际训练的过程中采用的是$\varepsilon\text{-greedy}$策略。这里可以看出，在训练过程中或者测试过程中，我们要手动的计算出$u(\text{f};\text{g})$的值。下面我们详细的剖析模型的训练过程。模型训练对于传统的强化学习算法，例如Q-learning，其训练过程是一个在线学习的过程，也即其训练集是一个一个进行输入的，每输入一次都进行一次参数的更新。由于Q-learning以及DFP都是采用了MC (Monte Carlo) 策略，这种训练过程可能十分不稳定 （由于训练最开始时我们的训练数据是通过一个随机策略与环境交互产生的），致使收敛速度很慢，需要很多的episodes进行训练。这里采用了和DQN (Deep Q-Network) 相同的 experience replay技术。具体来说，就是保存每次agent与环境交互后产生的数据对$\langle \mathbf{o}_i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle$ 到数据集$\mathcal{D}$中，即$\mathcal{D} = \{\langle \mathbf{o}_ i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle \}_{i=1}^N$. 注意这里的$N$个数据对并不是直接顺序产生的，而是从当前episode中到当前时间步时，所有的数据对中选取最近的$M$个，再从其中随机抽样$N$个。另外，每隔k步才进行一次参数的更新，因为$\mathbf{f}$的计算需要考虑到32个时间步之后的数据，因此$k \ge 32$（实验部分将详细介绍）。DQN 给出了具体的实现：另外需要注意的是有了训练集，我们现在定义w代价函数：$$\mathcal{L}(\theta) = \sum_{i=1}^{N} |F(\mathbf{o}_i, a_i, \mathbf{g}_i;\theta) - \mathbf{f}_i|^2$$我们来对比一下 DQN 的代价函数：$$L_i(\theta_i)= \mathbb{E}_{s, a \sim \rho(\cdot)} \left[ y_i - Q(s, a;\theta_i) \right],$$其中$y_i = \mathbb{E}_{s^{\prime} \sim \varepsilon}[ r + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime};\theta_{i-1}) ]$ 。这里的$y_i$是上一次模型的输出，其值随着更新次数的增加也在不断变化。因此从这里也能看出DFP是一个监督学习算法。训练过程中我们为了解决报告最开始提出的目标随着时间发生改变的问题，采用了两种目标进行测试：目标向量$\mathbf{g}$ （不是目标）在整个训练过程中不变目标向量在每个episode结束时随机变化网络结构下图是DFP模型的网络结构：从图中可以看出，该网络有三个输入模块。一个感知模块$S(s)$，一个测度模型$M(m)$以及一个目标模块$G(g)$。在实验中，$s$代表一张图片，$S$代表一个卷积神经网络。测度模块以及目标模块都是由一个全连接神经网络构成。三者的输出连接在一起，形成一个联合的输入表示，供后续算法使用：$$\mathbf{j} = J(\mathbf{s, m, g}) = \langle S(\mathbf{s}), M(\mathbf{m}), G(\mathbf{g}) \rangle$$DFP网络采用了DQN的做法，一次性输出所有action对应的measurement stream。但是我们希望能够着重关注对action之间差异的学习。因此采用了Wang et al. (ICML 2016) 这篇文章中才去的做法，将预测模块分为两个stream，一个期望stream $E(\text{j})$ 以及一个action stream $A(\text{j})$。注意这两个stream都是一个全连接的神经网络。期望stream的目标是预测所有action能够获得的measurement stream的期望。Action stream关注不同action之间的差异。其中，$A(\text{j}) = \langle A^1(\text{j}), \cdots, A^{w}(\text{j}) \rangle$，$w = |\mathcal{A}|$代表所有可能action的个数。同时我们还在加入了一个正则化层：$$\overline{A^{i}}(\mathbf{j}) = A^{i}(\mathbf{j}) - \frac{1}{w}\sum_{k=1}^{w} A^{k}(\mathbf{j})$$正则化层对每一个action的预测值减去了所有action预测值的期望，这样就强制期望stream去学习这个期望，这样action stream就可以着重关注不同action之间的差异。最后，网络的输出如下：$$\mathbf{p} = \langle \mathbf{p}^{a_1}, \cdots, \mathbf{p}^{a_w} \rangle = \langle \overline{A^1}(\mathbf{j})+E(\mathbf{j}), \cdots, \overline{A^w}(\mathbf{j})+E(\mathbf{j}) \rangle$$为了验证网络中使用的三个辅助结构（measurement stream输入，expectation-action分解以及action正则化层）的作用，我们进行了测试。我们基于D3场景（下面实验部分提及）随机产生了100个地图场景用以训练。同时采用basic网络 （下面实验部分提及），最后的实验结果如下：可以看出，expectation-action分解的作用最大，同时我们设计的measurement stream也是十分重要的。实验及结果具体的实验场景见下图：在前两个场景中，agent可以采取三个动作，向前移动、向左转、向右转。这样一共就有8种动作组合。采用的测度只有一种，就是血量。在后两个场景中，agent可以采取八个动作组合，分别是向前移动、向后移动、向左转、向右转、向左扫射，向右扫射、奔跑以及射击。这样一共就有256个动作组合。采用的测度一共有三种，血量，弹药数以及击杀数。这里我认为存在一个可以改进的地方，应该排除掉不合理的动作组合，例如同时向左转以及向右转。这样可以减少搜索空间，加速收敛，同时可以提高策略的质量。实验中网络的结构与DQN的结构十分类似，参数也尽可能相近，就是为了比较起来比较公平。具体来说，实验中采用了两种网络，basic以及large，结构相同，但是参数数量不同：Basic网络的参数与DQN比较接近，以便比较。两个网络在所有的非终止层后都加入了一个非线性层，采用的激活函数为Leaky ReLU，具体函数为：$$\mathbf{LReLU}(x) = \max(x, 0.2x)$$参数初始化方法采用了He Initialization，代码实现如下：12import numpy as npW = np.random.randn(node_in, node_out) / np.sqrt(node_in / 2)Agent以episode为单位进行训练和测试。每一个episode拥有525个时间步（大约一分钟），如果agent死亡那么episode也会终止。同时将时间偏置$\tau_1, \cdots, \tau_n$设置为1, 2, 4, 8, 16, 32。最后结果表明只有最新的三个时间步（8, 16, 32）对结果有贡献，贡献比例为 1:1:2。另外，输入图像被转换成灰度图像，measurement stream并不是直接输入，而是进行了正则化 （除以标准差）。同时，我们还在训练以及测试过程中使用frame skipping技术。Agent每隔4帧采取一次action。这些被忽略的帧所采取的action与其之后的帧的action一致，相当于进行了一次简单的复制。另外，由于人类的反应速度肯定是比不上计算机的，因此fram skipping使得agent的行为更加接近人类。对于之前提到的experience replay技术，实验中将M值设为20000， N设为64，k也设为64（$\ge32$）。同时为了能够更高效的获得训练集$\mathcal{D}$，我们同时采用8个agent并行运行。训练时采用的梯度下降算法为Adam算法，参数设置如下：$\beta_1=0.95, \;\beta_2=0.999,\;\varepsilon=10^{-4}$。Basic网络训练了800,000次mini-batch迭代，large网络训练了2,000,000次。算法实现https://github.com/IntelVCL/DirectFuturePrediction。下面介绍我们的baselines。我们同三个算法进行了比较：DQN (Mnih et al., 2015), A3C (Mnih et al., 2016), 以及 DSR (Kulkarni et al., 2016b)。DQN由于其在Atari游戏上的优异效果成为了视觉控制的标准baseline。A3C更是这个领域中的最好的算法。DSR也在Virtual Doom平台上进行了实验。所以我们挑选了这三个具有代表意义的算法。对于这三个算法我们都使用了Github上的开源实现：DQN (https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner) 、DSR (https://github.com/Ardavans/DSR), 以及 A3C (https://github.com/muupan/async-rl)。前两个都是作者提供的源码，最后的A3C是一个独立开发者的个人实现。对于DQN以及DSR我们测试了三个学习速率：默认值（0.00025），0.00005以及0.00002。其他参数直接采用默认值。对于A3C算法，为了训练更快，前两个任务我们采用了5个学习速率 ({2, 4, 8, 16, 32} · $10^{-4}$)。后两个任务我们训练了20个模型，每个模型的学习速率从一个范围从$10^{-4}$到$10^{-2}$的log-uniform分布中进行采样，$\beta$值（熵正则项）从一个范围从$10^{-4}$到$10^{-}$的lo1g-uniform分布中进行采样。结果选取最好的。最终结果如下所示：在前两个游戏场景中，模型尝试最大化血量；在后两个场景中尝试最大化血量、弹药数以及击杀数的一个线性组合，参数为0.5, 0.5, 1。因为游戏更加侧重于通过击杀数判断胜负。所有的数据都是对三次训练结果进行平均，曲线图采样点的个数为$3 \times 50,000$。可以看出，DFP模型取得了最好的结果。其中DSR算法由于训练速度过慢，所以我们只在D1场景（也进行了将近10天的训练）进行了测试。下面进行模型泛化能力的测试，我们基于D3以及D4两个场景分别随机产生100个随机场景。其中90个用于训练，剩下10个用于测试。最后结果如下：其中最后一列采用了large网络。可以看出，从复杂场景训练之后，在简单场景上的泛化能力往往不错，虽然两者规则不同。但是反之则不可以。接下来进行学习变化目标能力的测试。结果见下图：其中采用第二列的策略时，agent并不知道每一个measurement的相对重要性；最后一列，agent事先并不知道哪一个measurement是不需要考虑的。但是最后测试时，效果都很好，而且在固定目标策略没有见过的目标上的效果要更好。说明DFP模型对于变化目标的学习能力优异。最后我们单独对measurement stream时间偏置的重要性进行测试，我们采用的是D3-tx训练集，最后结果如图：相比较而言，采用更多的时间偏置可以达到更好的效果。总结现在强化学习问题关注的重点还是在value function的估计上，深度强化学习模型一般采用一个深度网络直接对value function进行估计。这篇论文的创新点在于，在使用深度网络之前，对value function进行了两次额外的映射。第一次是用measurement stream来代替reward，使得reward具有更强的状态表示能力；其次，对measurement stream再次进行了一个函数映射，采用了时间偏置，借鉴了n-step Q-learning的思想。最后，再将输出作为深度网络的输入，进行value function的估计。最后的实验结果证明这种想法是有其正确性的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Monte Carlo Methods (Reinforcement Learning)]]></title>
      <url>%2F2017%2F06%2F02%2FMonte-Carlo-Methods-Reinforcement-Learning%2F</url>
      <content type="text"><![CDATA[Here we consider our first learning methods for estimating value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte Carlo methods require only experience – sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Although a model is required, the model need only generate sample transition, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP).Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample return. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. Only on the completion of an episode are value estimates and policies changed.To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in earlier for DP.Suppose we wish to estimate $v_{\pi}(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing though $s$. Each occurrence of state $s$ in an episode is called a visit to $s$. Let us call the first time it is visited in an episode the first visit to $s$. The first-visit MC method estimates $v_{\pi}(s)$ as the average of the returns following first visits to $s$, whereas the every-visit MC method averages the returns following all visits to $s$.First-visit MC policy evaluation (returns $V \approx v_{\pi}$)Initialize:​ $\pi \leftarrow$ policy to be evaluated​ $V \leftarrow $ an arbitrary state-value function​ $Return(s) \leftarrow$ an empty list, for all $s \in \mathcal{S}$Repeat forever:​ Generate an episode using $\pi$​ For each state $s$ appearing in the episode:​ $G \leftarrow$ return following the first occurrence of $s$​ Append $G$ to $Return(s)$​ $V(s) \leftarrow$ $\text{average}(Return(s))$Next, we’ll use this algorithm to solve a naive problem that defined as follows:The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the game is draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stop (sticks) or excepted 21 (goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome–win, lose, draw–is determined by whose final sum is closer to 21.Playing blackjack is naturally formulated as an episode finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ($\gamma=1$); therefore these terminal rewards are also returns. The player’s actions are to hit or to stick. We assume that cards are dealt from an infinite deck (i.e., with replacement). If the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable. Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.Note that, although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events–in particular, they require the quantities $p(s^{\prime}, r|s, a)$–and it is not easy to determine these for blackjack. For example, suppose the play’s sum is 14 and he chooses to sticks. What is his excepted reward as a function of the dealer’s showing card? All of these rewards and transition probabilities must be computed before DP can be applied, and such computations are often complex and error-prone.The conceptual diagram of the experimental results is as follows:Figure 1The first we define some auxiliary variables and methods:12345678910111213141516171819202122232425262728293031323334# actions: hit or stand (stick)ACTION_HIT = 0ACTION_STAND = 1actions = [ACTION_HIT, ACTION_STAND]# policy for playerpolicyPlayer = np.zeros(22)for i in range(12, 20): policyPlayer[i] = ACTION_HITpolicyPlayer[20] = ACTION_STANDpolicyPlayer[21] = ACTION_STAND# function form of target policy of playerdef targetPolicyPlayer(usableAcePlayer, playerSum, dealerCard): return policyPlayer[playerSum]# function form of behavior policy of playerdef behaviorPolicyPlayer(usableAcePlayer, playerSum, dealerCard): if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HIT# policy for dealerpolicyDealer = np.zeros(22)for i in range(12, 17): policyDealer[i] = ACTION_HITfor i in range(17, 22): policyDealer[i] = ACTION_STAND# get a new carddef getCard(): card = np.random.randint(1, 14) card = min(card, 10) return cardFurthermore, we also have a print method:1234567891011121314151617181920# print the state valuefigureIndex = 0def prettyPrint(data, tile, zlabel='reward'): global figureIndex fig = plt.figure(figureIndex) figureIndex += 1 fig.suptitle(tile) ax = fig.add_subplot(111, projection='3d') axisX = [] axisY = [] axisZ = [] for i in range(12, 22): for j in range(1, 11): axisX.append(i) axisY.append(j) axisZ.append(data[i - 12, j - 1]) ax.scatter(axisX, axisY, axisZ) ax.set_xlabel('player sum') ax.set_ylabel('dealer showing') ax.set_zlabel(zlabel)In order to get the figure above, we wrote the following code:12345678def onPolicy(): statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(10000) statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(500000) prettyPrint(statesUsableAce1, 'Usable Ace, 10000 Episodes') prettyPrint(statesNoUsableAce1, 'No Usable Ace, 10000 Episodes') prettyPrint(statesUsableAce2, 'Usable Ace, 500000 Episodes') prettyPrint(statesNoUsableAce2, 'No Usable Ace, 500000 Episodes') plt.show()There is a term named on policy, we’ll explain this term later. Now let us jump into the monteCarloOnPolicy method:12345678910111213141516171819# Monte Carlo Sample with On-Policydef monteCarloOnPolicy(nEpisodes): statesUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesUsableAceCount = np.ones((10, 10)) statesNoUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesNoUsableAceCount = np.ones((10, 10)) for i in range(0, nEpisodes): state, reward, _ = play(targetPolicyPlayer) state[1] -= 12 state[2] -= 1 if state[0]: statesUsableAceCount[state[1], state[2]] += 1 statesUsableAce[state[1], state[2]] += reward else: statesNoUsableAceCount[state[1], state[2]] += 1 statesNoUsableAce[state[1], state[2]] += reward return statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCountWe ignore he first four variables now and explain them later. nEpisodes represents the number of the episodes and the play method simulates the process of the blackjack cards game. So what is it like? This method is too long (more than 100 rows) so we partially explain it. In the first place, this method define some auxiliary variables:12345678910111213141516# play a game# @policyPlayerFn: specify policy for player# @initialState: [whether player has a usable Ace, sum of player's cards, one card of dealer]# @initialAction: the initial actiondef play(policyPlayerFn, initialState=None, initialAction=None): # player status # sum of player playerSum = 0 # trajectory of player playerTrajectory = [] # whether player uses Ace as 11 usableAcePlayer = False # dealer status dealerCard1 = 0 dealerCard2 = 0 usableAceDealer = FalseThen, we generate a random initial state if the initial state is null. If not, we load the initial state from initialState variable:12345678910111213141516171819202122232425262728293031323334353637383940414243444546if initialState is None: # generate a random initial state numOfAce = 0 # initialize cards of player while playerSum &lt; 12: # if sum of player is less than 12, always hit card = getCard() # if get an Ace, use it as 11 if card == 1: numOfAce += 1 card = 11 usableAcePlayer = True playerSum += card # if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible if playerSum &gt; 21: # use the Ace as 1 rather than 11 playerSum -= 10 # if the player only has one Ace, then he doesn't have usable Ace any more if numOfAce == 1: usableAcePlayer = False # initialize cards of dealer, suppose dealer will show the first card he gets dealerCard1 = getCard() dealerCard2 = getCard() else: # use specified initial state usableAcePlayer = initialState[0] playerSum = initialState[1] dealerCard1 = initialState[2] dealerCard2 = getCard() # initial state of the game state = [usableAcePlayer, playerSum, dealerCard1] # initialize dealer's sum dealerSum = 0 if dealerCard1 == 1 and dealerCard2 != 1: dealerSum += 11 + dealerCard2 usableAceDealer = True elif dealerCard1 != 1 and dealerCard2 == 1: dealerSum += dealerCard1 + 11 usableAceDealer = True elif dealerCard1 == 1 and dealerCard2 == 1: dealerSum += 1 + 11 usableAceDealer = True else: dealerSum += dealerCard1 + dealerCard2Game start! Above all is player’s turn:1234567891011121314151617181920212223242526# player's turnwhile True: if initialAction is not None: action = initialAction initialAction = None else: # get action based on current sum action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1) # track player's trajectory for importance sampling playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)]) if action == ACTION_STAND: break # if hit, get new card playerSum += getCard() # player busts if playerSum &gt; 21: # if player has a usable Ace, use it as 1 to avoid busting and continue if usableAcePlayer == True: playerSum -= 10 usableAcePlayer = False else: # otherwise player loses return state, -1, playerTrajectoryThen is the dealer’s turn if the player’s turn is end:12345678910111213141516while True: # get action based on current sum action = policyDealer[dealerSum] if action == ACTION_STAND: break # if hit, get a new card dealerSum += getCard() # dealer busts if dealerSum &gt; 21: if usableAceDealer == True: # if dealer has a usable Ace, use it as 1 to avoid busting and continue dealerSum -= 10 usableAceDealer = False else: # otherwise dealer loses return state, 1, playerTrajectoryIf the both sides have finished the game:1234567# compare the sum between player and dealerif playerSum &gt; dealerSum: return state, 1, playerTrajectoryelif playerSum == dealerSum: return state, 0, playerTrajectoryelse: return state, -1, playerTrajectoryNow, let us come back the mentoCarloOnPolicy method:123456789101112131415161718def monteCarloOnPolicy(nEpisodes): statesUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesUsableAceCount = np.ones((10, 10)) statesNoUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesNoUsableAceCount = np.ones((10, 10)) for i in range(0, nEpisodes): state, reward, _ = play(targetPolicyPlayer) state[1] -= 12 state[2] -= 1 if state[0]: statesUsableAceCount[state[1], state[2]] += 1 statesUsableAce[state[1], state[2]] += reward else: statesNoUsableAceCount[state[1], state[2]] += 1 statesNoUsableAce[state[1], state[2]] += reward return statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCountIn this method we ignore the player’s trajectory (represent by the playerTrajectory variable). If you remember a sentence in the game definition (as follows) it will easy to understand.Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.This row (as follows) is to calculate the average returns of each state:1return statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCountRecall the beginning of the code and let’s see what results are like:Figure 2 (from up to down). (1) Usable Ace, 10000 Episodes. (2) No Usable Ace, 10000 Episodes. (3) Usable Ace, 500000 Episodes. (4) No Usable Ace, 500000 Episodes.If a model is not available, then it is particularly useful to estimate action values (the value of state-value pairs) rather than state values. With a model, state values alone are sufficient to determine a policy; one simply look ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the earlier on DP. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of out primary goals for Monte Carlo methods is to estimate $q_{\star}$. To achieve this, we first consider the policy evaluation problem for action values.The policy evaluation problem for action values is estimate $q_{\pi}(s, a)$. The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state-action pair rather than to a state. The only complication is that many state-value pairs may never be visited. This is the general problem of maintaining exploration, as discussed in the context of the k-armed bandit problem in here. One way to do this is by specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. We call this the assumption of exploring starts.We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with a arbitrary policy $\pi_{0}$ and ending with the optimal policy and optimal action-value function:$$\pi_{0} \stackrel{E}\longrightarrow q_{\pi{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow q_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow q_{\star},$$We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. We postpone consideration of the first assumption until later.The primary approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo to complete policy evaluation before returning to policy improvement. For Monte Carlo policy evaluation it it natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation,and then the policy is improved at all the states visited in the episode.Monte Carlo ES (Exploring Starts)Initialize, for all $s \in \mathcal{S},\; a \in \mathcal{A(s)}$:​ $Q(s,a) \leftarrow \text{arbitrary}$​ $\pi(s) \leftarrow \text{arbitrary}$​ $Returns(s,a) \leftarrow \text{empty list}$Repeat forever:​ Choose $S_{0} \in \mathcal{S}$ and $A_{0} \in \mathcal{A(S_{0})}$ s.t. all pairs have probability &gt; 0​ Generate an episode starting from $S_0, A_0$, following $\pi$​ For each pair $s, a$ appearing in the episode:​ $G \leftarrow \text{return following the first occurrence of} \; s, a$​ Append $G$ to $Returns(s,a)$​ $Q(s,a) \leftarrow \text{average}(Returns(s,a))$​ For each $s$ in the episode:​ $\pi(s) \leftarrow \arg\max_{a} Q(s,a)$Recall the blackjack card game. It is straightforward to apply Monte Carlo ES to blackjack.123456789101112131415161718def figure5_3(): stateActionValues = monteCarloES(500000) stateValueUsableAce = np.zeros((10, 10)) stateValueNoUsableAce = np.zeros((10, 10)) # get the optimal policy actionUsableAce = np.zeros((10, 10), dtype='int') actionNoUsableAce = np.zeros((10, 10), dtype='int') for i in range(10): for j in range(10): stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, 0, :]) stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, 1, :]) actionNoUsableAce[i, j] = argmax(stateActionValues[i, j, 0, :]) actionUsableAce[i, j] = argmax(stateActionValues[i, j, 1, :]) prettyPrint(stateValueUsableAce, 'Optimal state value with usable Ace') prettyPrint(stateValueNoUsableAce, 'Optimal state value with no usable Ace') prettyPrint(actionUsableAce, 'Optimal policy with usable Ace', 'Action (0 Hit, 1 Stick)') prettyPrint(actionNoUsableAce, 'Optimal policy with no usable Ace', 'Action (0 Hit, 1 Stick)') plt.show()Run the code we’ll get the conceptual diagram like follows:Let us to see the implementation (monteCarloES method) of this algorithm. Note that, some auxiliary variables are defined earlier.12345678910111213141516171819202122232425262728293031# Monte Carlo with Exploring Startsdef monteCarloES(nEpisodes): # (playerSum, dealerCard, usableAce, action) stateActionValues = np.zeros((10, 10, 2, 2)) # set default to 1 to avoid being divided by 0 stateActionPairCount = np.ones((10, 10, 2, 2)) # behavior policy is greedy def behaviorPolicy(usableAce, playerSum, dealerCard): usableAce = int(usableAce) playerSum -= 12 dealerCard -= 1 return argmax(stateActionValues[playerSum, dealerCard, usableAce, :]) # play for several episodes for episode in range(nEpisodes): print('episode:', episode) # for each episode, use a randomly initialized state and action initialState = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))] initialAction = np.random.choice(actions) _, reward, trajectory = play(behaviorPolicy, initialState, initialAction) for action, (usableAce, playerSum, dealerCard) in trajectory: usableAce = int(usableAce) playerSum -= 12 dealerCard -= 1 # update values of state-action pairs stateActionValues[playerSum, dealerCard, usableAce, action] += reward stateActionPairCount[playerSum, dealerCard, usableAce, action] += 1 return stateActionValues / stateActionPairCountYou can see we use the trajectory variable now. The difference between Monte Carlo On Policy and Monte Carlo ES is prior calculates the average return of each state and later calculates the average return of each state-action pair.The results are as follows:How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call on-policy (Do you remember this term?) methods and off-policy methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.. The First-visit Monte Carlo method and the Monte Carlo ES method are an example of an on-policy method. Here we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. Off-policy methods are considered later.The on-policy method we presented in here uses $\epsilon \text{-} greedy$ policies. The complete algorithm is given below.On-policy first-visit MC control (for $\epsilon \text{-soft}$ policies)Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:​ $Q(s,a ) \leftarrow \text{arbitrary}$​ $Returns(s,a) \leftarrow \text{empty list}$​ $\pi(a|s) \leftarrow \text{an arbitrary} \; \epsilon \text{-soft policy}$Repeat forever:​ (a) Generate an episode using $\pi$​ (b) For each pair $s, a$ appearing in the episode:​ $G \leftarrow $ return following the first occurrence of $s, a$​ Append $G$ to $Returns(s,a)$​ $Q(s, a) \leftarrow \text{average}(Returns(s,a))$​ (c) For each s in the episode:​ $A^{\star} \leftarrow \arg\max_{a} Q(s,a)$​ For all $a \in \mathcal{A(s)}$:​ $\pi(a|s) \leftarrow \left\{ \begin{array}{c} 1 - \epsilon + \epsilon/|\mathcal{A(s)}|\;\;\;\text{if} \;a=A^{\star} \\ \epsilon/|\mathcal{A(s)}|\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if} \;a\neq A^{\star} \end{array} \right.$All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise–it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to user two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data “off” the target policy, and the overall process is termed off-policy learning.We begin the study of off-policy methods by considering the prediction problem, in which both target and behavior policies are fixed. That is, suppose we wish to estimate $v_{\pi}$ or $q_{\pi}$, but all we have are episodes following another policy $\mu$, where $\mu \neq \pi$. In this case, $\pi$ is the target policy, $\mu$ is the behavior policy, and both policies are considered fixed and given.In order to use episodes from $\mu$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $\mu$. That is, we require that $\pi(a|s) &gt; 0$ implies $\mu(a|s) &gt; 0$. This is called the assumption of converge. It follows from converge that $\mu$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic. In here, we consider the prediction problem, in which $\pi$ is unchanging and given.Almost all off-policy methods utilize importance samplingddd, a general technique for estimating excepted values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectory occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state $S_{t}$, the probability of the subsequent state-action trajectory , $A_{t}, S_{t+1}, A_{t+1, \cdots, S_{T}}$, occurring under any policy $\pi$ is$$\prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1} | S_k, A_k),$$where $p$ here is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies (the important-sampling ratio) is$$\rho_{t}^{T} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} \mu(A_k|S_k)p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{\mu (A_k | S_k)}$$Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy $\mu$ to estimate $v_{\pi}(s)$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $t=101$. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state $s$ is visited, denote $\tau(s)$. This is for an every-visit method; for a first-visit method, $\tau(s)$ would only include time steps that were first visits to $s$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up though $T(t)$. Then $\{G_t\}_{t \in \tau(s)}$ are returns that pertain to state $s$, and $\{\rho_{t}^{T(t)}\}_{t \in \tau(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and average the results:$$V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{|\tau(s)|}.$$When importance sampling is done as a simple average in this way it is called ordinary importance sampling.An important alternative is weighted importance sampling, which uses a weighted average, defined as$$V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{\sum_{t \in \tau(s)} \rho_{t}^{T(t)}},$$or zero if the denominator is zero.We applied both ordinary and weighted importance-sampling methods to estimate the value of as single blackjack state from off-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimated for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generate by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21. The value of this state under the target policy is approximately -0.27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both off-policy methods closely approximated this value after 1000 off-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. The implementation details are as follows:12345678910111213141516171819202122232425262728293031# Monte Carlo Sample with Off-Policydef monteCarloOffPolicy(nEpisodes): initialState = [True, 13, 2] sumOfImportanceRatio = [0] sumOfRewards = [0] for i in range(0, nEpisodes): _, reward, playerTrajectory = play(behaviorPolicyPlayer, initialState=initialState) # get the importance ratio importanceRatioAbove = 1.0 importanceRatioBelow = 1.0 for action, (usableAce, playerSum, dealerCard) in playerTrajectory: if action == targetPolicyPlayer(usableAce, playerSum, dealerCard): importanceRatioBelow *= 0.5 else: importanceRatioAbove = 0.0 break importanceRatio = importanceRatioAbove / importanceRatioBelow sumOfImportanceRatio.append(sumOfImportanceRatio[-1] + importanceRatio) sumOfRewards.append(sumOfRewards[-1] + reward * importanceRatio) del sumOfImportanceRatio[0] del sumOfRewards[0] sumOfRewards= np.asarray(sumOfRewards) sumOfImportanceRatio= np.asarray(sumOfImportanceRatio) ordinarySampling = sumOfRewards / np.arange(1, nEpisodes + 1) with np.errstate(divide='ignore',invalid='ignore'): weightedSampling = np.where(sumOfImportanceRatio != 0, sumOfRewards / sumOfImportanceRatio, 0) return ordinarySampling, weightedSamplingNote that the behaviorPolicyPlayer that is a function that define the behavior policy:12345# function form of behavior policy of playerdef behaviorPolicyPlayer(usableAcePlayer, playerSum, dealerCard): if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HITAnd the calculation of the numerator and denominator of the importance-sampling are the summation operation. As the number of iterations increases, we need to accumulate the result from each episode. So we need to store the previous results. The sumOfRewards and sumOfImportanceRatio are used for this purpose.Then we need to show the result (mean square error):123456789101112131415161718192021# Figure 5.4def offPolicy(): trueValue = -0.27726 nEpisodes = 10000 nRuns = 100 ordinarySampling = np.zeros(nEpisodes) weightedSampling = np.zeros(nEpisodes) for i in range(0, nRuns): ordinarySampling_, weightedSampling_ = monteCarloOffPolicy(nEpisodes) # get the squared error ordinarySampling += np.power(ordinarySampling_ - trueValue, 2) weightedSampling += np.power(weightedSampling_ - trueValue, 2) ordinarySampling /= nRuns weightedSampling /= nRuns axisX = np.log10(np.arange(1, nEpisodes + 1)) plt.plot(axisX, ordinarySampling, label='Ordinary Importance Sampling') plt.plot(axisX, weightedSampling, label='Weighted Importance Sampling') plt.xlabel('Episodes (10^x)') plt.ylabel('Mean square error') plt.legend() plt.show()Result is as follows:Formally, the difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased (the bias converges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximate that we explore later.The estimates of ordinary importance sampling will typical have infinite variance, and this can easily happen in off-policy learning when trajectories contains loops. A simple example is shown below:There is only one nonterminal state $s$ and two action, end and back. The end action causes a deterministic transition to termination, whereas the back action transitions, with probability 0.9, back to $s$ or, with probability 0.1, on to termination. The rewards are +1 on latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to $s$ followed by termination with a reward and return of +1. Thus the value of $s$ under the target policy is 1. Suppose we are estimating this value from off-policy data using the behavior policy that selects end and back with equal probability.The implementation details are as follows. We first define the two policies:12345678910ACTION_BACK = 0ACTION_END = 1# behavior policydef behaviorPolicy(): return np.random.binomial(1, 0.5)# target policydef targetPolicy(): return ACTION_BACKThen we define how an episode runs:1234567891011# one turndef play(): # track the action for importance ratio trajectory = [] while True: action = behaviorPolicy() trajectory.append(action) if action == ACTION_END: return 0, trajectory if np.random.binomial(1, 0.9) == 0: return 1, trajectoryNow we start our off-policy (first-visit MC) learning process:123456789101112131415161718192021# Figure 5.5def monteCarloSample(): runs = 10 episodes = 100000 axisX = np.log10(np.arange(1, episodes + 1)) for run in range(0, runs): sumOfRewards = [0] for episode in range(0, episodes): reward, trajectory = play() if trajectory[-1] == ACTION_END: importanceRatio = 0 # Because it is impossible on the target policy else: importanceRatio = 1.0 / pow(0.5, len(trajectory)) sumOfRewards.append(sumOfRewards[-1] + importanceRatio * reward) del sumOfRewards[0] estimations = np.asarray(sumOfRewards) / np.arange(1, episodes + 1) plt.plot(axisX, estimations) plt.xlabel('Episodes (10^x)') plt.ylabel('Ordinary Importance Sampling') plt.show() returnResult is as follows:The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value.At last, we proposed two fancy algorithms, that is, the Incremental off-policy every-visit MC policy evaluation and the Off-policy every-visit MC control.Incremental off-policy every-visit MC policy evaluationInitialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:​ $Q(s,a) \leftarrow$ arbitrary​ $C(s,a) \leftarrow$ 0​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy​ $\pi(a|s) \leftarrow$ an arbitrary target policyRepeat forever:​ Generate an episode using $\mu$:​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$​ $G \leftarrow 0$​ $W \leftarrow 1$​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:​ $G \leftarrow \gamma G + R_{t+1}$​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$​ $W \leftarrow W\frac{\pi(A_t | S_t)}{\mu(A_t | S_t)}$​ If $W = 0$ then ExitForLoopOff-policy every-visit MC control (returns $\pi \approx \pi_{\star}$)Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:​ $Q(s,a) \leftarrow$ arbitrary​ $C(s,a) \leftarrow$ 0​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy​ $\pi(s) \leftarrow$ an deterministic policy that is greedy with respect $Q$Repeat forever:​ Generate an episode using $\mu$:​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$​ $G \leftarrow 0$​ $W \leftarrow 1$​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:​ $G \leftarrow \gamma G + R_{t+1}$​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$​ $\pi(S_t) \leftarrow \arg \max_{a}Q(S_t, a)$ (with ties broken consistently)​ If $A_t \neq \pi(S_t)$ then ExitForLoop​ $W \leftarrow W\frac{1}{\mu(A_t | S_t)}$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dynamic Programming]]></title>
      <url>%2F2017%2F05%2F31%2FDynamic-Programming%2F</url>
      <content type="text"><![CDATA[The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are provides an essential foundation for the understanding of the methods presented later. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. In here we show how DP can be used to compute the value functions defined in earlier. As discussed there, we can easily obtain optimal policies once we have found the optimal value functions $v_{\star}$ or $q_{\star}$ which satisfy the Bellman optimality equations:$$\begin{align}v_{\star}(s) &amp;= \max_{a} \mathbb{E} [R_{t+1} + \gamma v_{\star}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\&amp;= \max_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) \left[r + \gamma v_{\star}(s^{\prime})\right]\end{align}$$or$$\begin{align}q_{\star}(s, a) &amp;= \mathbb{E} [R_{t+1} + \gamma \max_{a^{\prime}} q_{\star}(S_{t+1}, a^{\prime}) \ | \ S_{t}=s, A_{t}=a] \\&amp;= \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma \max_{a^{\prime}} q_{\star}(s^{\prime}, a^{\prime})]\end{align}$$for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}, \; \text{and} \; s^{\prime} \in \mathcal{S^{+}}$. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.First we consider how to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem. Recall that for all $s \in \mathcal{S}$,$$\begin{align}v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots \ | \ S_{t}=s] \\&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t}=s] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{\pi}(s^{\prime})]\end{align}$$If the environment’s dynamics are complete known, then (7) is a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns (the $v_{\pi}(s), s \in \mathcal{S}$). In principle, its solution is a straightforward, if tedious, computation. For our purpose, iterative solution methods are most suitable. The initial approximation, $v_0$, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for $v_{\pi}$ as an update rule:$$\begin{align}v_{k+1}(s) &amp;\doteq \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r|s, a) [r + \gamma v_{k} (s^{\prime})]\end{align}$$This algorithm is called iterative policy evaluation.Iterative policy evaluationInput $\pi$, the policy to be evaluatedInitialize an array $V(s) = 0$, for all $s \in \mathcal{S^{+}}$Repeat​ $\Delta \leftarrow 0$​ for each $s \in \mathcal{S}$:​ $v \leftarrow V(s)$​ $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$until $\Delta &lt; \theta$ (a small positive number)Output $V \approx v_{\pi}$We can see the algorithm used in the grid world problem just is the iterative policy evaluation.Our reason for computing the value function for a policy is to help find better policies. Once a policy $\pi$ has been improved using $v_{\pi}$ to yield a better policy $\pi^{\prime}$, we can then compute $v_{\pi^{\prime}}$and improve it again to yield an even better $\pi^{\prime\prime}$. We can thus obtain a sequence of monotonically improving policies and value functions:$$\pi_{0} \stackrel{E}\longrightarrow v_{\pi_{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow v_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow v_{\star},$$where $\stackrel{E}\longrightarrow$ denotes a policy evaluation and $\stackrel{I}\longrightarrow$ denotes a policy improvement. This way of finding an optimal policy is called policy iteration.Policy iteration (using iterative policy evaluation)Initialization$V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A(s)}$ arbitrarily for all $s \in \mathcal{S}$Policy EvaluationRepeat​ $\Delta \leftarrow 0$​ For each $s \in \mathcal{S}$:​ $v \leftarrow V(s)$​ $V(s) \leftarrow \sum_{s^{\prime}, r} p(s^{\prime}, r | s, \pi(s)) [r + \gamma v(s^{\prime})]$​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$until $\Delta &lt; \theta$ (a small positive number)Policy Improvementpolicy-stable $\leftarrow$ trueFor each $s \in \mathcal{S}$:​ old-action $\leftarrow$ $\pi_(s)$​ $\pi (s) \leftarrow argmax_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$​ If old-action $\neq \pi(s)$, then policy-stable $\leftarrow$ falseIf policy-stable, then stop and return $V \approx v_{\star} \; \text{and} \; \pi \approx \pi_{\star}$; else go to 2.Let us solve a problem used by policy iteration. The problem defined as follows:Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and it credited \$10 by the national company. If he out of cats at that location, then the business is lost. Cars become available for renting the day after they are returned. To ensure that cars are available where they are needed, Jack ca move them between the two locations overnight, at a cost of \$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\frac{\lambda^{n}}{n!}e^{-\lambda}$, where $\lambda$ is the excepted number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\lambda=0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight.The excepted result is as follows:Figure 1The first, we define some facts of this problem:1234567891011121314151617# maximum # of cars in each locationMAX_CARS = 20# maximum # of cars to move during nightMAX_MOVE_OF_CARS = 5# expectation for rental requests in first locationRENTAL_REQUEST_FIRST_LOC = 3# expectation for rental requests in second locationRENTAL_REQUEST_SECOND_LOC = 4# expectation for # of cars returned in first locationRETURNS_FIRST_LOC = 3# expectation for # of cars returned in second locationRETURNS_SECOND_LOC = 2DISCOUNT = 0.9# credit earned by a carRENTAL_CREDIT = 10# cost of moving a carMOVE_CAR_COST = 2From the problem definition, we know that in this MDP the states is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight. Each action is a integer that positive number represents the number of cars moving from the first location to second location and vice verse.12345678# current policypolicy = np.zeros((MAX_CARS + 1, MAX_CARS + 1))# current state valuestateValue = np.zeros((MAX_CARS + 1, MAX_CARS + 1))# all possible statesstates = []# all possible actionsactions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)For visualization (Figure 1) convenient, we define a method:123456789101112131415161718192021222324# axes for printing useAxisXPrint = []AxisYPrint = []for i in range(0, MAX_CARS + 1): for j in range(0, MAX_CARS + 1): AxisXPrint.append(i) AxisYPrint.append(j) states.append([i, j])# plot a policy/state value matrixfigureIndex = 0def prettyPrint(data, labels): global figureIndex fig = plt.figure(figureIndex) figureIndex += 1 ax = fig.add_subplot(111, projection='3d') AxisZ = [] for i, j in states: AxisZ.append(data[i, j]) ax.scatter(AxisXPrint, AxisYPrint, AxisZ) ax.set_xlabel(labels[0]) ax.set_ylabel(labels[1]) ax.set_zlabel(labels[2])Next, we define a Poisson function that return the probability:12345678910111213# An up bound for poisson distribution# If n is greater than this value, then the probability of getting n is truncated to 0POISSON_UP_BOUND = 11# Probability for poisson distribution# @lam: lambda should be less than 10 for this functionpoissonBackup = dict()def poisson(n, lam): global poissonBackup key = n * 10 + lam if key not in poissonBackup.keys(): poissonBackup[key] = exp(-lam) * pow(lam, n) / factorial(n) return poissonBackup[key]Now, the preparation is done. We’ll implement the policy iteration algorithm as follows:12345678910111213141516171819202122232425262728293031323334353637newStateValue = np.zeros((MAX_CARS + 1, MAX_CARS + 1))improvePolicy = FalsepolicyImprovementInd = 0while True: if improvePolicy == True: # start policy improvement print('Policy improvement', policyImprovementInd) policyImprovementInd += 1 newPolicy = np.zeros((MAX_CARS + 1, MAX_CARS + 1)) for i, j in states: actionReturns = [] # go through all actions and select the best one for action in actions: if (action &gt;= 0 and i &gt;= action) or (action &lt; 0 and j &gt;= abs(action)): actionReturns.append(expectedReturn([i, j], action, stateValue)) else: actionReturns.append(-float('inf')) bestAction = argmax(actionReturns) newPolicy[i, j] = actions[bestAction] # if policy is stable policyChanges = np.sum(newPolicy != policy) print('Policy for', policyChanges, 'states changed') if policyChanges == 0: policy = newPolicy break policy = newPolicy improvePolicy = False # start policy evaluation for i, j in states: newStateValue[i, j] = expectedReturn([i, j], policy[i, j], stateValue) if np.sum(np.abs(newStateValue - stateValue)) &lt; 1e-4: stateValue[:] = newStateValue improvePolicy = True continue stateValue[:] = newStateValueWe can see the logistic is the same as the pseudocode of the policy iteration algorithm. There is a core method in the code, that is, exceptedReturn() is used to calculate the reward of cars rental.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# @state: [# of cars in first location, # of cars in second location]# @action: positive if moving cars from first location to second location,# negative if moving cars from second location to first location# @stateValue: state value matrixdef expectedReturn(state, action, stateValue): # initailize total return returns = 0.0 # cost for moving cars returns -= MOVE_CAR_COST * abs(action) # go through all possible rental requests for rentalRequestFirstLoc in range(0, POISSON_UP_BOUND): for rentalRequestSecondLoc in range(0, POISSON_UP_BOUND): # moving cars numOfCarsFirstLoc = int(min(state[0] - action, MAX_CARS)) numOfCarsSecondLoc = int(min(state[1] + action, MAX_CARS)) # valid rental requests should be less than actual # of cars realRentalFirstLoc = min(numOfCarsFirstLoc, rentalRequestFirstLoc) realRentalSecondLoc = min(numOfCarsSecondLoc, rentalRequestSecondLoc) # get credits for renting reward = (realRentalFirstLoc + realRentalSecondLoc) * RENTAL_CREDIT numOfCarsFirstLoc -= realRentalFirstLoc numOfCarsSecondLoc -= realRentalSecondLoc # probability for current combination of rental requests prob = poisson(rentalRequestFirstLoc, RENTAL_REQUEST_FIRST_LOC) * \ poisson(rentalRequestSecondLoc, RENTAL_REQUEST_SECOND_LOC) # if set True, model is simplified such that the # of cars returned in daytime becomes constant # rather than a random value from poisson distribution, which will reduce calculation time # and leave the optimal policy/value state matrix almost the same constantReturnedCars = True if constantReturnedCars: # get returned cars, those cars can be used for renting tomorrow returnedCarsFirstLoc = RETURNS_FIRST_LOC returnedCarsSecondLoc = RETURNS_SECOND_LOC numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS) numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS) returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc]) else: numOfCarsFirstLoc_ = numOfCarsFirstLoc numOfCarsSecondLoc_ = numOfCarsSecondLoc prob_ = prob for returnedCarsFirstLoc in range(0, POISSON_UP_BOUND): for returnedCarsSecondLoc in range(0, POISSON_UP_BOUND): numOfCarsFirstLoc = numOfCarsFirstLoc_ numOfCarsSecondLoc = numOfCarsSecondLoc_ prob = prob_ numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS) numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS) prob = poisson(returnedCarsFirstLoc, RETURNS_FIRST_LOC) * \ poisson(returnedCarsSecondLoc, RETURNS_SECOND_LOC) * prob returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc]) return returnsThe comments are very clear, and we’re going to do a lot of this. Finally, let us print the result:123prettyPrint(policy, ['# of cars in first location', '# of cars in second location', '# of cars to move during night'])prettyPrint(stateValue, ['# of cars in first location', '# of cars in second location', 'expected returns'])plt.show()The results are as follows:12345678910Policy improvement 0Policy for 332 states changedPolicy improvement 1Policy for 286 states changedPolicy improvement 2Policy for 83 states changedPolicy improvement 3Policy for 19 states changedPolicy improvement 4Policy for 0 states changedOne drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\pi}$ occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called value iteration. It can be written as a particular simple backup operation that combines the policy improvement and truncated policy evaluation steps:$$\begin{align}v_{k+1} &amp;\doteq \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\&amp;= \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{k}(s^{\prime})],\end{align}$$for all $s \in \mathcal{S}$.Value iterationInitialize array $V$ arbitrarily (e.g. $V(s) = 0$ for all $s \in \mathcal{S^{+}}$)Repeat​ $\Delta \leftarrow 0$​ For each $s \in \mathcal{S}$:​ $v \leftarrow V(s)$​ $V(s) \leftarrow \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$until $\Delta &lt; \theta$ (a small positive number)Output a deterministic policy, $\pi \approx \pi_{\star}$, such that​ $\pi(s) = \arg\max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$Let us use the value iteration algorithm to solve a Gambler’s Problem. The problem defined as follows:A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he staked on the flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of \$100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer number of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gambler’s capital, $s \in \{1, 2, \cdots, 99\}$ and the actions are stakes, $a \in \{0, 1, \cdots, \min(s, 100-s)\}$. The reward is zero on all transitions excepted those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let $p_h$ denote the probability of the coin coming up heads. If $p_h$ is known, then the entire problem is known and it can be solved, for instance, by value iteration.OK, now let us to solve this problem by use the value iteration algorithm.The first we defined some facts and some auxiliary data structure:1234567891011# goalGOAL = 100# all states, including state 0 and state 100states = np.arange(GOAL + 1)# probability of headheadProb = 0.4# optimal policypolicy = np.zeros(GOAL + 1)# state valuestateValue = np.zeros(GOAL + 1)stateValue[GOAL] = 1.0The step of value iteration:123456789101112131415# value iterationwhile True: delta = 0.0 for state in states[1:GOAL]: # get possilbe actions for current state actions = np.arange(min(state, GOAL - state) + 1) actionReturns = [] for action in actions: actionReturns.append(headProb * stateValue[state + action] + (1 - headProb) * stateValue[state - action]) newValue = np.max(actionReturns) delta += np.abs(stateValue[state] - newValue) # update state value stateValue[state] = newValue if delta &lt; 1e-9: breakCalculate the optimal policy:12345678# calculate the optimal policyfor state in states[1:GOAL]: actions = np.arange(min(state, GOAL - state) + 1) actionReturns = [] for action in actions: actionReturns.append(headProb * stateValue[state + action] + (1 - headProb) * stateValue[state - action]) # due to tie and precision, can't reproduce the optimal policy in book policy[state] = actions[argmax(actionReturns)]Print the results:123456789plt.figure(1)plt.xlabel('Capital')plt.ylabel('Value estimates')plt.plot(stateValue)plt.figure(2)plt.scatter(states, policy)plt.xlabel('Capital')plt.ylabel('Final policy (stake)')plt.show()The results are as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The GridWorld problem]]></title>
      <url>%2F2017%2F05%2F29%2FThe-GridWorld-problem%2F</url>
      <content type="text"><![CDATA[A reinforcement learning task that satisfied the Markov property is called Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).A particular finite MDP is defined by its state and action sets and by the one-step dynamics of the environment. Given any state and action s and a, the probability of each possible pair of next state and reward, s’, r, is denoted$$p(s^{\prime}, r | s, a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime}, R_{t+1}=r \ | \ S_{t}=s, A_{t}=a \}$$Given that, one can compute anything else one might want to know about the environment, such as the excepted rewards of state-action pairs,$$r(s,a) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a] = \sum_{r \in \mathcal{R}}r\sum_{s^{\prime} \in \mathcal{S}}p(s^{\prime},r|s, a)$$the state-transition probabilities,$$p(s^{\prime}|s,a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime} \ | \ S_{t}=s, A_{t}=a\} = \sum_{r \in \mathcal{R}} p(s^{\prime},r|s, a)$$and the excepted rewards for state-action-next-state triples,$$r(s, a, s^{\prime}) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a, S_{t+1}=s^{\prime}] = \frac{\sum_{r \in \mathcal{R}}rp(s^{\prime},r|s,a)}{p(s^{\prime}|s,a)}$$Almost all reinforcement learning algorithms involve estimating value functions–functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).Recall that a policy, $\pi$, is a mapping from a each state, $s \in \mathcal{S}$, and action, $a \in \mathcal{A}(s)$, to the probability $\pi(a|s)$ of taking action a when in state s. Informally, the value of a state s under a policy $\pi$, denoted $v_{\pi}(s)$, is the excepted return when starting in s and following $\pi$ thereafter. For MDPs, we can define $v_{\pi}(s)$ formally as$$v_{\pi}(s) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right]$$Note that the value of the terminal state, if any, is always zero. We call the function $v_{\pi}$ the state-value function for policy $\pi$.Similarly, we define the value of taking action a in state s under a policy $\pi$, denoted $q_{\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:$$q_{\pi}(s,a) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s, A_{t}=a] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s, A_{t}=a\right]$$We call $q_{\pi}$ the action-value function for policy $\pi$.A fundamental property of the value functions used in reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:$$\begin{align}v_{\pi}(s) &amp;\doteq \mathbb{E_{\pi}[G_{t} \ | \ S_{t}=s]} \\&amp;= \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right] \\&amp;= \mathbb{E_{\pi}}\left[R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} \ | \ S_{t}=s \right] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}}\sum_{r}p(s^{\prime},r|s,a) \left[ r + \gamma \mathbb{E_{\pi}} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} | S_{t+1}=s^{\prime} \right] \right] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},\end{align}$$Equation (11) is the Bellman equation for $v_{\pi}$.Figure 1 (left) shows a rectangular grid world representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Action would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions result in a reward of 0, excepted those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to $\mathrm{A^{\prime}}$. From state B, all actions yield a reward +5 and take the agent to $\mathrm{B^{\prime}}$.Figure 1Suppose the agent selects all four actions with equal probability in all states. Figure 1 (right) shows the value function, $v_{\pi}$, for this policy, for the discounted reward case with $\gamma = 0.9$. This value function was computed by solving the system of linear equations (11).OK, now let us solve this problem. The first, we need to define the grid world by code.12345678WORLD_SIZE = 5A_POS = [0, 1]A_PRIME_POS = [4, 1]B_POS = [0, 3]B_PRIME_POS = [2, 3]discount = 0.9world = np.zeros((WORLD_SIZE, WORLD_SIZE))This world has 5 by 5 cells, and there are four special cells: A, A’, B, B’. Discount represents the $\gamma $ in equation (11). We know that the agent in the world selects all four actions with equal probability in all states (cells). So we have:12345678# left, up, right, downactions = ['L', 'U', 'R', 'D']actionProb = []for i in range(0, WORLD_SIZE): actionProb.append([]) for j in range(0, WORLD_SIZE): actionProb[i].append(dict(&#123;'L':0.25, 'U':0.25, 'R':0.25, 'D':0.25&#125;))The actionProb is a list that has five items. Each item represents a row in the grid and it also is a list that has five items that represents a column in corresponding row, that is, each item in a row represents a cell in the grid. In all cells (states), there are four direction could be selected with equal probability 0.25. Then, we’ll define a undirected graph with weights. The node represented the cell in grid. If between two node has a edge then the agent could move between this two nodes (cells). The weight on the edges represents the reward do this move.12345678910111213141516171819202122232425262728293031323334353637383940414243444546nextState = []actionReward = []for i in range(0, WORLD_SIZE): nextState.append([]) actionReward.append([]) for j in range(0, WORLD_SIZE): next = dict() reward = dict() if i == 0: next['U'] = [i, j] reward['U'] = -1.0 else: next['U'] = [i - 1, j] reward['U'] = 0.0 if i == WORLD_SIZE - 1: next['D'] = [i, j] reward['D'] = -1.0 else: next['D'] = [i + 1, j] reward['D'] = 0.0 if j == 0: next['L'] = [i, j] reward['L'] = -1.0 else: next['L'] = [i, j - 1] reward['L'] = 0.0 if j == WORLD_SIZE - 1: next['R'] = [i, j] reward['R'] = -1.0 else: next['R'] = [i, j + 1] reward['R'] = 0.0 if [i, j] == A_POS: next['L'] = next['R'] = next['D'] = next['U'] = A_PRIME_POS reward['L'] = reward['R'] = reward['D'] = reward['U'] = 10.0 if [i, j] == B_POS: next['L'] = next['R'] = next['D'] = next['U'] = B_PRIME_POS reward['L'] = reward['R'] = reward['D'] = reward['U'] = 5.0 nextState[i].append(next) actionReward[i].append(reward)The nextState and actionReward are the same as actionProb that we explained earlier.Now, we could solve this problem by use the equation (11):$$\begin{align}v_{\pi}(s) &amp;\doteq \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},\end{align}$$Let us jump into the implementation detail.1234567891011121314while True: # keep iteration until convergence newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE)) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): for action in actions: newPosition = nextState[i][j][action] # bellman equation newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]]) if np.sum(np.abs(world - newWorld)) &lt; 1e-4: print('Random Policy') print(newWorld) break world = newWorldThe core code is:1newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]])The += represents the first sum notation in the equation (11). If we ensure the current state (cell) and action will take in this world, then the next state and reward also will be ensured. So $\sum_{s^{\prime},r} p(s^{\prime}, r | s, a)$ is equal to 1.The result as follows:123456Random Policy[[ 3.30902999 8.78932551 4.42765281 5.3224012 1.49221235] [ 1.52162172 2.9923515 2.25017358 1.90760531 0.5474363 ] [ 0.05085614 0.73820423 0.67314689 0.35821982 -0.40310755] [-0.97355865 -0.43546179 -0.35484864 -0.58557148 -1.18304148] [-1.8576669 -1.34519762 -1.22923364 -1.42288454 -1.97514545]]We can see the value of all states is the same as the Figure 1.Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi^{\prime}$ if its excepted return is greater than or equal to that of $\pi^{\prime}$ for all states. In other words, $\pi \ge \pi^{\prime}$ if and only if $v_{\pi}(s) \ge v_{\pi^{\prime}}(s)$ for all $s \in \mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the optimal state-value function, denote $v_{\star}$, and defined as$$v_{\star}(s) \doteq \max_{\pi} v_{\pi}(s),$$for all $s \in \mathcal{S}$.Optimal policies also share the same optimal action-value function, denoted $q_{\star}$, and defined as$$q_{\star}(s, a) \doteq \max_{\pi} q_{\pi}(s, a)$$for all $s \in \mathcal{S}$ and $a \in \mathcal{A(s)}$. For the state-action pair (s, a), this function gives the excepted return for taking action a in state s and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $v_{\star}$ as follows:$$q_{\star}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\star} \ | \ S_{t}=s, A_{t}=a]$$Suppose we solve the Bellman equation for $v_{\star}$ for the simple grid task introduced in earlier and shown again in Figure 2 (left). Recall that state A is followed by a reward of +10 and transition to state A’. while state B is followed by a reward of +5 and transition to state B’. Figure 2 (middle) shows the optimal value function, and Figure 2 (right) shows the corresponding optimal policies. Where there are multiple arrows in a cell, any of the corresponding actions are optimal.Figure 2Now, let us solve this problem:1234567891011121314151617world = np.zeros((WORLD_SIZE, WORLD_SIZE))while True: # keep iteration until convergence newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE)) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): values = [] for action in actions: newPosition = nextState[i][j][action] # value iteration values.append(actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]]) newWorld[i][j] = np.max(values) if np.sum(np.abs(world - newWorld)) &lt; 1e-4: print('Optimal Policy') print(newWorld) break world = newWorldWe can see the core code is as follows:1newWorld[i][j] = np.max(values)The only difference between this code and the earlier code is the prior only uses the maximum value and the latter uses the weighted average.The result is below:123456Optimal Policy[[ 21.97744338 24.41938153 21.97744338 19.41938153 17.47744338] [ 19.77969904 21.97744338 19.77969904 17.80172914 16.02153504] [ 17.80172914 19.77969904 17.80172914 16.02153504 14.41938153] [ 16.02153504 17.80172914 16.02153504 14.41938153 12.97744338] [ 14.41938153 16.02153504 14.41938153 12.97744338 11.67969904]]It is not doubt that the result is the same as the Figure 2 (middle).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[k-Armed Bandit Problem]]></title>
      <url>%2F2017%2F05%2F27%2Fk-Armed-Bandit-Problem%2F</url>
      <content type="text"><![CDATA[Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.This is the original form of the k-armed bandit problem. Each of the k actions has an excepted or mean reward given that action is selected; let us call this value of that action. We denote the action selected on time step t as $A_{t}$, and the corresponding reward as $R_{t}$. The value then of an arbitrary action a, denoted $q_{\star}(a)$, is the excepted reward given that a is selected:$$q_{\star}(a) = \mathbb{E}[R_t|A_t=a]$$If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time t as $Q_{t}(a) \approx q_{\star}(a)$.We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when action is selected. One natural way to estimate this is by averaging the rewards actually received:$$Q_t(a) \doteq \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbf{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbf{1}_{A_i=a}}$$where $\mathbf{1}_{\text{predicate}}$ denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $Q_1(a) = 0$. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_{\star} (a)$. We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards.The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step t one of the greedy actions, $A_t^\star$ for which $Q_t(A_t^\star) = \max_a Q_t(a)$. This greedy action selection method can be written as$$A_t \doteq argmax_a Q_t(a)$$Naturally, we could use the $\epsilon$-greedy method rather the greedy method. We’ll show their difference on the performance. Now, let’s jump into the implementation details. In order to be able to see the results quickly, we set to k to be 10. The first, we generate 10 stationary probability distributions that we’ll sample from to generate action values. The generate method is below:1data=np.random.randn(200,10) + np.random.randn(10)We first generate randomly 10 true excepted values by np.random.randn(10), then I’m going to change the variance to 1. So we get 10 stationary probability distributions. The visualizations are as follows:We’re going to compare how different $\epsilon$ values affect the end result.123456789101112131415161718192021def epsilonGreedy(nBandits, time): epsilons = [0, 0.1, 0.01] bandits = [] for epsInd, eps in enumerate(epsilons): bandits.append([Bandit(epsilon=eps, sampleAverages=True) for _ in range(0, nBandits)]) bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 for eps, counts in zip(epsilons, bestActionCounts): plt.plot(counts, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.figure(figureIndex) figureIndex += 1 for eps, rewards in zip(epsilons, averageRewards): plt.plot(rewards, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('average reward') plt.legend()Before we go into the details, we introduce the Bandit object first.123456789101112class Bandit: # @kArm: # of arms # @epsilon: probability for exploration in epsilon-greedy algorithm # @initial: initial estimation for each action # @stepSize: constant step size for updating estimations # @sampleAverages: if True, use sample averages to update estimations instead of constant step size # @UCB: if not None, use UCB algorithm to select action # @gradient: if True, use gradient based bandit algorithm # @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm def __init__(self, kArm=10, epsilon=0., initial=0., stepSize=0.1, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=0.): def getAction(self): def takeAction(self, action):For now we just introduce sample-average method, so skip other methods parameters. Let us see the initialization method.12345678910111213141516171819202122232425262728293031def __init__(self, kArm=10, epsilon=0., initial=0., stepSize=0.1, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=0.): self.k = kArm self.stepSize = stepSize self.sampleAverages = sampleAverages self.indices = np.arange(self.k) self.time = 0 self.UCBParam = UCBParam self.gradient = gradient self.gradientBaseline = gradientBaseline self.averageReward = 0 self.trueReward = trueReward # real reward for each action self.qTrue = [] # estimation for each action self.qEst = np.zeros(self.k) # # of chosen times for each action self.actionCount = [] self.epsilon = epsilon # initialize real rewards with N(0,1) distribution and estimations with desired initial value for i in range(0, self.k): self.qTrue.append(np.random.randn() + trueReward) self.qEst[i] = initial self.actionCount.append(0) self.bestAction = np.argmax(self.qTrue)There are some important attributes. time is a number that represents the time steps now. actionCount is the times that correspond actions have been taken prior to current time steps. qTrue is a list. And each item is the true excepted value corresponding to each action. qEst is the estimate value of each action. It’s initialized to zero. epsilon is the $\epsilon$. Next, in the for loop, we initialize real rewards with N(0, 1) distribution and estimations with desired initial value. At last, the bestAction store the current best action will be take.The next method tell us how to get the next action should be take:1234567891011121314151617def getAction(self): # explore if self.epsilon &gt; 0: if np.random.binomial(1, self.epsilon) == 1: np.random.shuffle(self.indices) return self.indices[0] # exploit if self.UCBParam is not None: UCBEst = self.qEst + \ self.UCBParam * np.sqrt(np.log(self.time + 1) / (np.asarray(self.actionCount) + 1)) return np.argmax(UCBEst) if self.gradient: expEst = np.exp(self.qEst) self.actionProb = expEst / np.sum(expEst) return np.random.choice(self.indices, p=self.actionProb) return np.argmax(self.qEst)We can skip the second and the third if statements (we’ll introduce this two methods later). If we use greedy method, we just return the action that has highest value. Otherwise, we’re choosing randomly at $\epsilon$ probability.12345678910111213141516171819202122def takeAction(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.qTrue[action] self.time += 1 self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time self.actionCount[action] += 1 if self.sampleAverages: # update estimation using sample averages self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action]) elif self.gradient: oneHot = np.zeros(self.k) oneHot[action] = 1 if self.gradientBaseline: baseline = self.averageReward else: baseline = 0 self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb) else: # update estimation with constant step size self.qEst[action] += self.stepSize * (reward - self.qEst[action]) return rewardSimilarly, we just skip other if statements and focus on this row:1self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])This formula seems different with the earlier one. The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and per-time-step computation.To simplify notation we concentrate on a single action. Let $R_i$ now denote the reward received after ith selection of this action, and let $Q_n$ denote the estimate of its action value after it has been select $n-1$ times, which we can now write simply as$$Q_n \doteq \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}$$The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, in this case the memory and computational requirements would grow over time as more rewards are seen.It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given $Q_n$ and the nth reward, $R_n$, the new average of all n rewards can be computed by$$\begin{align}Q_{n+1} &amp;\doteq \frac{1}{n}\sum_{i=1}^{n} R_i \\&amp;= \frac{1}{n}(R_n + \sum_{i=1}^{n-1} R_i) \\&amp;= \frac{1}{n}(R_n + (n-1) \frac{1}{n-1}\sum_{i=1}^{n-1} R_i) \\&amp;= \frac{1}{n}(R_n + (n-1) Q_n) \\&amp;= \frac{1}{n}(R_n + n Q_n - Q_n) \\&amp;= Q_n + \frac{1}{n}[R_n - Q_n]\end{align}$$So this is why the code is look like this:1self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])Back to epsilonGreedy() method:123456789101112131415161718192021def epsilonGreedy(nBandits, time): epsilons = [0, 0.1, 0.01] bandits = [] for epsInd, eps in enumerate(epsilons): bandits.append([Bandit(epsilon=eps, sampleAverages=True) for _ in range(0, nBandits)]) bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 for eps, counts in zip(epsilons, bestActionCounts): plt.plot(counts, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.figure(figureIndex) figureIndex += 1 for eps, rewards in zip(epsilons, averageRewards): plt.plot(rewards, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('average reward') plt.legend()Now, we get nBandits bandits and each bandit has 10 arm. Then we use a bandit simulation to simulate this process.1234567891011121314def banditSimulation(nBandits, time, bandits): bestActionCounts = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))] averageRewards = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))] for banditInd, bandit in enumerate(bandits): for i in range(0, nBandits): for t in range(0, time): action = bandit[i].getAction() reward = bandit[i].takeAction(action) averageRewards[banditInd][t] += reward if action == bandit[i].bestAction: bestActionCounts[banditInd][t] += 1 bestActionCounts[banditInd] /= nBandits averageRewards[banditInd] /= nBandits return bestActionCounts, averageRewardsThe bandits is a list that has three item. Each item is a list that contains nBandits bandits that has a corresponding epsilon value. We calculate the average total reward of 2000 bandits is to eliminate the effect of noise. Ok, let us see the final result.We can see the algorithm reaches the best performance when epsilon is set to 0.1.The averaging methods discussed so far are appropriate in a stationary environment, but not if the bandit is changing over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to weight recent rewards more heavily than long-past ones. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule for updating an average $Q_n$ of the $n-1$ past rewards is modified to be$$Q_{n+1} \doteq Q_n + \alpha [R_n - Q_n]$$where the step-size parameter $\alpha \in (0, 1]$ is constant. This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:$$\begin{align}Q_{n+1} &amp;\doteq Q_n + \alpha [R_n - Q_n] \\&amp;= \alpha R_n + (1 - \alpha) Q_n \\&amp;= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 Q_{n-1} \\&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 R_{n-1} + \cdots + (1 - \alpha) ^ {n-1} \alpha R_1 + (1 - \alpha) ^ n Q_1 \\&amp;= (1 - \alpha) ^ n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha) ^ {n-i} R_i\end{align}$$We call this a weighted average because the sum of the weights is 1. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. According, this is sometimes called an exponential, recency-weighted average.Sometimes it is convenient to vary the step-size parameter from step to step. Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after nth selection of action a. As we noted, the choice $\alpha_n(a)=\frac{1}{n}$ results in the sample-average method, which is guaranteed to converge to the true action values by the law of the large numbers. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:$$\sum_{n=1}^{\infty}\alpha_{n}(a) = \infty \; \text{and} \; \sum_{n=1}^{\infty}\alpha_{n}^{2}(a) &lt; \infty$$All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, $Q_1(a)$. In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant $\alpha$ (weighted avetrage), the bias is permanent, though decreasing over time. In practice, this kind of bias is usually not a problem and can sometimes be very helpful. The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be excepted. So, let us do a experiment. The first we set them all to zero and the we set them all to a constant, 5.1234567891011121314151617181920def optimisticInitialValues(nBandits, time): bandits = [[], []] bandits[0] = [Bandit(epsilon=0, initial=5, stepSize=0.1) for _ in range(0, nBandits)] bandits[1] = [Bandit(epsilon=0.1, initial=0, stepSize=0.1) for _ in range(0, nBandits)] bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 plt.plot(bestActionCounts[0], label='epsilon = 0, q = 5') plt.plot(bestActionCounts[1], label='epsilon = 0.1, q = 0') plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.figure(figureIndex) figureIndex += 1 plt.plot(averageRewards[0], label='epsilon=0, initial=5, stepSize=0.1') plt.plot(averageRewards[1], label='epsilon=0.1, initial=0, stepSize=0.1') plt.xlabel('Steps') plt.ylabel('average reward') plt.legend()The Bandit object’s takeAction() has a little difference:1self.qEst[action] += self.stepSize * (reward - self.qEst[action])The result is as follows:We can see it reaches the better performance than the $\epsilon$-greedy method, when they are all used the weighted average method. Notice that the $\epsilon$-greedy with weighted-average method is worsen than the $\epsilon$-greedy with sample-average method.We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial state in any special way is unlikely to help with the general nonstationary case.Exploration is needed because the estimates of the action values are uncertain. The greedy actions are those that look best at present, but some of the other actions may actually be better. $\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions as$$A_t \doteq argmax_a \left [Q_t(a) + c\sqrt{\frac{\ln{t}}{N_t(a)}}\ \right]$$where $N_t(a)$ denotes the number of times that action a has been selected prior to time t, and the number $c&gt;0$ controls the degree of exploration. If $N_t(a)=0$, then a is considered to be a maximizing action. The idea of this is called upper confidence bound (UCB). Let us implement it.12345678910111213def ucb(nBandits, time): bandits = [[], []] bandits[0] = [Bandit(epsilon=0, stepSize=0.1, UCBParam=2) for _ in range(0, nBandits)] bandits[1] = [Bandit(epsilon=0.1, stepSize=0.1) for _ in range(0, nBandits)] _, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 plt.plot(averageRewards[0], label='UCB c = 2') plt.plot(averageRewards[1], label='epsilon greedy epsilon = 0.1') plt.xlabel('Steps') plt.ylabel('Average reward') plt.legend()We note that the UCBParam=2. The Bandit object explains this. The getAction() method and takeAction() method are as follows:1234567891011121314151617181920212223242526272829303132333435363738394041def getAction(self): # explore if self.epsilon &gt; 0: if np.random.binomial(1, self.epsilon) == 1: np.random.shuffle(self.indices) return self.indices[0] # exploit if self.UCBParam is not None: UCBEst = self.qEst + \ self.UCBParam * np.sqrt(np.log(self.time + 1) / (np.asarray(self.actionCount) + 1)) return np.argmax(UCBEst) if self.gradient: expEst = np.exp(self.qEst) self.actionProb = expEst / np.sum(expEst) return np.random.choice(self.indices, p=self.actionProb) return np.argmax(self.qEst)# take an action, update estimation for this actiondef takeAction(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.qTrue[action] self.time += 1 self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time self.actionCount[action] += 1 if self.sampleAverages: # update estimation using sample averages self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action]) elif self.gradient: oneHot = np.zeros(self.k) oneHot[action] = 1 if self.gradientBaseline: baseline = self.averageReward else: baseline = 0 self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb) else: # update estimation with constant step size self.qEst[action] += self.stepSize * (reward - self.qEst[action]) return rewardWe can see the policy get next action has changed but the update policy has not changed. The result is here:We omit the figure about the optimal action because the trend of this two figures are the same. UCB will often perform well, but is more difficult than $\epsilon$-greedy to extend beyond bandits to the more general reinforcement learning setting considered in the more advanced problems. In these more advanced settings there is currently no known practical way of utilizing the idea of UCB action selection.So far we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. Now, we consider learning a numerical preference $H_t(a)$ for each action a. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the preferences there is no effect on the action probabilities, which are determined according to a softmax distribution as follows:$$Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)$$where here we have also introduced a useful new notation $\pi_t(a)$ for the probability of taking action a at time t. Initially all preferences are the same (e.g., $H_1(a)=0, \forall a$) so that all actions have an equal probability of being selected.There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. On each step, after selection the action $A_t$ and receiving the reward $R_t$, the preferences are updated by:$$\begin{align}H_{t+1}(A_t) \doteq H_t(A_t) + \alpha (R_t - \overline{R_t}) (1 - \pi_t(A_t)), \; \text{and} \\H_{t+1}(a) \doteq H_t(a) - \alpha (R_t - \overline{R_t}) \pi_t(a), \;\;\;\;\;\; \forall{a \neq A_t}\end{align}$$where $\alpha&gt;0$ is a ste-size parameter, and $\overline{R_t} \in \mathbb{R}$ is the average of all the rewards up through and including time t, which can be computed incrementally. The $\overline{R_t}$ term serves as a baseline with which the reward is compared. Let us see the implement details (takeAction() method and getAction() method).1234if self.gradient: expEst = np.exp(self.qEst) self.actionProb = expEst / np.sum(expEst) return np.random.choice(self.indices, p=self.actionProb)12345678elif self.gradient: oneHot = np.zeros(self.k) oneHot[action] = 1 if self.gradientBaseline: baseline = self.averageReward else: baseline = 0 self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot self.actionProb)The below figure shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true excepted rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before).Despite their simplicity, in our opinion the methods presented in here can fairly be considered the state of the art. Finally, we do a parameter study of the various bandit algorithms presented in here.123456789101112131415161718192021222324252627def figure2_6(nBandits, time): labels = ['epsilon-greedy', 'gradient bandit', 'UCB', 'optimistic initialization'] generators = [lambda epsilon: Bandit(epsilon=epsilon, sampleAverages=True), lambda alpha: Bandit(gradient=True, stepSize=alpha, gradientBaseline=True), lambda coef: Bandit(epsilon=0, stepSize=0.1, UCBParam=coef), lambda initial: Bandit(epsilon=0, initial=initial, stepSize=0.1)] parameters = [np.arange(-7, -1), np.arange(-5, 2), np.arange(-4, 3), np.arange(-2, 3)] bandits = [[generator(math.pow(2, param)) for _ in range(0, nBandits)] for generator, parameter in zip(generators, parameters) for param in parameter] _, averageRewards = banditSimulation(nBandits, time, bandits) rewards = np.sum(averageRewards, axis=1)/time global figureIndex plt.figure(figureIndex) figureIndex += 1 i = 0 for label, parameter in zip(labels, parameters): l = len(parameter) plt.plot(parameter, rewards[i:i+l], label=label) i += l plt.xlabel('Parameter(2^x)') plt.ylabel('Average reward') plt.legend()The results as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Tic-Tac-Toe Game]]></title>
      <url>%2F2017%2F05%2F26%2FTic-tac-toe%2F</url>
      <content type="text"><![CDATA[What is the Tic-Tac-Toe game? Two players take turns playing on a three-by-three board. One player plays Xs and the other Os until one player wins by placing three marks in a row, horizontally, vertically, or diagonally, as the X player has in the game shown to the blew. If the board fills up with neither player getting three in a row, the game is a draw.There have three steps. Train, compete and play.The first, let’s to see the train period. Follow is the train() method.123456789101112131415161718def train(epochs=20000): player1 = Player() player2 = Player() judger = Judger(player1, player2) player1Win = 0.0 player2Win = 0.0 for i in range(0, epochs): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / epochs) print(player2Win / epochs) player1.savePolicy() player2.savePolicy()Train() method create two Player objects first, and then let them to play the tic-tac-toe through a Judger object. It’s a very simple process.Next. let to get into the Player object.Follow is the code of the Player object. For understand easily, I omitted the implementation details of each method.1234567891011121314class Player: # @stepSize: step size to update estimations # @exploreRate: possibility to explore def __init__(self, stepSize=0.1, exploreRate=0.1): def reset(self): def setSymbol(self, symbol): # accept a state def feedState(self, state): # update estimation according to reward def feedReward(self, reward): # determine next action def takeAction(self): def savePolicy(self): def loadPolicy(self):As a Player, the important thing during the train process is to learn a policy. The policy is a selection when the player faces a state. So there are two method savePolicy() and loadPolicy(). When the train process end, the player save its learned policy and load the same policy when the player compete with someone else later.Follow is the implementation details:123456789def savePolicy(self): fw = open('optimal_policy_' + str(self.symbol), 'wb') pickle.dump(self.estimations, fw) fw.close()def loadPolicy(self): fr = open('optimal_policy_' + str(self.symbol), 'rb') self.estimations = pickle.load(fr) fr.close()And, let’s to jump into the initialization method, the below is its source code:123456def __init__(self, stepSize=0.1, exploreRate=0.1): self.allStates = allStates self.estimations = dict() self.stepSize = stepSize self.exploreRate = exploreRate self.states = []Every player hold a dictionary. For each item in the dictionary, the key is the state, and the value is the estimation of the probability to win from this state. We use the TD(0) method to solve the problem. That is, we need to update .the state-value function step by step. The update rule is below:$$ V(s) = V(s) + \alpha [V(s’) - V(s)] ​$$The $\alpha$ is the step size, and the $s’$ is the next state, $s$ is the current state. $V(\star)$ is the estimation of the probability to win from $*$ state.So, what is the explore rate. We need to know how to choose the next action at current state if we want to understand the explore rate. For every state, first we find the every state it can transfer to. Then we look up the dictionary to find the state that has the highest estimation value. This state is our action will take. The method called greedy policy. But, the value of each state is our estimation, so we can’t say it’s the true probability. So the greedy policy has some error. There is a method to solve this problem. At every state, we not only select the next state that has the highest probability but also choose the next state randomly by explore rate probability. Formerly, if we use the symbol $\epsilon$ represents the explore rate, then the method is called $\epsilon$-greedy method.Next, let’s see what is the allStates variable.1self.allStates = allStateswe can see12# all possible board configurationsallStates = getAllStates()So what is the getAllStates() look like?1234567def getAllStates(): currentSymbol = 1 currentState = State() allStates = dict() allStates[currentState.getHash()] = (currentState, currentState.isEnd()) getAllStatesImpl(currentState, currentSymbol, allStates) return allStatesUntil now you may ask what is the STATE? Below is the definition:123456class State: def __init__(self): def getHash(self): def isEnd(self): def nextState(self, i, j, symbol): def show(self):One state is one arrangement of pieces on the board. So one state has some extra attributions. Such as who is the winner at current state, if the state is the terminal state or not and so on. Specially, each state has a hash value for representation convenient. The board is represented by a n by n array, that is, one state is a n by n array.123456789def __init__(self): # the board is represented by a n * n array, # 1 represents chessman of the player who moves first, # -1 represents chessman of another player # 0 represents empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hashVal = None self.end = NoneBelow is how to calculate the hash value of a state:123456789# calculate the hash value for one state, it's uniquedef getHash(self): if self.hashVal is None: self.hashVal = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: i = 2 self.hashVal = self.hashVal * 3 + i return int(self.hashVal)Below is how to judge if a state is end or not:12345678910111213141516171819202122232425262728293031323334353637383940# determine whether a player has won the game, or it's a tiedef isEnd(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.endThere are two scenarios for the end of the game: Someone wins or ties. Because player A’s chessman is represents by 1 and play B is -1. So if A wins, then one row ‘s sum is n or one column’s sum is n or one diagnose’s sum is n. Otherwise is -n. And the state’s winner attribute is 1 or -1, that is, player A or player B. if the sum of the absolute value of the all chessman in the board is n by n, then the game is tie, winner is 0 (that is no one wins).When someone put a chessman in the board, then the state is change and transfer to another state. How to get the state?12345def nextState(self, i, j, symbol): newState = State() newState.data = np.copy(self.data) newState.data[i, j] = symbol return newStateAnd the last, we are play a game so we need a GUI. This is what the show() to do.123456789101112131415# print the boarddef show(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------')Let’s come back to the getAllStates() method.1234567def getAllStates(): currentSymbol = 1 currentState = State() allStates = dict() allStates[currentState.getHash()] = (currentState, currentState.isEnd()) getAllStatesImpl(currentState, currentSymbol, allStates) return allStatesNow we know what is a state and the next we need to generate all possible state. The first, we need build a data structure to store the all states. So we define a dictionary allStates. It’s key is the hash value of the state, and it’s value is a tuple. The first item of the tuple is the state (a n by n array) and the second item is a flag that represent the state whether is a terminal state or not. For generate the all states, we jump into the getAllStatesImpl() method.1234567891011def getAllStatesImpl(currentState, currentSymbol, allStates): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if currentState.data[i][j] == 0: newState = currentState.nextState(i, j, currentSymbol) newHash = newState.getHash() if newHash not in allStates.keys(): isEnd = newState.isEnd() allStates[newHash] = (newState, isEnd) if not isEnd: getAllStatesImpl(newState, -currentSymbol, allStates)The getAllStatesImpl() method start with a empty board (currentState), and generate the states step by step (because it recursive calls itself). Because the game is very simple, so we could generate all possible states. But for the larger game, this is impossible.Tada~Let’s come back to the Player object. We put the code here again for convenience.1234567891011121314class Player: # @stepSize: step size to update estimations # @exploreRate: possibility to explore def __init__(self, stepSize=0.1, exploreRate=0.1): def reset(self): def setSymbol(self, symbol): # accept a state def feedState(self, state): # update estimation according to reward def feedReward(self, reward): # determine next action def takeAction(self): def savePolicy(self): def loadPolicy(self):We has explained the initialization method. It’s worth to notice that the Player object has a attribute states. We’ll explain it later.Below is the reset() method:12def reset(self): self.states = []and below is the setSymbol() method:1234567891011def setSymbol(self, symbol): self.symbol = symbol for hash in self.allStates.keys(): (state, isEnd) = self.allStates[hash] if isEnd: if state.winner == self.symbol: self.estimations[hash] = 1.0 else: self.estimations[hash] = 0 else: self.estimations[hash] = 0.5We know that every player’s chessman in the board has a symbol (1 or -1). This method is set a symbol to the player. Furthermore, this method initialize the estimate state-value dictionary (we mentioned it earlier).And the feedState() method:12def feedState(self, state): self.states.append(state)The same as the states variable, we’ll explain it later.Go on, below is the feedForward() method.This method not only the core of the Player object, but also it’s the core of the method that solve this game. That is, it’s the core of the TD(0) method.1234567891011def feedReward(self, reward): if len(self.states) == 0: return self.states = [state.getHash() for state in self.states] target = reward for latestState in reversed(self.states): value = self.estimations[ latestState] + self.stepSize * (target - self.estimations[latestState]) self.estimations[latestState] = value target = value self.states = []We mentioned the update rule earlier. Below is it’s implementation:12value = self.estimations[ latestState] + self.stepSize * (target - self.estimations[latestState])Notice that we can see there are two row in the code:12self.estimations[latestState] = valuetarget = valueSo the update rule is a chain-like update rule. Specially, the states variable is set to empty (In the same way, we’ll explain it later).The next method (implement the $\epsilon$-greedy policy) also is very important, because it tells the player how to take the next action:12345678910111213141516171819202122232425262728def takeAction(self): state = self.states[-1] nextStates = [] nextPositions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: nextPositions.append([i, j]) nextStates.append(state.nextState( i, j, self.symbol).getHash()) if np.random.binomial(1, self.exploreRate): np.random.shuffle(nextPositions) # Not sure if truncating is the best way to deal with exploratory step # Maybe it's better to only skip this step rather than forget all # the history self.states = [] action = nextPositions[0] action.append(self.symbol) return action values = [] for hash, pos in zip(nextStates, nextPositions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) action = values[0][1] action.append(self.symbol) return actionWe’ll see that the return action is a list that the first item is a list contains the next position and the second item is the symbol that represents the player.Ok, the travel about the Player object is over. Then, we’ll look into the Judger object. Before that, let’s recall the train() process.123456789101112131415161718def train(epochs=20000): player1 = Player() player2 = Player() judger = Judger(player1, player2) player1Win = 0.0 player2Win = 0.0 for i in range(0, epochs): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / epochs) print(player2Win / epochs) player1.savePolicy() player2.savePolicy()We can see that the Judger object accept two parameters, that is, two player object. The definition of Judger is below:123456789101112class Judger: # @player1: player who will move first, its chessman will be 1 # @player2: another player with chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2, feedback=True): # give reward to two players def giveReward(self): def feedCurrentState(self): def reset(self): # @show: if True, print each board during the game def play(self, show=False):Notice that the rewards only receive at the end of the game. The first, let’s see the initialization method.1234567891011def __init__(self, player1, player2, feedback=True): self.p1 = player1 self.p2 = player2 self.feedback = feedback self.currentPlayer = None self.p1Symbol = 1 self.p2Symbol = -1 self.p1.setSymbol(self.p1Symbol) self.p2.setSymbol(self.p2Symbol) self.currentState = State() self.allStates = allStatesp1 and p2 is the two player that play the game. The feedback represents if the reward propagation back or not. On the train process the feedback is true and on the compete process and play process the feedback is false. currentPlayer represents who should move next. and next the judger set symbol for each player. The currentState is the start state (the board is empty).Go on. Below is the giveReward() method:12345678910def giveReward(self): if self.currentState.winner == self.p1Symbol: self.p1.feedReward(1) self.p2.feedReward(0) elif self.currentState.winner == self.p2Symbol: self.p1.feedReward(0) self.p2.feedReward(1) else: self.p1.feedReward(0) self.p2.feedReward(0)Just like we say earlier, the rewards only receive at the end of the game. So if player A wins, then we give him a reward 1 and otherwise we give him a reward 0. If ties, then all reward is 0. We explain the feedCurrentState() later. Now we explain reset() method first.12345def reset(self): self.p1.reset() self.p2.reset() self.currentState = State() self.currentPlayer = NoneIt’s simple right? Let’s skip it and go to the core method:1234567891011121314151617181920def play(self, show=False): self.reset() self.feedCurrentState() while True: # set current player if self.currentPlayer == self.p1: self.currentPlayer = self.p2 else: self.currentPlayer = self.p1 if show: self.currentState.show() [i, j, symbol] = self.currentPlayer.takeAction() self.currentState = self.currentState.nextState(i, j, symbol) hashValue = self.currentState.getHash() self.currentState, isEnd = self.allStates[hashValue] self.feedCurrentState() if isEnd: if self.feedback: self.giveReward() return self.currentState.winnerWe can see the two player alternate to play chess. Each reached state on the game will feed to the players’ states attribute.1self.feedCurrentState()So below is the method like:123def feedCurrentState(self): self.p1.feedState(self.currentState) self.p2.feedState(self.currentState)12def feedState(self, state): self.states.append(state)Let’s explain the states now. Each player only update the states that the game reached in one game. Each reached state on the game will feed to the players’ states attribute. Note that, the player just update part of the states of the all states. Only after a lot of games, the all states could be updated. So all TD methods need a lot of epochs.Ouch! Finally three core objects are explained. Now we’ll clear about the three process: train, compete and play.123456789101112131415161718def train(epochs=20000): player1 = Player() player2 = Player() judger = Judger(player1, player2) player1Win = 0.0 player2Win = 0.0 for i in range(0, epochs): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / epochs) print(player2Win / epochs) player1.savePolicy() player2.savePolicy()123456789101112131415161718def compete(turns=500): player1 = Player(exploreRate=0) player2 = Player(exploreRate=0) judger = Judger(player1, player2, False) player1.loadPolicy() player2.loadPolicy() player1Win = 0.0 player2Win = 0.0 for i in range(0, turns): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / turns) print(player2Win / turns)12345678910111213def play(): while True: player1 = Player(exploreRate=0) player2 = HumanPlayer() judger = Judger(player1, player2, False) player1.loadPolicy() winner = judger.play(True) if winner == player2.symbol: print("Win!") elif winner == player1.symbol: print("Lose!") else: print("Tie!")It’s worth noting that there is a HumanPlayer object.1234567891011121314151617181920212223242526272829class HumanPlayer: def __init__(self, stepSize=0.1, exploreRate=0.1): self.symbol = None self.currentState = None return def reset(self): return def setSymbol(self, symbol): self.symbol = symbol return def feedState(self, state): self.currentState = state return def feedReward(self, reward): return def takeAction(self): data = int(input("Input your position:")) data -= 1 i = data // int(BOARD_COLS) j = data % BOARD_COLS if self.currentState.data[i, j] != 0: return self.takeAction() return (i, j, self.symbol)We’ll see that this object do nothing. It just put a chess to on the board.OK, you’re done! Finally, we put the complete code here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[How to generate a unique ID in a distribute system]]></title>
      <url>%2F2017%2F05%2F25%2FHow-to-generate-a-unique-ID-in-a-distribute-system%2F</url>
      <content type="text"><![CDATA[现今所有的企业级应用都需要处理海量的数据对象，这些对象都需要一个唯一的ID与其他的对象区分开来。在关系型数据库中，我们一般是创建主键来达到这个目的。一些数据库支持内建的列类型（AUTO_INCREMENT/AUTO_NUMBER）来产生一个单调递增的64位长的数。有些人喜欢在他们的应用层中生成id，以便获得对这代人的更多控制，然后使用数据层保存记录。但是，第二种方法通过缓存最新生成的数字，并且通过某种持久性技术保存已经生成的id的轨迹来避免主键冲突。上述两种方法本身都有各自的优点和缺点，但它们都有一个共同的缺点，即在分布式架构的情况下，这些都不具有弹性。那么需要考虑数据分片在多个数据库节点之间时，第一个技术如何确保不同节点中的表不会产生相同的auto_increment数或想象一个拓扑，在多个节点上运行应用程序，那么第二种技术如何满足所有节点的需求。没有一种方法可以满足所有的需求，下面是在许多大型应用程序中使用的最流行的方法。1. 数据库自增长序列或字段最常见的方式。利用数据库，全数据库唯一。优点：1）简单，代码方便，性能可以接受。2）数字ID天然排序，对分页或者需要排序的结果很有帮助。缺点：1）不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理。2）在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。3）在性能达不到要求的情况下，比较难于扩展。4）如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦。5）分表分库的时候会有麻烦。优化方案：1）针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 1，4，7，10，Master2生成的是2,5,8,11 Master3生成的是 3,6,9,12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。2. UUID常见的方式。可以利用数据库也可以利用程序生成，一般来说全球唯一。优点：1）简单，代码方便。2）生成ID性能非常好，基本不会有性能问题。3）全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。缺点：1）没有排序，无法保证趋势递增。2）UUID往往是使用字符串存储，查询的效率比较低。3）存储空间比较大，如果是海量数据库，就需要考虑存储量的问题。4）传输数据量大5）不可读。3. UUID的变种1）为了解决UUID不可读，可以使用UUID to Int64的方法。及12345678/// &lt;summary&gt;/// 根据GUID获取唯一数字序列/// &lt;/summary&gt;public static long GuidToInt64()&#123; byte[] bytes = Guid.NewGuid().ToByteArray(); return BitConverter.ToInt64(bytes, 0);&#125;2）为了解决UUID无序的问题，NHibernate在其主键生成方式中提供了Comb算法（combined guid/timestamp）。保留GUID的10个字节，用另6个字节表示GUID生成的时间（DateTime）。12345678910111213141516171819202122232425262728293031323334/// &lt;summary&gt; /// Generate a new &lt;see cref="Guid"/&gt; using the comb algorithm. /// &lt;/summary&gt; private Guid GenerateComb()&#123; byte[] guidArray = Guid.NewGuid().ToByteArray(); DateTime baseDate = new DateTime(1900, 1, 1); DateTime now = DateTime.Now; // Get the days and milliseconds which will be used to build //the byte string TimeSpan days = new TimeSpan(now.Ticks - baseDate.Ticks); TimeSpan msecs = now.TimeOfDay; // Convert to a byte array // Note that SQL Server is accurate to 1/300th of a // millisecond so we divide by 3.333333 byte[] daysArray = BitConverter.GetBytes(days.Days); byte[] msecsArray = BitConverter.GetBytes((long) (msecs.TotalMilliseconds / 3.333333)); // Reverse the bytes to match SQL Servers ordering Array.Reverse(daysArray); Array.Reverse(msecsArray); // Copy the bytes into the guid Array.Copy(daysArray, daysArray.Length - 2, guidArray, guidArray.Length - 6, 2); Array.Copy(msecsArray, msecsArray.Length - 4, guidArray, guidArray.Length - 4, 4); return new Guid(guidArray);&#125;用上面的算法测试一下，得到如下的结果：作为比较，前面3个是使用COMB算法得出的结果，最后12个字符串是时间序（统一毫秒生成的3个UUID），过段时间如果再次生成，则12个字符串会比图示的要大。后面3个是直接生成的GUID。如果想把时间序放在前面，可以生成后改变12个字符串的位置，也可以修改算法类的最后两个Array.Copy。4. Redis生成ID当使用数据库来生成ID性能不够要求的时候，我们可以尝试使用Redis来生成ID。这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。可以使用Redis集群来获取更高的吞吐量。假如一个集群中有5台Redis。可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5。各个Redis生成的ID为：A：1,6,11,16,21B：2,7,12,17,22C：3,8,13,18,23D：4,9,14,19,24E：5,10,15,20,25这个，随便负载到哪个机确定好，未来很难做修改。但是3-5台服务器基本能够满足器上，都可以获得不同的ID。但是步长和初始值一定需要事先需要了。使用Redis集群也可以方式单点故障的问题。另外，比较适合使用Redis来生成每天从0开始的流水号。比如订单号=日期+当日自增长号。可以每天在Redis中生成一个Key，使用INCR进行累加。优点：1）不依赖于数据库，灵活方便，且性能优于数据库。2）数字ID天然排序，对分页或者需要排序的结果很有帮助。缺点：1）如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。2）需要编码和配置的工作量比较大。5. Twitter的snowflake算法snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。C#代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/// &lt;summary&gt; /// From: https://github.com/twitter/snowflake /// An object that generates IDs. /// This is broken into a separate class in case /// we ever want to support multiple worker threads /// per process /// &lt;/summary&gt; public class IdWorker &#123; private long workerId; private long datacenterId; private long sequence = 0L; private static long twepoch = 1288834974657L; private static long workerIdBits = 5L; private static long datacenterIdBits = 5L; private static long maxWorkerId = -1L ^ (-1L &lt;&lt; (int)workerIdBits); private static long maxDatacenterId = -1L ^ (-1L &lt;&lt; (int)datacenterIdBits); private static long sequenceBits = 12L; private long workerIdShift = sequenceBits; private long datacenterIdShift = sequenceBits + workerIdBits; private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private long sequenceMask = -1L ^ (-1L &lt;&lt; (int)sequenceBits); private long lastTimestamp = -1L; private static object syncRoot = new object(); public IdWorker(long workerId, long datacenterId) &#123; // sanity check for workerId if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new ArgumentException(string.Format("worker Id can't be greater than %d or less than 0", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new ArgumentException(string.Format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; public long nextId() &#123; lock (syncRoot) &#123; long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; throw new ApplicationException(string.Format("Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); &#125; if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) &#123; timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123; sequence = 0L; &#125; lastTimestamp = timestamp; return ((timestamp - twepoch) &lt;&lt; (int)timestampLeftShift) | (datacenterId &lt;&lt; (int)datacenterIdShift) | (workerId &lt;&lt; (int)workerIdShift) | sequence; &#125; &#125; protected long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; protected long timeGen() &#123; return (long)(DateTime.UtcNow - new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc)).TotalMilliseconds; &#125; &#125;测试代码如下：1234567891011121314151617181920212223242526272829303132333435363738private static void TestIdWorker() &#123; HashSet&lt;long&gt; set = new HashSet&lt;long&gt;(); IdWorker idWorker1 = new IdWorker(0, 0); IdWorker idWorker2 = new IdWorker(1, 0); Thread t1 = new Thread(() =&gt; DoTestIdWoker(idWorker1, set)); Thread t2 = new Thread(() =&gt; DoTestIdWoker(idWorker2, set)); t1.IsBackground = true; t2.IsBackground = true; t1.Start(); t2.Start(); try &#123; Thread.Sleep(30000); t1.Abort(); t2.Abort(); &#125; catch (Exception e) &#123; &#125; Console.WriteLine("done"); &#125; private static void DoTestIdWoker(IdWorker idWorker, HashSet&lt;long&gt; set) &#123; while (true) &#123; long id = idWorker.nextId(); if (!set.Add(id)) &#123; Console.WriteLine("duplicate:" + id); &#125; Thread.Sleep(1); &#125; &#125;snowflake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。优点：1）不依赖于数据库，灵活方便，且性能优于数据库。2）ID按照时间在单机上是递增的。缺点：1）在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。Snowflake 的其他变种Snowflake 有一些变种, 各个应用结合自己的实际场景对 Snowflake 做了一些改动. 这里主要介绍 3 种.5.1 Boundary flake变化:ID 长度扩展到 128 bits:最高 64 bits 时间戳;然后是 48 bits 的 Worker 号 (和 Mac 地址一样长);最后是 16 bits 的 Seq Number由于它用 48 bits 作为 Worker ID, 和 Mac 地址的长度一样, 这样启动时不需要和 Zookeeper 通讯获取 Worker ID. 做到了完全的去中心化基于 Erlang它这样做的目的是用更多的 bits 实现更小的冲突概率, 这样就支持更多的 Worker 同时工作. 同时, 每毫秒能分配出更多的 ID5.2 SimpleflakeSimpleflake 的思路是取消 Worker 号, 保留 41 bits 的 Timestamp, 同时把 sequence number 扩展到 22 bits;Simpleflake 的特点:sequence number 完全靠随机产生 (这样也导致了生成的 ID 可能出现重复)没有 Worker 号, 也就不需要和 Zookeeper 通讯, 实现了完全去中心化Timestamp 保持和 Snowflake 一致, 今后可以无缝升级到 SnowflakeSimpleflake 的问题就是 sequence number 完全随机生成, 会导致生成的 ID 重复的可能. 这个生成 ID 重复的概率随着每秒生成的 ID 数的增长而增长.所以, Simpleflake 的限制就是每秒生成的 ID 不能太多 (最好小于 100次/秒, 如果大于 100次/秒的场景, Simpleflake 就不适用了, 建议切换回 Snowflake).5.3 instagram 的做法先简单介绍一下 instagram 的分布式存储方案:先把每个 Table 划分为多个逻辑分片 (logic Shard), 逻辑分片的数量可以很大, 例如 2000 个逻辑分片然后制定一个规则, 规定每个逻辑分片被存储到哪个数据库实例上面; 数据库实例不需要很多. 例如, 对有 2 个 PostgreSQL 实例的系统 (instagram 使用 PostgreSQL); 可以使用奇数逻辑分片存放到第一个数据库实例, 偶数逻辑分片存放到第二个数据库实例的规则每个 Table 指定一个字段作为分片字段 (例如, 对用户表, 可以指定 uid 作为分片字段)插入一个新的数据时, 先根据分片字段的值, 决定数据被分配到哪个逻辑分片 (logic Shard)然后再根据 logic Shard 和 PostgreSQL 实例的对应关系, 确定这条数据应该被存放到哪台 PostgreSQL 实例上instagram unique ID 的组成:41 bits: Timestamp (毫秒)13 bits: 每个 logic Shard 的代号 (最大支持 8 x 1024 个 logic Shards)10 bits: sequence number; 每个 Shard 每毫秒最多可以生成 1024 个 ID生成 unique ID 时, 41 bits 的 Timestamp 和 Snowflake 类似, 这里就不细说了.主要介绍一下 13 bits 的 logic Shard 代号 和 10 bits 的 sequence number 怎么生成.logic Shard 代号:假设插入一条新的用户记录, 插入时, 根据 uid 来判断这条记录应该被插入到哪个 logic Shard 中.假设当前要插入的记录会被插入到第 1341 号 logic Shard 中 (假设当前的这个 Table 一共有 2000 个 logic Shard)新生成 ID 的 13 bits 段要填的就是 1341 这个数字sequence number 利用 PostgreSQL 每个 Table 上的 auto-increment sequence 来生成:如果当前表上已经有 5000 条记录, 那么这个表的下一个 auto-increment sequence 就是 5001 (直接调用 PL/PGSQL 提供的方法可以获取到)然后把 这个 5001 对 1024 取模就得到了 10 bits 的 sequence numberinstagram 这个方案的优势在于:利用 logic Shard 号来替换 Snowflake 使用的 Worker 号, 就不需要到中心节点获取 Worker 号了. 做到了完全去中心化另外一个附带的好处就是, 可以通过 ID 直接知道这条记录被存放在哪个 logic Shard 上同时, 今后做数据迁移的时候, 也是按 logic Shard 为单位做数据迁移的, 所以这种做法也不会影响到今后的数据迁移6. 利用zookeeper生成唯一IDzookeeper主要通过其znode数据版本来生成序列号，可以生成32位和64位的数据版本号，客户端可以使用这个版本号来作为唯一的序列号。很少会使用zookeeper来生成唯一ID。主要是由于需要依赖zookeeper，并且是多步调用API，如果在竞争较大的情况下，需要考虑使用分布式锁。因此，性能在高并发的分布式环境下，也不甚理想。7. MongoDB的ObjectIdMongoDB的ObjectId和snowflake算法类似。它设计成轻量型的，不同的机器都能用全局唯一的同种方法方便地生成它。MongoDB 从一开始就设计用来作为分布式数据库，处理多个节点是一个核心要求。使其在分片环境中要容易生成得多。其格式如下：前4 个字节是从标准纪元开始的时间戳，单位为秒。时间戳，与随后的5 个字节组合起来，提供了秒级别的唯一性。由于时间戳在前，这意味着ObjectId 大致会按照插入的顺序排列。这对于某些方面很有用，如将其作为索引提高效率。这4 个字节也隐含了文档创建的时间。绝大多数客户端类库都会公开一个方法从ObjectId 获取这个信息。接下来的3 字节是所在主机的唯一标识符。通常是机器主机名的散列值。这样就可以确保不同主机生成不同的ObjectId，不产生冲突。为了确保在同一台机器上并发的多个进程产生的ObjectId 是唯一的，接下来的两字节来自产生ObjectId 的进程标识符（PID）。前9 字节保证了同一秒钟不同机器不同进程产生的ObjectId 是唯一的。后3 字节就是一个自动增加的计数器，确保相同进程同一秒产生的ObjectId 也是不一样的。同一秒钟最多允许每个进程拥有2563（16 777 216）个不同的ObjectId。8. Flickr 的全局主键生成方案flickr巧妙地使用了MySQL的自增ID，及replace into语法，十分简洁地实现了分片ID生成功能。比如创建64位的自增id：首先，创建一个表：123456CREATE TABLE `uid_sequence` ( `id` bigint(20) unsigned NOT NULL auto_increment, `stub` char(1) NOT NULL default '', PRIMARY KEY (`id`), UNIQUE KEY `stub` (`stub`)) ENGINE=MyISAM;123456123456SELECT * from uid_sequence 输出：+——————-+——+| id | stub |+——————-+——+| 72157623227190423 | a |如果我需要一个全局的唯一的64位uid，则执行：12REPLACE INTO uid_sequence (stub) VALUES ('a');SELECT LAST_INSERT_ID();1212说明：用 REPLACE INTO 代替 INSERT INTO 的好处是避免表行数太大，还要另外定期清理。stub 字段要设为唯一索引，这个 sequence 表只有一条纪录，但也可以同时为多张表生成全局主键，例如user_order_id。除非你需要表的主键是连续的，那么就另建一个 user_order_id_sequence 表。经过实际对比测试，使用 MyISAM 比 Innodb 有更高的性能。这里flickr使用两台数据库（也可以更多）作为自增序列生成，通过这两台机器做主备和负载均衡。1234567TicketServer1:auto-increment-increment = 2auto-increment-offset = 1TicketServer2:auto-increment-increment = 2auto-increment-offset = 212345671234567优点：简单可靠。缺点：id只是一个ID，没有带入时间，shardingId等信息。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Reproduce DQN result]]></title>
      <url>%2F2017%2F05%2F24%2FReproduce-DQN-result%2F</url>
      <content type="text"><![CDATA[论文链接：https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html源代码地址：https://sites.google.com/a/deepmind.com/dqn/由于源代码中只有训练阶段，没有测试阶段，因此我才用了这个项目的测试脚本，并且生成游戏动图。实验复现的步骤如下（这里引用作者原文）：DQN 3.0This project contains the source code of DQN 3.0, a Lua-based deep reinforcement learning architecture, necessary to reproduce the experiments described in the paper “Human-level control through deep reinforcement learning”, Nature 518, 529–533 (26 February 2015) doi:10.1038/nature14236.To replicate the experiment results, a number of dependencies need to be installed, namely:LuaJIT and Torch 7.0nngraphXitari (fork of the Arcade Learning Environment (Bellemare et al., 2013))AleWrap (a lua interface to Xitari) An install script for these dependencies is provided.Two run scripts are provided: run_cpu and run_gpu. As the names imply, the former trains the DQN network using regular CPUs, while the latter uses GPUs (CUDA), which typically results in a significant speed-up.Installation instructionsThe installation requires Linux with apt-get.Note: In order to run the GPU version of DQN, you should additionally have the NVIDIA® CUDA® (version 5.5 or later) toolkit installed prior to the Torch installation below. This can be downloaded from https://developer.nvidia.com/cuda-toolkit and installation instructions can be found in http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linuxTo train DQN on Atari games, the following components must be installed:LuaJIT and Torch 7.0nngraphXitariAleWrapTo install all of the above in a subdirectory called ‘torch’, it should be enough to run1./install_dependencies.shfrom the base directory of the package.Note: The above install script will install the following packages via apt-get: build-essential, gcc, g++, cmake, curl, libreadline-dev, git-core, libjpeg-dev, libpng-dev, ncurses-dev, imagemagick, unzipTraining DQN on Atari gamesPrior to running DQN on a game, you should copy its ROM in the ‘roms’ subdirectory. It should then be sufficient to run the script1./run_cpu &lt;game name&gt;Or, if GPU support is enabled,1./run_gpu &lt;game name&gt;Note: On a system with more than one GPU, DQN training can be launched on a specified GPU by setting the environment variable GPU_ID, e.g. by1GPU_ID=2 ./run_gpu &lt;game name&gt;If GPU_ID is not specified, the first available GPU (ID 0) will be used by default.这之后是我采用另一个项目的测试步骤：Storing a .gif for a trained networkOnce you have a snapshot of a network you can run1./test_gpu &lt;game name&gt; &lt;snapshopt filename&gt;to make it play one game and store the .gif under gifs. For example1./test_gpu breakout DQN3_0_1_breakout_FULL_Y.t7OptionsOptions to DQN are set within run_cpu (respectively, run_gpu). You may, for example, want to change the frequency at which information is output to stdout by setting ‘prog_freq’ to a different value.我在实验过程中碰到了一系列的问题，实验环境为Windows 10 Vmware Workstation中运行的Ubuntu 16.04 LTS虚拟机。需要声明的是，我才用原始代码并没有运行成功，经分析应该是虚拟机的问题，但以下碰到的问题应当是具有一般性地，下一步准备采用测试项目代码运行。Some Problem./run_cpu之后出现Segmentation fault错误可能是因为其后参数名中出现大写字母可能是因为内存不足，可以尝试换一个游戏运行./test_cpu之后提示找不到gd这个时候需要手动安装gd，具体安装方法如下：下载地址：https://ittner.github.io/lua-gd/manual.html#download我下载的是这个版本http://files.luaforge.net/releases/lua-gd/lua-gd/lua-gd-2.0.33r2forLua5.1/lua-gd-2.0.33r2.tar.gz下载解压后，进到对应的目录，执行命令：makemake成功后，执行：sudo make install如果中间出现错误的话，请把下面的几个包都安装上：sudo apt-get install lua5.1sudo apt-get install lua5.1-0-devsudo apt-get install liblua5.1-0-devsudo apt-get install libgd2-dev安装成功之后会有如下提示：12345678910111213gcc -o gd.so `gdlib-config --features |sed -e &quot;s/GD_/-DGD_/g&quot;``gdlib-config --cflags` `pkg-config lua5.1 --cflags` -O3 -Wall -shared`gdlib-config --ldflags` `gdlib-config --libs` `pkg-config lua5.1 --libs`-lgd luagd.clua test_features.luaLua-GD version: lua-gd 2.0.33r2Lua-GD features: PNG support ..................... Enabled GIF support ..................... Enabled JPEG support .................... Enabled XPM/XBM support ................. Enabled FreeType support ................ Enabled Fontconfig support .............. Enabled安装gd时进行make的时候出现gd.h: No such file or directoryTry to install this package if you are in debian : libgd2-noxpm-dev安装gd时进行make的时候出现srlua makefile error lua.h No such file or directorysudo apt-get install liblua5.1-0-dev在解决以上问题后依然通不过编译这里引用了https://groups.google.com/forum/#!topic/bamboo-cn/myYzVk5XLgc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Question Answering: A Very Brief Introduction]]></title>
      <url>%2F2017%2F05%2F13%2FQuestion-Answering-A-Very-Brief-Introduction%2F</url>
      <content type="text"><![CDATA[Find answers to (natual language) questions by machineTypes of questionsFactoidDefinitionYes-NoOpinionComparisonMultiple Intelligences in Modern in QA SystemsKnowledge-QA结构化的，基于知识库，其实就是一个图，结点是实体，边是语义关系。关键是能够提取中问题中的实体以及实体之间的语义关系Document-QA非结构化的Social-QA类似Quora, Zhihu, Stackoverflow等]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Bot Sample: MultiDialog]]></title>
      <url>%2F2017%2F05%2F12%2FBot-Sample-MultiDialog%2F</url>
      <content type="text"><![CDATA[MessageController.cs1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162namespace MultiDialogsBot&#123; using System.Net; using System.Net.Http; using System.Threading.Tasks; using System.Web.Http; using Dialogs; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [BotAuthentication] public class MessagesController : ApiController &#123; /// &lt;summary&gt; /// POST: api/Messages /// Receive a message from a user and reply to it /// &lt;/summary&gt; public async Task&lt;HttpResponseMessage&gt; Post([FromBody]Activity activity) &#123; if (activity.Type == ActivityTypes.Message) &#123; await Conversation.SendAsync(activity, () =&gt; new RootDialog()); &#125; else &#123; this.HandleSystemMessage(activity); &#125; var response = Request.CreateResponse(HttpStatusCode.OK); return response; &#125; private Activity HandleSystemMessage(Activity message) &#123; if (message.Type == ActivityTypes.DeleteUserData) &#123; // Implement user deletion here // If we handle user deletion, return a real message &#125; else if (message.Type == ActivityTypes.ConversationUpdate) &#123; // Handle conversation state changes, like members being added and removed // Use Activity.MembersAdded and Activity.MembersRemoved and Activity.Action for info // Not available in all channels &#125; else if (message.Type == ActivityTypes.ContactRelationUpdate) &#123; // Handle add/remove from contact lists // Activity.From + Activity.Action represent what happened &#125; else if (message.Type == ActivityTypes.Typing) &#123; // Handle knowing tha the user is typing &#125; else if (message.Type == ActivityTypes.Ping) &#123; &#125; return null; &#125; &#125;&#125;RootDialog.cs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990namespace MultiDialogsBot.Dialogs&#123; using System; using System.Collections.Generic; using System.Threading; using System.Threading.Tasks; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [Serializable] public class RootDialog : IDialog&lt;object&gt; &#123; private const string FlightsOption = "Flights"; private const string HotelsOption = "Hotels"; public async Task StartAsync(IDialogContext context) &#123; context.Wait(this.MessageReceivedAsync); &#125; public virtual async Task MessageReceivedAsync(IDialogContext context, IAwaitable&lt;IMessageActivity&gt; result) &#123; var message = await result; if (message.Text.ToLower().Contains("help") || message.Text.ToLower().Contains("support") || message.Text.ToLower().Contains("problem")) &#123; await context.Forward(new SupportDialog(), this.ResumeAfterSupportDialog, message, CancellationToken.None); &#125; else &#123; this.ShowOptions(context); &#125; &#125; private void ShowOptions(IDialogContext context) &#123; PromptDialog.Choice(context, this.OnOptionSelected, new List&lt;string&gt;() &#123; FlightsOption, HotelsOption &#125;, "Are you looking for a flight or a hotel?", "Not a valid option", 3); &#125; private async Task OnOptionSelected(IDialogContext context, IAwaitable&lt;string&gt; result) &#123; try &#123; string optionSelected = await result; switch (optionSelected) &#123; case FlightsOption: context.Call(new FlightsDialog(), this.ResumeAfterOptionDialog); break; case HotelsOption: context.Call(new HotelsDialog(), this.ResumeAfterOptionDialog); break; &#125; &#125; catch (TooManyAttemptsException ex) &#123; await context.PostAsync($"Ooops! Too many attemps :(. But don't worry, I'm handling that exception and you can try again!"); context.Wait(this.MessageReceivedAsync); &#125; &#125; private async Task ResumeAfterSupportDialog(IDialogContext context, IAwaitable&lt;int&gt; result) &#123; var ticketNumber = await result; await context.PostAsync($"Thanks for contacting our support team. Your ticket number is &#123;ticketNumber&#125;."); context.Wait(this.MessageReceivedAsync); &#125; private async Task ResumeAfterOptionDialog(IDialogContext context, IAwaitable&lt;object&gt; result) &#123; try &#123; var message = await result; &#125; catch (Exception ex) &#123; await context.PostAsync($"Failed with message: &#123;ex.Message&#125;"); &#125; finally &#123; context.Wait(this.MessageReceivedAsync); &#125; &#125; &#125;&#125;SupportDialog.cs123456789101112131415161718192021222324252627namespace MultiDialogsBot.Dialogs&#123; using System; using System.Threading.Tasks; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [Serializable] public class SupportDialog : IDialog&lt;int&gt; &#123; public async Task StartAsync(IDialogContext context) &#123; context.Wait(this.MessageReceivedAsync); &#125; public virtual async Task MessageReceivedAsync(IDialogContext context, IAwaitable&lt;IMessageActivity&gt; result) &#123; var message = await result; var ticketNumber = new Random().Next(0, 20000); await context.PostAsync($"Your message '&#123;message.Text&#125;' was registered. Once we resolve it; we will get back to you."); context.Done(ticketNumber); &#125; &#125;&#125;FlightsDialog.cs12345678910111213141516namespace MultiDialogsBot.Dialogs&#123; using System; using System.Threading.Tasks; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [Serializable] public class FlightsDialog : IDialog&lt;object&gt; &#123; public async Task StartAsync(IDialogContext context) &#123; context.Fail(new NotImplementedException("Flights Dialog is not implemented and is instead being used to show context.Fail")); &#125; &#125;&#125;HotelsDialog.cs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126namespace MultiDialogsBot.Dialogs&#123; using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using System.Web; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Builder.FormFlow; using Microsoft.Bot.Connector; [Serializable] public class HotelsDialog : IDialog&lt;object&gt; &#123; public async Task StartAsync(IDialogContext context) &#123; await context.PostAsync("Welcome to the Hotels finder!"); var hotelsFormDialog = FormDialog.FromForm(this.BuildHotelsForm, FormOptions.PromptInStart); context.Call(hotelsFormDialog, this.ResumeAfterHotelsFormDialog); &#125; private IForm&lt;HotelsQuery&gt; BuildHotelsForm() &#123; OnCompletionAsyncDelegate&lt;HotelsQuery&gt; processHotelsSearch = async (context, state) =&gt; &#123; await context.PostAsync($"Ok. Searching for Hotels in &#123;state.Destination&#125; from &#123;state.CheckIn.ToString("MM/dd")&#125; to &#123;state.CheckIn.AddDays(state.Nights).ToString("MM/dd")&#125;..."); &#125;; return new FormBuilder&lt;HotelsQuery&gt;() .Field(nameof(HotelsQuery.Destination)) .Message("Looking for hotels in &#123;Destination&#125;...") .AddRemainingFields() .OnCompletion(processHotelsSearch) .Build(); &#125; private async Task ResumeAfterHotelsFormDialog(IDialogContext context, IAwaitable&lt;HotelsQuery&gt; result) &#123; try &#123; var searchQuery = await result; var hotels = await this.GetHotelsAsync(searchQuery); await context.PostAsync($"I found in total &#123;hotels.Count()&#125; hotels for your dates:"); var resultMessage = context.MakeMessage(); resultMessage.AttachmentLayout = AttachmentLayoutTypes.Carousel; resultMessage.Attachments = new List&lt;Attachment&gt;(); foreach (var hotel in hotels) &#123; HeroCard heroCard = new HeroCard() &#123; Title = hotel.Name, Subtitle = $"&#123;hotel.Rating&#125; starts. &#123;hotel.NumberOfReviews&#125; reviews. From $&#123;hotel.PriceStarting&#125; per night.", Images = new List&lt;CardImage&gt;() &#123; new CardImage() &#123; Url = hotel.Image &#125; &#125;, Buttons = new List&lt;CardAction&gt;() &#123; new CardAction() &#123; Title = "More details", Type = ActionTypes.OpenUrl, Value = $"https://www.bing.com/search?q=hotels+in+" + HttpUtility.UrlEncode(hotel.Location) &#125; &#125; &#125;; resultMessage.Attachments.Add(heroCard.ToAttachment()); &#125; await context.PostAsync(resultMessage); &#125; catch (FormCanceledException ex) &#123; string reply; if (ex.InnerException == null) &#123; reply = "You have canceled the operation. Quitting from the HotelsDialog"; &#125; else &#123; reply = $"Oops! Something went wrong :( Technical Details: &#123;ex.InnerException.Message&#125;"; &#125; await context.PostAsync(reply); &#125; finally &#123; context.Done&lt;object&gt;(null); &#125; &#125; private async Task&lt;IEnumerable&lt;Hotel&gt;&gt; GetHotelsAsync(HotelsQuery searchQuery) &#123; var hotels = new List&lt;Hotel&gt;(); // Filling the hotels results manually just for demo purposes for (int i = 1; i &lt;= 5; i++) &#123; var random = new Random(i); Hotel hotel = new Hotel() &#123; Name = $"&#123;searchQuery.Destination&#125; Hotel &#123;i&#125;", Location = searchQuery.Destination, Rating = random.Next(1, 5), NumberOfReviews = random.Next(0, 5000), PriceStarting = random.Next(80, 450), Image = $"https://placeholdit.imgix.net/~text?txtsize=35&amp;txt=Hotel+&#123;i&#125;&amp;w=500&amp;h=260" &#125;; hotels.Add(hotel); &#125; hotels.Sort((h1, h2) =&gt; h1.PriceStarting.CompareTo(h2.PriceStarting)); return hotels; &#125; &#125;&#125;Hotel.cs1234567891011121314151617181920namespace MultiDialogsBot&#123; using System; [Serializable] public class Hotel &#123; public string Name &#123; get; set; &#125; public int Rating &#123; get; set; &#125; public int NumberOfReviews &#123; get; set; &#125; public int PriceStarting &#123; get; set; &#125; public string Image &#123; get; set; &#125; public string Location &#123; get; set; &#125; &#125;&#125;HotelsQuery.cs12345678910111213141516171819namespace MultiDialogsBot&#123; using System; using Microsoft.Bot.Builder.FormFlow; [Serializable] public class HotelsQuery &#123; [Prompt("Please enter your &#123;&amp;&#125;")] public string Destination &#123; get; set; &#125; [Prompt("When do you want to &#123;&amp;&#125;?")] public DateTime CheckIn &#123; get; set; &#125; [Numeric(1, int.MaxValue)] [Prompt("How many &#123;&amp;&#125; do you want to stay?")] public int Nights &#123; get; set; &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The First Course of C#]]></title>
      <url>%2F2017%2F05%2F10%2FThe-First-Course-of-C%2F</url>
      <content type="text"><![CDATA[基本语法一个例子:123456789101112131415using System;namespace HelloWorldApplication&#123; /* 类名为 HelloWorld */ class HelloWorld &#123; /* main函数 */ static void Main(string[] args) &#123; /* 我的第一个 C# 程序 */ Console.WriteLine(&quot;Hello World!&quot;); Console.ReadKey(); &#125; &#125;&#125;对象类型是 C# 通用类型系统（Common Type System - CTS）中所有数据类型的终极基类。Object 是 System.Object 类的别名。所以对象（Object）类型可以被分配任何其他类型（值类型、引用类型、预定义类型或用户自定义类型）的值。但是，在分配值之前，需要先进行类型转换。当一个值类型转换为对象类型时，则被称为 装箱；另一方面，当一个对象类型转换为值类型时，则被称为 拆箱。12object obj;obj = 100; // 这是装箱动态类型您可以存储任何类型的值在动态数据类型变量中。这些变量的类型检查是在运行时发生的。声明动态类型的语法：1dynamic &lt;variable_name&gt; = value;例如：1dynamic d = 20;动态类型与对象类型相似，但是对象类型变量的类型检查是在编译时发生的，而动态类型变量的类型检查是在运行时发生的。字符串的特殊定义方式字符串（String）类型允许您给变量分配任何字符串值。字符串（String）类型是 System.String 类的别名。它是从对象（Object）类型派生的。字符串（String）类型的值可以通过两种形式进行分配：引号和 @引号。例如：1String str = &quot;runoob.com&quot;;一个 @引号字符串：1@&quot;runoob.com&quot;;C# string 字符串的前面可以加 @（称作”逐字字符串”）将转义字符（\）当作普通字符对待，比如：1string str = @&quot;C:\Windows&quot;;等价于：1string str = &quot;C:\\Windows&quot;;@ 字符串中可以任意换行，换行符及缩进空格都计算在字符串长度之内。1234string str = @&quot;&lt;script type=&quot;&quot;text/javascript&quot;&quot;&gt; &lt;!-- --&gt;&lt;/script&gt;&quot;;显式类型转换方式1234567891011121314151617181920namespace TypeConversionApplication&#123; class StringConversion &#123; static void Main(string[] args) &#123; int i = 75; float f = 53.005f; double d = 2345.7652; bool b = true; Console.WriteLine(i.ToString()); Console.WriteLine(f.ToString()); Console.WriteLine(d.ToString()); Console.WriteLine(b.ToString()); Console.ReadKey(); &#125; &#125;&#125;命令行输入System命名空间中的Console类提供了一个函数 ReadLine()，用于接收来自用户的输入，并把它存储到一个变量中。例如：12int num;num = Convert.ToInt32(Console.ReadLine());函数 Convert.ToInt32()把用户输入的数据转换为int 数据类型，因为 Console.ReadLine()只接受字符串格式的数据。特殊运算符运算符描述实例sizeof()返回数据类型的大小。sizeof(int)，将返回 4.typeof()返回 class 的类型。typeof(StreamReader);&amp;返回变量的地址。&a; 将得到变量的实际地址。*变量的指针。*a; 将指向一个变量。? :条件表达式如果条件为真 ? 则为 X : 否则为 Yis判断对象是否为某一类型。If( Ford is Car) // 检查 Ford 是否是 Car 类的一个对象。as强制转换，即使转换失败也不会抛出异常。Object obj = new StringReader(“Hello”);StringReader r = obj as StringReader;特殊访问修饰符Internal 访问修饰符Internal 访问说明符允许一个类将其成员变量和成员函数暴露给当前程序中的其他函数和对象。换句话说，带有 internal 访问修饰符的任何成员可以被定义在该成员所定义的应用程序内的任何类或方法访问。类的默认访问标识符是 internal，成员的默认访问标识符是 private。下面的实例说明了这点：123456789101112131415161718192021222324252627282930313233using System;namespace RectangleApplication&#123; class Rectangle &#123; //成员变量 internal double length; internal double width; double GetArea() &#123; return length * width; &#125; public void Display() &#123; Console.WriteLine(&quot;长度： &#123;0&#125;&quot;, length); Console.WriteLine(&quot;宽度： &#123;0&#125;&quot;, width); Console.WriteLine(&quot;面积： &#123;0&#125;&quot;, GetArea()); &#125; &#125;//end class Rectangle class ExecuteRectangle &#123; static void Main(string[] args) &#123; Rectangle r = new Rectangle(); r.length = 4.5; r.width = 3.5; r.Display(); Console.ReadLine(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：123长度： 4.5宽度： 3.5面积： 15.75在上面的实例中，请注意成员函数 GetArea() 声明的时候不带有任何访问修饰符。如果没有指定访问修饰符，则使用类成员的默认访问修饰符，即为 private。Protected Internal 访问修饰符Protected Internal 访问修饰符允许在本类,派生类或者包含该类的程序集中访问。这也被用于实现继承。按引用传递参数引用参数是一个对变量的内存位置的引用。当按引用传递参数时，与值参数不同的是，它不会为这些参数创建一个新的存储位置。引用参数表示与提供给方法的实际参数具有相同的内存位置。在 C# 中，使用 ref 关键字声明引用参数。下面的实例演示了这点：1234567891011121314151617181920212223242526272829303132333435using System;namespace CalculatorApplication&#123; class NumberManipulator &#123; public void swap(ref int x, ref int y) &#123; int temp; temp = x; /* 保存 x 的值 */ x = y; /* 把 y 赋值给 x */ y = temp; /* 把 temp 赋值给 y */ &#125; static void Main(string[] args) &#123; NumberManipulator n = new NumberManipulator(); /* 局部变量定义 */ int a = 100; int b = 200; Console.WriteLine(&quot;在交换之前，a 的值： &#123;0&#125;&quot;, a); Console.WriteLine(&quot;在交换之前，b 的值： &#123;0&#125;&quot;, b); /* 调用函数来交换值 */ n.swap(ref a, ref b); Console.WriteLine(&quot;在交换之后，a 的值： &#123;0&#125;&quot;, a); Console.WriteLine(&quot;在交换之后，b 的值： &#123;0&#125;&quot;, b); Console.ReadLine(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：1234在交换之前，a 的值：100在交换之前，b 的值：200在交换之后，a 的值：200在交换之后，b 的值：100结果表明，swap 函数内的值改变了，且这个改变可以在 Main 函数中反映出来。按输出传递参数return 语句可用于只从函数中返回一个值。但是，可以使用 输出参数 来从函数中返回两个值。输出参数会把方法输出的数据赋给自己，其他方面与引用参数相似。下面的实例演示了这点：1234567891011121314151617181920212223242526272829using System;namespace CalculatorApplication&#123; class NumberManipulator &#123; public void getValue(out int x ) &#123; int temp = 5; x = temp; &#125; static void Main(string[] args) &#123; NumberManipulator n = new NumberManipulator(); /* 局部变量定义 */ int a = 100; Console.WriteLine(&quot;在方法调用之前，a 的值： &#123;0&#125;&quot;, a); /* 调用函数来获取值 */ n.getValue(out a); Console.WriteLine(&quot;在方法调用之后，a 的值： &#123;0&#125;&quot;, a); Console.ReadLine(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12在方法调用之前，a 的值： 100在方法调用之后，a 的值： 5提供给输出参数的变量不需要赋值。当需要从一个参数没有指定初始值的方法中返回值时，输出参数特别有用。请看下面的实例，来理解这一点：1234567891011121314151617181920212223242526272829using System;namespace CalculatorApplication&#123; class NumberManipulator &#123; public void getValues(out int x, out int y ) &#123; Console.WriteLine(&quot;请输入第一个值： &quot;); x = Convert.ToInt32(Console.ReadLine()); Console.WriteLine(&quot;请输入第二个值： &quot;); y = Convert.ToInt32(Console.ReadLine()); &#125; static void Main(string[] args) &#123; NumberManipulator n = new NumberManipulator(); /* 局部变量定义 */ int a , b; /* 调用函数来获取值 */ n.getValues(out a, out b); Console.WriteLine(&quot;在方法调用之后，a 的值： &#123;0&#125;&quot;, a); Console.WriteLine(&quot;在方法调用之后，b 的值： &#123;0&#125;&quot;, b); Console.ReadLine(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果（取决于用户输入）：123456请输入第一个值：7请输入第二个值：8在方法调用之后，a 的值： 7在方法调用之后，b 的值： 8可空类型C# 提供了一个特殊的数据类型，nullable 类型（可空类型），可空类型可以表示其基础值类型正常范围内的值，再加上一个 null 值。例如，Nullable&lt; Int32 &gt;，读作”可空的 Int32“，可以被赋值为 -2,147,483,648 到 2,147,483,647 之间的任意值，也可以被赋值为 null 值。类似的，Nullable&lt; bool &gt; 变量可以被赋值为 true 或 false 或 null。在处理数据库和其他包含可能未赋值的元素的数据类型时，将 null 赋值给数值类型或布尔型的功能特别有用。例如，数据库中的布尔型字段可以存储值 true 或 false，或者，该字段也可以未定义。声明一个 nullable类型（可空类型）的语法如下：1&lt; data_type&gt; ? &lt;variable_name&gt; = null;下面的实例演示了可空数据类型的用法：123456789101112131415161718192021222324using System;namespace CalculatorApplication&#123; class NullablesAtShow &#123; static void Main(string[] args) &#123; int? num1 = null; int? num2 = 45; double? num3 = new double?(); double? num4 = 3.14157; bool? boolval = new bool?(); // 显示值 Console.WriteLine(&quot;显示可空类型的值： &#123;0&#125;, &#123;1&#125;, &#123;2&#125;, &#123;3&#125;&quot;, num1, num2, num3, num4); Console.WriteLine(&quot;一个可空的布尔值： &#123;0&#125;&quot;, boolval); Console.ReadLine(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12显示可空类型的值： , 45, , 3.14157一个可空的布尔值：Null合并运算符Null 合并运算符用于定义可空类型和引用类型的默认值。Null 合并运算符为类型转换定义了一个预设值，以防可空类型的值为 Null。Null 合并运算符把操作数类型隐式转换为另一个可空（或不可空）的值类型的操作数的类型。如果第一个操作数的值为 null，则运算符返回第二个操作数的值，否则返回第一个操作数的值。下面的实例演示了这点：123456789101112131415161718192021using System;namespace CalculatorApplication&#123; class NullablesAtShow &#123; static void Main(string[] args) &#123; double? num1 = null; double? num2 = 3.14157; double num3; num3 = num1 ?? 5.34; Console.WriteLine(&quot;num3 的值： &#123;0&#125;&quot;, num3); num3 = num2 ?? 5.34; Console.WriteLine(&quot;num3 的值： &#123;0&#125;&quot;, num3); Console.ReadLine(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12num3 的值： 5.34num3 的值： 3.14157foreach在前面的实例中，我们使用一个 for 循环来访问每个数组元素。您也可以使用一个 foreach 语句来遍历数组。123456789101112131415161718192021222324252627using System;namespace ArrayApplication&#123; class MyArray &#123; static void Main(string[] args) &#123; int [] n = new int[10]; /* n 是一个带有 10 个整数的数组, 赋值时初始化需要用大括号*/ /* 初始化数组 n 中的元素 */ for ( int i = 0; i &lt; 10; i++ ) &#123; n[i] = i + 100; &#125; /* 输出每个数组元素的值 */ foreach (int j in n ) &#123; int i = j-100; Console.WriteLine(&quot;Element[&#123;0&#125;] = &#123;1&#125;&quot;, i, j); &#125; Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12345678910Element[0] = 100Element[1] = 101Element[2] = 102Element[3] = 103Element[4] = 104Element[5] = 105Element[6] = 106Element[7] = 107Element[8] = 108Element[9] = 109多维数组您可以声明一个 string 变量的二维数组，如下：1string [,] names;或者，您可以声明一个 int 变量的三维数组，如下：1int [ , , ] m;多维数组可以通过在括号内为每行指定值来进行初始化。下面是一个带有 3 行 4 列的数组。12345int [,] a = new int [3,4] &#123; &#123;0, 1, 2, 3&#125; , /* 初始化索引号为 0 的行 */ &#123;4, 5, 6, 7&#125; , /* 初始化索引号为 1 的行 */ &#123;8, 9, 10, 11&#125; /* 初始化索引号为 2 的行 */&#125;;二维数组中的元素是通过使用下标（即数组的行索引和列索引）来访问的。例如：1int val = a[2,3];交错数组交错数组是数组的数组。您可以声明一个带有 int 值的交错数组 scores，如下所示：1int [][] scores;声明一个数组不会在内存中创建数组。创建上面的数组：12345int[][] scores = new int[5][];for (int i = 0; i &lt; scores.Length; i++) &#123; scores[i] = new int[4];&#125;您可以初始化一个交错数组，如下所示：1int[][] scores = new int[2][]&#123;new int[]&#123;92,93,94&#125;,new int[]&#123;85,66,87,88&#125;&#125;;其中，scores 是一个由两个整型数组组成的数组 – scores[0] 是一个带有 3 个整数的数组，scores[1] 是一个带有 4 个整数的数组。123456789101112131415161718192021222324252627下面的实例演示了如何使用交错数组：using System;namespace ArrayApplication&#123; class MyArray &#123; static void Main(string[] args) &#123; /* 一个由 5 个整型数组组成的交错数组 */ int[][] a = new int[][]&#123;new int[]&#123;0,0&#125;,new int[]&#123;1,2&#125;, new int[]&#123;2,4&#125;,new int[]&#123; 3, 6 &#125;, new int[]&#123; 4, 8 &#125; &#125;; int i, j; /* 输出数组中每个元素的值 */ for (i = 0; i &lt; 5; i++) &#123; for (j = 0; j &lt; 2; j++) &#123; Console.WriteLine(&quot;a[&#123;0&#125;][&#123;1&#125;] = &#123;2&#125;&quot;, i, j, a[i][j]); &#125; &#125; Console.ReadKey(); &#125; &#125;&#125;参数数组有时，当声明一个方法时，您不能确定要传递给函数作为参数的参数数目。C# 参数数组解决了这个问题，参数数组通常用于传递未知数量的参数给函数。params 关键字在使用数组作为形参时，C# 提供了 params 关键字，使调用数组为形参的方法时，既可以传递数组实参，也可以只传递一组数组。params 的使用格式为：1public 返回类型 方法名称( params 类型名称[] 数组名称 )实例下面的实例演示了如何使用参数数组：12345678910111213141516171819202122232425262728using System;namespace ArrayApplication&#123; class ParamArray &#123; public int AddElements(params int[] arr) &#123; int sum = 0; foreach (int i in arr) &#123; sum += i; &#125; return sum; &#125; &#125; class TestClass &#123; static void Main(string[] args) &#123; ParamArray app = new ParamArray(); int sum = app.AddElements(512, 720, 250, 567, 889); Console.WriteLine(&quot;总和是： &#123;0&#125;&quot;, sum); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：1总和是： 2938Array类Array 类是 C# 中所有数组的基类，它是在 System 命名空间中定义。Array 类提供了各种用于数组的属性和方法。下面的程序演示了 Array 类的一些方法的用法：12345678910111213141516171819202122232425262728293031323334353637383940using System;namespace ArrayApplication&#123; class MyArray &#123; static void Main(string[] args) &#123; int[] list = &#123; 34, 72, 13, 44, 25, 30, 10 &#125;; int[] temp = list; Console.Write(&quot;原始数组： &quot;); foreach (int i in list) &#123; Console.Write(i + &quot; &quot;); &#125; Console.WriteLine(); // 逆转数组 Array.Reverse(temp); Console.Write(&quot;逆转数组： &quot;); foreach (int i in temp) &#123; Console.Write(i + &quot; &quot;); &#125; Console.WriteLine(); // 排序数组 Array.Sort(list); Console.Write(&quot;排序数组： &quot;); foreach (int i in list) &#123; Console.Write(i + &quot; &quot;); &#125; Console.WriteLine(); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：123原始数组： 34 72 13 44 25 30 10逆转数组： 10 30 25 44 13 72 34排序数组： 10 13 25 30 34 44 72结构体在 C# 中，结构是值类型数据结构。它使得一个单一变量可以存储各种数据类型的相关数据。struct 关键字用于创建结构。结构是用来代表一个记录。假设您想跟踪图书馆中书的动态。您可能想跟踪每本书的以下属性：TitleAuthorSubjectBook ID定义结构为了定义一个结构，您必须使用 struct 语句。struct 语句为程序定义了一个带有多个成员的新的数据类型。例如，您可以按照如下的方式声明 Book 结构：1234567struct Books&#123; public string title; public string author; public string subject; public int book_id;&#125;;下面的程序演示了结构的用法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546using System; struct Books&#123; public string title; public string author; public string subject; public int book_id;&#125;; public class testStructure&#123; public static void Main(string[] args) &#123; Books Book1; /* 声明 Book1，类型为 Book */ Books Book2; /* 声明 Book2，类型为 Book */ /* book 1 详述 */ Book1.title = &quot;C Programming&quot;; Book1.author = &quot;Nuha Ali&quot;; Book1.subject = &quot;C Programming Tutorial&quot;; Book1.book_id = 6495407; /* book 2 详述 */ Book2.title = &quot;Telecom Billing&quot;; Book2.author = &quot;Zara Ali&quot;; Book2.subject = &quot;Telecom Billing Tutorial&quot;; Book2.book_id = 6495700; /* 打印 Book1 信息 */ Console.WriteLine( &quot;Book 1 title : &#123;0&#125;&quot;, Book1.title); Console.WriteLine(&quot;Book 1 author : &#123;0&#125;&quot;, Book1.author); Console.WriteLine(&quot;Book 1 subject : &#123;0&#125;&quot;, Book1.subject); Console.WriteLine(&quot;Book 1 book_id :&#123;0&#125;&quot;, Book1.book_id); /* 打印 Book2 信息 */ Console.WriteLine(&quot;Book 2 title : &#123;0&#125;&quot;, Book2.title); Console.WriteLine(&quot;Book 2 author : &#123;0&#125;&quot;, Book2.author); Console.WriteLine(&quot;Book 2 subject : &#123;0&#125;&quot;, Book2.subject); Console.WriteLine(&quot;Book 2 book_id : &#123;0&#125;&quot;, Book2.book_id); Console.ReadKey(); &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12345678Book 1 title : C ProgrammingBook 1 author : Nuha AliBook 1 subject : C Programming TutorialBook 1 book_id : 6495407Book 2 title : Telecom BillingBook 2 author : Zara AliBook 2 subject : Telecom Billing TutorialBook 2 book_id : 6495700C# 结构的特点您已经用了一个简单的名为 Books 的结构。在 C# 中的结构与传统的 C 或 C++ 中的结构不同。C# 中的结构有以下特点：结构可带有方法、字段、索引、属性、运算符方法和事件。结构可定义构造函数，但不能定义析构函数。但是，您不能为结构定义默认的构造函数。默认的构造函数是自动定义的，且不能被改变。与类不同，结构不能继承其他的结构或类。结构不能作为其他结构或类的基础结构。结构可实现一个或多个接口。结构成员不能指定为 abstract、virtual 或 protected。当您使用 New 操作符创建一个结构对象时，会调用适当的构造函数来创建结构。与类不同，结构可以不使用 New 操作符即可被实例化。如果不使用 New 操作符，只有在所有的字段都被初始化之后，字段才被赋值，对象才被使用。类 vs 结构类和结构有以下几个基本的不同点：类是引用类型，结构是值类型。结构不支持继承。结构不能声明默认的构造函数。针对上述讨论，让我们重写前面的实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051using System; struct Books&#123; private string title; private string author; private string subject; private int book_id; public void getValues(string t, string a, string s, int id) &#123; title = t; author = a; subject = s; book_id = id; &#125; public void display() &#123; Console.WriteLine(&quot;Title : &#123;0&#125;&quot;, title); Console.WriteLine(&quot;Author : &#123;0&#125;&quot;, author); Console.WriteLine(&quot;Subject : &#123;0&#125;&quot;, subject); Console.WriteLine(&quot;Book_id :&#123;0&#125;&quot;, book_id); &#125;&#125;; public class testStructure&#123; public static void Main(string[] args) &#123; Books Book1 = new Books(); /* 声明 Book1，类型为 Book */ Books Book2 = new Books(); /* 声明 Book2，类型为 Book */ /* book 1 详述 */ Book1.getValues(&quot;C Programming&quot;, &quot;Nuha Ali&quot;, &quot;C Programming Tutorial&quot;,6495407); /* book 2 详述 */ Book2.getValues(&quot;Telecom Billing&quot;, &quot;Zara Ali&quot;, &quot;Telecom Billing Tutorial&quot;, 6495700); /* 打印 Book1 信息 */ Book1.display(); /* 打印 Book2 信息 */ Book2.display(); Console.ReadKey(); &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12345678Title : C ProgrammingAuthor : Nuha AliSubject : C Programming TutorialBook_id : 6495407Title : Telecom BillingAuthor : Zara AliSubject : Telecom Billing TutorialBook_id : 6495700多态性多态：一个接口多个功能。静态多态性：编译时发生函数响应（调用）；动态多态性：运行时发生函数响应。静态绑定（早期绑定）：编译时函数和对象的连接机制。两种技术实现静态多态性：函数重载/运算符重载。函数重载：在同一范围内对相同函数名有多个定义，可以是参数类型或参数个数的不同，但不许只有返回值类型不同。运算符重载：关键字 abstract 声明抽象类：用于接口部分类的实现（派生类继承抽象类时，实现完成）。抽象类包含抽象方法，抽象方法可被派生类实现。抽象类规则：1.不能创建抽象类的实例2.不能在抽象类外定义抽象方法3.不能把抽象类声明为sealed（类前带关键字sealed代表该类是密封类，不能被继承）关键字virtual声明虚方法:用于方法在继承类中的实现（在不同的继承类中有不同的实现）。抽象类和虚方法共同实现动态多态性。注：继承类中的重写虚函数需要声明关键字 override，在方法参数传入中写（类名 形参名）例如 public void CallArea(Shape sh)，意思是传入一个 shape 类型的类。运算符重载您可以重定义或重载 C# 中内置的运算符。因此，程序员也可以使用用户自定义类型的运算符。重载运算符是具有特殊名称的函数，是通过关键字 operator 后跟运算符的符号来定义的。与其他函数一样，重载运算符有返回类型和参数列表。例如，请看下面的函数：12345678public static Box operator+ (Box b, Box c)&#123; Box box = new Box(); box.length = b.length + c.length; box.breadth = b.breadth + c.breadth; box.height = b.height + c.height; return box;&#125;上面的函数为用户自定义的类 Box 实现了加法运算符（+）。它把两个 Box 对象的属性相加，并返回相加后的 Box 对象。运算符重载的实现下面的程序演示了完整的实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677using System;namespace OperatorOvlApplication&#123; class Box &#123; private double length; // 长度 private double breadth; // 宽度 private double height; // 高度 public double getVolume() &#123; return length * breadth * height; &#125; public void setLength( double len ) &#123; length = len; &#125; public void setBreadth( double bre ) &#123; breadth = bre; &#125; public void setHeight( double hei ) &#123; height = hei; &#125; // 重载 + 运算符来把两个 Box 对象相加 public static Box operator+ (Box b, Box c) &#123; Box box = new Box(); box.length = b.length + c.length; box.breadth = b.breadth + c.breadth; box.height = b.height + c.height; return box; &#125; &#125; class Tester &#123; static void Main(string[] args) &#123; Box Box1 = new Box(); // 声明 Box1，类型为 Box Box Box2 = new Box(); // 声明 Box2，类型为 Box Box Box3 = new Box(); // 声明 Box3，类型为 Box double volume = 0.0; // 体积 // Box1 详述 Box1.setLength(6.0); Box1.setBreadth(7.0); Box1.setHeight(5.0); // Box2 详述 Box2.setLength(12.0); Box2.setBreadth(13.0); Box2.setHeight(10.0); // Box1 的体积 volume = Box1.getVolume(); Console.WriteLine(&quot;Box1 的体积： &#123;0&#125;&quot;, volume); // Box2 的体积 volume = Box2.getVolume(); Console.WriteLine(&quot;Box2 的体积： &#123;0&#125;&quot;, volume); // 把两个对象相加 Box3 = Box1 + Box2; // Box3 的体积 volume = Box3.getVolume(); Console.WriteLine(&quot;Box3 的体积： &#123;0&#125;&quot;, volume); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：123Box1 的体积： 210Box2 的体积： 1560Box3 的体积： 5400可重载和不可重载运算符下表描述了 C# 中运算符重载的能力：运算符描述+, -, !, ~, ++, –这些一元运算符只有一个操作数，且可以被重载。+, -, *, /, %这些二元运算符带有两个操作数，且可以被重载。==, !=, &lt;, &gt;, &lt;=, &gt;=这些比较运算符可以被重载。&amp;&amp;, \\这些条件逻辑运算符不能被直接重载。+=, -=, *=, /=, %=这些赋值运算符不能被重载。=, ., ?:, -&gt;, new, is, sizeof, typeof这些运算符不能被重载。实例针对上述讨论，让我们扩展上面的实例，重载更多的运算符：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178using System;namespace OperatorOvlApplication&#123; class Box &#123; private double length; // 长度 private double breadth; // 宽度 private double height; // 高度 public double getVolume() &#123; return length * breadth * height; &#125; public void setLength( double len ) &#123; length = len; &#125; public void setBreadth( double bre ) &#123; breadth = bre; &#125; public void setHeight( double hei ) &#123; height = hei; &#125; // 重载 + 运算符来把两个 Box 对象相加 public static Box operator+ (Box b, Box c) &#123; Box box = new Box(); box.length = b.length + c.length; box.breadth = b.breadth + c.breadth; box.height = b.height + c.height; return box; &#125; public static bool operator == (Box lhs, Box rhs) &#123; bool status = false; if (lhs.length == rhs.length &amp;&amp; lhs.height == rhs.height &amp;&amp; lhs.breadth == rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator !=(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length != rhs.length || lhs.height != rhs.height || lhs.breadth != rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &lt;(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &lt; rhs.length &amp;&amp; lhs.height &lt; rhs.height &amp;&amp; lhs.breadth &lt; rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &gt;(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &gt; rhs.length &amp;&amp; lhs.height &gt; rhs.height &amp;&amp; lhs.breadth &gt; rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &lt;=(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &lt;= rhs.length &amp;&amp; lhs.height &lt;= rhs.height &amp;&amp; lhs.breadth &lt;= rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &gt;=(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &gt;= rhs.length &amp;&amp; lhs.height &gt;= rhs.height &amp;&amp; lhs.breadth &gt;= rhs.breadth) &#123; status = true; &#125; return status; &#125; public override string ToString() &#123; return String.Format(&quot;(&#123;0&#125;, &#123;1&#125;, &#123;2&#125;)&quot;, length, breadth, height); &#125; &#125; class Tester &#123; static void Main(string[] args) &#123; Box Box1 = new Box(); // 声明 Box1，类型为 Box Box Box2 = new Box(); // 声明 Box2，类型为 Box Box Box3 = new Box(); // 声明 Box3，类型为 Box Box Box4 = new Box(); double volume = 0.0; // 体积 // Box1 详述 Box1.setLength(6.0); Box1.setBreadth(7.0); Box1.setHeight(5.0); // Box2 详述 Box2.setLength(12.0); Box2.setBreadth(13.0); Box2.setHeight(10.0); // 使用重载的 ToString() 显示两个盒子 Console.WriteLine(&quot;Box1： &#123;0&#125;&quot;, Box1.ToString()); Console.WriteLine(&quot;Box2： &#123;0&#125;&quot;, Box2.ToString()); // Box1 的体积 volume = Box1.getVolume(); Console.WriteLine(&quot;Box1 的体积： &#123;0&#125;&quot;, volume); // Box2 的体积 volume = Box2.getVolume(); Console.WriteLine(&quot;Box2 的体积： &#123;0&#125;&quot;, volume); // 把两个对象相加 Box3 = Box1 + Box2; Console.WriteLine(&quot;Box3： &#123;0&#125;&quot;, Box3.ToString()); // Box3 的体积 volume = Box3.getVolume(); Console.WriteLine(&quot;Box3 的体积： &#123;0&#125;&quot;, volume); //comparing the boxes if (Box1 &gt; Box2) Console.WriteLine(&quot;Box1 大于 Box2&quot;); else Console.WriteLine(&quot;Box1 不大于 Box2&quot;); if (Box1 &lt; Box2) Console.WriteLine(&quot;Box1 小于 Box2&quot;); else Console.WriteLine(&quot;Box1 不小于 Box2&quot;); if (Box1 &gt;= Box2) Console.WriteLine(&quot;Box1 大于等于 Box2&quot;); else Console.WriteLine(&quot;Box1 不大于等于 Box2&quot;); if (Box1 &lt;= Box2) Console.WriteLine(&quot;Box1 小于等于 Box2&quot;); else Console.WriteLine(&quot;Box1 不小于等于 Box2&quot;); if (Box1 != Box2) Console.WriteLine(&quot;Box1 不等于 Box2&quot;); else Console.WriteLine(&quot;Box1 等于 Box2&quot;); Box4 = Box3; if (Box3 == Box4) Console.WriteLine(&quot;Box3 等于 Box4&quot;); else Console.WriteLine(&quot;Box3 不等于 Box4&quot;); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：123456789101112Box1： (6, 7, 5)Box2： (12, 13, 10)Box1 的体积： 210Box2 的体积： 1560Box3： (18, 20, 15)Box3 的体积： 5400Box1 不大于 Box2Box1 小于 Box2Box1 不大于等于 Box2Box1 小于等于 Box2Box1 不等于 Box2Box3 等于 Box4FileStream12345678910111213141516171819202122232425262728using System;using System.IO;namespace FileIOApplication&#123; class Program &#123; static void Main(string[] args) &#123; FileStream F = new FileStream(&quot;test.dat&quot;, FileMode.OpenOrCreate, FileAccess.ReadWrite); for (int i = 1; i &lt;= 20; i++) &#123; F.WriteByte((byte)i); &#125; F.Position = 0; for (int i = 0; i &lt;= 20; i++) &#123; Console.Write(F.ReadByte() + &quot; &quot;); &#125; F.Close(); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：11 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 -1StreamReader 和 StreamWriterStreamReader 类下面的实例演示了读取名为 Jamaica.txt 的文件。文件如下：123456789101112131415161718192021222324252627282930313233343536373839Down the way where the nights are gayAnd the sun shines daily on the mountain topI took a trip on a sailing shipAnd when I reached JamaicaI made a stopusing System;using System.IO;namespace FileApplication&#123; class Program &#123; static void Main(string[] args) &#123; try &#123; // 创建一个 StreamReader 的实例来读取文件 // using 语句也能关闭 StreamReader using (StreamReader sr = new StreamReader(&quot;c:/jamaica.txt&quot;)) &#123; string line; // 从文件读取并显示行，直到文件的末尾 while ((line = sr.ReadLine()) != null) &#123; Console.WriteLine(line); &#125; &#125; &#125; catch (Exception e) &#123; // 向用户显示出错消息 Console.WriteLine(&quot;The file could not be read:&quot;); Console.WriteLine(e.Message); &#125; Console.ReadKey(); &#125; &#125;&#125;当您编译和执行上面的程序时，它会显示文件的内容。StreamWriter 类下面的实例演示了使用 StreamWriter 类向文件写入文本数据：123456789101112131415161718192021222324252627282930313233using System;using System.IO;namespace FileApplication&#123; class Program &#123; static void Main(string[] args) &#123; string[] names = new string[] &#123;&quot;Zara Ali&quot;, &quot;Nuha Ali&quot;&#125;; using (StreamWriter sw = new StreamWriter(&quot;names.txt&quot;)) &#123; foreach (string s in names) &#123; sw.WriteLine(s); &#125; &#125; // 从文件中读取并显示每行 string line = &quot;&quot;; using (StreamReader sr = new StreamReader(&quot;names.txt&quot;)) &#123; while ((line = sr.ReadLine()) != null) &#123; Console.WriteLine(line); &#125; &#125; Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12Zara AliNuha AliBinaryReader 和 BinaryWriter下面的实例演示了读取和写入二进制数据：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273using System;using System.IO;namespace BinaryFileApplication&#123; class Program &#123; static void Main(string[] args) &#123; BinaryWriter bw; BinaryReader br; int i = 25; double d = 3.14157; bool b = true; string s = &quot;I am happy&quot;; // 创建文件 try &#123; bw = new BinaryWriter(new FileStream(&quot;mydata&quot;, FileMode.Create)); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot create file.&quot;); return; &#125; // 写入文件 try &#123; bw.Write(i); bw.Write(d); bw.Write(b); bw.Write(s); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot write to file.&quot;); return; &#125; bw.Close(); // 读取文件 try &#123; br = new BinaryReader(new FileStream(&quot;mydata&quot;, FileMode.Open)); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot open file.&quot;); return; &#125; try &#123; i = br.ReadInt32(); Console.WriteLine(&quot;Integer data: &#123;0&#125;&quot;, i); d = br.ReadDouble(); Console.WriteLine(&quot;Double data: &#123;0&#125;&quot;, d); b = br.ReadBoolean(); Console.WriteLine(&quot;Boolean data: &#123;0&#125;&quot;, b); s = br.ReadString(); Console.WriteLine(&quot;String data: &#123;0&#125;&quot;, s); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot read from file.&quot;); return; &#125; br.Close(); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：1234Integer data: 25Double data: 3.14157Boolean data: TrueString data: I am happy属性属性（Property） 是类（class）、结构（structure）和接口（interface）的命名（named）成员。类或结构中的成员变量或方法称为 域（Field）。属性（Property）是域（Field）的扩展，且可使用相同的语法来访问。它们使用 访问器（accessors） 让私有域的值可被读写或操作。属性（Property）不会确定存储位置。相反，它们具有可读写或计算它们值的 访问器（accessors）。例如，有一个名为 Student 的类，带有 age、name 和 code 的私有域。我们不能在类的范围以外直接访问这些域，但是我们可以拥有访问这些私有域的属性。访问器（Accessors）属性（Property）的访问器（accessor）包含有助于获取（读取或计算）或设置（写入）属性的可执行语句。访问器（accessor）声明可包含一个 get 访问器、一个 set 访问器，或者同时包含二者。例如：1234567891011121314151617181920212223242526272829303132333435363738// 声明类型为 string 的 Code 属性public string Code&#123; get &#123; return code; &#125; set &#123; code = value; &#125;&#125;// 声明类型为 string 的 Name 属性public string Name&#123; get &#123; return name; &#125; set &#123; name = value; &#125;&#125;// 声明类型为 int 的 Age 属性public int Age&#123; get &#123; return age; &#125; set &#123; age = value; &#125;&#125;实例下面的实例演示了属性（Property）的用法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172using System;namespace tutorialspoint&#123; class Student &#123; private string code = &quot;N.A&quot;; private string name = &quot;not known&quot;; private int age = 0; // 声明类型为 string 的 Code 属性 public string Code &#123; get &#123; return code; &#125; set &#123; code = value; &#125; &#125; // 声明类型为 string 的 Name 属性 public string Name &#123; get &#123; return name; &#125; set &#123; name = value; &#125; &#125; // 声明类型为 int 的 Age 属性 public int Age &#123; get &#123; return age; &#125; set &#123; age = value; &#125; &#125; public override string ToString() &#123; return &quot;Code = &quot; + Code +&quot;, Name = &quot; + Name + &quot;, Age = &quot; + Age; &#125; &#125; class ExampleDemo &#123; public static void Main() &#123; // 创建一个新的 Student 对象 Student s = new Student(); // 设置 student 的 code、name 和 age s.Code = &quot;001&quot;; s.Name = &quot;Zara&quot;; s.Age = 9; Console.WriteLine(&quot;Student Info: &#123;0&#125;&quot;, s); // 增加年龄 s.Age += 1; Console.WriteLine(&quot;Student Info: &#123;0&#125;&quot;, s); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12Student Info: Code = 001, Name = Zara, Age = 9Student Info: Code = 001, Name = Zara, Age = 10抽象属性（Abstract Properties）抽象类可拥有抽象属性，这些属性应在派生类中被实现。下面的程序说明了这点：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485using System;namespace tutorialspoint&#123; public abstract class Person &#123; public abstract string Name &#123; get; set; &#125; public abstract int Age &#123; get; set; &#125; &#125; class Student : Person &#123; private string code = &quot;N.A&quot;; private string name = &quot;N.A&quot;; private int age = 0; // 声明类型为 string 的 Code 属性 public string Code &#123; get &#123; return code; &#125; set &#123; code = value; &#125; &#125; // 声明类型为 string 的 Name 属性 public override string Name &#123; get &#123; return name; &#125; set &#123; name = value; &#125; &#125; // 声明类型为 int 的 Age 属性 public override int Age &#123; get &#123; return age; &#125; set &#123; age = value; &#125; &#125; public override string ToString() &#123; return &quot;Code = &quot; + Code +&quot;, Name = &quot; + Name + &quot;, Age = &quot; + Age; &#125; &#125; class ExampleDemo &#123; public static void Main() &#123; // 创建一个新的 Student 对象 Student s = new Student(); // 设置 student 的 code、name 和 age s.Code = &quot;001&quot;; s.Name = &quot;Zara&quot;; s.Age = 9; Console.WriteLine(&quot;Student Info:- &#123;0&#125;&quot;, s); // 增加年龄 s.Age += 1; Console.WriteLine(&quot;Student Info:- &#123;0&#125;&quot;, s); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12Student Info: Code = 001, Name = Zara, Age = 9Student Info: Code = 001, Name = Zara, Age = 10索引器索引器（Indexer） 允许一个对象可以像数组一样被索引。当您为类定义一个索引器时，该类的行为就会像一个 虚拟数组（virtual array） 一样。您可以使用数组访问运算符（[ ]）来访问该类的实例。语法一维索引器的语法如下：1234567891011121314element-type this[int index] &#123; // get 访问器 get &#123; // 返回 index 指定的值 &#125; // set 访问器 set &#123; // 设置 index 指定的值 &#125;&#125;索引器（Indexer）的用途索引器的行为的声明在某种程度上类似于属性（property）。就像属性（property），您可使用 get 和 set 访问器来定义索引器。但是，属性返回或设置一个特定的数据成员，而索引器返回或设置对象实例的一个特定值。换句话说，它把实例数据分为更小的部分，并索引每个部分，获取或设置每个部分。定义一个属性（property）包括提供属性名称。索引器定义的时候不带有名称，但带有 this 关键字，它指向对象实例。下面的实例演示了这个概念：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556using System;namespace IndexerApplication&#123; class IndexedNames &#123; private string[] namelist = new string[size]; static public int size = 10; public IndexedNames() &#123; for (int i = 0; i &lt; size; i++) namelist[i] = &quot;N. A.&quot;; &#125; public string this[int index] &#123; get &#123; string tmp; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; tmp = namelist[index]; &#125; else &#123; tmp = &quot;&quot;; &#125; return ( tmp ); &#125; set &#123; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; namelist[index] = value; &#125; &#125; &#125; static void Main(string[] args) &#123; IndexedNames names = new IndexedNames(); names[0] = &quot;Zara&quot;; names[1] = &quot;Riz&quot;; names[2] = &quot;Nuha&quot;; names[3] = &quot;Asif&quot;; names[4] = &quot;Davinder&quot;; names[5] = &quot;Sunil&quot;; names[6] = &quot;Rubic&quot;; for ( int i = 0; i &lt; IndexedNames.size; i++ ) &#123; Console.WriteLine(names[i]); &#125; Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12345678910ZaraRizNuhaAsifDavinderSunilRubicN. A.N. A.N. A.重载索引器（Indexer）索引器（Indexer）可被重载。索引器声明的时候也可带有多个参数，且每个参数可以是不同的类型。没有必要让索引器必须是整型的。C# 允许索引器可以是其他类型，例如，字符串类型。下面的实例演示了重载索引器：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778using System;namespace IndexerApplication&#123; class IndexedNames &#123; private string[] namelist = new string[size]; static public int size = 10; public IndexedNames() &#123; for (int i = 0; i &lt; size; i++) &#123; namelist[i] = &quot;N. A.&quot;; &#125; &#125; public string this[int index] &#123; get &#123; string tmp; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; tmp = namelist[index]; &#125; else &#123; tmp = &quot;&quot;; &#125; return ( tmp ); &#125; set &#123; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; namelist[index] = value; &#125; &#125; &#125; public int this[string name] &#123; get &#123; int index = 0; while(index &lt; size) &#123; if (namelist[index] == name) &#123; return index; &#125; index++; &#125; return index; &#125; &#125; static void Main(string[] args) &#123; IndexedNames names = new IndexedNames(); names[0] = &quot;Zara&quot;; names[1] = &quot;Riz&quot;; names[2] = &quot;Nuha&quot;; names[3] = &quot;Asif&quot;; names[4] = &quot;Davinder&quot;; names[5] = &quot;Sunil&quot;; names[6] = &quot;Rubic&quot;; // 使用带有 int 参数的第一个索引器 for (int i = 0; i &lt; IndexedNames.size; i++) &#123; Console.WriteLine(names[i]); &#125; // 使用带有 string 参数的第二个索引器 Console.WriteLine(names[&quot;Nuha&quot;]); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：1234567891011ZaraRizNuhaAsifDavinderSunilRubicN. A.N. A.N. A.2委托C# 中的委托（Delegate）类似于 C 或 C++ 中函数的指针。委托（Delegate） 是存有对某个方法的引用的一种引用类型变量。引用可在运行时被改变。委托（Delegate）特别用于实现事件和回调方法。所有的委托（Delegate）都派生自 System.Delegate 类。声明委托（Delegate）委托声明决定了可由该委托引用的方法。委托可指向一个与其具有相同标签的方法。例如，假设有一个委托：1public delegate int MyDelegate (string s);上面的委托可被用于引用任何一个带有一个单一的 string 参数的方法，并返回一个 int 类型变量。声明委托的语法如下：1delegate &lt;return type&gt; &lt;delegate-name&gt; &lt;parameter list&gt;实例化委托（Delegate）一旦声明了委托类型，委托对象必须使用 new 关键字来创建，且与一个特定的方法有关。当创建委托时，传递到 new 语句的参数就像方法调用一样书写，但是不带有参数。例如：1234public delegate void printString(string s);...printString ps1 = new printString(WriteToScreen);printString ps2 = new printString(WriteToFile);下面的实例演示了委托的声明、实例化和使用，该委托可用于引用带有一个整型参数的方法，并返回一个整型值。1234567891011121314151617181920212223242526272829303132333435363738using System;delegate int NumberChanger(int n);namespace DelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static int AddNum(int p) &#123; num += p; return num; &#125; public static int MultNum(int q) &#123; num *= q; return num; &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // 创建委托实例 NumberChanger nc1 = new NumberChanger(AddNum); NumberChanger nc2 = new NumberChanger(MultNum); // 使用委托对象调用方法 nc1(25); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); nc2(5); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12Value of Num: 35Value of Num: 175委托的多播（Multicasting of a Delegate）委托对象可使用 “+” 运算符进行合并。一个合并委托调用它所合并的两个委托。只有相同类型的委托可被合并。”-“ 运算符可用于从合并的委托中移除组件委托。使用委托的这个有用的特点，您可以创建一个委托被调用时要调用的方法的调用列表。这被称为委托的 多播（multicasting），也叫组播。下面的程序演示了委托的多播：123456789101112131415161718192021222324252627282930313233343536373839using System;delegate int NumberChanger(int n);namespace DelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static int AddNum(int p) &#123; num += p; return num; &#125; public static int MultNum(int q) &#123; num *= q; return num; &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // 创建委托实例 NumberChanger nc; NumberChanger nc1 = new NumberChanger(AddNum); NumberChanger nc2 = new NumberChanger(MultNum); nc = nc1; nc += nc2; // 调用多播 nc(5); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：1Value of Num: 75委托（Delegate）的用途委托多播实例：例如小明叫小张买完车票，之后接着又让他带张电影票：1234567891011121314151617181920212223242526272829303132// 小张类public class MrZhang &#123; // 其实买车票的悲情人物是小张 public static void BuyTicket() &#123; Console.WriteLine(&quot;NND,每次都让我去买票，鸡人呀！&quot;); &#125; public static void BuyMovieTicket() &#123; Console.WriteLine(&quot;我去，自己泡妞，还要让我带电影票！&quot;); &#125;&#125;//小明类class MrMing&#123; // 声明一个委托，其实就是个“命令” public delegate void BugTicketEventHandler(); public static void Main(string[] args) &#123; // 这里就是具体阐述这个命令是干什么的，本例是MrZhang.BuyTicket“小张买车票” BugTicketEventHandler myDelegate = new BugTicketEventHandler(MrZhang.BuyTicket); myDelegate += MrZhang.BuyMovieTicket; // 这时候委托被附上了具体的方法 myDelegate(); Console.ReadKey(); &#125;&#125;事件事件（Event） 基本上说是一个用户操作，如按键、点击、鼠标移动等等，或者是一些出现，如系统生成的通知。应用程序需要在事件发生时响应事件。例如，中断。事件是用于进程间通信。通过事件使用委托事件在类中声明且生成，且通过使用同一个类或其他类中的委托与事件处理程序关联。包含事件的类用于发布事件。这被称为 发布器（publisher） 类。其他接受该事件的类被称为 订阅器（subscriber） 类。事件使用 发布-订阅（publisher-subscriber） 模型。发布器（publisher） 是一个包含事件和委托定义的对象。事件和委托之间的联系也定义在这个对象中。发布器（publisher）类的对象调用这个事件，并通知其他的对象。订阅器（subscriber） 是一个接受事件并提供事件处理程序的对象。在发布器（publisher）类中的委托调用订阅器（subscriber）类中的方法（事件处理程序）。声明事件（Event）在类的内部声明事件，首先必须声明该事件的委托类型。例如：1public delegate void BoilerLogHandler(string status);然后，声明事件本身，使用 event 关键字：12// 基于上面的委托定义事件public event BoilerLogHandler BoilerEventLog;上面的代码定义了一个名为 BoilerLogHandler 的委托和一个名为 BoilerEventLog 的事件，该事件在生成的时候会调用委托。实例 112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849using System;namespace SimpleEvent&#123; using System; public class EventTest &#123; private int value; public delegate void NumManipulationHandler(); public event NumManipulationHandler ChangeNum; protected virtual void OnNumChanged() &#123; if (ChangeNum != null) &#123; ChangeNum(); &#125; else &#123; Console.WriteLine(&quot;Event fired!&quot;); &#125; &#125; public EventTest(int n ) &#123; SetValue(n); &#125; public void SetValue(int n) &#123; if (value != n) &#123; value = n; OnNumChanged(); &#125; &#125; &#125; public class MainClass &#123; public static void Main() &#123; EventTest e = new EventTest(5); e.SetValue(7); e.SetValue(11); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：123Event Fired!Event Fired!Event Fired!实例 2本实例提供一个简单的用于热水锅炉系统故障排除的应用程序。当维修工程师检查锅炉时，锅炉的温度和压力会随着维修工程师的备注自动记录到日志文件中。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100using System;using System.IO;namespace BoilerEventAppl&#123; // boiler 类 class Boiler &#123; private int temp; private int pressure; public Boiler(int t, int p) &#123; temp = t; pressure = p; &#125; public int getTemp() &#123; return temp; &#125; public int getPressure() &#123; return pressure; &#125; &#125; // 事件发布器 class DelegateBoilerEvent &#123; public delegate void BoilerLogHandler(string status); // 基于上面的委托定义事件 public event BoilerLogHandler BoilerEventLog; public void LogProcess() &#123; string remarks = &quot;O. K&quot;; Boiler b = new Boiler(100, 12); int t = b.getTemp(); int p = b.getPressure(); if(t &gt; 150 || t &lt; 80 || p &lt; 12 || p &gt; 15) &#123; remarks = &quot;Need Maintenance&quot;; &#125; OnBoilerEventLog(&quot;Logging Info:\n&quot;); OnBoilerEventLog(&quot;Temparature &quot; + t + &quot;\nPressure: &quot; + p); OnBoilerEventLog(&quot;\nMessage: &quot; + remarks); &#125; protected void OnBoilerEventLog(string message) &#123; if (BoilerEventLog != null) &#123; BoilerEventLog(message); &#125; &#125; &#125; // 该类保留写入日志文件的条款 class BoilerInfoLogger &#123; FileStream fs; StreamWriter sw; public BoilerInfoLogger(string filename) &#123; fs = new FileStream(filename, FileMode.Append, FileAccess.Write); sw = new StreamWriter(fs); &#125; public void Logger(string info) &#123; sw.WriteLine(info); &#125; public void Close() &#123; sw.Close(); fs.Close(); &#125; &#125; // 事件订阅器 public class RecordBoilerInfo &#123; static void Logger(string info) &#123; Console.WriteLine(info); &#125;//end of Logger static void Main(string[] args) &#123; BoilerInfoLogger filelog = new BoilerInfoLogger(&quot;e:\\boiler.txt&quot;); DelegateBoilerEvent boilerEvent = new DelegateBoilerEvent(); boilerEvent.BoilerEventLog += new DelegateBoilerEvent.BoilerLogHandler(Logger); boilerEvent.BoilerEventLog += new DelegateBoilerEvent.BoilerLogHandler(filelog.Logger); boilerEvent.LogProcess(); Console.ReadLine(); filelog.Close(); &#125;//end of main &#125;//end of RecordBoilerInfo&#125;当上面的代码被编译和执行时，它会产生下列结果：123456Logging info:Temperature 100Pressure 12Message: O. K集合集合（Collection）类是专门用于数据存储和检索的类。这些类提供了对栈（stack）、队列（queue）、列表（list）和哈希表（hash table）的支持。大多数集合类实现了相同的接口。集合（Collection）类服务于不同的目的，如为元素动态分配内存，基于索引访问列表项等等。这些类创建 Object 类的对象的集合。在 C# 中，Object 类是所有数据类型的基类。各种集合类和它们的用法下面是各种常用的 System.Collection 命名空间的类。点击下面的链接查看细节。类描述和用法动态数组（ArrayList）它代表了可被单独索引的对象的有序集合。它基本上可以替代一个数组。但是，与数组不同的是，您可以使用索引在指定的位置添加和移除项目，动态数组会自动重新调整它的大小。它也允许在列表中进行动态内存分配、增加、搜索、排序各项。哈希表（Hashtable）它使用键来访问集合中的元素。当您使用键访问元素时，则使用哈希表，而且您可以识别一个有用的键值。哈希表中的每一项都有一个键/值对。键用于访问集合中的项目。排序列表（SortedList）它可以使用键和索引来访问列表中的项。排序列表是数组和哈希表的组合。它包含一个可使用键或索引访问各项的列表。如果您使用索引访问各项，则它是一个动态数组（ArrayList），如果您使用键访问各项，则它是一个哈希表（Hashtable）。集合中的各项总是按键值排序。堆栈（Stack）它代表了一个后进先出的对象集合。当您需要对各项进行后进先出的访问时，则使用堆栈。当您在列表中添加一项，称为推入元素，当您从列表中移除一项时，称为弹出元素。队列（Queue）它代表了一个先进先出的对象集合。当您需要对各项进行先进先出的访问时，则使用队列。当您在列表中添加一项，称为入队，当您从列表中移除一项时，称为出队。点阵列（BitArray）它代表了一个使用值 1 和 0 来表示的二进制数组。当您需要存储位，但是事先不知道位数时，则使用点阵列。您可以使用整型索引从点阵列集合中访问各项，索引从零开始。泛型泛型（Generic） 允许您延迟编写类或方法中的编程元素的数据类型的规范，直到实际在程序中使用它的时候。换句话说，泛型允许您编写一个可以与任何数据类型一起工作的类或方法。您可以通过数据类型的替代参数编写类或方法的规范。当编译器遇到类的构造函数或方法的函数调用时，它会生成代码来处理指定的数据类型。下面这个简单的实例将有助于您理解这个概念：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556using System;using System.Collections.Generic;namespace GenericApplication&#123; public class MyGenericArray&lt;T&gt; &#123; private T[] array; public MyGenericArray(int size) &#123; array = new T[size + 1]; &#125; public T getItem(int index) &#123; return array[index]; &#125; public void setItem(int index, T value) &#123; array[index] = value; &#125; &#125; class Tester &#123; static void Main(string[] args) &#123; // 声明一个整型数组 MyGenericArray&lt;int&gt; intArray = new MyGenericArray&lt;int&gt;(5); // 设置值 for (int c = 0; c &lt; 5; c++) &#123; intArray.setItem(c, c*5); &#125; // 获取值 for (int c = 0; c &lt; 5; c++) &#123; Console.Write(intArray.getItem(c) + &quot; &quot;); &#125; Console.WriteLine(); // 声明一个字符数组 MyGenericArray&lt;char&gt; charArray = new MyGenericArray&lt;char&gt;(5); // 设置值 for (int c = 0; c &lt; 5; c++) &#123; charArray.setItem(c, (char)(c+97)); &#125; // 获取值 for (int c = 0; c &lt; 5; c++) &#123; Console.Write(charArray.getItem(c) + &quot; &quot;); &#125; Console.WriteLine(); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：120 5 10 15 20a b c d e泛型（Generic）的特性使用泛型是一种增强程序功能的技术，具体表现在以下几个方面：它有助于您最大限度地重用代码、保护类型的安全以及提高性能。您可以创建泛型集合类。.NET 框架类库在 System.Collections.Generic 命名空间中包含了一些新的泛型集合类。您可以使用这些泛型集合类来替代 System.Collections 中的集合类。您可以创建自己的泛型接口、泛型类、泛型方法、泛型事件和泛型委托。您可以对泛型类进行约束以访问特定数据类型的方法。关于泛型数据类型中使用的类型的信息可在运行时通过使用反射获取。泛型（Generic）方法在上面的实例中，我们已经使用了泛型类，我们可以通过类型参数声明泛型方法。下面的程序说明了这个概念：123456789101112131415161718192021222324252627282930313233343536373839404142using System;using System.Collections.Generic;namespace GenericMethodAppl&#123; class Program &#123; static void Swap&lt;T&gt;(ref T lhs, ref T rhs) &#123; T temp; temp = lhs; lhs = rhs; rhs = temp; &#125; static void Main(string[] args) &#123; int a, b; char c, d; a = 10; b = 20; c = &apos;I&apos;; d = &apos;V&apos;; // 在交换之前显示值 Console.WriteLine(&quot;Int values before calling swap:&quot;); Console.WriteLine(&quot;a = &#123;0&#125;, b = &#123;1&#125;&quot;, a, b); Console.WriteLine(&quot;Char values before calling swap:&quot;); Console.WriteLine(&quot;c = &#123;0&#125;, d = &#123;1&#125;&quot;, c, d); // 调用 swap Swap&lt;int&gt;(ref a, ref b); Swap&lt;char&gt;(ref c, ref d); // 在交换之后显示值 Console.WriteLine(&quot;Int values after calling swap:&quot;); Console.WriteLine(&quot;a = &#123;0&#125;, b = &#123;1&#125;&quot;, a, b); Console.WriteLine(&quot;Char values after calling swap:&quot;); Console.WriteLine(&quot;c = &#123;0&#125;, d = &#123;1&#125;&quot;, c, d); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12345678Int values before calling swap:a = 10, b = 20Char values before calling swap:c = I, d = VInt values after calling swap:a = 20, b = 10Char values after calling swap:c = V, d = I泛型（Generic）委托您可以通过类型参数定义泛型委托。例如：1delegate T NumberChanger&lt;T&gt;(T n);下面的实例演示了委托的使用：123456789101112131415161718192021222324252627282930313233343536373839using System;using System.Collections.Generic;delegate T NumberChanger&lt;T&gt;(T n);namespace GenericDelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static int AddNum(int p) &#123; num += p; return num; &#125; public static int MultNum(int q) &#123; num *= q; return num; &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // 创建委托实例 NumberChanger&lt;int&gt; nc1 = new NumberChanger&lt;int&gt;(AddNum); NumberChanger&lt;int&gt; nc2 = new NumberChanger&lt;int&gt;(MultNum); // 使用委托对象调用方法 nc1(25); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); nc2(5); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：12Value of Num: 35Value of Num: 175匿名方法我们已经提到过，委托是用于引用与其具有相同标签的方法。换句话说，您可以使用委托对象调用可由委托引用的方法。匿名方法（Anonymous methods） 提供了一种传递代码块作为委托参数的技术。匿名方法是没有名称只有主体的方法。在匿名方法中您不需要指定返回类型，它是从方法主体内的 return 语句推断的。编写匿名方法的语法匿名方法是通过使用 delegate 关键字创建委托实例来声明的。例如：123456delegate void NumberChanger(int n);...NumberChanger nc = delegate(int x)&#123; Console.WriteLine(&quot;Anonymous Method: &#123;0&#125;&quot;, x);&#125;;代码块 Console.WriteLine(&quot;Anonymous Method: {0}&quot;, x); 是匿名方法的主体。委托可以通过匿名方法调用，也可以通过命名方法调用，即，通过向委托对象传递方法参数。例如：1nc(10);实例下面的实例演示了匿名方法的概念：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950using System;delegate void NumberChanger(int n);namespace DelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static void AddNum(int p) &#123; num += p; Console.WriteLine(&quot;Named Method: &#123;0&#125;&quot;, num); &#125; public static void MultNum(int q) &#123; num *= q; Console.WriteLine(&quot;Named Method: &#123;0&#125;&quot;, num); &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // 使用匿名方法创建委托实例 NumberChanger nc = delegate(int x) &#123; Console.WriteLine(&quot;Anonymous Method: &#123;0&#125;&quot;, x); &#125;; // 使用匿名方法调用委托 nc(10); // 使用命名方法实例化委托 nc = new NumberChanger(AddNum); // 使用命名方法调用委托 nc(5); // 使用另一个命名方法实例化委托 nc = new NumberChanger(MultNum); // 使用命名方法调用委托 nc(2); Console.ReadKey(); &#125; &#125;&#125;当上面的代码被编译和执行时，它会产生下列结果：123Anonymous Method: 10Named Method: 15Named Method: 30多线程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GRADIENTS, BATCH NORMALIZATION AND LAYER NORMALIZATION]]></title>
      <url>%2F2017%2F05%2F09%2FGRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION%2F</url>
      <content type="text"><![CDATA[In this post, we will take a look at common pit falls with optimization and solutions to some of these issues. The main topics that will be covered are:GradientsExploding gradientsVanishing gradientsLSTMs (pertaining to vanishing gradients)NormalizationAnd then we will see how to implement batch and layer normalization and apply them to our cells.GRADIENTSFirst, we will take a closer look at gradients and backpropagation during optimization. Our example will be a simple MLP but we will extend to an RNN later on.I want to go over what a gradient means. Let’s say we have a very simple MLP with 1 set of weights W_1 which is used to calcualte some y. We devise a very simple loss function J, and our gradient becomes dJ/dW_1 (d = partials). Sure we can take the derivative and apply chain rule and get a number, but what does this value even mean? The gradient can be thought of as several things. One is that the magnitude of the gradient represents the sensitivity or impact this weight has on determining y which determines our loss. This can be seen below:CS231nWhat the gradients (dfdx, dfdy, dfdz, dfdq, dfdz) tell us is the sensitivity of each variable on our result f. In an MLP, we will produce a result (logits) and compare it with our targets to determine the deviance in what we got and what we should have gotten. From this we can use backpropagation to determine how much adjusting needs to be made for each variable along the way, all the way to the beginning.The gradient also holds another key piece of information. It repesents how much we need to change the weights in order to move towards our goal (minimizing the loss, maximizing some objective, etc.). With simple SGD, we get the gradient and we apply an update to the weights (W_i_new = W_i_old – alpha * gradient). If we follow the direction of the gradient, we will be maximizing the goal function. Our loss functions (NLL or cross entropy) are functions we wish to minimize, so we subtract the gradient. We use the learning parameter alpha to control how quickly we change. This is where all of the normalization techniques in this post will come in handy.If we have an alpha that is 1 or larger, we will allow the gradient to directly impact our weights. In the beginning of training a neural net, our weight initializations are bound to be far off from the weights we actually need. This creates a large error and so, results in large gradients. If we choose to update our weights with these large gradients, we will be never reach the minimum point for our loss function. We will keep overshooting and bouncing back and forth. So, we use this alpha (small value) to control how much impact the gradient has. Eventually, the gradient will get smaller as well because of less error and we will reach our goal, but with such a small alpha, this can take a while. With techniques, such as batch normalization and layer normalization, we can afford to use large alpha because the gradients will be controlled due to controlled outputs from the neurons.Now, even with a simple RNN structure, backpropagation can pose several issues. When we get our result, we need to backpropagate all the way back to the very first cell in order to complete our updates. The main principles to really understand are: if I multiply a number greater than 1 over and over, I will reach infinity (explosion) and vice versa, if I multiply a number less than 1 over and over, I will reach 0 (vanishing).EXPLODING GRADIENTSThe first issue is that our gradients can be greater than 1. As we backpropagate the gradient through the network, we can end up with massive gradients. So far, the solution to exploding gradients is a very hacky but cheap solution; just clip the norm of the gradient at some threshold.VANISHING GRADIENTSWe could also experience the other issue where the gradient is less than 1 to start with and as we backpropagate, the effect of the gradient weakens and it will eventually be negligible. A common scenario where this occurs is when we have saturation at the tails of the sigmoidal function (0 or 1). This is problematic because now the derivative will always be near 0. During backpropagation, we will be multiplying this near zero derivative with our error repeatedly.Let’s look at the sigmoidal activation function. You can replicate this example for tanh too.To solve this issue, we can use rectified linear units (ReLU) which don’t suffer from this tail saturation as much. The derivative is 1 if x &gt; 0, so now error signal won’t weaken as it backpropagates through the network. But we do have the problem in the negative region (x &lt;0) where the derivative is zero. This can nullify our error signal so it’s best to add a leaky factor (http://arxiv.org/abs/1502.01852) to the ReLU unit, where the negative region will have some small negative slope. This parameter can be fixed or be a randomized parameter and be fixed after training. There’s also maxout (http://arxiv.org/abs/1302.4389) but this will have twice the amount of weights as a regular ReLU unit.LSTMS (VANISHING GRADIENTS)As for how LSTMs solve the vanishing gradient issue, they don’t have to worry about the error signal weakening as with a regular basic RNN cell. It’s a bit complicated but the basic idea is that they have a forget gate that determines how much previous memory is stored in the network. This architecture allows the error signal to be transferred effectively to the previous time step. This is usually referred to as the constant error carousel (CEC).NORMALIZATIONThere are several types of normalization techniques but the idea behind all of them is the same, which is shifting our inputs to a zero mean and unit variance.Techniques like batch norm (https://arxiv.org/abs/1502.03167) may help with the gradient issues as a side effect but the main object is to improve overall optimization. When we first initialize our weights, we are bound to have very large deviances from the true weights. These outliers need to be compensated for by the gradients and this further delays convergence during training. Batchnorm helps us here by normalizing the gradients (reducing influence from weight deviances) on a batched implementation and allows us to train faster (can even safely use larger learning rates now).With batch norm, the main idea is to normalize at each layer for every minibatch. We initially may normalize our inputs, but as they travel through the layers, the inputs are operated on by weights and neurons and effectively change. As this progresses, the deviances get larger and larger and our backpropagation will need to account for these large deviances. This restricts us to using a small learning rate to prevent gradient explosion/vanishing. With batch norm, we will normalize the inputs (activations coming from the previous layer) going into each layer using the mean and variance of the activations for the entire minibatch. The normalization is a bit different during training and inference but it is beyond the scope of this post. (details in paper).Batch normalization is very nice but it is based on minibatch size and so it’s a bit difficult to use with recurrent architectures. With layer normalization, we instead compute the mean and variance using ALL of the summed inputs to the neurons in a layer for EVERY single training**case. This removes the dependency on a minibatch size. Unlike batch normalization, the normalization operation for layer norm is same for training and inference. More details can be found on Hinton’s paper here**.######IMPLEMENTING BATCH NORMALIZATIONAs stated above, the main goal of batch normalization is optimization. By normalizing the inputs to a layer to zero mean and unit variance, we can help our net learn faster by minimizing the effects from large errors (especially during initial training).Batch norm is given by the operation below, where \epsilon is a small random noise (for stability). When we apply batch norm on a layer, we are restricting the inputs to follow a normal distribution, which ultimately will restrict the nets ability to learn. In order to fix this, we multiply by a scale parameter (\alpha) and add a shift parameter (\beta). Both of these parameters are trainable.Note that both alpha and beta are applied element wise, so there will be a scale and shift for each neuron in the subsequent layer. With batchnorm, we compute mean and variance across an entire batch and we have a value for each neuron we are feeding our normalized inputs into.So for a given layer, the mean during BN will be 1X. Each training data gets this mean subtracted from it and divided by sqrt(var + epsilon) and then shifted and scaled. To find the mean and var, we use all the examples in the training batch.In order to accurately evaluate the effectiveness of batchnorm, we will use a simple MLP to classify MNIST digits. We will run a normal MLP and an MLP with batchnorm, both initialized with the same starting weights. Let’s take a look at both the naive and TF implementations.First, the naive version:12345678910# Naive BN layerscale1 = tf.Variable(tf.ones([100]))shift1 = tf.Variable(tf.zeros([100]))W1_BN = tf.Variable(W1_init)b1_BN = tf.Variable(tf.zeros([100]))z1_BN = tf.matmul(X,W1_BN)+b1_BNmean1, var1 = tf.nn.moments(z1_BN, [0])BN1 = (z1_BN - mean1) / tf.sqrt(var1 + FLAGS.epsilon)BN1 = scale1*BN1 + shift1fc1_BN = tf.nn.relu(BN1)TF implementation:123456789# TF BN layerscale2 = tf.Variable(tf.ones([100]))shift2 = tf.Variable(tf.zeros([100]))W2_BN = tf.Variable(W2_init)b2_BN = tf.Variable(tf.zeros([100]))z2_BN = tf.matmul(fc1_BN,W2_BN)+b2_BNmean2, var2 = tf.nn.moments(z2_BN, [0])BN2 = tf.nn.batch_normalization(z2_BN,mean2,var2,shift2,scale2,FLAGS.epsilon)fc2_BN = tf.nn.relu(BN2)We first need to compute the mean and variance of the inputs coming into the layer. Then normalize them and scale/shift and then apply the activation function and pass to the next layer.Let’s compare the performance of the normal MLP and the MLP with batchnorm. We will focus of the massive impact on our cost with and without BN. Other interesting features to look at would be gradient norm, neuron inputs, etc.CROSS ENTROPY LOSS###NUANCE:Training is all fine and well, but what about testing. When doing BN on our test set, with the implementation from above, we will be using the mean and variance from our test set. Now think about what will happen if our test set is very small or even size 1. This will homogenize all the outputs we get since all inputs will be close to mean 0 and variance 1. The solution to this is to calculate the population mean and variance during testing and then use those values during testing.Now there are couple ways we can try to calculate the population, even simple as taking the average of the training batch and using it for testing. This isn’t the true population measure so we will calculate the unbiased mean and variance as they do in the original paper. But first, let’s see the accuracy when we feed in test samples of size 1.Not exactly state of the art anymore. So let’s see how to calculate population mean and variance.We will be updating the population mean and variance after each training batch and we will use them for inference. In fact we can simple replace the inference batchnorm process with a simple linear transformation:Below is the tensorflow implementation for batchnorm with the exponential moving average to use during inference. Take a look here for more implementation specifications for batch_norm but the required parameters for us is the actual input that we wish to normalize and wether or not we are training. Note: TF batchnorm with inference is in batch_norm2.py1234567891011121314from tensorflow.contrib.layers import ( batch_norm)...with tf.variable_scope('BN_1') as BN_1: self.BN1 = tf.cond(self.is_training_ph, lambda: batch_norm( self.z1_BN, is_training=True, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=BN_1), lambda: batch_norm( self.z1_BN, is_training=False, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=BN_1, reuse=True))Here are the inference results with the population mean and variance:######IMPLEMENTING LAYER NORMALIZATIONLayernorm is very similar to batch normalization in many ways as you can see with the equation below but it usually reserved for use with recurrent architectures.Layernorm acts on a per layer per sample basis, where the mean and variance are calculated for a specific layer for a specific training point. To understand the different between layernorm and batchnorm let’s see how these mean and variances are computed for both with figures.With layernorm it’s a bit different from BN. We compute the mean and var for every single sample for each layer independently and then do the LN operations using those computed values.First, we will make a function that will apply batch norm given an input tensor.1234567891011121314151617181920# LN funcitiondef ln(inputs, epsilon = 1e-5, scope = None): """ Computer LN given an input tensor. We get in an input of shape [N X D] and with LN we compute the mean and var for each individual training point across all it's hidden dimensions rather than across the training batch as we do in BN. This gives us a mean and var of shape [N X 1]. """ mean, var = tf.nn.moments(inputs, [1], keep_dims=True) with tf.variable_scope(scope + 'LN'): scale = tf.get_variable('alpha', shape=[inputs.get_shape()[1]], initializer=tf.constant_initializer(1)) shift = tf.get_variable('beta', shape=[inputs.get_shape()[1]], initializer=tf.constant_initializer(0)) LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift return LNNow we can apply our LN function to a GRUCell class. Note that I am using tensorflow’s GRUCell class but we can apply LN to all of their other RNN variants as well (LSTM, peephole LSTM, etc.)1234567891011121314151617181920212223242526272829303132333435class GRUCell(RNNCell): """Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""" def __init__(self, num_units, input_size=None, activation=tanh): if input_size is not None: logging.warn("%s: The input_size parameter is deprecated.", self) self._num_units = num_units self._activation = activation @property def state_size(self): return self._num_units @property def output_size(self): return self._num_units def __call__(self, inputs, state, scope=None): """Gated recurrent unit (GRU) with nunits cells.""" with vs.variable_scope(scope or type(self).__name__): # "GRUCell" with vs.variable_scope("Gates"): # Reset gate and update gate. # We start with bias of 1.0 to not reset and not update. r, u = array_ops.split(1, 2, _linear([inputs, state], 2 * self._num_units, True, 1.0)) # Apply Layer Normalization to the two gates r = ln(r, scope = 'r/') u = ln(r, scope = 'u/') r, u = sigmoid(r), sigmoid(u) with vs.variable_scope("Candidate"): c = self._activation(_linear([inputs, r * state], self._num_units, True)) new_h = u * state + (1 - u) * c return new_h, new_hSHAPES:I received quite a few PMs about some confusing aspects of BN and LN, mostly centered around what is actually the input. Let’s look at BN first. The input to a hidden layer will be [NXH]. Applying BN involves calculating the mean value for each H across all N samples. So we will have a mean of shape [1XH]. This “batch” mean will be used for BN, basically subtracting this batch mean from each sample.Now for LN, let’s imagine a simple RNN situation. Batch major inputs are of shape [N, M, H], where N is the batch size, M is the max number of time steps and H is the number of hidden units. Before feeing to an RNN, we can reshape to time-major which becomes [M, N, H]. Now we feed in one time step at a time into the RNN, so the shape of each time-step’s input is [N,H]. Applying LN involves calculating the mean for sample across dimension [1], which means looking at all hidden states for each sample (for this particular time step). This gives us a mean of size [NX1]. We use this “layer” mean for each sample.Source page is HERE.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[What Is Local Response Normalization In Convolutional Neural Networks]]></title>
      <url>%2F2017%2F05%2F04%2FWhat-Is-Local-Response-Normalization-In-Convolutional-Neural-Networks%2F</url>
      <content type="text"><![CDATA[Convolutional Neural Networks (CNNs) have been doing wonders in the field of image recognition in recent times. CNN is a type of deep neural network in which the layers are connected using spatially organized patterns. This is in line with how the human visual cortex processes image data. Researchers have been working on coming up with better architectures over the last few years. In this blog post, we will discuss a particular type of layer that has been used consistently across many famous architectures. This layer is called Local Response Normalization layer and it plays an important role. What does it do? What’s the advantage of having this in our network?Why do we need normalization layers in the first place?A typical CNN consists of the following layers: convolution, pooling, rectified linear unit (ReLU), fully connected, and loss. If the previous sentence didn’t make sense, you may want to go through a quick CNN tutorial before proceeding further. Anyway, the reason we may want to have normalization layers in our CNN is that we want to have some kind of inhibition scheme.In neurobiology, there is a concept called “lateral inhibition”. Now what does that mean? This refers to the capacity of an excited neuron to subdue its neighbors. We basically want a significant peak so that we have a form of local maxima. This tends to create a contrast in that area, hence increasing the sensory perception. Increasing the sensory perception is a good thing! We want to have the same thing in our CNNs.What exactly is Local Response Normalization?Local Response Normalization (LRN) layer implements the lateral inhibition we were talking about in the previous section. This layer is useful when we are dealing with ReLU neurons. Why is that? Because ReLU neurons have unbounded activations and we need LRN to normalize that. We want to detect high frequency features with a large response. If we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors.At the same time, it will dampen the responses that are uniformly large in any given local neighborhood. If all the values are large, then normalizing those values will diminish all of them. So basically we want to encourage some kind of inhibition and boost the neurons with relatively larger activations. This has been discussed nicely in Section 3.3 of the original paper by Krizhevsky et al.How is it done in practice?There are two types of normalizations available in Caffe. You can either normalize within the same channel or you can normalize across channels. Both these methods tend to amplify the excited neuron while dampening the surrounding neurons. When you are normalizing within the same channel, it’s just like considering a 2D neighborhood of dimension N x N, where N is the size of the normalization window. You normalize this window using the values in this neighborhood. If you are normalizing across channels, you will consider a neighborhood along the third dimension but at a single location. You need to consider an area of shape N x 1 x 1. Here 1 x 1 refers to a single value in a 2D matrix and N refers to the normalization size.Source page is here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The first course of the Docker]]></title>
      <url>%2F2017%2F05%2F01%2FThe-first-course-of-the-Docker%2F</url>
      <content type="text"><![CDATA[Docker command:docker images 查看本机所有镜像docker pull NAME 从仓库下载镜像docker run [-d -p 8080:80] or [-P] NAME 启动镜像（-d 后台运行 -p 端口映射 -P 随机映射）docker exec [-i -t] NAME bash 进入容器并执行bashdocker ps 查看后台容器docker stop ID 停止docker容器docker restart ID 重启容器​Docker netowrk type: bridgeDocker port map: host(eth0:80) &lt;–&gt; dicker0(bridge) &lt;–&gt; docker container(eth0:80)Build DockerDockerfiledocker build [-t] 建立Docker，指定TAGA Dcokerfile example (based tomcat):12345from tomcat MAINTAINER ewan ewanlee@yeah.netCOPY jpress-web-newest.war /usr/local/tomcat/webapps搭建第一个Web app为了介绍方便，所以使用了开源的java实现的wordpress，也就是Jpress[1]下载相应的war包，并存到工作目录下[2]下载一个tomcat的Docker镜像docker pull tomcat1234567891011121314151617Using default tag: latestlatest: Pulling from library/tomcatcd0a524342ef: Pull complete e39c3ffe4133: Pull complete aac3320edf40: Pull complete 4d9e109682f7: Pull complete 0a59efcf9553: Pull complete 43a404e523e0: Pull complete 806f07b1dce8: Pull complete 0cad96dccb4c: Pull complete 04073e2a9145: Pull complete d9e4bf4be89c: Pull complete 739005fdecc9: Pull complete 8bd03d99f1b2: Pull complete d586afbd7622: Pull complete Digest: sha256:88483873b279aaea5ced002c98dde04555584b66de29797a4476d5e94874e6deStatus: Downloaded newer image for tomcat:latest[3]写一个Dockerfile，也就是之前的example[4]建立镜像docker build -t jpress:latest .结果如下：1234567891011Sending build context to Docker daemon 20.8 MBStep 1 : FROM tomcat ---&gt; d71978506e58Step 2 : MAINTAINER ewan ewanlee@yeah.net ---&gt; Running in dfa1902d1ea4 ---&gt; 956612ba6987Removing intermediate container dfa1902d1ea4Step 3 : COPY jpress-web-newest.war /usr/local/tomcat/webapps ---&gt; dd6eecd741e7Removing intermediate container 1fe7f943071bSuccessfully built dd6eecd741e7[5]下载一个mysql的docker镜像docker pull mysql123456789101112131415Using default tag: latestlatest: Pulling from library/mysqlcd0a524342ef: Already exists d9c95f06c17e: Pull complete 46b2d578f59a: Pull complete 10fbc2bcc6e9: Pull complete 91b1a29c3956: Pull complete 5bf9316bd602: Pull complete 69bd23f08b55: Pull complete 4fb778132e94: Pull complete 6913628d7744: Pull complete a477f36dc2e0: Pull complete c954124ae935: Pull complete Digest: sha256:e44b9a3ae88db013a3e8571a89998678ba44676ed4ae9f54714fd31e108f8b58Status: Downloaded newer image for mysql:latest[6]运行mysql并创建一个数据库1docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=000000 -e MYSQL_DATABASE=jpress mysql[7]运行自己建立的jpress镜像docker run -d -p 8888:8080 jpress下面进行浏览器页面的配置，在浏览器输入localhost:8888将出现以下界面：在地址栏后加入后缀jpress-web-newest填写配置信息，注意服务器地址是docker0网卡的ip结果安装过程中出现了一个bug，就是在进行配置后我退出了，再次进去重新配置出错，最后发现原因是表前缀需要改一下，因为之前配置成功了，数据库中已经有了一个相同的表前缀​:P是不是很方便，完全不用手动安装任何东西~Referenceshttp://www.imooc.com/learn/824]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HMM implemented by hmmlearn]]></title>
      <url>%2F2017%2F05%2F01%2FHMM-implemented-by-hmmlearn%2F</url>
      <content type="text"><![CDATA[Sampling from HMMThis script shows how to sample points from a Hidden Markov Model (HMM): we use a 4-components with specified mean and covariance.The plot show the sequence of observations generated with the transitions between them. We can see that, as specified by our transition matrix, there are no transition between component 1 and 3.123456print(__doc__)import numpy as npimport matplotlib.pyplot as pltfrom hmmlearn import hmmPrepare parameters for a 4-components HMM Initial population probability123456789101112131415161718192021222324252627282930313233343536373839startprob = np.array([0.6, 0.3, 0.1, 0.0])# The transition matrix, note that there are no transitions possible# between component 1 and 3transmat = np.array([[0.7, 0.2, 0.0, 0.1], [0.3, 0.5, 0.2, 0.0], [0.0, 0.3, 0.5, 0.2], [0.2, 0.0, 0.2, 0.6]])# The means of each componentmeans = np.array([[0.0, 0.0], [0.0, 11.0], [9.0, 10.0], [11.0, -1.0]])# The covariance of each componentcovars = .5 * np.tile(np.identity(2), (4, 1, 1))# Build an HMM instance and set parametersmodel = hmm.GaussianHMM(n_components=4, covariance_type="full")# Instead of fitting it from the data, we directly set the estimated# parameters, the means and covariance of the componentsmodel.startprob_ = startprobmodel.transmat_ = transmatmodel.means_ = meansmodel.covars_ = covars# Generate samplesX, Z = model.sample(500)# Plot the sampled dataplt.plot(X[:, 0], X[:, 1], ".-", label="observations", ms=6, mfc="orange", alpha=0.7)# Indicate the component numbersfor i, m in enumerate(means): plt.text(m[0], m[1], 'Component %i' % (i + 1), size=17, horizontalalignment='center', bbox=dict(alpha=.7, facecolor='w'))plt.legend(loc='best')plt.show()Total running time of the script: ( 0 minutes 0.676 seconds)Gaussian HMM of stock dataThis script shows how to use Gaussian HMM on stock price data from Yahoo! finance. For more information on how to visualize stock prices with matplotlib, please refer to date_demo1.py of matplotlib.12345678910111213141516171819from __future__ import print_functionimport datetimeimport numpy as npfrom matplotlib import cm, pyplot as pltfrom matplotlib.dates import YearLocator, MonthLocatortry: from matplotlib.finance import quotes_historical_yahoo_ochlexcept ImportError: # For Matplotlib prior to 1.5. from matplotlib.finance import ( quotes_historical_yahoo as quotes_historical_yahoo_ochl )from hmmlearn.hmm import GaussianHMMprint(__doc__)Get quotes from Yahoo! finance1234567891011121314151617quotes = quotes_historical_yahoo_ochl( "INTC", datetime.date(1995, 1, 1), datetime.date(2012, 1, 6))# Unpack quotesdates = np.array([q[0] for q in quotes], dtype=int)close_v = np.array([q[2] for q in quotes])volume = np.array([q[5] for q in quotes])[1:]# Take diff of close value. Note that this makes# ``len(diff) = len(close_t) - 1``, therefore, other quantities also# need to be shifted by 1.diff = np.diff(close_v)dates = dates[1:]close_v = close_v[1:]# Pack diff and volume for training.X = np.column_stack([diff, volume])Run Gaussian HMM123456789print("fitting to HMM and decoding ...", end="")# Make an HMM instance and execute fitmodel = GaussianHMM(n_components=4, covariance_type="diag", n_iter=1000).fit(X)# Predict the optimal sequence of internal hidden statehidden_states = model.predict(X)print("done")Out:1fitting to HMM and decoding ...donePrint trained parameters and plot1234567891011121314151617181920212223242526print("Transition matrix")print(model.transmat_)print()print("Means and vars of each hidden state")for i in range(model.n_components): print("&#123;0&#125;th hidden state".format(i)) print("mean = ", model.means_[i]) print("var = ", np.diag(model.covars_[i])) print()fig, axs = plt.subplots(model.n_components, sharex=True, sharey=True)colours = cm.rainbow(np.linspace(0, 1, model.n_components))for i, (ax, colour) in enumerate(zip(axs, colours)): # Use fancy indexing to plot data in each state. mask = hidden_states == i ax.plot_date(dates[mask], close_v[mask], ".-", c=colour) ax.set_title("&#123;0&#125;th hidden state".format(i)) # Format the ticks. ax.xaxis.set_major_locator(YearLocator()) ax.xaxis.set_minor_locator(MonthLocator()) ax.grid(True)plt.show()Out:12345678910111213141516171819202122Transition matrix[[ 9.79220773e-01 2.57382344e-15 2.72061945e-03 1.80586073e-02] [ 1.12216188e-12 7.73561269e-01 1.85019044e-01 4.14196869e-02] [ 3.25313504e-03 1.12692615e-01 8.83368021e-01 6.86228435e-04] [ 1.18741799e-01 4.20310643e-01 1.18670597e-18 4.60947557e-01]]Means and vars of each hidden state0th hidden statemean = [ 2.33331888e-02 4.97389989e+07]var = [ 6.97748259e-01 2.49466578e+14]1th hidden statemean = [ 2.12401671e-02 8.81882861e+07]var = [ 1.18665023e-01 5.64418451e+14]2th hidden statemean = [ 7.69658065e-03 5.43135922e+07]var = [ 5.02315562e-02 1.54569357e+14]3th hidden statemean = [ -3.53210673e-01 1.53080943e+08]var = [ 2.55544137e+00 5.88210257e+15]Total running time of the script: ( 0 minutes 2.903 seconds)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The Basic of Hidden Markov Model]]></title>
      <url>%2F2017%2F04%2F30%2FThe-basic-of-Hidden-Markov-Model%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WGAN implemented by PyTorch]]></title>
      <url>%2F2017%2F04%2F29%2FWGAN-implemented-by-PyTorch%2F</url>
      <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#!/usr/bin/env python# Wasserstein Generative Adversarial Networks (WGAN) example in PyTorch.import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variable# Data paramsdata_mean = 4data_stddev = 1.25# Model paramsg_input_size = 1 # Random noise dimension coming into generator, per output vectorg_hidden_size = 50 # Generator complexityg_output_size = 1 # size of generated output vectord_input_size = 100 # Minibatch size - cardinality of distributionsd_hidden_size = 50 # Discriminator complexityd_output_size = 1 # Single dimension for 'real' vs. 'fake'minibatch_size = d_input_sized_learning_rate = 2e-4 # 2e-4g_learning_rate = 2e-4# optim_betas = (0.9, 0.999)num_epochs = 30000print_interval = 200# d_steps = 1 # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generatord_steps = 5g_steps = 1# ### Uncomment only one of these#(name, preprocess, d_input_func) = ("Raw data", lambda data: data, lambda x: x)(name, preprocess, d_input_func) = ("Data and variances", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)print("Using data [%s]" % (name))# ##### DATA: Target data and generator input datadef get_distribution_sampler(mu, sigma): return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n))) # Gaussiandef get_generator_input_sampler(): return lambda m, n: torch.rand(m, n) # Uniform-dist data into generator, _NOT_ Gaussian# ##### MODELS: Generator model and discriminator modelclass Generator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Generator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.sigmoid(self.map2(x)) return self.map3(x)class Discriminator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Discriminator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.elu(self.map2(x)) # return F.sigmoid(self.map3(x)) return self.map3(x)def extract(v): return v.data.storage().tolist()def stats(d): return [np.mean(d), np.std(d)]def decorate_with_diffs(data, exponent): mean = torch.mean(data.data, 1) mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0]) diffs = torch.pow(data - Variable(mean_broadcast), exponent) return torch.cat([data, diffs], 1)d_sampler = get_distribution_sampler(data_mean, data_stddev)gi_sampler = get_generator_input_sampler()G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)# criterion = nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss# d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)# g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)for epoch in range(num_epochs): for d_index in range(d_steps): # 1. Train D on real+fake D.zero_grad() # 1A: Train D on real d_real_data = Variable(d_sampler(d_input_size)) d_real_decision = D(preprocess(d_real_data)) # d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones = true d_real_error = -torch.mean(d_real_decision) d_real_error.backward() # compute/store gradients, but don't change params # 1B: Train D on fake d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) d_fake_data = G(d_gen_input).detach() # detach to avoid training G on these labels d_fake_decision = D(preprocess(d_fake_data.t())) # d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = fake d_fake_error = torch.mean(d_fake_decision) d_fake_error.backward() d_optimizer.step() # Only optimizes D's parameters; changes based on stored gradients from backward() # Weight Clipping for p in D.parameters(): p.data.clamp_(-0.01, 0.01) for g_index in range(g_steps): # 2. Train G on D's response (but DO NOT train D on these labels) G.zero_grad() gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) g_fake_data = G(gen_input) dg_fake_decision = D(preprocess(g_fake_data.t())) # g_error = criterion(dg_fake_decision, Variable(torch.ones(1))) # we want to fool, so pretend it's all genuine g_error = -torch.mean(dg_fake_decision) g_error.backward() g_optimizer.step() # Only optimizes G's parameters if epoch % print_interval == 0: print("%s: D: %s/%s G: %s (Real: %s, Fake: %s) " % (epoch, extract(d_real_error)[0], extract(d_fake_error)[0], extract(g_error)[0], stats(extract(d_real_data)), stats(extract(d_fake_data))))与之前的文章所做的修改仅仅只有以下几点（理论支持参考我之前转发的一篇博文）:判别模型最后一层直接用线型激活函数，而不是用Sigmoid函数123456789101112class Discriminator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Discriminator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.elu(self.map2(x)) # return F.sigmoid(self.map3(x)) return self.map3(x)生成模型与判别模型的loss函数进行修改12345# 生成模型# d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones = trued_real_error = -torch.mean(d_real_decision)# d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = faked_fake_error = torch.mean(d_fake_decision)123# 判别模型# g_error = criterion(dg_fake_decision, Variable(torch.ones(1))) # we want to fool, so pretend it's all genuineg_error = -torch.mean(dg_fake_decision)每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c (这里取的是0.01)123# Weight Clippingfor p in D.parameters(): p.data.clamp_(-0.01, 0.01)不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行1234# d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)# g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)​实验结果如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152ewan@ubuntu:~/Documents/gan/pytorch-generative-adversarial-networks$ python wgan_pytorch.py Using data [Data and variances]0: D: -0.00291868206114/-0.0098686888814 G: 0.0101090818644 (Real: [3.9948547959327696, 1.1746644935894675], Fake: [-0.49681734740734101, 0.012067284766516822]) 200: D: -0.607654631138/0.150195807219 G: -0.148662015796 (Real: [3.8201908415555952, 1.2529761319208725], Fake: [1.3578049659729003, 0.068574913818859801]) 400: D: -0.463035583496/0.187745466828 G: -0.199109002948 (Real: [3.9679448902606964, 1.0966020511088672], Fake: [2.7924281167984009, 0.10128610818888226]) 600: D: -0.195529654622/-0.0762325078249 G: 0.0709114596248 (Real: [4.0289887523651124, 1.130490874393266], Fake: [3.2025665378570558, 0.11113662831727719]) 800: D: -0.267909675837/-0.0125531600788 G: 0.0149036226794 (Real: [3.8386318933963777, 1.1596351907184081], Fake: [2.9168305301666262, 0.18930262941797507]) 1000: D: -0.305421292782/0.0375043526292 G: -0.0430304855108 (Real: [4.036220012307167, 1.2074152140825467], Fake: [2.980299861431122, 0.34328656032877736]) 1200: D: -0.52364641428/0.34957420826 G: -0.336933553219 (Real: [4.2644650164060298, 1.3088487291781874], Fake: [3.5564545428752901, 0.93418534418781807]) 1400: D: 0.0167735591531/-0.0165516249835 G: 0.0153960846364 (Real: [4.005841153860092, 1.2205788960289556], Fake: [3.6258796131610871, 1.3573166859479273]) 1600: D: 0.00350501108915/-0.0680181980133 G: 0.0898797661066 (Real: [4.0096039956808092, 1.3040836884406217], Fake: [4.2868031549453738, 1.1195239069375269]) 1800: D: -0.017161777243/-0.0345846936107 G: 0.00348377227783 (Real: [3.8140131759643556, 1.2696980193364791], Fake: [3.6976867783069611, 1.3915195404268279]) 2000: D: 0.0342473760247/-0.0408688522875 G: 0.042895399034 (Real: [3.8277990472316743, 1.2935257967493754], Fake: [4.0553032100200657, 1.0920039067237071]) 2200: D: -0.0247789677233/-0.0973515734076 G: 0.0561916455626 (Real: [4.0955437314510341, 1.3877739508665123], Fake: [4.2196925377845762, 1.1830430815754616]) 2400: D: 0.0279140714556/-0.0485894307494 G: 0.051317743957 (Real: [4.1299532175064089, 1.2504224526907901], Fake: [3.6290897476673125, 1.4143234578612853]) 2600: D: -0.0277859847993/0.0174758173525 G: -0.0226532723755 (Real: [4.1205433750152585, 1.1041964193630893], Fake: [4.1067905998229977, 1.1112897398730086]) 2800: D: 0.0298485141248/-0.0404594913125 G: 0.0436173528433 (Real: [3.8474615824222567, 1.376119005659207], Fake: [4.1015409564971925, 1.1240560154112995]) 3000: D: -0.00891616754234/-0.0320432707667 G: -0.00200085714459 (Real: [4.2869654643535613, 1.2452766642692439], Fake: [4.0315418589115142, 1.1215360762164166]) 3200: D: 0.125043600798/-0.141845062375 G: 0.180229827762 (Real: [4.1041129958629607, 1.2669502216408666], Fake: [3.9350157177448271, 1.2041076720740758]) 3400: D: 0.00801010616124/-0.0085571501404 G: 0.00837498996407 (Real: [4.1750692510604859, 1.1555020360853467], Fake: [3.7647246885299683, 1.3171958013324914]) 3600: D: -0.0108975172043/0.00422720238566 G: 0.0679717883468 (Real: [4.2474800306558613, 1.1525478772018374], Fake: [3.9568253087997438, 1.2016376545965635]) 3800: D: 0.174184441566/-0.0896890684962 G: 0.132265836 (Real: [3.6444931725133212, 1.4372372290167961], Fake: [4.1011261808872224, 1.2724649929743026]) 4000: D: 0.0152352238074/-0.0211527496576 G: 0.0241769701242 (Real: [4.298748409748077, 1.2334686924805018], Fake: [3.8711180412769317, 1.2375391560481097]) 4200: D: 0.00989393051714/-0.00974932964891 G: 0.00978021323681 (Real: [3.8817882406711579, 1.2274675510251392], Fake: [4.4020989084243771, 1.1135816847780859]) 4400: D: 0.110887765884/-0.195888444781 G: 0.185447320342 (Real: [4.0501037514209743, 1.3391687317184524], Fake: [3.9222843647003174, 1.0870922014501809]) 4600: D: 0.0116609586403/0.0201185699552 G: -0.0251631941646 (Real: [4.097090389728546, 1.190104784646782], Fake: [4.0819661796092985, 1.3105115963188185]) 4800: D: 0.00524073652923/-0.00464708916843 G: 0.0057549579069 (Real: [3.8242294645309447, 1.2650652243397946], Fake: [4.1804288566112522, 1.2938617118884317]) 5000: D: -0.142288714647/0.0809833407402 G: -0.128578931093 (Real: [3.7870366251468659, 1.1074026548781364], Fake: [3.9050006806850432, 1.298625653396472]) 5200: D: 0.00282126059756/-0.000789406709373 G: 0.00220172246918 (Real: [3.8225140625238421, 1.2743034472730719], Fake: [4.1409763026237485, 1.1529764181372026]) 5400: D: 0.0688827335835/-0.143126890063 G: 0.177940413356 (Real: [3.9872682169079781, 1.3030584347635661], Fake: [4.1435868382453922, 1.1051301998899086]) 5600: D: -0.0711650624871/0.0871955379844 G: -0.134067937732 (Real: [3.9407234787940979, 1.1742557675838305], Fake: [4.2017855679988863, 1.2602829191705458]) 5800: D: 0.000587910413742/0.000934307463467 G: 0.00103192776442 (Real: [4.0573597419261933, 1.2623953329979454], Fake: [3.8340791404247283, 1.339685454959999]) 6000: D: 0.00821333751082/-0.12042221427 G: 0.0573511943221 (Real: [4.1211176145076749, 1.2369626300361085], Fake: [3.6600258636474607, 1.3520569881721223]) 6200: D: 0.00682129478082/0.001195830293 G: 0.00338123179972 (Real: [4.0544225633144375, 1.2749644040623289], Fake: [4.1039247584342959, 1.2693975476155579]) 6400: D: -0.00134055688977/0.00293467193842 G: -0.00249383598566 (Real: [4.0987548109889032, 1.4076174670922545], Fake: [3.8387181401252746, 1.0786043697026602]) 6600: D: -0.0879130512476/0.00771049968898 G: 0.0105132861063 (Real: [4.0482780200242994, 1.3183274437573238], Fake: [4.1890638065338131, 1.0659647273618436]) 6800: D: -0.0613053664565/0.00630968250334 G: 0.00345144513994 (Real: [3.9884191691875457, 1.2496578805847449], Fake: [4.0083020174503323, 1.1951200826269044]) 7000: D: -0.00451065413654/0.0126703362912 G: -0.0153036154807 (Real: [4.1685840785503387, 1.0996732796623405], Fake: [3.8199899888038633, 1.3533216043161698]) 7200: D: -0.00164794549346/-0.026672417298 G: 0.00926311034709 (Real: [3.9697488701343535, 1.1614389493998623], Fake: [4.0069102811813355, 1.332521020789126]) 7400: D: 0.0479753166437/-0.00875021051615 G: 0.0273390654474 (Real: [3.9136831092834474, 1.3941734665017038], Fake: [3.9792356503009798, 1.2934269648663987]) 7600: D: 0.0299390181899/-0.0244860406965 G: 0.0235633179545 (Real: [3.9529241484403612, 1.3003400363613378], Fake: [4.1008431494235991, 1.1966721541073959]) 7800: D: -0.106096304953/-0.00319136725739 G: 0.0128062078729 (Real: [3.8472019118070602, 1.3776392180901436], Fake: [3.9847766911983489, 1.1441746730859625]) 8000: D: -0.0541454330087/0.0360651388764 G: -0.0368629023433 (Real: [4.001156520843506, 1.2686070678293795], Fake: [3.7170648825168611, 1.2630303399418346]) 8200: D: 0.0385981723666/-0.0308057032526 G: 0.0258536860347 (Real: [4.0773776215314861, 1.1340129155680212], Fake: [4.025383379459381, 1.327217397616157]) 8400: D: 0.0323679596186/-0.0363558754325 G: 0.0379030331969 (Real: [4.068932784795761, 1.1369141540559231], Fake: [3.9889052593708039, 1.292853623065962]) 8600: D: -0.00726405344903/-0.0198955982924 G: -0.0463897511363 (Real: [4.1387977415323256, 1.2983278993502099], Fake: [3.9634271264076233, 1.2541944672524785]) 8800: D: 0.0214307252318/-0.0323143824935 G: 0.0147992642596 (Real: [3.8878944924473764, 1.2858782523769321], Fake: [3.9738967609405518, 1.2617951400969825]) 9000: D: 0.0408670082688/-0.0408971831203 G: 0.0338222235441 (Real: [3.8935359448194502, 1.2102182389881371], Fake: [4.1026345968246458, 1.1619291320679421]) 9200: D: 0.0334619283676/-0.0487795248628 G: 0.043896459043 (Real: [4.0024692767858507, 1.3035652548917089], Fake: [4.2494437253475192, 1.1284849306040097]) 9400: D: -0.0662252604961/0.0567465648055 G: -0.0975001305342 (Real: [3.9983484780788423, 1.2727864024938771], Fake: [4.1652837800979619, 1.2757452301144367]) 9600: D: -0.0437398403883/0.0547546446323 G: -0.0755473896861 (Real: [3.9568819630146028, 1.2089398910557572], Fake: [4.0577589499950406, 1.254854081501209]) 9800: D: 0.00763822672889/-0.00536214653403 G: 0.00614025257528 (Real: [4.0391950635612011, 1.3067671354062065], Fake: [3.8441065263748171, 1.3304282270617658]) 10000: D: 0.0420219749212/-0.000623900443316 G: 0.0955700650811 (Real: [4.0145307508111001, 1.2332284552616837], Fake: [4.1720886218547824, 1.3184165599194013]) 10200: D: -0.0580518990755/-0.0247586201876 G: 0.0602744668722 (Real: [3.9131186211109164, 1.1547087942243295], Fake: [3.8442363095283509, 1.3100046689992075]) 10400: D: 0.0350324884057/-0.0446610674262 G: 0.0443669557571 (Real: [3.9732863992452621, 1.0900301299537192], Fake: [4.1616083049774169, 1.1977412391369193]) 10600: D: 0.0309124011546/-0.0327286012471 G: 0.0324002951384 (Real: [4.1375643616914752, 1.3491791182650394], Fake: [4.1360740911960603, 1.2026694938475944]) 10800: D: 0.0251356009394/-0.0600365921855 G: 0.0182816889137 (Real: [3.9463955080509185, 1.209152327657528], Fake: [4.0492063975334167, 1.1931266255697688]) 11000: D: -0.0226037632674/0.0645630285144 G: -0.00730620510876 (Real: [4.0881260240077975, 1.1610880829221104], Fake: [4.1015665113925932, 1.2508656591000114]) 11200: D: -0.203874662519/0.129180550575 G: -0.137796327472 (Real: [3.9598375034332274, 1.3812077142172803], Fake: [4.0204527139663693, 1.2581185304639424]) 11400: D: -0.0908113643527/0.0762611478567 G: -0.0800914615393 (Real: [4.0449822235107424, 1.3556268019161497], Fake: [3.6170706963539123, 1.2538775159913775]) 11600: D: 0.0127945197746/-0.0136474575847 G: 0.0115108992904 (Real: [3.8434849847108126, 1.4191038384690144], Fake: [3.6834572017192841, 1.3749317238019667]) 11800: D: -0.0162955205888/0.00703074596822 G: 0.0635928660631 (Real: [4.0656388866901398, 1.1733235519103811], Fake: [4.2119219648838042, 1.2884029757138897]) 12000: D: 0.00804834254086/0.0114726442844 G: -0.0416676998138 (Real: [4.0812106788158413, 1.2768383065648503], Fake: [3.8802548873424532, 1.1682818121544778]) 12200: D: 0.00880087539554/-0.00853784382343 G: 0.00878115184605 (Real: [3.9501210238039492, 1.2609298922930623], Fake: [4.016851776838303, 1.1958214043365074]) 12400: D: -0.0908231809735/0.0565089061856 G: -0.0594271346927 (Real: [4.2189184671640394, 1.2027120432908258], Fake: [4.0232754671573643, 1.0601718488768348]) 12600: D: 0.0851941630244/-0.0584048479795 G: 0.0588090792298 (Real: [3.7772543743252753, 1.130624908263915], Fake: [3.9319257283210756, 1.2051865367836399]) 12800: D: -0.0560997053981/-0.0248175561428 G: -0.0423211455345 (Real: [4.1257915179431439, 1.3557555020469465], Fake: [3.9178791642189026, 1.1446278900771538]) 13000: D: -0.021879715845/-0.0102085536346 G: 0.049164660275 (Real: [3.8891402572393416, 1.340302981622111], Fake: [4.1098264539241791, 1.1973190716986095]) 13200: D: 0.00609071925282/0.000411780551076 G: 0.000873317010701 (Real: [4.0079734873771669, 1.0734378076269375], Fake: [4.16044829249382, 1.24589904041035]) 13400: D: 0.0619652941823/-0.0918542221189 G: 0.0685269758105 (Real: [4.0059312301874161, 1.2294789910478197], Fake: [3.935395474433899, 1.2204450041984987]) 13600: D: -0.0172225553542/0.0116953141987 G: -0.0139160379767 (Real: [3.9669277960062028, 1.2823045137798716], Fake: [3.9422059106826781, 1.1863138013678882]) 13800: D: -0.0343380719423/-0.0341883003712 G: 0.0315745696425 (Real: [3.9349321211874484, 1.3515663905606217], Fake: [4.0361522984504701, 1.1889982801815446]) 14000: D: -0.0781251713634/0.0379043146968 G: -0.0811991766095 (Real: [3.9622140777111055, 1.3270647840200485], Fake: [3.958692445755005, 1.1882249562538854]) 14200: D: -0.00332566350698/0.00831608474255 G: -0.00968919880688 (Real: [4.0868309581279751, 1.2649052154720533], Fake: [3.9996533656120299, 1.2424544463340046]) 14400: D: 0.00310544949025/-0.00344840623438 G: 0.002937767189 (Real: [3.9016156983375549, 1.3394072373207904], Fake: [3.8578492951393129, 1.2802578210924642]) 14600: D: 0.00954662263393/-0.00955961830914 G: 0.00952168926597 (Real: [3.951248247921467, 1.3720542385537113], Fake: [3.9343765902519228, 1.3196731296807518]) 14800: D: -0.118950776756/-0.0234697107226 G: -0.0475859940052 (Real: [4.224924056529999, 1.2198087928062376], Fake: [3.8152624690532684, 1.407979253312801]) 15000: D: -0.0943605676293/0.0735622048378 G: -0.104274556041 (Real: [3.8776874673366546, 1.2303474890793162], Fake: [3.8042025637626646, 1.2641632638711853]) 15200: D: -0.000172574073076/-0.0136091653258 G: -0.0342488661408 (Real: [3.9725669431686401, 1.3636566655582356], Fake: [3.7739255595207215, 1.286560381931142]) 15400: D: 0.0314685925841/-0.0321847423911 G: 0.0224884226918 (Real: [3.9619563330709933, 1.191049295263032], Fake: [3.7949125266075132, 1.144446158701051]) 15600: D: 0.00764724984765/-0.00575984269381 G: 0.0064948592335 (Real: [3.7679578655958177, 1.3149928065248815], Fake: [4.2461013138294224, 1.0951171764483221]) 15800: D: -0.0777092948556/0.0849689692259 G: -0.0924058929086 (Real: [3.932852659225464, 1.2573061632959293], Fake: [4.1913282787799835, 1.2836186853339466]) 16000: D: -0.050300322473/-0.0388206243515 G: 0.0357397347689 (Real: [4.0962446802854542, 1.4029011906591213], Fake: [4.070586755275726, 1.1271350494375147]) 16200: D: 0.0753296241164/-0.0198806431144 G: 0.0808434784412 (Real: [3.8760965394973756, 1.1409524988246751], Fake: [3.8057461333274842, 1.2098168757605468]) 16400: D: -0.0372299104929/0.0351875349879 G: -0.0454745069146 (Real: [4.0939353704452515, 1.2848196043395506], Fake: [3.9558720147609709, 1.2728235384902225]) 16600: D: -0.0101340338588/0.0110626723617 G: -0.0111222248524 (Real: [3.986977145075798, 1.3259823635587689], Fake: [3.9554380464553831, 1.2907862191410846]) 16800: D: -0.0494117587805/0.0523075163364 G: -0.0535500720143 (Real: [3.8448826253414152, 1.3117905469567066], Fake: [3.7438095784187317, 1.2535150365672076]) 17000: D: 0.0156182665378/-0.0128254238516 G: 0.0146374739707 (Real: [3.9421124708652497, 1.1052540236280552], Fake: [3.8871842885017394, 1.2453511923222738]) 17200: D: 0.0429224148393/-0.0480623096228 G: 0.0399292707443 (Real: [3.9799196243286135, 1.2941615666073001], Fake: [4.1375756561756134, 1.2109081564509361]) 17400: D: 0.00968278944492/-0.00968171562999 G: 0.00966327264905 (Real: [3.935849468111992, 1.2695645007229639], Fake: [3.8996728241443632, 1.3144268300578967]) 17600: D: -0.00301436148584/-0.000785265117884 G: 0.00103102996945 (Real: [3.9284519279003143, 1.2341036313393001], Fake: [3.6972431838512421, 1.3855687155856462]) 17800: D: 0.116903491318/-0.0937560945749 G: 0.172590240836 (Real: [4.2645069471001626, 1.3080363040531007], Fake: [3.9567726898193358, 1.2967345311449683]) 18000: D: -0.0608675032854/0.0476493611932 G: -0.00500288326293 (Real: [4.0269851100444791, 1.2116770270672328], Fake: [4.1152600276470181, 1.281199668474674]) 18200: D: -0.0734401643276/0.0987718477845 G: -0.0819599106908 (Real: [3.8394976514577865, 1.2749873300796422], Fake: [4.0419886147975923, 1.327963817546014]) 18400: D: 0.0497582927346/-0.155175164342 G: 0.13303783536 (Real: [3.7719902545213699, 1.0897407967420649], Fake: [3.7615046393871308, 1.3089916470515932]) 18600: D: 0.0239700898528/-0.0381186343729 G: 0.0276864990592 (Real: [4.188409751355648, 1.285584105229516], Fake: [4.0233318042755126, 1.2681527004757882]) 18800: D: 0.00111512281001/-0.0264507420361 G: 0.0286112166941 (Real: [3.9199141567945479, 1.2738313063627613], Fake: [4.1139781177043915, 1.330488711219485]) 19000: D: -0.0473541393876/0.111352369189 G: -0.0523310601711 (Real: [3.7932651308923959, 1.3147127405682739], Fake: [3.7947627007961273, 1.0531299503292175]) 19200: D: -0.0304779503495/0.045797213912 G: -0.0440187454224 (Real: [4.0896886540949344, 1.3392233824907658], Fake: [3.8646358847618103, 1.304593284039177]) 19400: D: 0.194737583399/-0.192367076874 G: 0.230072781444 (Real: [3.9661449289321897, 1.2822216197459986], Fake: [4.0850893747806545, 1.3070266600721223]) 19600: D: -0.195656016469/0.194369539618 G: -0.204969212413 (Real: [3.9445683220028878, 1.2908669424594961], Fake: [4.0273511683940884, 1.3484937484757897]) 19800: D: 0.276149004698/-0.262592494488 G: 0.261271834373 (Real: [3.9244625726342202, 1.2138755313418907], Fake: [3.896045311689377, 1.3239168205792633]) 20000: D: -0.037402831018/0.0541176348925 G: -0.0254273694009 (Real: [3.7887831997871397, 1.0838328443531984], Fake: [4.1803205323219297, 1.2069399210575202]) 20200: D: -0.14391182363/0.154710128903 G: -0.127932995558 (Real: [3.9718186306953429, 1.1938920103826984], Fake: [3.8623993241786958, 1.1992380687067719]) 20400: D: 0.277315825224/-0.276595175266 G: 0.280247867107 (Real: [3.9932824140787124, 1.2951435399231526], Fake: [3.9807376277446749, 1.1784780448683547]) 20600: D: -0.213297829032/0.245908752084 G: -0.243222758174 (Real: [3.8720276713371278, 1.2542419688526467], Fake: [3.8206098222732545, 1.1661960388796837]) 20800: D: 0.114619217813/-0.100926779211 G: 0.0922625884414 (Real: [3.9682870441675187, 1.3188621677189192], Fake: [3.5771069145202636, 1.1369803011602813]) 21000: D: -0.303231596947/0.294602781534 G: -0.288874447346 (Real: [3.991482014656067, 1.0697520343686426], Fake: [3.674229063987732, 1.162594834704991]) 21200: D: -0.074034973979/0.0798109993339 G: -0.0742214098573 (Real: [3.5809044003486634, 1.1568557007313405], Fake: [4.0297869884967801, 1.262183063172349]) 21400: D: 0.262162327766/-0.297971874475 G: 0.296678453684 (Real: [4.0233621561527251, 1.1153293685921177], Fake: [4.3256152606010438, 1.293378983535336]) 21600: D: 0.253285288811/-0.265974611044 G: 0.271079391241 (Real: [3.8655495065450669, 1.3046362904478612], Fake: [4.0383575105667111, 1.1593536714254398]) 21800: D: -0.668483495712/0.693548798561 G: -0.597621560097 (Real: [4.0561192989349362, 1.3785832256993071], Fake: [4.0196917986869813, 1.1727416034901368]) 22000: D: -0.247271433473/0.260498434305 G: -0.254284113646 (Real: [4.0449540507793422, 1.1182831642815363], Fake: [3.9410277414321899, 1.35662918383663]) 22200: D: 0.0106530245394/-0.0105826444924 G: 0.010412142612 (Real: [3.9709725368022917, 1.1935909496194108], Fake: [3.6618342864513398, 1.1302755516153604]) 22400: D: -0.0474079549313/0.0512998178601 G: -0.0483585894108 (Real: [4.0366528975963591, 1.255590190060166], Fake: [4.4536384451389317, 1.1817009846117434]) 22600: D: -0.322408914566/0.294503211975 G: -0.294557034969 (Real: [4.1648625326156612, 1.2910376071493044], Fake: [3.9514351594448089, 1.2428792207747439]) 22800: D: -0.0832418426871/0.0778618454933 G: -0.0830294713378 (Real: [4.1286677682399748, 1.2808552112825371], Fake: [4.0503418278694152, 1.2931609764101457]) 23000: D: -0.369321852922/0.350715816021 G: -0.379378199577 (Real: [4.0539671546220779, 1.2841527209665038], Fake: [3.7385779893398285, 1.226034767157562]) 23200: D: -0.20978730917/0.198253154755 G: -0.20125605166 (Real: [3.8997612628340721, 1.2476609639285596], Fake: [3.9131766259670258, 1.1745094337139723]) 23400: D: -0.0713088735938/0.070287771523 G: -0.0685144215822 (Real: [3.8823761761188509, 1.2554855061572396], Fake: [3.916521146297455, 1.1589148704590277]) 23600: D: 0.0427192002535/-0.0458992123604 G: 0.0468493178487 (Real: [4.2497683775424955, 1.3534774394799314], Fake: [3.7455072367191313, 1.2035723328660535]) 23800: D: 0.0886824280024/-0.089180290699 G: 0.0824339240789 (Real: [4.1368276840448379, 1.3053732424006685], Fake: [3.7440953600406646, 1.3403098424499473]) 24000: D: 0.0765529945493/-0.0702198073268 G: 0.067143753171 (Real: [4.1424573111534118, 1.1894154051554844], Fake: [3.9408028304576872, 1.311870950939225]) 24200: D: -0.0332999974489/0.0289861243218 G: -0.0238233078271 (Real: [4.0625021523237228, 1.3193496247910601], Fake: [4.0214765596389768, 1.3626613178115112]) 24400: D: 0.0116833550856/-0.0433083474636 G: 0.0294151268899 (Real: [4.155729653835297, 1.2443573708805233], Fake: [4.0276014816761014, 1.2064370896635035]) 24600: D: -0.143586605787/0.176585748792 G: -0.18224260211 (Real: [4.1486411762237552, 1.1859516848633762], Fake: [4.1132693731784817, 1.1922180729014844]) 24800: D: -0.0138712525368/0.0168411824852 G: -0.0119427125901 (Real: [4.1591709744930263, 1.2359258557380455], Fake: [4.1677398359775539, 1.3845231707709731]) 25000: D: 0.255919009447/-0.294253230095 G: 0.279962956905 (Real: [3.9463270044326784, 1.1874795319708413], Fake: [4.2903580510616299, 1.3555421660554561]) 25200: D: -0.0276325326413/0.0174208488315 G: -0.0236964281648 (Real: [3.9243721216917038, 1.0837602743237815], Fake: [3.6880193889141082, 1.3551960082382857]) 25400: D: 0.0133695462719/-0.0217840373516 G: 0.0382910817862 (Real: [3.9248281943798067, 1.3498579423514441], Fake: [3.9377611076831815, 1.3147392264391]) 25600: D: 0.0533282607794/-0.0582511797547 G: 0.0426382124424 (Real: [3.9252138528227807, 1.2343049898537437], Fake: [4.1364144349098204, 1.2410536065514364]) 25800: D: -0.00288704037666/0.00770187750459 G: -0.0114914979786 (Real: [3.9242496091127395, 1.2788150012319115], Fake: [4.0345127677917478, 1.1882337663095883]) 26000: D: -0.0608727261424/0.0541118755937 G: -0.0474198237062 (Real: [4.0897465288639072, 1.3095601996023096], Fake: [4.1400825273990627, 1.2148829163174772]) 26200: D: -0.130559697747/0.0733794793487 G: -0.104144588113 (Real: [4.2607862049341199, 1.2942193499055861], Fake: [3.8867506885528567, 1.1942672801186012]) 26400: D: -0.0439343079925/0.0573879256845 G: -0.0878697857261 (Real: [3.7808335113525389, 1.0880880845236942], Fake: [3.9782328522205352, 1.1620106342824015]) 26600: D: 0.0152015341446/0.00366508681327 G: 0.041159953922 (Real: [3.8900859886407853, 1.1779470629112894], Fake: [3.7596992158889773, 1.2139592079531667]) 26800: D: 0.0352714285254/-0.1031877473 G: 0.067874789238 (Real: [4.0695308989286421, 1.1837713697563146], Fake: [4.0929770147800442, 1.0965869589580517]) 27000: D: -0.0881021544337/0.0813493356109 G: -0.0242269244045 (Real: [3.9890777540206908, 1.2553969722414431], Fake: [3.7988330614566803, 1.2567013288504758]) 27200: D: 0.0763045027852/-0.0917293503881 G: 0.114218316972 (Real: [4.0028738850355152, 1.3423566094628674], Fake: [3.9770897746086122, 1.3219552807466088]) 27400: D: 0.0594872310758/-0.0451167076826 G: 0.0368666872382 (Real: [4.0800592017173765, 1.2152901513624952], Fake: [3.9476736617088317, 1.2989705597833583]) 27600: D: 0.0153470486403/-0.0201481245458 G: -0.000402322039008 (Real: [4.1604018148779867, 1.3359014716469342], Fake: [3.9977971708774565, 1.2944576179632961]) 27800: D: -0.00789823569357/0.00908922962844 G: -0.0111076626927 (Real: [4.0212037134170533, 1.1874018724012747], Fake: [4.1083386635780332, 1.2509297017041943]) 28000: D: 0.00757996272296/-0.00654019229114 G: 0.00611820165068 (Real: [3.7911120998859404, 1.1977103659955959], Fake: [4.0841165268421173, 1.1898253993115502]) 28200: D: 0.0131957577541/0.00322831980884 G: -0.00111622922122 (Real: [4.1888789300620557, 1.3496568725947327], Fake: [4.0611115002632143, 1.3183184144220856]) 28400: D: -0.0306499581784/0.0331647247076 G: -0.0338053703308 (Real: [4.1849153059720994, 1.3391440077022734], Fake: [3.8500063753128053, 1.3092803392722017]) 28600: D: -0.0750854164362/0.0745137408376 G: -0.0692436397076 (Real: [4.2219353467226028, 1.3228632865628431], Fake: [3.9156518685817718, 1.322625042830196]) 28800: D: 0.0400990955532/-0.0271217841655 G: 0.0072197439149 (Real: [4.1668396210670471, 1.1685380084057959], Fake: [3.8380984902381896, 1.362370341203504]) 29000: D: -0.0643707811832/0.0576644167304 G: -0.100686855614 (Real: [3.8912058281898498, 1.1764897014192157], Fake: [4.1498241519927976, 1.2432322677870791]) 29200: D: 0.0442187860608/-0.0331076569855 G: 0.0377507209778 (Real: [3.995900819301605, 1.1999502583881319], Fake: [3.9349853229522704, 1.3676764998638458]) 29400: D: -0.0614512637258/0.0583380833268 G: -0.059112302959 (Real: [4.1833238875865932, 1.4038158613161691], Fake: [4.1426575899124147, 1.2694314780433735]) 29600: D: -0.0337703973055/0.0392336845398 G: -0.0504648312926 (Real: [4.1217511665821078, 1.2264251023812502], Fake: [3.838116307258606, 1.2309841481033876]) 29800: D: 0.129453405738/-0.13672092557 G: 0.143395990133 (Real: [3.8660407388210296, 1.2221890139039508], Fake: [4.0156518769264222, 1.3044469158238432])]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The awesome Wasserstein GAN]]></title>
      <url>%2F2017%2F04%2F29%2FThe-awesome-Wasserstein-GAN%2F</url>
      <content type="text"><![CDATA[原帖地址：https://zhuanlan.zhihu.com/p/25071913本文后续：Wasserstein GAN最新进展：从weight clipping到gradient penalty，更加先进的Lipschitz限制手法在GAN的相关研究如火如荼甚至可以说是泛滥的今天，一篇新鲜出炉的arXiv论文《Wasserstein GAN》却在Reddit的Machine Learning频道火了，连Goodfellow都在帖子里和大家热烈讨论，这篇论文究竟有什么了不得的地方呢？要知道自从2014年Ian Goodfellow提出以来，GAN就存在着训练困难、生成器和判别器的loss无法指示训练进程、生成样本缺乏多样性等问题。从那时起，很多论文都在尝试解决，但是效果不尽人意，比如最有名的一个改进DCGAN依靠的是对判别器和生成器的架构进行实验枚举，最终找到一组比较好的网络架构设置，但是实际上是治标不治本，没有彻底解决问题。而今天的主角Wasserstein GAN（下面简称WGAN）成功地做到了以下爆炸性的几点：彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度基本解决了collapse mode的问题，确保了生成样本的多样性训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高（如题图所示）以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到那以上好处来自哪里？这就是令人拍案叫绝的部分了——实际上作者整整花了两篇论文，在第一篇《Towards Principled Methods for Training Generative Adversarial Networks》里面推了一堆公式定理，从理论上分析了原始GAN的问题所在，从而针对性地给出了改进要点；在这第二篇《Wasserstein GAN》里面，又再从这个改进点出发推了一堆公式定理，最终给出了改进的算法实现流程，而改进后相比原始GAN的算法实现流程却只改了四点：判别器最后一层去掉sigmoid生成器和判别器的loss不取log每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行算法截图如下：改动是如此简单，效果却惊人地好，以至于Reddit上不少人在感叹：就这样？没有别的了？ 太简单了吧！这些反应让我想起了一个颇有年头的鸡汤段子，说是一个工程师在电机外壳上用粉笔划了一条线排除了故障，要价一万美元——画一条线，1美元；知道在哪画线，9999美元。上面这四点改进就是作者Martin Arjovsky划的简简单单四条线，对于工程实现便已足够，但是知道在哪划线，背后却是精巧的数学分析，而这也是本文想要整理的内容。本文内容分为五个部分：原始GAN究竟出了什么问题？（此部分较长）WGAN之前的一个过渡解决方案Wasserstein距离的优越性质从Wasserstein距离到WGAN总结理解原文的很多公式定理需要对测度论、 拓扑学等数学知识有所掌握，本文会从直观的角度对每一个重要公式进行解读，有时通过一些低维的例子帮助读者理解数学背后的思想，所以不免会失于严谨，如有引喻不当之处，欢迎在评论中指出。以下简称《Wassertein GAN》为“WGAN本作”，简称《Towards Principled Methods for Training Generative Adversarial Networks》为“WGAN前作”。WGAN源码实现：martinarjovsky/WassersteinGAN第一部分：原始GAN究竟出了什么问题？回顾一下，原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例：$$-\mathbb{E}_{x \sim P_r}[log D(x)] - \mathbb{E}_{x \sim P_{g}}[log(1-D(x))]$$其中$P_r$是真实样本分布，$P_g$是由生成器产生的样本分布。对于生成器，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是$$ \mathbb{E}_{x \sim P_g}[log(1-D(x))]$$$$ \mathbb{E}_{x \sim P_g}[-log D(x)]$$后者在WGAN两篇论文中称为“the - log D alternative”或“the - log D trick”。WGAN前作分别分析了这两种形式的原始GAN各自的问题所在，下面分别说明。第一种原始GAN形式的问题一句话概括：判别器越好，生成器梯度消失越严重。WGAN前作从两个角度进行了论证，第一个角度是从生成器的等价损失函数切入的。首先从公式1可以得到，在生成器G固定参数时最优的判别器D应该是什么。对于一个具体的样本$x$，它可能来自真实分布也可能来自生成分布，它对公式1损失函数的贡献是$$- P_r(x)logD(x) - p_g(x)log[1 - D(x)]$$令其关于$D(x)$的导数为0，得$$-\frac{P_r(x)}{D(x)} + \frac{P_g(x)}{1 - D(x)} = 0$$化简得最优判别器为：$$D^{\star}(x) = \frac{P_r(x)}{P_r(x) + P_g(x)}$$这个结果从直观上很容易理解，就是看一个样本$x$来自真实分布和生成分布的可能性的相对比例。如果$P_r(x) = 0$且$P_g(x) \neq 0$，最优判别器就应该非常自信地给出概率0；如果$P_r(x) = P_g(x)$，说明该样本是真是假的可能性刚好一半一半，此时最优判别器也应该给出概率0.5。然而GAN训练有一个trick，就是别把判别器训练得太好，否则在实验中生成器会完全学不动（loss降不下去），为了探究背后的原因，我们就可以看看在极端情况——判别器最优时，生成器的损失函数变成什么。给公式2加上一个不依赖于生成器的项，使之变成$$\mathbb{E}_{x \sim P_r}[log D(x)] - \mathbb{E}_{x \sim P_{g}}[log(1-D(x))]$$注意，最小化这个损失函数等价于最小化公式2，而且它刚好是判别器损失函数的反。代入最优判别器即公式4，再进行简单的变换可以得到$$\mathbb{E}_{x \sim P_r} \log \frac{P_r(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} + \mathbb{E}_{x \sim P_g} \log \frac{P_g(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} - 2\log 2$$变换成这个样子是为了引入Kullback–Leibler divergence（简称KL散度）和Jensen-Shannon divergence（简称JS散度）这两个重要的相似度衡量指标，后面的主角之一Wasserstein距离，就是要来吊打它们两个的。所以接下来介绍这两个重要的配角——KL散度和JS散度：$$KL(P_1||P_2) = \mathbb{E}_{x \sim P_1} \log \frac{P_1}{P_2}$$$$JS(P_1 || P_2) = \frac{1}{2}KL(P_1||\frac{P_1 + P_2}{2}) + \frac{1}{2}KL(P_2||\frac{P_1 + P_2}{2})$$于是公式5就可以继续写成$$2JS(P_r || P_g) - 2\log 2$$到这里读者可以先喘一口气，看看目前得到了什么结论：根据原始GAN定义的判别器loss，我们可以得到最优判别器的形式；而在最优判别器的下，我们可以把原始GAN定义的生成器loss等价变换为最小化真实分布$P_r$与生成分布$P_g$之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss也就会越近似于最小化$P_r$和$P_g$之间的JS散度。问题就出在这个JS散度上。我们会希望如果两个分布之间越接近它们的JS散度越小，我们通过优化JS散度就能将$P_g$“拉向”$P_r$，最终以假乱真。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分，或者它们重叠的部分可忽略（下面解释什么叫可忽略），它们的JS散度是多少呢？答案是$\log 2$，因为对于任意一个x只有四种可能：$$P_1(x) = 0且P_2(x) = 0$$$$P_1(x) \neq 0且P_2(x) \neq 0$$$$P_1(x) = 0且P_2(x) \neq 0$$$$P_1(x) \neq 且P_2(x) = 0$$第一种对计算JS散度无贡献，第二种情况由于重叠部分可忽略所以贡献也为0，第三种情况对公式7右边第一个项的贡献是$\log \frac{P_2}{\frac{1}{2}(P_2 + 0)} = \log 2$，第四种情况与之类似，所以最终$JS(P_1||P_2) = \log 2$。换句话说，无论$P_r$跟$P_g$是远在天边，还是近在眼前，只要它们俩没有一点重叠或者重叠部分可忽略，JS散度就固定是常数$\log 2$，而这对于梯度下降方法意味着——梯度为0！此时对于最优判别器来说，生成器肯定是得不到一丁点梯度信息的；即使对于接近最优的判别器来说，生成器也有很大机会面临梯度消失的问题。但是$P_r$与$P_g$不重叠或重叠部分可忽略的可能性有多大？不严谨的答案是：非常大。比较严谨的答案是：当$P_r$与$P_g$的支撑集（support）是高维空间中的低维流形（manifold）时，$P_r$与$P_g$重叠部分测度（measure）为0的概率为1。不用被奇怪的术语吓得关掉页面，虽然论文给出的是严格的数学表述，但是直观上其实很容易理解。首先简单介绍一下这几个概念：支撑集（support）其实就是函数的非零部分子集，比如ReLU函数的支撑集就是$(0, +\infty)$，一个概率分布的支撑集就是所有概率密度非零部分的集合。流形（manifold）是高维空间中曲线、曲面概念的拓广，我们可以在低维上直观理解这个概念，比如我们说三维空间中的一个曲面是一个二维流形，因为它的本质维度（intrinsic dimension）只有2，一个点在这个二维流形上移动只有两个方向的自由度。同理，三维空间或者二维空间中的一条曲线都是一个一维流形。测度（measure）是高维空间中长度、面积、体积概念的拓广，可以理解为“超体积”。回过头来看第一句话，“当$P_r$与$P_g$的支撑集是高维空间中的低维流形时”，基本上是成立的。原因是GAN中的生成器一般是从某个低维（比如100维）的随机分布中采样出一个编码向量，再经过一个神经网络生成出一个高维样本（比如64x64的图片就有4096维）。当生成器的参数固定时，生成样本的概率分布虽然是定义在4096维的空间上，但它本身所有可能产生的变化已经被那个100维的随机分布限定了，其本质维度就是100，再考虑到神经网络带来的映射降维，最终可能比100还小，所以生成样本分布的支撑集就在4096维空间中构成一个最多100维的低维流形，“撑不满”整个高维空间。“撑不满”就会导致真实分布与生成分布难以“碰到面”，这很容易在二维空间中理解：一方面，二维平面中随机取两条曲线，它们之间刚好存在重叠线段的概率为0；另一方面，虽然它们很大可能会存在交叉点，但是相比于两条曲线而言，交叉点比曲线低一个维度，长度（测度）为0，可忽略。三维空间中也是类似的，随机取两个曲面，它们之间最多就是比较有可能存在交叉线，但是交叉线比曲面低一个维度，面积（测度）是0，可忽略。从低维空间拓展到高维空间，就有了如下逻辑：因为一开始生成器随机初始化，所以$P_g$几乎不可能与$P_r$有什么关联，所以它们的支撑集之间的重叠部分要么不存在，要么就比$P_r$和$P_g$的最小维度还要低至少一个维度，故而测度为0。所谓“重叠部分测度为0”，就是上文所言“不重叠或者重叠部分可忽略”的意思。我们就得到了WGAN前作中关于生成器梯度消失的第一个论证：在（近似）最优判别器下，最小化生成器的loss等价于最小化$P_r$与$P_g$之间的JS散度，而由于$P_r$与$P_g$几乎不可能有不可忽略的重叠，所以无论它们相距多远JS散度都是常数$\log 2$，最终导致生成器的梯度（近似）为0，梯度消失。接着作者写了很多公式定理从第二个角度进行论证，但是背后的思想也可以直观地解释：首先，$P_r$与$P_g$之间几乎不可能有不可忽略的重叠，所以无论它们之间的“缝隙”多狭小，都肯定存在一个最优分割曲面把它们隔开，最多就是在那些可忽略的重叠处隔不开而已。由于判别器作为一个神经网络可以无限拟合这个分隔曲面，所以存在一个最优判别器，对几乎所有真实样本给出概率1，对几乎所有生成样本给出概率0，而那些隔不开的部分就是难以被最优判别器分类的样本，但是它们的测度为0，可忽略。最优判别器在真实分布和生成分布的支撑集上给出的概率都是常数（1和0），导致生成器的loss梯度为0，梯度消失。有了这些理论分析，原始GAN不稳定的原因就彻底清楚了：判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，四处乱跑。只有判别器训练得不好不坏才行，但是这个火候又很难把握，甚至在同一轮训练的前后不同阶段这个火候都可能不一样，所以GAN才那么难训练。实验辅证如下：WGAN前作Figure 2。先分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器重新随机初始化从头开始训练，对于第一种形式的生成器loss产生的梯度可以打印出其尺度的变化曲线，可以看到随着判别器的训练，生成器的梯度均迅速衰减。注意y轴是对数坐标轴。第二种原始GAN形式的问题一句话概括：最小化第二种生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足。WGAN前作又是从两个角度进行了论证，下面只说第一个角度，因为对于第二个角度我难以找到一个直观的解释方式，感兴趣的读者还是去看论文吧（逃）。如前文所说，Ian Goodfellow提出的“- log D trick”是把生成器loss改成$$\mathbb{E}_{x\sim P_g}[- \log D(x)]$$上文推导已经得到在最优判别器$D^*$下$$\mathbb{E}_{x\sim P_r}[\log D^*(x)]$$我们可以把KL散度（注意下面是先g后r）变换成含的形式：$$\begin{align}KL(P_g || P_r) &amp;= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x)}{P_r(x)}] \\&amp;= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x) / (P_r(x) + P_g(x))}{P_r(x) / (P_r(x) + P_g(x))}] \\&amp;= \mathbb{E}_{x \sim P_g} [\log \frac{1 - D^(x)}{D^(x)}] \\&amp;= \mathbb{E}_{x \sim P_g} \log [1 - D^(x)] - \mathbb{E}_{x \sim P_g} \log D^(x)\end{align} \\$$可得最小化目标的等价变形$$\begin{align}\mathbb{E}_{x \sim P_g} [-\log D^(x)] &amp;= KL(P_g || P_r) - \mathbb{E}_{x \sim P_g} \log [1 - D^(x)] \\&amp;= KL(P_g || P_r) - 2JS(P_r || P_g) + 2\log 2 + \mathbb{E}_{x\sim P_r}[\log D^*(x)]\end{align}$$注意上式最后两项不依赖于生成器G，最终得到最小化公式3等价于最小化$$KL(P_g || P_r) - 2JS(P_r || P_g)$$这个等价最小化目标存在两个严重的问题。第一是它同时要最小化生成分布与真实分布的KL散度，却又要最大化两者的JS散度，一个要拉近，一个却要推远！这在直观上非常荒谬，在数值上则会导致梯度不稳定，这是后面那个JS散度项的毛病。第二，即便是前面那个正常的$KL$散度项也有毛病。因为$KL$散度不是一个对称的衡量，$KL(P_g || P_r)$与$KL(P_r || P_g)$是有差别的。以前者为例当$P_g(x)\rightarrow 0$而$P_r(x)\rightarrow 1$时，$P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow 0$，对$KL(P_g || P_r)$贡献趋近0当$P_g(x)\rightarrow 1$而$P_r(x)\rightarrow 0$时，$P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow +\infty$，对$KL(P_g || P_r)$贡献趋近正无穷换言之，$KL(P_g || P_r)$对于上面两种错误的惩罚是不一样的，第一种错误对应的是“生成器没能生成真实的样本”，惩罚微小；第二种错误对应的是“生成器生成了不真实的样本” ，惩罚巨大。第一种错误对应的是缺乏多样性，第二种错误对应的是缺乏准确性。这一放一打之下，生成器宁可多生成一些重复但是很“安全”的样本，也不愿意去生成多样性的样本，因为那样一不小心就会产生第二种错误，得不偿失。这种现象就是大家常说的collapse mode。第一部分小结：在原始GAN的（近似）最优判别器下，第一种生成器loss面临梯度消失问题，第二种生成器loss面临优化目标荒谬、梯度不稳定、对多样性与准确性惩罚不平衡导致mode collapse这几个问题。实验辅证如下：WGAN前作Figure 3。先分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器重新随机初始化从头开始训练，对于第二种形式的生成器loss产生的梯度可以打印出其尺度的变化曲线，可以看到随着判别器的训练，蓝色和绿色曲线中生成器的梯度迅速增长，说明梯度不稳定，红线对应的是DCGAN相对收敛的状态，梯度才比较稳定。第二部分：WGAN之前的一个过渡解决方案原始GAN问题的根源可以归结为两点，一是等价优化的距离衡量（KL散度、JS散度）不合理，二是生成器随机初始化后的生成分布很难与真实分布有不可忽略的重叠。WGAN前作其实已经针对第二点提出了一个解决方案，就是对生成样本和真实样本加噪声，直观上说，使得原本的两个低维流形“弥散”到整个高维空间，强行让它们产生不可忽略的重叠。而一旦存在重叠，JS散度就能真正发挥作用，此时如果两个分布越靠近，它们“弥散”出来的部分重叠得越多，JS散度也会越小而不会一直是一个常数，于是（在第一种原始GAN形式下）梯度消失的问题就解决了。在训练过程中，我们可以对所加的噪声进行退火（annealing），慢慢减小其方差，到后面两个低维流形“本体”都已经有重叠时，就算把噪声完全拿掉，JS散度也能照样发挥作用，继续产生有意义的梯度把两个低维流形拉近，直到它们接近完全重合。以上是对原文的直观解释。在这个解决方案下我们可以放心地把判别器训练到接近最优，不必担心梯度消失的问题。而当判别器最优时，对公式9取反可得判别器的最小loss为其中$P_{r+\epsilon}$和$P_{g+\epsilon}$分别是加噪后的真实分布与生成分布。反过来说，从最优判别器的loss可以反推出当前两个加噪分布的JS散度。两个加噪分布的JS散度可以在某种程度上代表两个原本分布的距离，也就是说可以通过最优判别器的loss反映训练进程！……真的有这样的好事吗？并没有，因为加噪JS散度的具体数值受到噪声的方差影响，随着噪声的退火，前后的数值就没法比较了，所以它不能成为$P_r$和$P_g$距离的本质性衡量。因为本文的重点是WGAN本身，所以WGAN前作的加噪方案简单介绍到这里，感兴趣的读者可以阅读原文了解更多细节。加噪方案是针对原始GAN问题的第二点根源提出的，解决了训练不稳定的问题，不需要小心平衡判别器训练的火候，可以放心地把判别器训练到接近最优，但是仍然没能够提供一个衡量训练进程的数值指标。但是WGAN本作就从第一点根源出发，用Wasserstein距离代替JS散度，同时完成了稳定训练和进程指标的问题！作者未对此方案进行实验验证。第三部分：Wasserstein距离的优越性质Wasserstein距离又叫Earth-Mover（EM）距离，定义如下：$$ W(P_r, P_g) = \inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||$$解释如下：$ \Pi (P_r, P_g)$ 是 $P_r$ 和 $P_g$ 组合起来的所有可能的联合分布的集合，反过来说，$\Pi (P_r, P_g)$ 中每一个分布的边缘分布都是 $P_r$ 和 $P_g$ 。对于每一个可能的联合分布 $\gamma$ 而言，可以从中采样 $(x, y) \sim \gamma$ 得到一个真实样本 $x$ 和一个生成样本 $y$ ，并算出这对样本的距离 $||x-y||$ ，所以可以计算该联合分布 $\gamma$ 下样本对距离的期望值 $\mathbb{E}_{(x, y) \sim \gamma} [||x - y||$ 。在所有可能的联合分布中能够对这个期望值取到的下界 $\inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||$ ，就定义为Wasserstein距离。直观上可以把$\mathbb{E}_{(x, y) \sim \gamma} [||x - y||]$理解为在$\gamma$这个“路径规划”下把$P_r$这堆“沙土”挪到$P_g$“位置”所需的“消耗”，而$W(P_r, P_g)$就是“最优路径规划”下的“最小消耗”，所以才叫Earth-Mover（推土机）距离。Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。WGAN本作通过简单的例子展示了这一点。考虑如下二维空间中的两个分布$P_1$和$P_2$，$P_1$在线段AB上均匀分布，$P_2$在线段CD上均匀分布，通过控制参数$\theta$可以控制着两个分布的距离远近。此时容易得到（读者可自行验证）$$KL(P_1 || P_2) = KL(P_1 || P_2) =\begin{cases}+\infty &amp; \text{if $\theta \neq 0$} \\0 &amp; \text{if $\theta = 0$}\end{cases}$$$$JS(P_1||P_2)=\begin{cases}\log 2 &amp; \text{if $\theta \neq 0$} \\0 &amp; \text{if $\theta - 0$}\end{cases}$$$$W(P_0, P_1) = |\theta|$$KL散度和JS散度是突变的，要么最大要么最小，Wasserstein距离却是平滑的，如果我们要用梯度下降法优化$\theta$这个参数，前两者根本提供不了梯度，Wasserstein距离却可以。类似地，在高维空间中如果两个分布不重叠或者重叠部分可忽略，则KL和JS既反映不了远近，也提供不了梯度，但是Wasserstein却可以提供有意义的梯度。第四部分：从Wasserstein距离到WGAN既然Wasserstein距离有如此优越的性质，如果我们能够把它定义为生成器的loss，不就可以产生有意义的梯度来更新生成器，使得生成分布被拉向真实分布吗？没那么简单，因为Wasserstein距离定义中的$\inf_{\gamma \sim \Pi (P_r, P_g)}$没法直接求解，不过没关系，作者用了一个已有的定理把它变换为如下形式$$W(P_r, P_g) = \frac{1}{K} \sup_{||f||_L \leq K} \mathbb{E}_{x \sim P_r} [f(x)$$证明过程被作者丢到论文附录中了，我们也姑且不管，先看看上式究竟说了什么。首先需要介绍一个概念——Lipschitz连续。它其实就是在一个连续函数$f$上面额外施加了一个限制，要求存在一个常数$K\geq 0$使得定义域内的任意两个元素$x_1$和$x_2$都满足此时称函数$f$的Lipschitz常数为$K$。简单理解，比如说$f$的定义域是实数集合，那上面的要求就等价于$f$的导函数绝对值不超过$K$。再比如说$\log (x)$就不是Lipschitz连续，因为它的导函数没有上界。Lipschitz连续条件限制了一个连续函数的最大局部变动幅度。公式13的意思就是在要求函数$f$的Lipschitz常数$||f||_L$不超过$K$的条件下，对所有可能满足条件的$f$取到$\mathbb{E}_{x \sim P_r} [f(x)]$的上界，然后再除以$K$。特别地，我们可以用一组参数$w$来定义一系列可能的函数$f_w$，此时求解公式13可以近似变成求解如下形式$$K \cdot W(P_r, P_g) \approx \max_{w: |f_w|_L \leq K} \mathbb{E}_{x \sim P_r} [f_w(x)$$再用上我们搞深度学习的人最熟悉的那一套，不就可以把$f$用一个带参数$w$的神经网络来表示嘛！由于神经网络的拟合能力足够强大，我们有理由相信，这样定义出来的一系列$f_w$虽然无法囊括所有可能，但是也足以高度近似公式13要求的那个$sup_{||f||_L \leq K} $了。最后，还不能忘了满足 $||f_w||_L \leq K$ 这个限制。我们其实不关心具体的K是多少，只要它不是正无穷就行，因为它只是会使得梯度变大K倍，并不会影响梯度的方向。所以作者采取了一个非常简单的做法，就是限制神经网络$f_\theta$的所有参数$w_i$的不超过某个范围$[-c, c]$，比如$w_i \in [- 0.01, 0.01]$，此时关于输入样本x的导数$\frac{\partial f_w}{\partial x}$也不会超过某个范围，所以一定存在某个不知道的常数K使得$f_w$的局部变动幅度不会超过它，Lipschitz连续条件得以满足。具体在算法实现中，只需要每次更新完$w$后把它clip回这个范围就可以了。到此为止，我们可以构造一个含参数$w$、最后一层不是非线性激活层的判别器网络$f_w$，在限制$w$不超过某个范围的条件下，使得$$L = \mathbb{E}_{x \sim P_r} [f_w(x)$$尽可能取到最大，此时L就会近似真实分布与生成分布之间的Wasserstein距离（忽略常数倍数K）。注意原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器$f_w$做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。接下来生成器要近似地最小化Wasserstein距离，可以最小化$L$，由于Wasserstein距离的优良性质，我们不需要担心生成器梯度消失的问题。再考虑到$L$的第一项与生成器无关，就得到了WGAN的两个loss。$$\mathbb{E}_{x \sim P_g} [f_w(x)]$$（WGAN生成器loss函数）$$\mathbb{E}_{x \sim P_g} [f_w(x)$$（WGAN判别器loss函数）可以指示训练进程，其数值越小，表示真实分布与生成分布的Wasserstein距离越小，GAN训练得越好。WGAN完整的算法流程已经贴过了，为了方便读者此处再贴一遍：上文说过，WGAN与原始GAN第一种形式相比，只改了四点：判别器最后一层去掉sigmoid生成器和判别器的loss不取log每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行前三点都是从理论分析中得到的，已经介绍完毕；第四点却是作者从实验中发现的，属于trick，相对比较“玄”。作者发现如果使用Adam，判别器的loss有时候会崩掉，当它崩掉时，Adam给出的更新方向与梯度方向夹角的cos值就变成负数，更新方向与梯度方向南辕北辙，这意味着判别器的loss梯度是不稳定的，所以不适合用Adam这类基于动量的优化算法。作者改用RMSProp之后，问题就解决了，因为RMSProp适合梯度不稳定的情况。对WGAN作者做了不少实验验证，本文只提比较重要的三点。第一，判别器所近似的Wasserstein距离与生成器的生成图片质量高度相关，如下所示（此即题图）：第二，WGAN如果用类似DCGAN架构，生成图片的效果与DCGAN差不多：但是厉害的地方在于WGAN不用DCGAN各种特殊的架构设计也能做到不错的效果，比如如果大家一起拿掉Batch Normalization的话， DCGAN就崩了：如果WGAN和原始GAN都使用多层全连接网络（MLP），不用CNN，WGAN质量会变差些，但是原始GAN不仅质量变得更差，而且还出现了collapse mode，即多样性不足：第三，在所有WGAN的实验中未观察到collapse mode，作者也只说应该是解决了，最后补充一点论文没提到，但是我个人觉得比较微妙的问题。判别器所近似的Wasserstein距离能够用来指示单次训练中的训练进程，这个没错；接着作者又说它可以用于比较多次训练进程，指引调参，我倒是觉得需要小心些。比如说我下次训练时改了判别器的层数、节点数等超参，判别器的拟合能力就必然有所波动，再比如说我下次训练时改了生成器两次迭代之间，判别器的迭代次数，这两种常见的变动都会使得Wasserstein距离的拟合误差就与上次不一样。那么这个拟合误差的变动究竟有多大，或者说不同的人做实验时判别器的拟合能力或迭代次数相差实在太大，那它们之间还能不能直接比较上述指标，我都是存疑的。评论区的知友@Minjie Xu 进一步指出，相比于判别器迭代次数的改变，对判别器架构超参的改变会直接影响到对应的Lipschitz常数，进而改变近似Wasserstein距离的倍数，前后两轮训练的指标就肯定不能比较了，这是需要在实际应用中注意的。对此我想到了一个工程化的解决方式，不是很优雅：取同样一对生成分布和真实分布，让前后两个不同架构的判别器各自拟合到收敛，看收敛到的指标差多少倍，可以近似认为是后面的相对前面的变化倍数，于是就可以用这个变化倍数校正前后两轮训练的指标。第五部分：总结WGAN前作分析了Ian Goodfellow提出的原始GAN两种形式各自的问题，第一种形式等价在最优判别器下等价于最小化生成分布与真实分布之间的JS散度，由于随机生成分布很难与真实分布有不可忽略的重叠以及JS散度的突变特性，使得生成器面临梯度消失的问题；第二种形式在最优判别器下等价于既要最小化生成分布与真实分布直接的KL散度，又要最大化其JS散度，相互矛盾，导致梯度不稳定，而且KL散度的不对称性使得生成器宁可丧失多样性也不愿丧失准确性，导致collapse mode现象。WGAN前作针对分布重叠问题提出了一个过渡解决方案，通过对生成样本和真实样本加噪声使得两个分布产生重叠，理论上可以解决训练不稳定的问题，可以放心训练判别器到接近最优，但是未能提供一个指示训练进程的可靠指标，也未做实验验证。WGAN本作引入了Wasserstein距离，由于它相对KL散度与JS散度具有优越的平滑特性，理论上可以解决梯度消失问题。接着通过数学变换将Wasserstein距离写成可求解的形式，利用一个参数数值范围受限的判别器神经网络来最大化这个形式，就可以近似Wasserstein距离。在此近似最优判别器下优化生成器使得Wasserstein距离缩小，就能有效拉近生成分布与真实分布。WGAN既解决了训练不稳定的问题，也提供了一个可靠的训练进程指标，而且该指标确实与生成样本的质量高度相关。作者对WGAN进行了实验验证。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)]]></title>
      <url>%2F2017%2F04%2F28%2FGenerative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch%2F</url>
      <content type="text"><![CDATA[Source BlogPyTorch Install: http://pytorch.org/The models play two distinct (literally, adversarial) roles. Given some real data set R, G is the generator, trying to create fake data that looks just like the genuine data, while D is the discriminator, getting data from either the real set or G and labeling the difference. Goodfellow’s metaphor (and a fine one it is) was that G was like a team of forgers trying to match real paintings with their output, while D was the team of detectives trying to tell the difference. (Except that in this case, the forgers G never get to see the original data — only the judgments of D. They’re like blind forgers.)In the ideal case, both D and G would get better over time until G had essentially become a “master forger” of the genuine article and D was at a loss, “unable to differentiate between the two distributions.”In practice, what Goodfellow had shown was that G would be able to perform a form of unsupervised learning on the original dataset, finding some way of representing that data in a (possibly) much lower-dimensional manner. And as Yann LeCun famously stated, unsupervised learning is the “cake” of true AI.This powerful technique seems like it must require a metric ton of code just to get started, right? Nope. Using PyTorch, we can actually create a very simple GAN in under 50 lines of code. There are really only 5 components to think about:R: The original, genuine data setI: The random noise that goes into the generator as a source of entropyG: The generator which tries to copy/mimic the original data setD: The discriminator which tries to tell apart G’s output from RThe actual ‘training’ loop where we teach G to trick D and D to beware G.1.) R: In our case, we’ll start with the simplest possible R — a bell curve. This function takes a mean and a standard deviation and returns a function which provides the right shape of sample data from a Gaussian with those parameters. In our sample code, we’ll use a mean of 4.0 and a standard deviation of 1.25.2.) I: The input into the generator is also random, but to make our job a little bit harder, let’s use a uniform distribution rather than a normal one. This means that our model G can’t simply shift/scale the input to copy R, but has to reshape the data in a non-linear way.3.) G: The generator is a standard feedforward graph — two hidden layers, three linear maps. We’re using an ELU (exponential linear unit) becausethey’re the new black, yo. G is going to get the uniformly distributed data samples from I and somehow mimic the normally distributed samples from R.4.) D: The discriminator code is very similar to G’s generator code; a feedforward graph with two hidden layers and three linear maps. It’s going to get samples from either R or G and will output a single scalar between 0 and 1, interpreted as ‘fake’ vs. ‘real’. This is about as milquetoast as a neural net can get.5.) Finally, the training loop alternates between two modes: first training D on real data vs. fake data, with accurate labels (think of this as Police Academy); and then training G to fool D, with inaccurate labels (this is more like those preparation montages from Ocean’s Eleven). It’s a fight between good and evil, people.Even if you haven’t seen PyTorch before, you can probably tell what’s going on. In the first (green) section, we push both types of data through D and apply a differentiable criterion to D’s guesses vs. the actual labels. That pushing is the ‘forward’ step; we then call ‘backward()’ explicitly in order to calculate gradients, which are then used to update D’s parameters in the d_optimizer step() call. G is used but isn’t trained here.Then in the last (red) section, we do the same thing for G — note that we also run G’s output through D (we’re essentially giving the forger a detective to practice on) but we do not optimize or change D at this step. We don’t want the detective D to learn the wrong labels. Hence, we only call g_optimizer.step().And…that’s all. There’s some other boilerplate code but the GAN-specific stuff is just those 5 components, nothing else.After a few thousand rounds of this forbidden dance between D and G, what do we get? The discriminator D gets good very quickly (while G slowly moves up), but once it gets to a certain level of power, G has a worthy adversary and begins to improve. Really improve.Over 20,000 training rounds, the mean of G’s output overshoots 4.0 but then comes back in a fairly stable, correct range (left). Likewise, the standard deviation initially drops in the wrong direction but then rises up to the desired 1.25 range (right), matching R.Ok, so the basic stats match R, eventually. How about the higher moments? Does the shape of the distribution look right? After all, you could certainly have a uniform distribution with a mean of 4.0 and a standard deviation of 1.25, but that wouldn’t really match R. Let’s show the final distribution emitted by G.Not bad. The left tail is a bit longer than the right, but the skew and kurtosis are, shall we say, evocative of the original Gaussian.G recovers the original distribution R nearly perfectly — and D is left cowering in the corner, mumbling to itself, unable to tell fact from fiction. This is precisely the behavior we want (see Figure 1 in Goodfellow). From fewer than 50 lines of code.Goodfellow would go on to publish many other papers on GANs, including a 2016 gem describing some practical improvements, including the minibatch discrimination method adapted here. And here’s a 2-hour tutorial he presented at NIPS 2016. For TensorFlow users, here’s a parallel post from Aylien on GANs.Ok. Enough talk. Go look at the code.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#!/usr/bin/env python# Generative Adversarial Networks (GAN) example in PyTorch.# See related blog post at https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f#.sch4xgsa9import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variable# Data paramsdata_mean = 4data_stddev = 1.25# Model paramsg_input_size = 1 # Random noise dimension coming into generator, per output vectorg_hidden_size = 50 # Generator complexityg_output_size = 1 # size of generated output vectord_input_size = 100 # Minibatch size - cardinality of distributionsd_hidden_size = 50 # Discriminator complexityd_output_size = 1 # Single dimension for 'real' vs. 'fake'minibatch_size = d_input_sized_learning_rate = 2e-4 # 2e-4g_learning_rate = 2e-4optim_betas = (0.9, 0.999)num_epochs = 30000print_interval = 200d_steps = 1 # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generatorg_steps = 1# ### Uncomment only one of these#(name, preprocess, d_input_func) = ("Raw data", lambda data: data, lambda x: x)(name, preprocess, d_input_func) = ("Data and variances", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)print("Using data [%s]" % (name))# ##### DATA: Target data and generator input datadef get_distribution_sampler(mu, sigma): return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n))) # Gaussiandef get_generator_input_sampler(): return lambda m, n: torch.rand(m, n) # Uniform-dist data into generator, _NOT_ Gaussian# ##### MODELS: Generator model and discriminator modelclass Generator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Generator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.sigmoid(self.map2(x)) return self.map3(x)class Discriminator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Discriminator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.elu(self.map2(x)) return F.sigmoid(self.map3(x))def extract(v): return v.data.storage().tolist()def stats(d): return [np.mean(d), np.std(d)]def decorate_with_diffs(data, exponent): mean = torch.mean(data.data, 1) mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0]) diffs = torch.pow(data - Variable(mean_broadcast), exponent) return torch.cat([data, diffs], 1)d_sampler = get_distribution_sampler(data_mean, data_stddev)gi_sampler = get_generator_input_sampler()G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)criterion = nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bcelossd_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)for epoch in range(num_epochs): for d_index in range(d_steps): # 1. Train D on real+fake D.zero_grad() # 1A: Train D on real d_real_data = Variable(d_sampler(d_input_size)) d_real_decision = D(preprocess(d_real_data)) d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones = true d_real_error.backward() # compute/store gradients, but don't change params # 1B: Train D on fake d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) d_fake_data = G(d_gen_input).detach() # detach to avoid training G on these labels d_fake_decision = D(preprocess(d_fake_data.t())) d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = fake d_fake_error.backward() d_optimizer.step() # Only optimizes D's parameters; changes based on stored gradients from backward() for g_index in range(g_steps): # 2. Train G on D's response (but DO NOT train D on these labels) G.zero_grad() gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) g_fake_data = G(gen_input) dg_fake_decision = D(preprocess(g_fake_data.t())) g_error = criterion(dg_fake_decision, Variable(torch.ones(1))) # we want to fool, so pretend it's all genuine g_error.backward() g_optimizer.step() # Only optimizes G's parameters if epoch % print_interval == 0: print("%s: D: %s/%s G: %s (Real: %s, Fake: %s) " % (epoch, extract(d_real_error)[0], extract(d_fake_error)[0], extract(g_error)[0], stats(extract(d_real_data)), stats(extract(d_fake_data))))Result：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152ewan@ubuntu:~/Documents/gan/pytorch-generative-adversarial-networks$ python gan_pytorch.py Using data [Data and variances]0: D: 0.636019647121/0.687892377377 G: 0.692580163479 (Real: [4.0121619534492492, 1.3228379995364423], Fake: [0.36497069358825684, 0.0040907625909989871]) 200: D: 2.92067015835e-05/0.474851727486 G: 1.00973010063 (Real: [4.0935744738578794, 1.3016500752040552], Fake: [-0.5716635638475418, 0.019948046232028654]) 400: D: 0.0014917049557/0.502498149872 G: 0.943185687065 (Real: [4.198446000814438, 1.1262929992527102], Fake: [-0.21786054879426955, 0.0067362612730766476]) 600: D: 6.4969262894e-06/0.384293109179 G: 1.15257537365 (Real: [3.8602226501703263, 1.3292726136430937], Fake: [-0.29857088595628739, 0.03924369275813562]) 800: D: 1.84774467016e-06/0.211148008704 G: 1.67116880417 (Real: [4.0269100540876392, 1.2954351206409835], Fake: [-0.32296697288751602, 0.14901211840131676]) 1000: D: 9.02455067262e-05/0.0219078511 G: 4.19585323334 (Real: [3.9491306754946707, 1.3613105655283608], Fake: [0.13110455054789782, 0.5252103421913964]) 1200: D: 0.00441630883142/0.137605398893 G: 2.78980493546 (Real: [4.238747425079346, 1.1837142728845262], Fake: [2.3851456820964811, 0.69947230698573948]) 1400: D: 0.291683584452/0.824121117592 G: 0.26126781106 (Real: [3.8486315739154815, 1.2074486225815622], Fake: [3.4868409335613251, 1.2438192602257458]) 1600: D: 0.503275632858/1.08712184429 G: 0.628099560738 (Real: [3.7856648898124696, 1.1925325100947208], Fake: [3.9149187129735945, 1.5374543372663099]) 1800: D: 0.992162883282/0.955306172371 G: 0.215137541294 (Real: [3.9097139459848402, 1.3729001379532129], Fake: [4.9751595187187192, 1.2850838287273094]) 2000: D: 0.701098382473/0.634775817394 G: 0.389043629169 (Real: [3.9641699814796447, 1.1512756986625183], Fake: [5.0374661159515384, 1.5190411587235346]) 2200: D: 0.510353624821/0.350295126438 G: 1.5988701582 (Real: [4.0406568145751951, 1.3612318676859239], Fake: [5.4763065743446351, 1.2736378899688456]) 2400: D: 0.895085930824/0.400622785091 G: 0.922062814236 (Real: [3.8292097043991089, 1.1506111704583193], Fake: [4.5642045128345492, 1.7082890861364539]) 2600: D: 0.802581310272/0.717123866081 G: 0.572393655777 (Real: [4.0654918360710148, 1.2552944260604222], Fake: [5.1286249160766602, 1.0479449058428656]) 2800: D: 0.51098883152/0.489002883434 G: 0.842381119728 (Real: [4.0405197954177856, 1.136660175398452], Fake: [3.9549839448928834, 1.1751749984899784]) 3000: D: 0.496278882027/0.97537201643 G: 0.753688693047 (Real: [4.0026307255029678, 1.2446167315972034], Fake: [3.2340782660245897, 1.2949288892421307]) 3200: D: 0.696556508541/0.829834342003 G: 0.475445389748 (Real: [3.9983750417828561, 1.2828095340103229], Fake: [3.5434492731094362, 0.98673911467128028]) 3400: D: 0.479906737804/0.477254271507 G: 1.2421528101 (Real: [4.1585888534784319, 1.2672863214247221], Fake: [3.3173918831348419, 1.156708995162234]) 3600: D: 1.36562228203/0.508370876312 G: 0.550418972969 (Real: [4.0406067597866056, 1.1363201759386616], Fake: [4.4300824308395388, 1.0639278538481793]) 3800: D: 0.538426816463/0.622343420982 G: 0.786149024963 (Real: [4.0097330248355867, 1.1609232820569348], Fake: [4.5179304122924808, 1.2347411732817635]) 4000: D: 0.350504934788/0.361344873905 G: 0.728424191475 (Real: [3.7975878280401232, 1.2378775025626094], Fake: [4.3484812033176423, 1.4327683271077338]) 4200: D: 0.912463009357/0.779066801071 G: 0.840294659138 (Real: [3.9861780107021332, 1.2293009498211762], Fake: [4.0718169224262235, 1.2044778720046834]) 4400: D: 0.814347147942/0.794115483761 G: 0.889387726784 (Real: [3.9556436133384705, 1.1131208050960595], Fake: [3.6148070895671847, 1.1790021094109027]) 4600: D: 0.637132883072/0.639598190784 G: 0.835896074772 (Real: [4.0807307386398319, 1.1590112689981971], Fake: [3.6376679444313051, 1.2540016088688517]) 4800: D: 0.816388785839/0.629823803902 G: 0.6337043643 (Real: [4.1595975148677828, 1.2996693029809485], Fake: [4.0303308999538423, 1.3050560562935769]) 5000: D: 1.38226401806/0.714248239994 G: 1.17240273952 (Real: [3.9217003214359285, 1.3408209709046912], Fake: [4.4204820060729979, 1.0378887480226417]) 5200: D: 0.752707779408/0.432243227959 G: 0.735915839672 (Real: [4.033863249272108, 1.417255801501303], Fake: [3.7434970003366472, 1.4305561672741818]) 5400: D: 0.672449588776/0.694190680981 G: 0.671269893646 (Real: [3.9849637061357499, 1.3054745436415693], Fake: [3.7987613070011137, 1.1584021967574571]) 5600: D: 0.633513212204/0.678804934025 G: 0.736048042774 (Real: [3.8742538380622862, 1.1924929483627851], Fake: [4.0905960440635685, 1.0496450658176097]) 5800: D: 0.954816102982/0.619474828243 G: 0.847522497177 (Real: [4.0848416697978971, 1.2377045321962332], Fake: [4.5059887909889218, 1.0769809353783582]) 6000: D: 0.634225904942/0.653471052647 G: 0.402414888144 (Real: [3.9909452509880068, 1.2152347623325401], Fake: [3.9412865948677065, 1.2808620107297906]) 6200: D: 0.733776032925/0.414616316557 G: 0.969770550728 (Real: [4.0096452310681343, 1.2858629342885464], Fake: [3.4776910370588303, 1.4216167469252254]) 6400: D: 0.483776688576/0.456314682961 G: 0.42595911026 (Real: [4.16927042722702, 1.2557057135387499], Fake: [3.905275868177414, 1.3509040440658031]) 6600: D: 1.06177055836/0.443961560726 G: 0.910483181477 (Real: [4.0327691116929056, 1.1752792712434861], Fake: [4.1322225379943847, 1.3041032842304898]) 6800: D: 0.911615252495/0.851063728333 G: 0.822307884693 (Real: [4.0429812586307525, 1.0149434426406105], Fake: [4.181604235172272, 1.1091966315801844]) 7000: D: 0.859644412994/0.819373309612 G: 0.683367550373 (Real: [4.0413902151584624, 1.2697299173474621], Fake: [3.6461249232292174, 1.1392232969008105]) 7200: D: 0.697537004948/1.29639554024 G: 0.567749083042 (Real: [3.9289280462265013, 1.1476723124689931], Fake: [4.3612218284606934, 1.1698644305174593]) 7400: D: 0.892510712147/0.93148213625 G: 1.18729686737 (Real: [3.9838603484630584, 1.10640478112829], Fake: [4.1228645443916321, 1.2695625804586594]) 7600: D: 0.855136275291/0.683420717716 G: 0.87994658947 (Real: [4.1161885654926298, 1.1923004904972447], Fake: [3.6958885985612868, 1.3379389180110717]) 7800: D: 0.549697399139/1.37823116779 G: 0.398991644382 (Real: [4.2173074555397037, 1.2371073094023581], Fake: [3.8741448554396629, 1.3837623378110455]) 8000: D: 1.35398185253/0.410179078579 G: 0.527717351913 (Real: [3.9588229835033415, 1.3744496473744439], Fake: [3.9429207968711855, 1.3684983506717674]) 8200: D: 0.700774013996/0.295857429504 G: 0.803082704544 (Real: [3.8515358114242555, 1.2566173136350174], Fake: [3.7108538401126863, 1.3342916614304938]) 8400: D: 0.689352571964/0.590398311615 G: 0.698961615562 (Real: [3.965521250963211, 1.2231963456729893], Fake: [4.6866454958915709, 1.1286615282559416]) 8600: D: 0.19632807374/0.604559898376 G: 0.812706291676 (Real: [3.8928249645233155, 1.3264703109197318], Fake: [3.918080286383629, 1.2016505045193488]) 8800: D: 0.595732450485/0.572122216225 G: 0.738678693771 (Real: [3.7554583859443667, 1.2011572644775179], Fake: [3.8252914756536485, 1.1905187885079342]) 9000: D: 0.232542961836/1.26930451393 G: 0.834500789642 (Real: [3.9203160056471824, 1.2725988502730134], Fake: [4.1613124001026156, 1.2681795442466237]) 9200: D: 1.257376194/0.5735257864 G: 0.554405272007 (Real: [3.8860677522420883, 1.1041807259307903], Fake: [3.9102136331796644, 1.3811967247690093]) 9400: D: 0.610212028027/0.538761377335 G: 0.558459818363 (Real: [4.0015355503559116, 0.99711450973270277], Fake: [3.8555663478374482, 1.1037480705144518]) 9600: D: 0.702151358128/0.81621837616 G: 0.706716835499 (Real: [4.0513852632045744, 1.1984303669025829], Fake: [4.2933621263504032, 1.1478353305254103]) 9800: D: 0.511451423168/0.670217812061 G: 0.873916983604 (Real: [3.935146123766899, 1.3218541944694313], Fake: [4.2863738107681275, 1.1362357473661524]) 10000: D: 0.587130308151/0.764386773109 G: 0.714644312859 (Real: [4.0829932641983033, 1.1844677307174318], Fake: [4.2149634605646131, 1.1542778585504672]) 10200: D: 0.454408079386/0.390097141266 G: 0.694087386131 (Real: [3.9480907583236693, 1.2586832917742197], Fake: [3.9525690937042235, 1.3555640918653922]) 10400: D: 0.232991695404/0.377689123154 G: 0.839949011803 (Real: [3.9636431083083155, 1.2146210496905581], Fake: [4.0022356742620468, 1.0348462356745984]) 10600: D: 0.887756228447/0.452646583319 G: 0.776298880577 (Real: [4.1107078218460087, 1.3061081296488184], Fake: [4.3001403945684435, 1.3191353715419794]) 10800: D: 0.988030552864/0.472889751196 G: 2.00703763962 (Real: [4.1303015506267551, 1.2646447231333668], Fake: [4.2425211107730867, 1.2706986066792705]) 11000: D: 0.962553679943/1.00584948063 G: 0.458068579435 (Real: [4.1017441129684444, 1.1564779436003478], Fake: [3.861787896156311, 1.2478181443952361]) 11200: D: 0.404395908117/0.560545325279 G: 0.764987766743 (Real: [3.8819530367851258, 1.1290593525971337], Fake: [4.0393019503355028, 1.1760851438968263]) 11400: D: 1.04482722282/0.170368790627 G: 0.979512214661 (Real: [4.0775347077846531, 1.1743573984958275], Fake: [4.4076948529481887, 1.1430737801156545]) 11600: D: 0.767144262791/0.419019073248 G: 0.804197788239 (Real: [4.1507718646526337, 1.2935215526943189], Fake: [4.2565110635757444, 1.1195747875890809]) 11800: D: 0.328228145838/0.192100420594 G: 0.694948136806 (Real: [4.2615561389923098, 1.3187283101366121], Fake: [3.7841238260269163, 1.2796545407667934]) 12000: D: 0.939581632614/0.512252509594 G: 0.486280798912 (Real: [4.1770594882965089, 1.2492834466325793], Fake: [4.0997331076860428, 1.0701209918243111]) 12200: D: 0.964525461197/0.397465586662 G: 1.45534229279 (Real: [3.9129967219382524, 1.3473476671217695], Fake: [4.3561846733093263, 1.1667221650406194]) 12400: D: 0.516430974007/0.255626231432 G: 0.753806650639 (Real: [3.9942912605404852, 1.3623400447216258], Fake: [4.2171517282724382, 1.2046534326031684]) 12600: D: 0.050210531801/0.567070662975 G: 0.887824892998 (Real: [3.9560802054405211, 1.3569670682588555], Fake: [3.6434229278564452, 1.2798963544271591]) 12800: D: 0.566556215286/1.45121753216 G: 2.67591071129 (Real: [4.0868541407585148, 1.1440918337515926], Fake: [3.7308121472597122, 1.2567484994327229]) 13000: D: 0.285438686609/1.26493763924 G: 0.714931368828 (Real: [4.0406689298152925, 1.2295255598171184], Fake: [4.1976348906755447, 1.2778464434389283]) 13200: D: 0.420082330704/0.20268279314 G: 1.13221895695 (Real: [4.0006502330303189, 1.1790149224725006], Fake: [4.2336275362968445, 1.2803975596845565]) 13400: D: 0.219869300723/0.733704686165 G: 1.4634616375 (Real: [3.8348834168910981, 1.240605849665303], Fake: [3.8208065938949587, 1.3042463825727604]) 13600: D: 1.35286784172/0.161317944527 G: 2.29795908928 (Real: [4.0841373348236081, 1.2295542819596996], Fake: [4.0513113558292391, 1.2789595441318489]) 13800: D: 0.188396275043/0.38589566946 G: 1.38826131821 (Real: [4.0228236329555509, 1.3524482715610078], Fake: [4.2307587480545044, 1.2042737228043698]) 14000: D: 0.0101562952623/0.363918542862 G: 1.24292945862 (Real: [4.0695835274457934, 1.4484548400603423], Fake: [4.3588982570171355, 1.2305509242343933]) 14200: D: 0.308517187834/0.687216579914 G: 0.831201374531 (Real: [4.1314239382743834, 1.2039768851618762], Fake: [4.3469831347465515, 1.1622408025070994]) 14400: D: 1.05658388138/0.777651846409 G: 0.713593065739 (Real: [3.9307258637249469, 1.3932677098843045], Fake: [3.8781710839271546, 1.3920662615905985]) 14600: D: 0.428974717855/0.430344074965 G: 0.865560889244 (Real: [4.2443156433105464, 1.4786604488020483], Fake: [3.9386759352684022, 1.2173706417721266]) 14800: D: 0.358524769545/0.631785154343 G: 1.72760403156 (Real: [4.0897545439004901, 1.3611061267905207], Fake: [4.0185626268386843, 1.2011546705663261]) 15000: D: 0.451200634241/0.451773911715 G: 1.10325527191 (Real: [3.9933083570003509, 1.0881706638388742], Fake: [3.902902855873108, 1.1771562868487595]) 15200: D: 0.756480932236/0.419855684042 G: 0.942300021648 (Real: [4.1753564620018002, 1.3629881946025171], Fake: [3.8721090507507325, 1.189488508024922]) 15400: D: 0.219109147787/0.190036550164 G: 2.20304942131 (Real: [3.9836783826351168, 1.4838718408508595], Fake: [3.9491609585285188, 1.1700151592543104]) 15600: D: 1.01965582371/0.519556045532 G: 1.10594069958 (Real: [4.1213941669464109, 1.2398676800048194], Fake: [4.1908504700660707, 1.1195751576139747]) 15800: D: 0.733263611794/0.697221815586 G: 0.84056687355 (Real: [4.0593542096018789, 1.1946663317303297], Fake: [4.3031868946552274, 1.0306412415157991]) 16000: D: 0.400649875402/0.377974271774 G: 1.2899967432 (Real: [4.0140545344352718, 1.2630515897106358], Fake: [4.1656066524982451, 1.1779954377184654]) 16200: D: 0.34089872241/0.265896707773 G: 1.11251270771 (Real: [4.0408088731765748, 1.3839176416694203], Fake: [4.0593357777595518, 1.2213436233279213]) 16400: D: 0.00472234329209/0.513436615467 G: 1.63225841522 (Real: [4.1417997646331788, 1.2449733327544124], Fake: [3.7269023895263671, 1.1296458384504016]) 16600: D: 0.756382524967/0.66779255867 G: 0.536718785763 (Real: [3.9379871004819869, 1.278594816781579], Fake: [3.8750299978256226, 1.2829775944385431]) 16800: D: 0.879319548607/0.169020995498 G: 2.33787298203 (Real: [4.2075482982397077, 1.3725696551173026], Fake: [3.6744112837314606, 1.3225226221432227]) 17000: D: 0.0482731573284/1.43823099136 G: 1.15067052841 (Real: [4.0404629743099214, 1.218948521692204], Fake: [4.0387165582180025, 1.2794767516999943]) 17200: D: 2.88490628009e-05/0.57872825861 G: 0.495411038399 (Real: [3.9901529085636138, 1.4349120434336065], Fake: [4.0573103535175328, 1.1918079188127153]) 17400: D: 0.231002807617/1.2511702776 G: 1.33606302738 (Real: [3.7472488379478452, 1.1658634335870959], Fake: [3.9354779303073881, 1.2931455406139682]) 17600: D: 0.181431129575/0.149175107479 G: 2.51311731339 (Real: [4.1270963573455814, 1.312367798822683], Fake: [4.3470913958549495, 1.1818067904116243]) 17800: D: 0.830040276051/0.415931969881 G: 1.57710897923 (Real: [3.99146986246109, 1.0836663745208763], Fake: [4.3325731372833252, 1.266683405420135]) 18000: D: 0.20047518611/0.460676729679 G: 2.56421780586 (Real: [4.3388666504621503, 1.3881540592894346], Fake: [3.9820314025878907, 1.0436684747098013]) 18200: D: 0.0659740716219/0.428199917078 G: 0.931035280228 (Real: [3.8892200005054476, 1.2217018988161374], Fake: [3.8822696304321287, 1.304586899060783]) 18400: D: 0.791511416435/0.56503880024 G: 1.98549497128 (Real: [3.7894453473389147, 1.3567878969348022], Fake: [4.0909739780426024, 1.2361544714927677]) 18600: D: 1.15297484398/0.102882102132 G: 1.85704553127 (Real: [4.2316720616817474, 1.2603607958456993], Fake: [3.7415710711479186, 1.311454258421634]) 18800: D: 1.06078708172/0.366641134024 G: 0.914008259773 (Real: [3.9394708669185636, 1.2924449902046702], Fake: [3.9466111737489702, 1.137776845711856]) 19000: D: 0.374139517546/0.448283135891 G: 0.701639294624 (Real: [3.9492650532722475, 1.2348435624999976], Fake: [3.7365686148405075, 1.215777672310739]) 19200: D: 0.209440857172/0.522395193577 G: 0.707223057747 (Real: [3.8846979635953902, 1.2146658434075039], Fake: [4.1696245861053463, 1.2979841463522084]) 19400: D: 0.15654887259/0.133351936936 G: 1.43907415867 (Real: [4.0292040088772776, 1.2291287794070285], Fake: [3.8498308193683624, 1.1121767482065514]) 19600: D: 0.329566717148/0.222448319197 G: 0.429250627756 (Real: [3.7978928279876709, 1.1554982239517226], Fake: [3.5122534275054931, 1.2462801759237472]) 19800: D: 0.0176634714007/0.480926275253 G: 0.39424943924 (Real: [4.0822606313228604, 1.2484518469881001], Fake: [4.5482089626789097, 1.1266585202489452]) 20000: D: 0.45860773325/0.517112135887 G: 0.957448124886 (Real: [4.0875282829999922, 1.2310698313795749], Fake: [4.2767848205566406, 1.1186856033319335]) 20200: D: 1.71172118187/0.240745082498 G: 0.314642876387 (Real: [3.8525538909435273, 1.2094100771830765], Fake: [3.6543397814035417, 1.2917598911679764]) 20400: D: 0.583434104919/0.703361749649 G: 1.45571947098 (Real: [4.0388400733470915, 1.2267253073862441], Fake: [3.9019298100471498, 1.0292402192122965]) 20600: D: 0.176266431808/0.55411952734 G: 0.962469100952 (Real: [4.0694609802961352, 1.2276659305759301], Fake: [3.9728190612792971, 1.1212652107309595]) 20800: D: 1.17427504063/0.212535098195 G: 0.505771696568 (Real: [3.7983859290182589, 1.3565768879920506], Fake: [4.0766829651594163, 1.1742807548541911]) 21000: D: 0.247546881437/0.242251947522 G: 2.533826828 (Real: [4.048124186992645, 1.2074367711533176], Fake: [3.8443934541940687, 1.0964556009967605]) 21200: D: 0.000996549613774/1.77280521393 G: 0.741032421589 (Real: [3.8826335191726686, 1.3432952882949609], Fake: [4.0052364200353621, 1.0658632049377181]) 21400: D: 0.0162861924618/0.202122434974 G: 0.640827775002 (Real: [3.949158318042755, 1.2312223613675215], Fake: [3.9677765011787414, 1.1984950273079937]) 21600: D: 0.494586825371/0.368914216757 G: 1.73299539089 (Real: [4.2141097390651705, 1.3170628249721785], Fake: [3.9259325069189073, 1.2402090610341174]) 21800: D: 1.72856020927/0.280478566885 G: 0.301942139864 (Real: [3.9425574642419816, 1.3421295277895979], Fake: [4.1370714265108113, 1.3135434962232824]) 22000: D: 0.316263616085/0.425417006016 G: 4.6092467308 (Real: [3.9253722500801085, 1.1573266813219236], Fake: [3.7590440094470976, 1.2176312271677099]) 22200: D: 1.70313096046/0.166758075356 G: 1.76803898811 (Real: [4.1788750314712528, 1.3796412025948377], Fake: [4.4896411395072935, 0.88890948354147137]) 22400: D: 0.00245383195579/0.618139982224 G: 0.561835348606 (Real: [4.0531666296720505, 1.3030890495946361], Fake: [3.9800510057806968, 1.2769573713555427]) 22600: D: 0.0456999950111/0.270536243916 G: 0.719259619713 (Real: [3.8036734467744826, 1.2489490089903446], Fake: [4.2525720745325089, 1.3061806069103183]) 22800: D: 0.0318684391677/0.34651991725 G: 1.3301807642 (Real: [4.0768313544988635, 1.2930152979365797], Fake: [4.4993063497543337, 1.2277717696258752]) 23000: D: 1.38112533092/0.656377196312 G: 0.700986683369 (Real: [4.0261077487468722, 1.1634786009859657], Fake: [4.1274698692560197, 1.1909195549188023]) 23200: D: 0.7532761693/0.30048418045 G: 1.24321329594 (Real: [4.0255234652757643, 1.2277433432951119], Fake: [4.0463824319839476, 1.2493841122917879]) 23400: D: 1.54497790337/0.524266302586 G: 1.88104653358 (Real: [4.1244187545776363, 1.2126284333800423], Fake: [4.0199511092901226, 1.4125067136876193]) 23600: D: 0.838026106358/1.1139113903 G: 2.2735543251 (Real: [4.0352903008460999, 1.1687086536829701], Fake: [4.5685070466995237, 1.4508884769834012]) 23800: D: 0.869914472103/0.160864800215 G: 1.42444908619 (Real: [4.1635012495517731, 1.1441051019240691], Fake: [4.1520407730340958, 1.2022442680490875]) 24000: D: 0.0401677601039/0.240127012134 G: 1.21359109879 (Real: [4.0558859372138976, 1.1263029268841764], Fake: [3.8535136532783509, 0.99055012605544335]) 24200: D: 0.444084912539/0.761975646019 G: 1.18176090717 (Real: [4.1462872040271757, 1.1670976588949802], Fake: [4.0291124176979061, 1.4000525541431663]) 24400: D: 0.259448975325/0.206390738487 G: 0.850725114346 (Real: [4.2600694203376772, 1.3260391555100224], Fake: [4.7161277580261229, 1.3763624799621637]) 24600: D: 0.821855664253/0.381440609694 G: 0.898442983627 (Real: [3.9929001557826997, 1.316718033939094], Fake: [3.659836998283863, 1.033547623133473]) 24800: D: 0.869792580605/0.143853545189 G: 1.68244981766 (Real: [3.9503055346012115, 1.1980136516743376], Fake: [4.3753550618886949, 1.4268488751378543]) 25000: D: 0.533834278584/0.944993913174 G: 1.35653877258 (Real: [3.8403973925113677, 1.1415226099240794], Fake: [4.3022644245624546, 1.277824404897737]) 25200: D: 0.57686984539/1.21011674404 G: 0.49785476923 (Real: [4.1094828593730925, 1.0606124114518727], Fake: [3.8350191235542299, 1.1822398134788241]) 25400: D: 1.30570268631/0.127069279552 G: 2.14658904076 (Real: [3.8440176880359651, 1.2759016439053388], Fake: [4.2303895175457003, 1.2478330871411345]) 25600: D: 0.163877904415/0.356351107359 G: 1.50513041019 (Real: [3.9149920016527178, 1.3322359586431274], Fake: [4.5107577931880947, 1.37733363996175]) 25800: D: 0.0257995054126/0.501479804516 G: 0.846267580986 (Real: [4.0328698861598973, 1.0891363228332751], Fake: [4.2062628841400143, 1.2707193105443095]) 26000: D: 0.4208984375/0.45090213418 G: 1.24405300617 (Real: [4.0495267909765245, 1.3629959211491509], Fake: [3.881335927248001, 1.1534035700479874]) 26200: D: 1.0977101326/0.260044932365 G: 0.274282753468 (Real: [4.0526520502567287, 1.1354404896569923], Fake: [3.7989616423845289, 1.3036229409468019]) 26400: D: 0.836492598057/0.194570705295 G: 1.25769793987 (Real: [4.2580243301391603, 1.1229754918621602], Fake: [4.9420129108428954, 1.4595622988211396]) 26600: D: 0.0381172671914/0.229116663337 G: 3.23367476463 (Real: [3.9871047949790954, 1.2891811878363044], Fake: [5.5130027627944944, 1.3531596753079107]) 26800: D: 0.33750808239/0.0588937625289 G: 2.76632380486 (Real: [4.0901136839389798, 1.2240984948711151], Fake: [5.9970619964599612, 1.3296608494175821]) 27000: D: 0.403919011354/0.025144957006 G: 5.00026988983 (Real: [3.9684947764873506, 1.1928812330565042], Fake: [5.5821900677680967, 1.5869340992569609]) 27200: D: 1.26118826866/1.14945113659 G: 0.233536079526 (Real: [4.0953157800436024, 1.2000917970554563], Fake: [3.457775202393532, 1.2362199991432059]) 27400: D: 0.842516124249/0.577941656113 G: 0.518706798553 (Real: [3.8673747038841246, 1.1826108239366226], Fake: [3.6999527400732042, 1.2050256827670227]) 27600: D: 0.459548681974/0.516558885574 G: 1.69328427315 (Real: [4.0379843235015871, 1.267741160236167], Fake: [4.3069088852405546, 1.2883256614455194]) 27800: D: 0.757292568684/0.295852422714 G: 0.82683211565 (Real: [3.6750951480865477, 1.1881818498282759], Fake: [4.3079475378990173, 1.3863961893145142]) 28000: D: 1.0311729908/0.836829304695 G: 0.54562240839 (Real: [3.8109287106990815, 1.2699445078581264], Fake: [4.0800623488426204, 1.2420579399013889]) 28200: D: 0.662180066109/0.698618113995 G: 0.430238395929 (Real: [3.8820258617401122, 1.3192879801078357], Fake: [3.8678512275218964, 1.2100339116659864]) 28400: D: 0.857332766056/0.637849986553 G: 0.443328052759 (Real: [4.0044168281555175, 1.2977773729964786], Fake: [3.77621297955513, 1.10884790779666]) 28600: D: 0.518617451191/0.676390469074 G: 0.824631929398 (Real: [3.9321113193035124, 1.189980080467403], Fake: [4.1412628889083862, 1.4110153520360829]) 28800: D: 0.924657285213/0.57682287693 G: 0.867313206196 (Real: [3.8806186806410552, 1.2663798129949515], Fake: [3.7928846073150635, 0.96599856269415929]) 29000: D: 0.681347727776/0.833830595016 G: 0.880895376205 (Real: [4.0122552135586735, 1.3382642859979685], Fake: [3.8699622356891634, 1.5246898233773196]) 29200: D: 0.690975308418/0.571468651295 G: 0.539677977562 (Real: [3.9422134029865266, 1.2798402813873653], Fake: [3.4796924066543578, 1.0078584415562459]) 29400: D: 0.600927650928/0.692537486553 G: 0.785535871983 (Real: [4.0494313037395475, 1.2729051468200046], Fake: [4.0457676327228542, 1.2121629628604733]) 29600: D: 0.662378668785/0.552553355694 G: 0.665563106537 (Real: [3.8692034566402436, 1.1988600586203602], Fake: [4.3626180648803707, 1.3098951956607312]) 29800: D: 0.844242811203/0.719559967518 G: 0.89226102829 (Real: [3.8751950478553772, 1.1053984789259368], Fake: [3.9671442759037019, 1.1584875699071935])]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LSTM by Example using Tensorflow (Text Generate)]]></title>
      <url>%2F2017%2F04%2F26%2FLSTM-by-Example-using-Tensorflow-Text-Generate%2F</url>
      <content type="text"><![CDATA[In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term Memory (LSTM) because it is robust against the problems of long-term dependency.What seems to be lacking is a good documentation and example on how to build an easy to understand Tensorflow application based on LSTM. This is the motivation behind this article.Suppose we want to train a LSTM to predict the next word using a sample short story, Aesop’s Fables:long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .If we feed a LSTM with correct sequences from the text of 3 symbols as inputs and 1 labeled symbol, eventually the neural network will learn to predict the next symbol correctly.Technically, LSTM inputs can only understand real numbers. A way to convert symbol to number is to assign a unique integer to each symbol based on frequency of occurrence. For example, there are 112 unique symbols in the text above. The function in Listing 2 builds a dictionary with the following entries [ “,” : 0 ][ “the” : 1 ], …, [ “council” : 37 ],…,[ “spoke” : 111 ]. The reverse dictionary is also generated since it will be used in decoding the output of LSTM.1234567def build_dataset(words): count = collections.Counter(words).most_common() dictionary = dict() for word, _ in count: dictionary[word] = len(dictionary) reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return dictionary, reverse_dictionarySimilarly, the prediction is a unique integer identifying the index in the reverse dictionary of the predicted symbol. For example, if the prediction is 37, the predicted symbol is actually “council”.The generation of output may sound simple but actually LSTM produces a 112-element vector of probabilities of prediction for the next symbol normalized by the softmax() function. The index of the element with the highest probability is the predicted index of the symbol in the reverse dictionary (ie a one-hot vector).There is the source code:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180'''A Recurrent Neural Network (LSTM) implementation example using TensorFlow..Next word prediction after n_input words learned from text file.A story is automatically generated if the predicted word is fed back as input.Author: Rowel AtienzaProject: https://github.com/roatienza/Deep-Learning-Experiments'''from __future__ import print_functionimport numpy as npimport tensorflow as tffrom tensorflow.contrib import rnnimport randomimport collectionsimport timestart_time = time.time()def elapsed(sec): if sec&lt;60: return str(sec) + " sec" elif sec&lt;(60*60): return str(sec/60) + " min" else: return str(sec/(60*60)) + " hr"# Target log pathlogs_path = '/tmp/tensorflow/rnn_words'writer = tf.summary.FileWriter(logs_path)# Text file containing words for trainingtraining_file = 'belling_the_cat.txt'def read_data(fname): with open(fname) as f: content = f.readlines() content = [x.strip() for x in content] content = [content[i].split() for i in range(len(content))] content = np.array(content) content = np.reshape(content, [-1, ]) return contenttraining_data = read_data(training_file)print("Loaded training data...")def build_dataset(words): count = collections.Counter(words).most_common() dictionary = dict() for word, _ in count: dictionary[word] = len(dictionary) reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return dictionary, reverse_dictionarydictionary, reverse_dictionary = build_dataset(training_data)vocab_size = len(dictionary)# Parameterslearning_rate = 0.001training_iters = 50000display_step = 1000n_input = 3# number of units in RNN celln_hidden = 512# tf Graph inputx = tf.placeholder("float", [None, n_input, 1])y = tf.placeholder("float", [None, vocab_size])# RNN output node weights and biasesweights = &#123; 'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))&#125;biases = &#123; 'out': tf.Variable(tf.random_normal([vocab_size]))&#125;def RNN(x, weights, biases): # reshape to [1, n_input] x = tf.reshape(x, [-1, n_input]) # Generate a n_input-element sequence of inputs # (eg. [had] [a] [general] -&gt; [20] [6] [33]) x = tf.split(x,n_input,1) # 2-layer LSTM, each layer has n_hidden units. # Average Accuracy= 95.20% at 50k iter rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)]) # 1-layer LSTM with n_hidden units but with lower accuracy. # Average Accuracy= 90.60% 50k iter # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above # rnn_cell = rnn.BasicLSTMCell(n_hidden) # generate prediction outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32) # there are n_input outputs but # we only want the last output return tf.matmul(outputs[-1], weights['out']) + biases['out']pred = RNN(x, weights, biases)# Loss and optimizercost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)# Model evaluationcorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))# Initializing the variablesinit = tf.global_variables_initializer()# Launch the graphwith tf.Session() as session: session.run(init) step = 0 offset = random.randint(0,n_input+1) end_offset = n_input + 1 acc_total = 0 loss_total = 0 writer.add_graph(session.graph) while step &lt; training_iters: # Generate a minibatch. Add some randomness on selection process. if offset &gt; (len(training_data)-end_offset): offset = random.randint(0, n_input+1) symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ] symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1]) symbols_out_onehot = np.zeros([vocab_size], dtype=float) symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0 symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1]) _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \ feed_dict=&#123;x: symbols_in_keys, y: symbols_out_onehot&#125;) loss_total += loss acc_total += acc if (step+1) % display_step == 0: print("Iter= " + str(step+1) + ", Average Loss= " + \ "&#123;:.6f&#125;".format(loss_total/display_step) + ", Average Accuracy= " + \ "&#123;:.2f&#125;%".format(100*acc_total/display_step)) acc_total = 0 loss_total = 0 symbols_in = [training_data[i] for i in range(offset, offset + n_input)] symbols_out = training_data[offset + n_input] symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())] print("%s - [%s] vs [%s]" % (symbols_in,symbols_out,symbols_out_pred)) step += 1 offset += (n_input+1) print("Optimization Finished!") print("Elapsed time: ", elapsed(time.time() - start_time)) print("Run on command line.") print("\ttensorboard --logdir=%s" % (logs_path)) print("Point your web browser to: http://localhost:6006/") while True: prompt = "%s words: " % n_input sentence = input(prompt) sentence = sentence.strip() words = sentence.split(' ') if len(words) != n_input: continue try: symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))] for i in range(32): keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1]) onehot_pred = session.run(pred, feed_dict=&#123;x: keys&#125;) onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval()) sentence = "%s %s" % (sentence,reverse_dictionary[onehot_pred_index]) symbols_in_keys = symbols_in_keys[1:] symbols_in_keys.append(onehot_pred_index) print(sentence) except: print("Word not in dictionary")source blog: https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Xiaomi mini wifi cannot build the connection]]></title>
      <url>%2F2017%2F04%2F26%2FXiaomi-mini-wifi-cannot-build-the-connection%2F</url>
      <content type="text"><![CDATA[针对Win10不能正常使用的问题进入安装目录进入drivers文件夹进入Win81x64文件夹找到netr28ux.inf文件，右键安装之]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Movie Recommendation with MLlib]]></title>
      <url>%2F2017%2F04%2F26%2FMovie-Recommendation-with-MLlib%2F</url>
      <content type="text"><![CDATA[Spark Summit 2014https://databricks-training.s3.amazonaws.com/index.htmlwe will use MLlib to make personalized movie recommendations tailored for you. We will work with 10 million ratings from 72,000 users on 10,000 movies, collected by MovieLens. This dataset is pre-loaded in your USB drive under data/movielens/large. For quick testing of your code, you may want to use a smaller dataset under data/movielens/medium, which contains 1 million ratings from 6000 users on 4000 movies.DataSetWe will use two files from this MovieLens dataset: “ratings.dat” and “movies.dat”. All ratings are contained in the file “ratings.dat” and are in the following format:1UserID::MovieID::Rating::TimestampMovie information is in the file “movies.dat” and is in the following format:1MovieID::Title::GenresCollaborative filteringCollaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix, in our case, the user-movie rating matrix. MLlib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. In particular, we implement the alternating least squares (ALS) algorithm to learn these latent factors.Create training exampleshttps://github.com/ewanlee/spark-trainingTo make recommendation for you, we are going to learn your taste by asking you to rate a few movies. We have selected a small set of movies that have received the most ratings from users in the MovieLens dataset. You can rate those movies by running bin/rateMovies:1python bin/rateMoviesWhen you run the script, you should see prompt similar to the following:12Please rate the following movie (1-5 (best), or 0 if not seen):Toy Story (1995):After you’re done rating the movies, we save your ratings in personalRatings.txt in the MovieLens format, where a special user id 0 is assigned to you.rateMovies allows you to re-rate the movies if you’d like to see how your ratings affect your recommendations.If you don’t have python installed, please copy personalRatings.txt.template to personalRatings.txt and replace ?s with your ratings.SetupWe will be using a standalone project template for this exercise.In the training USB drive, this has been setup in1machine-learning/python/You should find the following items in the directory:MovieLensALS.py: Main Python program that you are going to edit, compile and runsolution: Directory containing the solution codeMovieLensALS.py should look as follows:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/env pythonimport sysimport itertoolsfrom math import sqrtfrom operator import addfrom os.path import join, isfile, dirnamefrom pyspark import SparkConf, SparkContextfrom pyspark.mllib.recommendation import ALSdef parseRating(line): """ Parses a rating record in MovieLens format userId::movieId::rating::timestamp . """ # ...def parseMovie(line): """ Parses a movie record in MovieLens format movieId::movieTitle . """ # ...def loadRatings(ratingsFile): """ Load ratings from file. """ # ...def computeRmse(model, data, n): """ Compute RMSE (Root Mean Squared Error). """ # ...if __name__ == "__main__": if (len(sys.argv) != 3): print "Usage: [usb root directory]/spark/bin/spark-submit --driver-memory 2g " + \ "MovieLensALS.py movieLensDataDir personalRatingsFile" sys.exit(1) # set up environment conf = SparkConf() \ .setAppName("MovieLensALS") \ .set("spark.executor.memory", "2g") sc = SparkContext(conf=conf) # load personal ratings myRatings = loadRatings(sys.argv[2]) myRatingsRDD = sc.parallelize(myRatings, 1) # load ratings and movie titles movieLensHomeDir = sys.argv[1] # ratings is an RDD of (last digit of timestamp, (userId, movieId, rating)) ratings = sc.textFile(join(movieLensHomeDir, "ratings.dat")).map(parseRating) # movies is an RDD of (movieId, movieTitle) movies = dict(sc.textFile(join(movieLensHomeDir, "movies.dat")).map(parseMovie).collect()) # your code here # clean up sc.stop()Let’s first take a closer look at our template code in a text editor, then we’ll start adding code to the template. Locate theMovieLensALS class and open it with a text editor.12usb/$ cd machine-learning/pythonvim MovieLensALS.py # Or your editor of choiceFor any Spark computation, we first create a SparkConf object and use it to create a SparkContext object. Since we will be using spark-submit to execute the programs in this tutorial (more on spark-submit in the next section), we only need to configure the executor memory allocation and give the program a name, e.g. “MovieLensALS”, to identify it in Spark’s web UI. In local mode, the web UI can be access at localhost:4040 during the execution of a program.This is what it looks like in our template code:1234conf = SparkConf() \ .setAppName("MovieLensALS") \ .set("spark.executor.memory", "2g") sc = SparkContext(conf=conf)Next, the code uses the SparkContext to read in ratings. Recall that the rating file is a text file with “::” as the delimiter. The code parses each line to create a RDD for ratings that contains (Int, Rating) pairs. We only keep the last digit of the timestamp as a random key. The Rating class is a wrapper around the tuple (user: Int, product: Int, rating: Double).1234movieLensHomeDir = sys.argv[1]# ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))ratings = sc.textFile(join(movieLensHomeDir, "ratings.dat")).map(parseRating)Next, the code read in movie ids and titles, collect them into a movie id to title map.12345def parseMovie(line): fields = line.split("::") return int(fields[0]), fields[1] movies = dict(sc.textFile(join(movieLensHomeDir, "movies.dat")).map(parseMovie).collect())Now, let’s make our first edit to add code to get a summary of the ratings.12345numRatings = ratings.count()numUsers = ratings.values().map(lambda r: r[0]).distinct().count()numMovies = ratings.values().map(lambda r: r[1]).distinct().count()print "Got %d ratings from %d users on %d movies." % (numRatings, numUsers, numMovies)Running the programBefore we compute movie recommendations, here is a quick reminder on how you can run the program at any point during this exercise. As mentioned above, we will use spark-submit to execute your program in local mode for this tutorial.Starting with Spark 1.0, spark-submit is the recommended way for running Spark applications, both on clusters and locally in standalone mode.1234usb/$ cd machine-learning/python# change the folder name from "medium" to "large" to run on the large data set[usb root directory]/spark/bin/spark-submit MovieLensALS.py [usb root directory]/data/movielens/medium/ ../personalRatings.txtYou should see output similar to the following on your screen:1Got 1000209 ratings from 6040 users on 3706 movies.Splitting training dataWe will use MLlib’s ALS to train a MatrixFactorizationModel, which takes a RDD[Rating] object as input in Scala and RDD[(user, product, rating)] in Python. ALS has training parameters such as rank for matrix factors and regularization constants. To determine a good combination of the training parameters, we split the data into three non-overlapping subsets, named training, test, and validation, based on the last digit of the timestamp, and cache them. We will train multiple models based on the training set, select the best model on the validation set based on RMSE (Root Mean Squared Error), and finally evaluate the best model on the test set. We also add your ratings to the training set to make recommendations for you. We hold the training, validation, and test sets in memory by calling cache because we need to visit them multiple times.12345678910111213141516171819numPartitions = 4training = ratings.filter(lambda x: x[0] &lt; 6) \ .values() \ .union(myRatingsRDD) \ .repartition(numPartitions) \ .cache()validation = ratings.filter(lambda x: x[0] &gt;= 6 and x[0] &lt; 8) \ .values() \ .repartition(numPartitions) \ .cache()test = ratings.filter(lambda x: x[0] &gt;= 8).values().cache()numTraining = training.count()numValidation = validation.count()numTest = test.count()print "Training: %d, validation: %d, test: %d" % (numTraining, numValidation, numTest)After the split, you should see1Training: 602251, validation: 198919, test: 199049.Training using ALSIn this section, we will use ALS.train to train a bunch of models, and select and evaluate the best. Among the training paramters of ALS, the most important ones are rank, lambda (regularization constant), and number of iterations. The trainmethod of ALS we are going to use is defined as the following:12345class ALS(object):def train(cls, ratings, rank, iterations=5, lambda_=0.01, blocks=-1): # ... return MatrixFactorizationModel(sc, mod)deally, we want to try a large number of combinations of them in order to find the best one. Due to time constraint, we will test only 8 combinations resulting from the cross product of 2 different ranks (8 and 12), 2 different lambdas (1.0 and 10.0), and two different numbers of iterations (10 and 20). We use the provided method computeRmse to compute the RMSE on the validation set for each model. The model with the smallest RMSE on the validation set becomes the one selected and its RMSE on the test set is used as the final metric.1234567891011121314151617181920212223242526ranks = [8, 12]lambdas = [1.0, 10.0]numIters = [10, 20]bestModel = NonebestValidationRmse = float("inf")bestRank = 0bestLambda = -1.0bestNumIter = -1for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters): model = ALS.train(training, rank, numIter, lmbda) validationRmse = computeRmse(model, validation, numValidation) print "RMSE (validation) = %f for the model trained with " % validationRmse + \ "rank = %d, lambda = %.1f, and numIter = %d." % (rank, lmbda, numIter) if (validationRmse &lt; bestValidationRmse): bestModel = model bestValidationRmse = validationRmse bestRank = rank bestLambda = lmbda bestNumIter = numItertestRmse = computeRmse(bestModel, test, numTest)# evaluate the best model on the test setprint "The best model was trained with rank = %d and lambda = %.1f, " % (bestRank, bestLambda) \ + "and numIter = %d, and its RMSE on the test set is %f." % (bestNumIter, testRmse)Spark might take a minute or two to train the models. You should see the following on the screen:1The best model was trained using rank 8 and lambda 10.0, and its RMSE on test is 0.8808492431998702.Recommending movies for youAs the last part of our tutorial, let’s take a look at what movies our model recommends for you. This is done by generating (0, movieId) pairs for all movies you haven’t rated and calling the model’s predict method to get predictions. 0 is the special user id assigned to you.12345class MatrixFactorizationModel(object): def predictAll(self, usersProducts): # ... return RDD(self._java_model.predict(usersProductsJRDD._jrdd), self._context, RatingDeserializer())After we get all predictions, let us list the top 50 recommendations and see whether they look good to you.12345678myRatedMovieIds = set([x[1] for x in myRatings])candidates = sc.parallelize([m for m in movies if m not in myRatedMovieIds])predictions = bestModel.predictAll(candidates.map(lambda x: (0, x))).collect()recommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50]print "Movies recommended for you:"for i in xrange(len(recommendations)): print ("%2d: %s" % (i + 1, movies[recommendations[i][1]])).encode('ascii', 'ignore')The output should be similar to123456789101112Movies recommended for you: 1: Silence of the Lambs, The (1991) 2: Saving Private Ryan (1998) 3: Godfather, The (1972) 4: Star Wars: Episode IV - A New Hope (1977) 5: Braveheart (1995) 6: Schindler's List (1993) 7: Shawshank Redemption, The (1994) 8: Star Wars: Episode V - The Empire Strikes Back (1980) 9: Pulp Fiction (1994)10: Alien (1979)...Comparing to a naive baselineDoes ALS output a non-trivial model? We can compare the evaluation result with a naive baseline model that only outputs the average rating (or you may try one that outputs the average rating per movie). Computing the baseline’s RMSE is straightforward:1234meanRating = training.union(validation).map(lambda x: x[2]).mean()baselineRmse = sqrt(test.map(lambda x: (meanRating - x[2]) ** 2).reduce(add) / numTest)improvement = (baselineRmse - testRmse) / baselineRmse * 100print "The best model improves the baseline by %.2f" % (improvement) + "%."The output should be similar to1The best model improves the baseline by 20.96%.It seems obvious that the trained model would outperform the naive baseline. However, a bad combination of training parameters would lead to a model worse than this naive baseline. Choosing the right set of parameters is quite important for this task.Solution code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156#!/usr/bin/env pythonimport sysimport itertoolsfrom math import sqrtfrom operator import addfrom os.path import join, isfile, dirnamefrom pyspark import SparkConf, SparkContextfrom pyspark.mllib.recommendation import ALSdef parseRating(line): """ Parses a rating record in MovieLens format userId::movieId::rating::timestamp . """ fields = line.strip().split("::") return long(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))def parseMovie(line): """ Parses a movie record in MovieLens format movieId::movieTitle . """ fields = line.strip().split("::") return int(fields[0]), fields[1]def loadRatings(ratingsFile): """ Load ratings from file. """ if not isfile(ratingsFile): print "File %s does not exist." % ratingsFile sys.exit(1) f = open(ratingsFile, 'r') ratings = filter(lambda r: r[2] &gt; 0, [parseRating(line)[1] for line in f]) f.close() if not ratings: print "No ratings provided." sys.exit(1) else: return ratingsdef computeRmse(model, data, n): """ Compute RMSE (Root Mean Squared Error). """ predictions = model.predictAll(data.map(lambda x: (x[0], x[1]))) predictionsAndRatings = predictions.map(lambda x: ((x[0], x[1]), x[2])) \ .join(data.map(lambda x: ((x[0], x[1]), x[2]))) \ .values() return sqrt(predictionsAndRatings.map(lambda x: (x[0] - x[1]) ** 2).reduce(add) / float(n))if __name__ == "__main__": if (len(sys.argv) != 3): print "Usage: /path/to/spark/bin/spark-submit --driver-memory 2g " + \ "MovieLensALS.py movieLensDataDir personalRatingsFile" sys.exit(1) # set up environment conf = SparkConf() \ .setAppName("MovieLensALS") \ .set("spark.executor.memory", "2g") sc = SparkContext(conf=conf) # load personal ratings myRatings = loadRatings(sys.argv[2]) myRatingsRDD = sc.parallelize(myRatings, 1) # load ratings and movie titles movieLensHomeDir = sys.argv[1] # ratings is an RDD of (last digit of timestamp, (userId, movieId, rating)) ratings = sc.textFile(join(movieLensHomeDir, "ratings.dat")).map(parseRating) # movies is an RDD of (movieId, movieTitle) movies = dict(sc.textFile(join(movieLensHomeDir, "movies.dat")).map(parseMovie).collect()) numRatings = ratings.count() numUsers = ratings.values().map(lambda r: r[0]).distinct().count() numMovies = ratings.values().map(lambda r: r[1]).distinct().count() print "Got %d ratings from %d users on %d movies." % (numRatings, numUsers, numMovies) # split ratings into train (60%), validation (20%), and test (20%) based on the # last digit of the timestamp, add myRatings to train, and cache them # training, validation, test are all RDDs of (userId, movieId, rating) numPartitions = 4 training = ratings.filter(lambda x: x[0] &lt; 6) \ .values() \ .union(myRatingsRDD) \ .repartition(numPartitions) \ .cache() validation = ratings.filter(lambda x: x[0] &gt;= 6 and x[0] &lt; 8) \ .values() \ .repartition(numPartitions) \ .cache() test = ratings.filter(lambda x: x[0] &gt;= 8).values().cache() numTraining = training.count() numValidation = validation.count() numTest = test.count() print "Training: %d, validation: %d, test: %d" % (numTraining, numValidation, numTest) # train models and evaluate them on the validation set ranks = [8, 12] lambdas = [0.1, 10.0] numIters = [10, 20] bestModel = None bestValidationRmse = float("inf") bestRank = 0 bestLambda = -1.0 bestNumIter = -1 for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters): model = ALS.train(training, rank, numIter, lmbda) validationRmse = computeRmse(model, validation, numValidation) print "RMSE (validation) = %f for the model trained with " % validationRmse + \ "rank = %d, lambda = %.1f, and numIter = %d." % (rank, lmbda, numIter) if (validationRmse &lt; bestValidationRmse): bestModel = model bestValidationRmse = validationRmse bestRank = rank bestLambda = lmbda bestNumIter = numIter testRmse = computeRmse(bestModel, test, numTest) # evaluate the best model on the test set print "The best model was trained with rank = %d and lambda = %.1f, " % (bestRank, bestLambda) \ + "and numIter = %d, and its RMSE on the test set is %f." % (bestNumIter, testRmse) # compare the best model with a naive baseline that always returns the mean rating meanRating = training.union(validation).map(lambda x: x[2]).mean() baselineRmse = sqrt(test.map(lambda x: (meanRating - x[2]) ** 2).reduce(add) / numTest) improvement = (baselineRmse - testRmse) / baselineRmse * 100 print "The best model improves the baseline by %.2f" % (improvement) + "%." # make personalized recommendations myRatedMovieIds = set([x[1] for x in myRatings]) candidates = sc.parallelize([m for m in movies if m not in myRatedMovieIds]) predictions = bestModel.predictAll(candidates.map(lambda x: (0, x))).collect() recommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50] print "Movies recommended for you:" for i in xrange(len(recommendations)): print ("%2d: %s" % (i + 1, movies[recommendations[i][1]])).encode('ascii', 'ignore') # clean up sc.stop()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning with MLlib of Spark]]></title>
      <url>%2F2017%2F04%2F25%2FMachine-Learning-with-MLlib-of-Spark%2F</url>
      <content type="text"><![CDATA[Example: Spam ClassificationThis program uses two MLlib algorithms: HashingTF, which builds term frequency feature vectors from text data, and LogisticRegressionWithSGD, which implements the logistic regression procedure using stochastic gradient descent (SGD). We assume that we start with two files, spam.txt an normal.txt, each of which contains examples of spam and non-spam emails, one per line. We then turn the text in each file into a feature vector with TF, and train a logistic regression model to separate the two types of messages.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the "License"); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#from pyspark import SparkContextfrom pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.classification import LogisticRegressionWithSGDfrom pyspark.mllib.feature import HashingTFif __name__ == "__main__": sc = SparkContext(appName="PythonBookExample") # Load 2 types of emails from text files: spam and ham (non-spam). # Each line has text from one email. spam = sc.textFile("file:///home/hduser/learning-spark/files/spam.txt") ham = sc.textFile("file:///home/hduser/learning-spark/files/ham.txt") # Create a HashingTF instance to map email text to vectors of 100 features. tf = HashingTF(numFeatures = 100) # Each email is split into words, and each word is mapped to one feature. spamFeatures = spam.map(lambda email: tf.transform(email.split(" "))) hamFeatures = ham.map(lambda email: tf.transform(email.split(" "))) # Create LabeledPoint datasets for positive (spam) and negative (ham) examples. positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features)) negativeExamples = hamFeatures.map(lambda features: LabeledPoint(0, features)) training_data = positiveExamples.union(negativeExamples) training_data.cache() # Cache data since Logistic Regression is an iterative algorithm. # Run Logistic Regression using the SGD optimizer. # regParam is model regularization, which can make models more robust. model = LogisticRegressionWithSGD.train(training_data) # Test on a positive example (spam) and a negative one (ham). # First apply the same HashingTF feature transformation used on the training data. posTestExample = tf.transform("O M G GET cheap stuff by sending money to ...".split(" ")) negTestExample = tf.transform("Hi Dad, I started studying Spark the other ...".split(" ")) # Now use the learned model to predict spam/ham for new emails. print "Prediction for positive test example: %g" % model.predict(posTestExample) print "Prediction for negative test example: %g" % model.predict(negTestExample) sc.stop()AlgorithmsHere only has some usual APIs.Feature ExtractionScalingMost machine learning algorithms consider the magnitude of each element in the feature vector, and thus work best when the features are scaled so they weigh equally (e.g., all features have a mean of 0 and standard deviation of 1). Once you have built feature vectors, you can use the StandardScaler class in MLlib to do this scaling, both for the mean and the standard deviation. You create a StandardScaler, call fit() on a dataset to obtain a StandardScalerModel (i.e., compute the mean and variance of each column), and then call transform() on the model to scale a dataset.123456789from pyspark.mllib.feature import StandardScalervectors = [Vectors.dense([-2.0, 5.0, 1.0]), Vectors.dense([2.0, 0.0, 1.0])]dataset = sc.parallelize(vectors)scaler = StandardScaler(withMean=True, withStd=True)model = scaler.fit(dataset)result = model.transform(dataset)# Result: &#123;[-0.7071, 0.7071, 0.0], [0.7071, -0.7071, 0.0]&#125;NormalizationSimply use Normalizer().transform(rdd). By default Normalizer uses the L 2 norm (i.e, Euclidean length), but you can also pass a power pto Normalizer to use the L p norm.123456789101112131415from pyspark.mllib.feature import Normalizerfrom pyspark.mllib.util import MLUtilsdata = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")labels = data.map(lambda x: x.label)features = data.map(lambda x: x.features)normalizer1 = Normalizer()normalizer2 = Normalizer(p=float("inf"))# Each sample in data1 will be normalized using $L^2$ norm.data1 = labels.zip(normalizer1.transform(features))# Each sample in data2 will be normalized using $L^\infty$ norm.data2 = labels.zip(normalizer2.transform(features))Word2VecOnce you have trained the model (withWord2Vec.fit(rdd)), you will receive a Word2VecModel that can be used to transform() each word into a vector. Note that the size of the models in Word2Vec will be equal to the number of words in your vocabulary times the size of a vector (by default, 100). You may wish to filter out words that are not in a standard dictionary to limit the size. In general, a good size for the vocabulary is 100,000 words.1234567891011from pyspark.mllib.feature import Word2Vecinp = sc.textFile("data/mllib/sample_lda_data.txt").map(lambda row: row.split(" "))word2vec = Word2Vec()model = word2vec.fit(inp)synonyms = model.findSynonyms('1', 5)for word, cosine_distance in synonyms: print("&#123;&#125;: &#123;&#125;".format(word, cosine_distance))StatisticsStatistics.colStats(rdd)Computes a statistical summary of an RDD of vectors, which stores the min, max, mean, and variance for each column in the set of vectors. This can be used to obtain a wide variety of statistics in one pass.Statistics.corr(rdd, method)Computes the correlation matrix between columns in an RDD of vectors, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).1234567891011121314151617from pyspark.mllib.stat import StatisticsseriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0]) # a series# seriesY must have the same number of partitions and cardinality as seriesXseriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])# Compute the correlation using Pearson's method. Enter "spearman" for Spearman's method.# If a method is not specified, Pearson's method will be used by default.print("Correlation is: " + str(Statistics.corr(seriesX, seriesY, method="pearson")))data = sc.parallelize( [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([5.0, 33.0, 366.0])]) # an RDD of Vectors# calculate the correlation matrix using Pearson's method. Use "spearman" for Spearman's method.# If a method is not specified, Pearson's method will be used by default.print(Statistics.corr(data, method="pearson"))Statistics.corr(rdd1, rdd2, method)Computes the correlation between two RDDs of floating-point values, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).Statistics.chiSqTest(rdd)Computes Pearson’s independence test for every feature with the label on an RDD of LabeledPoint objects. Returns an array of ChiSqTestResult objects that capture the p-value, test statistic, and degrees of freedom for each feature. Label and feature values must be categorical (i.e., discrete values).123456789101112131415161718192021222324252627282930313233343536from pyspark.mllib.linalg import Matrices, Vectorsfrom pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.stat import Statisticsvec = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25) # a vector composed of the frequencies of events# compute the goodness of fit. If a second vector to test against# is not supplied as a parameter, the test runs against a uniform distribution.goodnessOfFitTestResult = Statistics.chiSqTest(vec)# summary of the test including the p-value, degrees of freedom,# test statistic, the method used, and the null hypothesis.print("%s\n" % goodnessOfFitTestResult)mat = Matrices.dense(3, 2, [1.0, 3.0, 5.0, 2.0, 4.0, 6.0]) # a contingency matrix# conduct Pearson's independence test on the input contingency matrixindependenceTestResult = Statistics.chiSqTest(mat)# summary of the test including the p-value, degrees of freedom,# test statistic, the method used, and the null hypothesis.print("%s\n" % independenceTestResult)obs = sc.parallelize( [LabeledPoint(1.0, [1.0, 0.0, 3.0]), LabeledPoint(1.0, [1.0, 2.0, 0.0]), LabeledPoint(1.0, [-1.0, 0.0, -0.5])]) # LabeledPoint(feature, label)# The contingency table is constructed from an RDD of LabeledPoint and used to conduct# the independence test. Returns an array containing the ChiSquaredTestResult for every feature# against the label.featureTestResults = Statistics.chiSqTest(obs)for i, result in enumerate(featureTestResults): print("Column %d:\n%s" % (i + 1, result))Classification and RegressionMLlib includes a variety of methods for classification and regression, including simple linear methods and decision trees and forests.Linear regression123456from pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.regression import LinearRegressionWithSGDpoints = # (create RDD of LabeledPoint)model = LinearRegressionWithSGD.train(points, iterations=200, intercept=True)print "weights: %s, intercept: %s" % (model.weights, model.intercept)Logistic regressionThe logistic regression algorithm has a very similar API to linear regression, covered in the previous section. One difference is that there are two algorithms available for solving it: SGD and LBFGS. LBFGS is generally the best choice, but is not available in some earlier versions of MLlib (before Spark 1.2). These algorithms are available in the mllib.classification.LogisticRegressionWithLBFGS and WithSGD classes, which have interfaces similar to LinearRegressionWithSGD. They take all the same parameters as linear regression.1234567891011121314151617181920212223from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModelfrom pyspark.mllib.regression import LabeledPoint# Load and parse the datadef parsePoint(line): values = [float(x) for x in line.split(' ')] return LabeledPoint(values[0], values[1:])data = sc.textFile("data/mllib/sample_svm_data.txt")parsedData = data.map(parsePoint)# Build the modelmodel = LogisticRegressionWithLBFGS.train(parsedData)# Evaluating the model on training datalabelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())print("Training Error = " + str(trainErr))# Save and load modelmodel.save(sc, "target/tmp/pythonLogisticRegressionWithLBFGSModel")sameModel = LogisticRegressionModel.load(sc, "target/tmp/pythonLogisticRegressionWithLBFGSModel")Support Vector MachinesThey are available through the SVMWithSGD class, with similar parameters to linear and logisitic regression. The returned SVMModel uses a threshold for prediction like LogisticRegressionModel.12345678910111213141516171819202122from pyspark.mllib.classification import SVMWithSGD, SVMModelfrom pyspark.mllib.regression import LabeledPoint# Load and parse the datadef parsePoint(line): values = [float(x) for x in line.split(' ')] return LabeledPoint(values[0], values[1:])data = sc.textFile("data/mllib/sample_svm_data.txt")parsedData = data.map(parsePoint)# Build the modelmodel = SVMWithSGD.train(parsedData, iterations=100)# Evaluating the model on training datalabelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())print("Training Error = " + str(trainErr))# Save and load modelmodel.save(sc, "target/tmp/pythonSVMWithSGDModel")sameModel = SVMModel.load(sc, "target/tmp/pythonSVMWithSGDModel")Naive BayesIn MLlib, you can use Naive Bayes through themllib.classification.NaiveBayes class. It supports one parameter, lambda (or lambda_ in Python), used for smoothing. You can call it on an RDD of LabeledPoints, where the labels are between 0 and C–1 for C classes.123456789101112131415161718192021222324252627from pyspark.mllib.classification import NaiveBayes, NaiveBayesModelfrom pyspark.mllib.util import MLUtils# Load and parse the data file.data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")# Split data approximately into training (60%) and test (40%)training, test = data.randomSplit([0.6, 0.4])# Train a naive Bayes model.model = NaiveBayes.train(training, 1.0)# Make prediction and test accuracy.predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()print('model accuracy &#123;&#125;'.format(accuracy))# Save and load modeloutput_dir = 'target/tmp/myNaiveBayesModel'shutil.rmtree(output_dir, ignore_errors=True)model.save(sc, output_dir)sameModel = NaiveBayesModel.load(sc, output_dir)predictionAndLabel = test.map(lambda p: (sameModel.predict(p.features), p.label))accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()print('sameModel accuracy &#123;&#125;'.format(accuracy))Decision trees and random forestsIn MLlib, you can train trees using the mllib.tree.DecisionTree class, through the static methods trainClassifier() and trainRegressor(). Unlike in some of the other algorithms, the Java and Scala APIs also use static methods instead of a DecisionTree object with setters.123456789101112131415161718192021222324from pyspark.mllib.tree import DecisionTree, DecisionTreeModelfrom pyspark.mllib.util import MLUtils# Load and parse the data file into an RDD of LabeledPoint.data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a DecisionTree model.# Empty categoricalFeaturesInfo indicates all features are continuous.model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo=&#123;&#125;, impurity='gini', maxDepth=5, maxBins=32)# Evaluate model on test instances and compute test errorpredictions = model.predict(testData.map(lambda x: x.features))labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())print('Test Error = ' + str(testErr))print('Learned classification tree model:')print(model.toDebugString())# Save and load modelmodel.save(sc, "target/tmp/myDecisionTreeClassificationModel")sameModel = DecisionTreeModel.load(sc, "target/tmp/myDecisionTreeClassificationModel")ClusteringK-means1234567891011121314151617181920212223from numpy import arrayfrom math import sqrtfrom pyspark.mllib.clustering import KMeans, KMeansModel# Load and parse the datadata = sc.textFile("data/mllib/kmeans_data.txt")parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))# Build the model (cluster the data)clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode="random")# Evaluate clustering by computing Within Set Sum of Squared Errorsdef error(point): center = clusters.centers[clusters.predict(point)] return sqrt(sum([x**2 for x in (point - center)]))WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)print("Within Set Sum of Squared Error = " + str(WSSSE))# Save and load modelclusters.save(sc, "target/org/apache/spark/PythonKMeansExample/KMeansModel")sameModel = KMeansModel.load(sc, "target/org/apache/spark/PythonKMeansExample/KMeansModel")Collaborative Filtering and Recommendation12345678910111213141516171819202122from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating# Load and parse the datadata = sc.textFile("data/mllib/als/test.data")ratings = data.map(lambda l: l.split(','))\ .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))# Build the recommendation model using Alternating Least Squaresrank = 10numIterations = 10model = ALS.train(ratings, rank, numIterations)# Evaluate the model on training datatestdata = ratings.map(lambda p: (p[0], p[1]))predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()print("Mean Squared Error = " + str(MSE))# Save and load modelmodel.save(sc, "target/tmp/myCollaborativeFilter")sameModel = MatrixFactorizationModel.load(sc, "target/tmp/myCollaborativeFilter")The training exercises from the Spark Summit 2014 include a hands-on tutorial for personalized movie recommendation with spark.mllib.Dimensionality ReductionPrincipal component analysisPCA in Scala123456789101112import org.apache.spark.mllib.linalg.Matriximport org.apache.spark.mllib.linalg.distributed.RowMatrixval points: RDD[Vector] = // ...val mat: RowMatrix = new RowMatrix(points)val pc: Matrix = mat.computePrincipalComponents(2)// Project points to low-dimensional spaceval projected = mat.multiply(pc).rows// Train a k-means model on the projected 2-dimensional dataval model = KMeans.train(projected, 10)Singular value decompositionSVD in Scala1234567// Compute the top 20 singular values of a RowMatrix mat and their singular vectors.val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(20, computeU=true)val U: RowMatrix = svd.U // U is a distributed RowMatrix.val s: Vector = svd.s // Singular values are a local dense vector.val V: Matrix = svd.V // V is a local dense matrix.Pipeline APIPipeline API version of spam classification in Scala1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.spark.sql.SQLContextimport org.apache.spark.ml.Pipelineimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.ml.feature.&#123;HashingTF, Tokenizer&#125;import org.apache.spark.ml.tuning.&#123;CrossValidator, ParamGridBuilder&#125;import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator// A class to represent documents -- will be turned into a SchemaRDDcase class LabeledDocument(id: Long, text: String, label: Double)val documents = // (load RDD of LabeledDocument)val sqlContext = new SQLContext(sc)import sqlContext._// Configure an ML pipeline with three stages: tokenizer, tf, and lr; each stage// outputs a column in a SchemaRDD and feeds it to the next stage's input columnval tokenizer = new Tokenizer() // Splits each email into words .setInputCol("text") .setOutputCol("words")val tf = new HashingTF() // Maps email words to vectors of 10000 features .setNumFeatures(10000) .setInputCol(tokenizer.getOutputCol) .setOutputCol("features")val lr = new LogisticRegression() // Uses "features" as inputCol by defaultval pipeline = new Pipeline().setStages(Array(tokenizer, tf, lr))// Fit the pipeline to the training documentsval model = pipeline.fit(documents)// Alternatively, instead of fitting once with the parameters above, we can do a// grid search over some parameters and pick the best model via cross-validationval paramMaps = new ParamGridBuilder() .addGrid(tf.numFeatures, Array(10000, 20000)) .addGrid(lr.maxIter, Array(100, 200)) .build() // Builds all combinations of parametersval eval = new BinaryClassificationEvaluator()val cv = new CrossValidator() .setEstimator(lr) .setEstimatorParamMaps(paramMaps) .setEvaluator(eval)val bestModel = cv.fit(documents)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Solution for Bracket in markdown link address]]></title>
      <url>%2F2017%2F04%2F23%2F%C2%96Solution-for-Bracket-in-markdown-link-address%2F</url>
      <content type="text"><![CDATA[Markdown创造一个链接或者图片是使用 [title](link) 和 ![title](link).我们可以避免[]内出现中括号, 或者使用转义.但是在小括号的链接里面就可能会出问题. 有些网址上面会具有小括号. 例如,https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdf解决方法:%28 代替(, %29代替) 主要是后者会歧义链接部分的结束. 这是使用url符号码去代替ascii的符号. 能够解决这个问题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The first course of the Spark]]></title>
      <url>%2F2017%2F04%2F23%2FThe-first-course-of-the-Spark%2F</url>
      <content type="text"><![CDATA[简介扩充了MapReduce计算模型基于内存的计算能够进行批处理、迭代式计算、交互查询和流处理降低里维护成本提供了Python、Java、Scala、SQL的API和丰富的内置库可以与Hadoop、Kafka等整合组件Spark CoreSpark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. Spark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. RDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.Spark SQLSpark SQL is Spark’s package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Language (HQL)—and it supports many sources of data, including Hive tables, Parquet, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics. This tight integration with the rich computing environment provided by Spark makes Spark SQL unlike any other open source data warehouse tool. Spark SQL was added to Spark in version 1.0.Shark was an older SQL-on-Spark project out of the University of California, Berkeley, that modified Apache Hive to run on Spark. It has now been replaced by Spark SQL to provide better integration with the Spark engine and language APIs.Spark StreamingSpark Streaming is a Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. Spark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. Underneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.MLlibSpark comes with a library containing common machine learning (ML) functionality, called MLlib. MLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. It also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. All of these methods are designed to scale out across a cluster.GraphXGraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations. Like Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. GraphX also provides various operators for manipulating graphs (e.g., subgraph and mapVertices) and a library of common graph algorithms (e.g., PageRank and triangle counting).Cluster ManagersUnder the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. To achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including Hadoop YARN, Apache Mesos, and a simple cluster manager included in Spark itself called the Standalone Scheduler. If you are just installing Spark on an empty set of machines, the Standalone Scheduler provides an easy way to get started; if you already have a Hadoop YARN or Mesos cluster, however, Spark’s support for these cluster managers allows your applications to also run on them.安装Spark由Scala编写，运行于JVM上，运行环境为Java 7+如果使用Python API，需要安装Python 2.6+ 或者Python 3.4+Spark 1.6.2 – Scala 2.10 / Spark 2.0.0 – Scala 2.11下载http://spark.apache.org/downloads.html不需要Hadoop集群；如果已经搭建好Hadoop集群，可下载相应版本解压目录README.mdContains short instructions for getting started with Spark.binContains executable files that can be used to interact with Spark in various ways (e.g., the Spark shell, which we will cover later in this chapter).core, streaming, python, …Contains the source code of major components of the Spark project.examplesContains some helpful Spark standalone jobs that you can look at and run tolearn about the Spark API.ShellPython Shellbin/pysparkScala Shellbin/spark-shell开发环境搭建Scala安装https://www.scala-lang.org/download/注意版本对应IntelliJ IDEA安装https://www.jetbrains.com/idea/#chooseYourEdition可以申请教育账号插件安装File-Settings-Plugins 搜索Scala，安装项目创建File-New-Project-Scala-SBT同样注意版本匹配（这里用的是Spark 2.1.0, Scala 2.11.11）配置文件需要定义使用的Spark版本build.sbt追加123libraryDependencies ++= Seq( "org.apache.spark" %% "spark-core" % "2.1.0")重建项目即可源程序编写New-Scala Class-Class to ObjectWordCount.scala1234567891011121314151617import org.apache.spark.&#123;SparkContext, SparkConf&#125;/** * Created by root on 4/23/17. */object WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName("wordcount") val sc = new SparkContext(conf) val input = sc.textFile("/home/hduser/Anaconda2-4.3.1-Linux-x86_64.sh") val lines = input.flatMap(line =&gt; line.split(" ")) val count = lines.map(word =&gt; (word, 1)).reduceByKey&#123;case (x, y) =&gt; x + y&#125; val output = count.saveAsTextFile("/home/hduser/scala_wordcount_demo_output") &#125;&#125;打包File-Project Structure-Project Setting-Artifacts-Add-JAR-From modules with dependenciesBuild-Build Artifacts-Build启动集群启动mastersbin/start-master.sh启动workerbin/spark-class org.apache.spark.deploy.worker.Worker spark://Ubuntu:7077注意这里的spark服务器地址可以通过浏览器输入localhost:8080来查看提交作业bin/spark-submit --master spark://Ubuntu:7077 --class WordCount /home/hduser/scala_demo.jar注意这里的scala_demo.jar文件为打包阶段生成TODO: RDDsZaharia M., et al. Learning Spark (O’Reilly, 2015)(274s).pdfReferences慕课网 http://www.imooc.com/learn/814Zaharia M., et al. Learning Spark (O’Reilly, 2015)(274s).pdf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Implement k-means on the hadoop platform]]></title>
      <url>%2F2017%2F04%2F21%2FImplement-k-means-on-the-hadoop-platform%2F</url>
      <content type="text"><![CDATA[首先在单机上搭一个伪分布式环境，主要是对*-site.xml配置文件进行修改，具体修改如下：core-site.xml12345678&lt;?xml version="1.0"?&gt;&lt;!-- core-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost/&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;hdfs-site.xml12345678&lt;?xml version="1.0"?&gt;&lt;!-- hdfs-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;yarn-site.xml123456789101112&lt;?xml version="1.0"?&gt;&lt;!-- yarn-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;mapred-site.xml12345678&lt;?xml version="1.0"?&gt;&lt;!-- mapred-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;然后启动hadoop，启动的流程如下：start-dfs.shstart-yarn.shmr-jobhistory-daemon.sh start historyserver注意，以上命令能够得到正确执行的前提是已经将hadoop的安装目录下的bin目录加到环境变量中由于很久没有使用Java，所以采用Python实现，这里需要用到一个package，也就是mrjob首先计划一下实现步骤：[ Mapper ]Acceptsdataglobal constant representing the list of centersComputesthe nearest center for each data instanceEmitsnearest centers (key) and points (value).[ Reducer ]Acceptscenter instance / coordinate (key)points (value)Computesthe new centers based on clustersEmitsnew centersYou will provide the next epoch of K-Means with:the same data from your initial epochthe centers emitted from the reducer as global constantsRepeat until your stopping criteria are met.如果要用Python进行相关的Hadoop操作的话，肯定是要使用hadoop streaming的，但是存在一个问题，也就是streaming流程只能跑一遍，但是很显然，作为一个machine learning算法，k-means是类似于EM算法要经过多步迭代的，那么最容易想到的就是使用shell脚本多次调用相关命令，但是这样显得十分ugly，因此可以采用mrjob包来帮助我们完成这个工作。从上面看来，我们需要两个文件，一个是python实现的map-reduce，另一个是mrjob的job文件，相当于master，下面列出这两个文件，因为实现比较简单，因此不作过多解释.kmeans.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788from mrjob.job import MRJobimport mrjob# MRJob is a python class which will be overloadedfrom math import sqrtclass MRKMeans(MRJob): SORT_VALUES = True OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol def dist_vec(self, v1, v2): # calculate the ditance between two vectors (in two dimensions) return sqrt((v2[0] - v1[0]) * (v2[0] - v1[0]) + (v2[1] - v1[1]) * (v2[1] - v1[1])) def configure_options(self): super(MRKMeans, self).configure_options() # the line below define that the file folowing the --c option is the # centroid and is loadable self.add_file_option('--c') def get_centroids(self): """ Definition : extracts centroids from the centroids file define afetr --c flag Out : Return the list of centroids """ # self.options.c is the name of the file following --c option f = open(self.options.c, 'r') centroids = [] for line in f.read().split('\n'): if line: x, y = line.split(', ') centroids.append([float(x), float(y)]) f.close() return centroids def mapper(self, _, lines): """ Definition : Mapper take centroids extract form get_centroids() and the point cloud and for each point, calculate the distance to the centroids, find the mininum of it Out : yield the point with it's class """ centroids = self.get_centroids() for l in lines.split('\n'): x, y = l.split(', ') point = [float(x), float(y)] min_dist = 100000000.0 classe = 0 # iterate over the centroids (Here we know that we are doing a 3means) for i in range(3): dist = self.dist_vec(point, centroids[i]) if dist &lt; min_dist: min_dist = dist classe = i yield classe, point def combiner(self, k, v): """ Definition : Calculate for each class, at the end of the mapper, before reducer, the medium point of each class Out: return for each class, the centroids for each mapper """ count = 0 moy_x = moy_y = 0.0 for t in v: count += 1 moy_x += t[0] moy_y += t[1] yield k, (moy_x / count, moy_y / count) def reducer(self, k, v): """ Definition : for each class, get all the tmp centroids from each combiner and calculate the new centroids. """ # k is class and v are medium points linked to the class count = 0 moy_x = moy_y = 0.0 for t in v: count += 1 moy_x += t[0] moy_y += t[1] print str(k) + ", " + str(moy_x / count) + ", " + str(moy_y / count)if __name__ == '__main__': # just run mapreduce ! MRKMeans.run()main.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889from mrjob.job import MRJobfrom kmeans import MRKMeansimport sysimport os.pathimport shutilfrom math import sqrtimport timeinput_c = "centroids"CENTROIDS_FILE = "/home/hduser/tmp/centroid"def get_c(job, runner): c = [] for line in runner.stream_output(): # print "stream_output: ", line key, value = job.parse_output_line(line) c.append(key) return cdef get_first_c(fname): f = open(fname, 'r') centroids = [] for line in f.read().split('\n'): if line: x, y = line.split(', ') centroids.append([float(x), float(y)]) f.close() return centroidsdef write_c(centroids): f = open(CENTROIDS_FILE, "w") centroids.sort() for c in centroids: k, cx, cy = c.split(', ') # print c f.write("%s, %s\n" % (cx, cy)) f.close()def dist_vec(v1, v2): return sqrt((v2[0] - v1[0]) * (v2[0] - v1[0]) + (v2[1] - v1[1]) * (v2[1] - v1[1]))def diff(cs1, cs2): max_dist = 0.0 for i in range(3): dist = dist_vec(cs1[i], cs2[i]) if dist &gt; max_dist: max_dist = dist return max_distif __name__ == '__main__': args = sys.argv[1:] if not os.path.isfile(CENTROIDS_FILE): shutil.copy(input_c, CENTROIDS_FILE) old_c = get_first_c(input_c) i = 1 start = time.time() while True: print "Iteration #%i" % i mr_job = MRKMeans(args=args + ['--c=' + CENTROIDS_FILE]) # print "start runner.." with mr_job.make_runner() as runner: runner.run() centroids = get_c(mr_job, runner) # print "mr result: ", centroids write_c(centroids) n_c = get_first_c(CENTROIDS_FILE) # print "old_c", old_c # print "n_c", n_c max_d = diff(n_c, old_c) # print "dist max = "+str(max_d) if max_d &lt; 0.01: break else: old_c = n_c i = i + 1 print "used time: ", time.time() - start, 's'根据上面写的实现步骤可以看出，我们需要两个文件，一个存储输入数据，另一个存储centroids，由于只是一个demo，因此在这里我简化了具体问题。设所有的数据都是二维数据点，并且聚类个数为3。当然，如果真的是在大数据上进行工业级的处理的话，还是推荐使用Spark。下面列出这两个文件：kmeans_data12345678910111213141516171, 22, 31, 3.54, 3.53, 4.22, 1.65, 2.31.5, 2.33, 5.22, 31, 3.54, 3.53, 4.22, 1.65, 2.31.5, 2.33, 5centroids1231, 22, 31, 3.5按照以下方式运行：1python main.py kmeans_data -r hadoop结果显示如下：1234567891011Iteration #1No handlers could be found for logger "mrjob.hadoop"old_c [[1.0, 2.0], [2.0, 3.0], [1.0, 3.5]]n_c [[1.625, 1.95833333333], [3.4, 3.62], [1.0, 3.5]]Iteration #2old_c [[1.625, 1.95833333333], [3.4, 3.62], [1.0, 3.5]]n_c [[1.72916666667, 2.2625], [3.75, 3.775], [1.0, 3.5]]Iteration #3old_c [[1.72916666667, 2.2625], [3.75, 3.775], [1.0, 3.5]]n_c [[1.72916666667, 2.2625], [3.75, 3.775], [1.0, 3.5]]time: 148.277868032最后生成结果文件：centroid1231.72916666667, 2.26253.75, 3.7751.0, 3.5根据以上可以看出，对于小数据集，效率反而会比较低，因为整个程序运行过程中大部分的时间都没有花在实际的算法运行上。TODO：用常规方法实现，作为baseline在大数据集上继续实验，观察结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Something about cloudera]]></title>
      <url>%2F2017%2F04%2F20%2FSomething-about-cloudera%2F</url>
      <content type="text"><![CDATA[最近发现了一个神器，clouderahttps://www.cloudera.com/它其实是一个集成了Hadoop生态系统的CentOS 6.7的VM，可以跑在Docker、Virtual Box或者VMware上https://www.cloudera.com/downloads/quickstart_vms/5-10.html虚拟机配置的时候需要分配至少8G的RAM以及2个Cores。另外，第一次启动会有些慢，请耐心等待。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop Distributed Filesystem notes]]></title>
      <url>%2F2017%2F04%2F18%2FHadoop-Distributed-Filesystem-notes%2F</url>
      <content type="text"><![CDATA[HDFS ConceptsBlocks128 MB by defaultHDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks.Having a block abstraction for a distributed filesystem brings several benefitsA file can be larger than any single disk in the network.Simplifies the storage subsystem.Providing fault tolerance and availability.% hdfs fsck / -files -blocksNamenodes and DatanodesAn HDFS cluster has two types of nodes operating in a master−worker patternnamenode (the master)datanodes (workers)Without the namenode, the filesystem cannot be usedFor this reason, it is important to make the namenode resilient to failureThe first way is to back up the files that make up the persistent state of the filesystemmetadata.It is also possible to run a secondary namenodeBlock CachingFor frequently accessed files the blocks may be explicitly cached in the datanode’s memory, in an off-heap block cache. By default, a block is cached in only one datanode’s memory.HDFS Federationone namenode might manage all the files rooted under /user, say, and a second name‐node might handle files under /share.namespace volumeblock poolnamenodes do not communicate with one anotherHDFS High AvalibilityThe new namenode is not able to serve requests until it hasloaded its namespace image into memoryreplayed its edit logreceived enough block reports from the datanodes to leave safe mode.On large clusters with many files and blocks, the time it takes for a namenode to start from cold can be 30 minutes or more.Hadoop 2 remedied this situation by adding support for HDFS high availability (HA).there are a pair of namenodes in an active-standby configuration. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption.There are two choices for the highly available shared storageNFS filerquorum journal manager (QJM)The actual observed failover time will be longer in practice (around a minute or so)The transition from the active namenode to the standby is managed by a new entity inthe system called the failover controllerdefault implementation uses ZooKeeper to ensure that only one namenode is active.The QJM only allows one namenode to write to the edit log at one timeThe Command-Line InterfaceBasic Filesystem Operationscopying a file from the local filesystem to HDFS12% hadoop fs -copyFromLocal input/docs/quangle.txt \ hdfs://localhost/user/tom/quangle.txtcopy the file back to the local filesystem and check whether it’s the same1234% hadoop fs -copyToLocal quangle.txt quangle.copy.txt% md5 input/docs/quangle.txt quangle.copy.txte7891a2627cf263a079fb0f18256ffb2 input/docs/quangle.txtMD5 (quangle.copy.txt) = e7891a2627cf263a079fb0f18256ffb2create a directory first just to see how it is displayed in the listing12345% hadoop fs -mkdir books% hadoop fs -ls .Found 2 itemsdrwxr-xr-x - tom supergroup 0 2014-10-04 13:22 books-rw-r--r-- 1 tom supergroup 119 2014-10-04 13:21 quangle.txtThe Java InterfaceReading Data from a Hadoop URLExample. Displaying files from a Hadoop filesystem on standard output using a URLStreamHandler1234567891011121314151617181920212223242526// cc URLCat Displays files from a Hadoop filesystem on standard output using a URLStreamHandlerimport java.io.InputStream;import java.net.URL;import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;import org.a pache.hadoop.io.IOUtils;// vv URLCatpublic class URLCat &#123; static &#123; URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); &#125; public static void main(String[] args) throws Exception &#123; InputStream in = null; try &#123; in = new URL(args[0]).openStream(); IOUtils.copyBytes(in, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(in); &#125; &#125;&#125;// ^^ URLCatThere’s a little bit more work required to make Java recognize Hadoop’s hdfs URL scheme. This is achieved by calling the setURLStreamHandlerFactory() method on URL with an instance of FsUrlStreamHandlerFactory. This method can be called only once per JVM, so it is typically executed in a static block.Here’s a sample run:123456% export HADOOP_CLASSPATH=hadoop-examples.jar% hadoop URLCat hdfs://localhost/user/tom/quangle.txtOn the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.Reading Data Using the FileSystem APIExample. Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly1234567891011121314151617181920212223242526// cc FileSystemCat Displays files from a Hadoop filesystem on standard output by using the FileSystem directlyimport java.io.InputStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;// vv FileSystemCatpublic class FileSystemCat &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), conf); InputStream in = null; try &#123; in = fs.open(new Path(uri)); IOUtils.copyBytes(in, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(in); &#125; &#125;&#125;// ^^ FileSystemCatThe program runs as follows:12345% hadoop FileSystemCat hdfs://localhost/user/tom/quangle.txtOn the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.FSDataInputStreamExample. Displaying files from a Hadoop filesystem on standard output twice, by using seek()12345678910111213141516171819202122232425262728// cc FileSystemDoubleCat Displays files from a Hadoop filesystem on standard output twice, by using seekimport java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;// vv FileSystemDoubleCatpublic class FileSystemDoubleCat &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), conf); FSDataInputStream in = null; try &#123; in = fs.open(new Path(uri)); IOUtils.copyBytes(in, System.out, 4096, false); in.seek(0); // go back to the start of the file IOUtils.copyBytes(in, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(in); &#125; &#125;&#125;// ^^ FileSystemDoubleCatHere’s the result of running it on a small file:123456789% hadoop FileSystemDoubleCat hdfs://localhost/user/tom/quangle.txtOn the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.On the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.Writing DataExample. Copying a local file to a Hadoop filesystem123456789101112131415161718192021222324252627282930313233// cc FileCopyWithProgress Copies a local file to a Hadoop filesystem, and shows progressimport java.io.BufferedInputStream;import java.io.FileInputStream;import java.io.InputStream;import java.io.OutputStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.Progressable;// vv FileCopyWithProgresspublic class FileCopyWithProgress &#123; public static void main(String[] args) throws Exception &#123; String localSrc = args[0]; String dst = args[1]; InputStream in = new BufferedInputStream(new FileInputStream(localSrc)); Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(dst), conf); OutputStream out = fs.create(new Path(dst), new Progressable() &#123; public void progress() &#123; System.out.print("."); &#125; &#125;); IOUtils.copyBytes(in, out, 4096, true); &#125;&#125;// ^^ FileCopyWithProgressTypical usage:123% hadoop FileCopyWithProgress input/docs/1400-8.txthdfs://localhost/user/tom/1400-8.txt.................Querying the FilesystemThe FileStatus class encapsulates filesystem metadata for files and directories, including file length, block size, replication, modification time, ownership, and permission information.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class ShowFileStatusTest &#123; private MiniDFSCluster cluster; // use an in-process HDFS cluster for testing private FileSystem fs; @Before public void setUp() throws IOException &#123; Configuration conf = new Configuration(); if (System.getProperty("test.build.data") == null) &#123; System.setProperty("test.build.data", "/tmp"); &#125; cluster = new MiniDFSCluster.Builder(conf).build(); fs = cluster.getFileSystem(); OutputStream out = fs.create(new Path("/dir/file")); out.write("content".getBytes("UTF-8")); out.close(); &#125; @After public void tearDown() throws IOException &#123; if (fs != null) &#123; fs.close(); &#125; if (cluster != null) &#123; cluster.shutdown(); &#125; &#125; @Test(expected = FileNotFoundException.class) public void throwsFileNotFoundForNonExistentFile() throws IOException &#123; fs.getFileStatus(new Path("no-such-file")); &#125; @Test public void fileStatusForFile() throws IOException &#123; Path file = new Path("/dir/file"); FileStatus stat = fs.getFileStatus(file); assertThat(stat.getPath().toUri().getPath(), is("/dir/file")); assertThat(stat.isDirectory(), is(false)); assertThat(stat.getLen(), is(7L)); assertThat(stat.getModificationTime(), is(lessThanOrEqualTo(System.currentTimeMillis()))); assertThat(stat.getReplication(), is((short) 1)); assertThat(stat.getBlockSize(), is(128 * 1024 * 1024L)); assertThat(stat.getOwner(), is(System.getProperty("user.name"))); assertThat(stat.getGroup(), is("supergroup")); assertThat(stat.getPermission().toString(), is("rw-r--r--")); &#125; @Test public void fileStatusForDirectory() throws IOException &#123; Path dir = new Path("/dir"); FileStatus stat = fs.getFileStatus(dir); assertThat(stat.getPath().toUri().getPath(), is("/dir")); assertThat(stat.isDirectory(), is(true)); assertThat(stat.getLen(), is(0L)); assertThat(stat.getModificationTime(), is(lessThanOrEqualTo(System.currentTimeMillis()))); assertThat(stat.getReplication(), is((short) 0)); assertThat(stat.getBlockSize(), is(0L)); assertThat(stat.getOwner(), is(System.getProperty("user.name"))); assertThat(stat.getGroup(), is("supergroup")); assertThat(stat.getPermission().toString(), is("rwxr-xr-x")); &#125; &#125;Listing filesExample. Showing the file statuses for a collection of paths in a Hadoop filesystem123456789101112131415161718192021222324252627282930// cc ListStatus Shows the file statuses for a collection of paths in a Hadoop filesystem import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.FileUtil;import org.apache.hadoop.fs.Path;// vv ListStatuspublic class ListStatus &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), conf); Path[] paths = new Path[args.length]; for (int i = 0; i &lt; paths.length; i++) &#123; paths[i] = new Path(args[i]); &#125; FileStatus[] status = fs.listStatus(paths); Path[] listedPaths = FileUtil.stat2Paths(status); for (Path p : listedPaths) &#123; System.out.println(p); &#125; &#125;&#125;// ^^ ListStatusWe can use this program to find the union of directory listings for a collection of paths:1234% hadoop ListStatus hdfs://localhost/ hdfs://localhost/user/tomhdfs://localhost/userhdfs://localhost/user/tom/bookshdfs://localhost/user/tom/quangle.txtDataFlowAnatomy of a File ReadNetwork Topology and HadoopMathematically inclined readers will notice that this is an example of a distance metric.Anatomy of a File WriteA typical replica pipeline:Coherency ModelAfter creating a file, it is visible in the filesystem namespace, as expected:123Path p = new Path("p");fs.create(p);assertThat(fs.exists(p), is(true));However, any content written to the file is not guaranteed to be visible, even if the stream is flushed. So, the file appears to have a length of zero:12345Path p = new Path("p");OutputStream out = fs.create(p);out.write("content".getBytes("UTF-8"));out.flush();assertThat(fs.getFileStatus(p).getLen(), is(0L));HDFS provides a way to force all buffers to be flushed to the datanodes via the hflush() method on FSDataOutputStream. After a successful return from hflush(), HDFS guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers:12345Path p = new Path("p");FSDataOutputStream out = fs.create(p);out.write("content".getBytes("UTF-8"));out.hflush();assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));Note that hflush() does not guarantee that the datanodes have written the data to disk, only that it’s in the datanodes’ memory (so in the event of a data center power outage, for example, data could be lost). For this stronger guarantee, use hsync() instead.12345FileOutputStream out = new FileOutputStream(localFile);out.write("content".getBytes("UTF-8"));out.flush(); // flush to operating systemout.getFD().sync(); // sync to diskassertThat(localFile.length(), is(((long) "content".length())));Closing a file in HDFS performs an implicit hflush(), too:12345Path p = new Path("p");OutputStream out = fs.create(p);out.write("content".getBytes("UTF-8"));out.close();assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));Parallel Copying with distcpOne use for distcp is as an efficient replacement for hadoop fs -cp. For example, you can copy one file to another with:% hadoop distcp file1 file2You can also copy directories:% hadoop distcp dir1 dir2]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop namenode not getting started]]></title>
      <url>%2F2017%2F04%2F18%2FHadoop-namenode-not-getting-started%2F</url>
      <content type="text"><![CDATA[First delete all contents from temporary folder: rm -rf &lt;tmp dir&gt; (my was /usr/local/hadoop/tmp)Format the namenode: bin/hadoop namenode -formatStart all processes againbin/start-dfs.shbin/start-yarn.shbin/mr-jobhistory-daemon.sh start historyserverYou may consider rolling back as well using checkpoint (if you had it enabled).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop ch02 MapReduce notes]]></title>
      <url>%2F2017%2F04%2F17%2FHadoop-ch02-MapReduce-notes%2F</url>
      <content type="text"><![CDATA[MapReduce首先我们有一个数据集，关于天气的，然后它的每一条记录是这样的：123456789101112131415161718192021222324252627282930310057332130 # USAF weather station identifier99999 # WBAN weather station identifier19500101 # observation date0300 # observation time4+51317 # latitude (degrees x 1000)+028783 # longitude (degrees x 1000)FM-12+0171 # elevation (meters)99999V020320 # wind direction (degrees)1 # quality codeN0072100450 # sky ceiling height (meters)1 # quality codeCN010000 # visibility distance (meters)1 # quality codeN9-0128 # air temperature (degrees Celsius x 10)1 # quality code-0139 # dew point temperature (degrees Celsius x 10)1 # quality code10268 # atmospheric pressure (hectopascals x 10)1 # quality code当然以上数据是经过处理之后的，一开始它长这样：123450067011990999991950051507004...9999999N9+00001+99999999999...0043011990999991950051512004...9999999N9+00221+99999999999...0043011990999991950051518004...9999999N9-00111+99999999999...0043012650999991949032412004...0500001N9+01111+99999999999...0043012650999991949032418004...0500001N9+00781+99999999999...Hmmm….这个天气数据集按照气象站编号-年份的形式来组织的：12345678910010010-99999-1990.gz010014-99999-1990.gz010015-99999-1990.gz010016-99999-1990.gz010017-99999-1990.gz010030-99999-1990.gz010040-99999-1990.gz010080-99999-1990.gz010100-99999-1990.gz010150-99999-1990.gz这个原始数据显然用起来不方便，所以按照年份给它聚个类，用了如下方法：123456789hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -D mapred.reduce.tasks=0 \ -D mapred.map.tasks.speculative.execution=false \ -D mapred.task.timeout=12000000 \ -input ncdc_files.txt \ -inputformat org.apache.hadoop.mapred.lib.NLineInputFormat \ -output output \ -mapper load_ncdc_map.sh \ -file load_ncdc_map.sh然后里面用到的ncdc_files以及load_ncdc_map.sh这两个文件是这样的：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100s3n://hadoopbook/ncdc/raw/isd-1901.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1902.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1903.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1904.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1905.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1906.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1907.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1908.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1909.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1910.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1911.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1912.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1913.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1914.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1915.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1916.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1917.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1918.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1919.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1920.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1921.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1922.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1923.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1924.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1925.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1926.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1927.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1928.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1929.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1930.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1931.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1932.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1933.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1934.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1935.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1936.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1937.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1938.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1939.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1940.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1941.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1942.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1943.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1944.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1945.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1946.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1947.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1948.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1949.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1950.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1951.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1952.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1953.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1954.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1955.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1956.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1957.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1958.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1959.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1960.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1961.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1962.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1963.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1964.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1965.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1966.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1967.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1968.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1969.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1970.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1971.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1972.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1973.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1974.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1975.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1976.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1977.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1978.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1979.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1980.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1981.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1982.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1983.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1984.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1985.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1986.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1987.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1988.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1989.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1990.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1991.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1992.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1993.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1994.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1995.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1996.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1997.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1998.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1999.tar.bz2s3n://hadoopbook/ncdc/raw/isd-2000.tar.bz21234567891011121314151617181920212223242526#!/usr/bin/env bash# NLineInputFormat gives a single line: key is offset, value is S3 URIread offset s3file# Retrieve file from S3 to local diskecho "reporter:status:Retrieving $s3file" &gt;&amp;2$HADOOP_INSTALL/bin/hadoop fs -get $s3file .# Un-bzip and un-tar the local filetarget=`basename $s3file .tar.bz2`mkdir -p $targetecho "reporter:status:Un-tarring $s3file to $target" &gt;&amp;2tar jxf `basename $s3file` -C $target# Un-gzip each station file and concat into one fileecho "reporter:status:Un-gzipping $target" &gt;&amp;2for file in $target/*/*do gunzip -c $file &gt;&gt; $target.all echo "reporter:status:Processed $file" &gt;&amp;2done# Put gzipped version into HDFSecho "reporter:status:Gzipping $target and putting in HDFS" &gt;&amp;2gzip -c $target.all | $HADOOP_INSTALL/bin/hadoop fs -put - gz/$target.gz嗯…顺便说一句，这个文件是存在AWS上的，所以想用的话要有一个AWS账号，想要有个账号呢，你得先有个可以支付美刀的信用卡。Hmmmmm…其实作者给的sample data也挺好的我觉得，在这里.那么我们的问题就是说，找出每一年的最高的温度。先看看不用Hadoop的实现方法，事实证明我shell脚本还是宝刀未老的。12345678910#!/usr/bin/env bashfor year in all/*do echo -ne `basename $year .gz`"\t" gunzip -c $year | \ awk '&#123; temp = substr($0, 88, 5) + 0; q = substr($0, 93, 1); if (temp !=9999 &amp;&amp; q ~ /[01459]/ &amp;&amp; temp &gt; max) max = temp &#125; END &#123; print max &#125;'done结果如下：1234567% ./max_temperature.sh1901 3171902 2441903 2891904 2561905 283...啊嘞，还不错的样子，但是对于大数据速度还是慢了点儿，所以直接上Hadoop看看。对于以上的问题呢，MapReduce是这样解决的注意了，上面一行是hadoop的术语，下面呢，其实就是Unix的pipe了，这给我们不用Java来实现提供了可能。好了下面开始coding了，拿起键盘就是GAN为了实现我们的任务，我们需要三个java文件，一个mapper，一个reducer。这俩是苦工，还要一个监工。Mapper123456789101112131415161718192021222324252627282930313233// cc MaxTemperatureMapper Mapper for maximum temperature example// vv MaxTemperatureMapperimport java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class MaxTemperatureMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private static final int MISSING = 9999; @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String year = line.substring(15, 19); int airTemperature; if (line.charAt(87) == '+') &#123; // parseInt doesn't like leading plus signs airTemperature = Integer.parseInt(line.substring(88, 92)); &#125; else &#123; airTemperature = Integer.parseInt(line.substring(87, 92)); &#125; String quality = line.substring(92, 93); if (airTemperature != MISSING &amp;&amp; quality.matches("[01459]")) &#123; context.write(new Text(year), new IntWritable(airTemperature)); &#125; &#125;&#125;// ^^ MaxTemperatureMapperReducer123456789101112131415161718192021222324// cc MaxTemperatureReducer Reducer for maximum temperature example// vv MaxTemperatureReducerimport java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MaxTemperatureReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int maxValue = Integer.MIN_VALUE; for (IntWritable value : values) &#123; maxValue = Math.max(maxValue, value.get()); &#125; context.write(key, new IntWritable(maxValue)); &#125;&#125;// ^^ MaxTemperatureReducerJob12345678910111213141516171819202122232425262728293031323334// cc MaxTemperature Application to find the maximum temperature in the weather dataset// vv MaxTemperatureimport org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MaxTemperature &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"); System.exit(-1); &#125; Job job = new Job(); job.setJarByClass(MaxTemperature.class); job.setJobName("Max temperature"); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTemperatureMapper.class); job.setReducerClass(MaxTemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;// ^^ MaxTemperature然后这么运行：12% export HADOOP_CLASSPATH=hadoop-examples.jar% hadoop MaxTemperature input/ncdc/sample.txt output但是如果数据量非常大的话，需要在Mapper和Reducer之间传递大量的数据，这个时候可以引入Combiner，它的机理是这样的。假如我有两个mapper，它们的输出结果是这样子的：123(1950, 0)(1950, 20)(1950, 10)以及这样子的：12(1950, 25)(1950, 15)如果没有combiner的话，它们会先变成这样子：1(1950, [0, 20, 10, 25, 15])然后作为reducer的输入，但是如果加入了combiner的话，相当于上面的问题变成了这样max(0, 20, 10, 25, 15) = max(max(0, 20, 10), max(25, 15)) = max(20, 25) = 25是不是简单多了。但是注意了，并不是所有的问题都是这样，比如下面这个问题：mean(0, 20, 10, 25, 15) = 14mean(mean(0, 20, 10), mean(25, 15)) = mean(10, 20) = 15所以说要根据具体情况来定，不能直接套用。好了我们继续combiner的话题，我们怎么把这货加到hadoop的流程中去呢，其实很简单，这样就可以：123456789101112131415161718192021222324252627282930313233343536// cc MaxTemperatureWithCombiner Application to find the maximum temperature, using a combiner function for efficiencyimport org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;// vv MaxTemperatureWithCombinerpublic class MaxTemperatureWithCombiner &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("Usage: MaxTemperatureWithCombiner &lt;input path&gt; " + "&lt;output path&gt;"); System.exit(-1); &#125; Job job = new Job(); job.setJarByClass(MaxTemperatureWithCombiner.class); job.setJobName("Max temperature"); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTemperatureMapper.class); /*[*/job.setCombinerClass(MaxTemperatureReducer.class)/*]*/; job.setReducerClass(MaxTemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;// ^^ MaxTemperatureWithCombiner没错，combiner和reducer是一样的。其实仔细想想这也很自然，因为它们俩实际实现的功能是一样的。Hadoop Streaming作为一个machine learning专业的，有时候用Java还是感觉挺不爽的，哪有Python啊，Ruby啊这种脚本语言方便嘛。所以hadoop还是很人性地提供了解决方法，就是标题所表示的技术。直接看代码怎么用吧。RubyMap1234567#!/usr/bin/env rubySTDIN.each_line do |line| val = line year, temp, q = val[15,4], val[87,5], val[92,1] puts "#&#123;year&#125;\t#&#123;temp&#125;" if (temp != "+9999" &amp;&amp; q =~ /[01459]/)endReduce12345678910111213#!/usr/bin/env rubylast_key, max_val = nil, -1000000STDIN.each_line do |line| key, val = line.split("\t") if last_key &amp;&amp; last_key != key puts "#&#123;last_key&#125;\t#&#123;max_val&#125;" last_key, max_val = key, val.to_i else last_key, max_val = key, [max_val, val.to_i].max endendputs "#&#123;last_key&#125;\t#&#123;max_val&#125;" if last_key然后这样调用：12345% hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -input input/ncdc/sample.txt \ -output output \ -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \ -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb是不是很方便？如果要加上combiner的话，更方便了，都不用再写额外的文件：12345678% hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -files ch02-mr-intro/src/main/ruby/max_temperature_map.rb,\ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \ -input input/ncdc/all \ -output output \ -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \ -combiner ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \ -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb注意，以上的-files命令是为了在集群环境下运行时，将脚本复制到各子节点上。Python啊，Python大大出场，其实和Ruby没啥区别。Map12345678910#!/usr/bin/env pythonimport reimport sysfor line in sys.stdin: val = line.strip() (year, temp, q) = (val[15:19], val[87:92], val[92:93]) if (temp != "+9999" and re.match("[01459]", q)): print "%s\t%s" % (year, temp)Reduce123456789101112131415#!/usr/bin/env pythonimport sys(last_key, max_val) = (None, -sys.maxint)for line in sys.stdin: (key, val) = line.strip().split("\t") if last_key and last_key != key: print "%s\t%s" % (last_key, max_val) (last_key, max_val) = (key, int(val)) else: (last_key, max_val) = (key, max(max_val, int(val)))if last_key: print "%s\t%s" % (last_key, max_val)运行都是一样的，就不多做赘述了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning [ECNU] Assignment 1]]></title>
      <url>%2F2017%2F04%2F16%2FMachine-Learning-ECNU-Assignment-1%2F</url>
      <content type="text"><![CDATA[Use a crawler to get at least 20 webpages from a website.Count theoccurrences of words in the webpages on Hadoop.Hand in:Each one should crawl different websites, list the website URL, as well as the URLsof the crawled webpages.Count the word occurrence on Hadoop, code in both JAVA and another language such asPig Latin. print out your code.Print out your result.Home work due: 4/12You are allowed toform a group of no more than 4 fellow students.https://github.com/ewanlee/machine-learning-ECNU-/blob/master/Hadoop%20wordcount%20demo_cutted.pdf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignments [2 & 3]]]></title>
      <url>%2F2017%2F04%2F16%2Fcs231n-Assignments-2-3%2F</url>
      <content type="text"><![CDATA[Assignment 2In this assignment you will practice writing backpropagation code, and training Neural Networks and Convolutional Neural Networks. The goals of this assignment are as follows:understand Neural Networks and how they are arranged in layered architecturesunderstand and be able to implement (vectorized) backpropagationimplement various update rules used to optimize Neural Networksimplement batch normalization for training deep networksimplement dropout to regularize networkseffectively cross-validate and find the best hyperparameters for Neural Network architectureunderstand the architecture of Convolutional Neural Networks and train gain experience with training these models on dataSetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal.com.Working in the cloud on TerminalTerminal has created a separate subdomain to serve our class, www.stanfordterminalcloud.com. Register your account there. The Assignment 2 snapshot can then be found HERE. If you are registered in the class you can contact the TA (see Piazza for more information) to request Terminal credits for use on the assignment. Once you boot up the snapshot everything will be installed for you, and you will be ready to start on your assignment right away. We have written a small tutorial on Terminal here.Working locallyGet the code as a zip file here. As for the dependencies:[Option 1] Use Anaconda: The preferred approach for installing all the assignment dependencies is to useAnaconda, which is a Python distribution that includes many of the most popular Python packages for science, math, engineering and data analysis. Once you install it you can skip all mentions of requirements and you are ready to go directly to working on the assignment.[Option 2] Manual install, virtual environment: If you do not want to use Anaconda and want to go with a more manual and risky installation route you will likely want to create a virtual environment for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:1234567cd assignment2sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environmentDownload data: Once you have the starter code, you will need to download the CIFAR-10 dataset. Run the following from the assignment2 directory:12cd cs231n/datasets./get_datasets.shCompile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command:1python setup.py build_ext --inplaceStart IPython: After you have the CIFAR-10 data, you should start the IPython notebook server from the assignment2 directory. If you are unfamiliar with IPython, you should read our IPython tutorial.NOTE: If you are working in a virtual environment on OSX, you may encounter errors with matplotlib due to theissues described here. You can work around this issue by starting the IPython server using thestart_ipython_osx.sh script from the assignment2 directory; the script assumes that your virtual environment is named .env.Submitting your work:Whether you work on the assignment locally or using Terminal, once you are done working run the collectSubmission.sh script; this will produce a file called assignment2.zip. Upload this file under the Assignments tab on the coursework page for the course.Q1: Fully-connected Neural Network (30 points)The IPython notebook FullyConnectedNets.ipynb will introduce you to our modular layer design, and then use those layers to implement fully-connected networks of arbitrary depth. To optimize these models you will implement several popular update rules.Q2: Batch Normalization (30 points)In the IPython notebook BatchNormalization.ipynb you will implement batch normalization, and use it to train deep fully-connected networks.Q3: Dropout (10 points)The IPython notebook Dropout.ipynb will help you implement Dropout and explore its effects on model generalization.Q4: ConvNet on CIFAR-10 (30 points)In the IPython Notebook ConvolutionalNetworks.ipynb you will implement several new layers that are commonly used in convolutional networks. You will train a (shallow) convolutional network on CIFAR-10, and it will then be up to you to train the best network that you can.Q5: Do something extra! (up to +10 points)In the process of training your network, you should feel free to implement anything that you want to get better performance. You can modify the solver, implement additional layers, use different types of regularization, use an ensemble of models, or anything else that comes to mind. If you implement these or other ideas not covered in the assignment then you will be awarded some bonus points.](https://github.com/ewanlee/cs231n/tree/master/cs231n-assignments/assignmentAssignment 3In this assignment you will implement recurrent networks, and apply them to image captioning on Microsoft COCO. We will also introduce the TinyImageNet dataset, and use a pretrained model on this dataset to explore different applications of image gradients.The goals of this assignment are as follows:Understand the architecture of recurrent neural networks (RNNs) and how they operate on sequences by sharing weights over timeUnderstand the difference between vanilla RNNs and Long-Short Term Memory (LSTM) RNNsUnderstand how to sample from an RNN at test-timeUnderstand how to combine convolutional neural nets and recurrent nets to implement an image captioning systemUnderstand how a trained convolutional network can be used to compute gradients with respect to the input imageImplement and different applications of image gradients, including saliency maps, fooling images, class visualizations, feature inversion, and DeepDream.SetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal.com.Working in the cloud on TerminalTerminal has created a separate subdomain to serve our class, www.stanfordterminalcloud.com. Register your account there. The Assignment 3 snapshot can then be found HERE. If you are registered in the class you can contact the TA (see Piazza for more information) to request Terminal credits for use on the assignment. Once you boot up the snapshot everything will be installed for you, and you will be ready to start on your assignment right away. We have written a small tutorial on Terminal here.Working locallyGet the code as a zip file here. As for the dependencies:[Option 1] Use Anaconda: The preferred approach for installing all the assignment dependencies is to useAnaconda, which is a Python distribution that includes many of the most popular Python packages for science, math, engineering and data analysis. Once you install it you can skip all mentions of requirements and you are ready to go directly to working on the assignment.[Option 2] Manual install, virtual environment: If you do not want to use Anaconda and want to go with a more manual and risky installation route you will likely want to create a virtual environment for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:1234567cd assignment3sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environmentDownload data: Once you have the starter code, you will need to download the processed MS-COCO dataset, the TinyImageNet dataset, and the pretrained TinyImageNet model. Run the following from the assignment3directory:1234cd cs231n/datasets./get_coco_captioning.sh./get_tiny_imagenet_a.sh./get_pretrained_model.shCompile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command:1python setup.py build_ext --inplaceStart IPython: After you have the data, you should start the IPython notebook server from the assignment3directory. If you are unfamiliar with IPython, you should read our IPython tutorial.NOTE: If you are working in a virtual environment on OSX, you may encounter errors with matplotlib due to theissues described here. You can work around this issue by starting the IPython server using thestart_ipython_osx.sh script from the assignment3 directory; the script assumes that your virtual environment is named .env.Submitting your work:Whether you work on the assignment locally or using Terminal, once you are done working run the collectSubmission.sh script; this will produce a file called assignment3.zip. Upload this file under the Assignments tab on the coursework page for the course.Q1: Image Captioning with Vanilla RNNs (40 points)The IPython notebook RNN_Captioning.ipynb will walk you through the implementation of an image captioning system on MS-COCO using vanilla recurrent networks.Q2: Image Captioning with LSTMs (35 points)The IPython notebook LSTM_Captioning.ipynbwill walk you through the implementation of Long-Short Term Memory (LSTM) RNNs, and apply them to image captioning on MS-COCO.Q3: Image Gradients: Saliency maps and Fooling Images (10 points)The IPython notebook ImageGradients.ipynb will introduce the TinyImageNet dataset. You will use a pretrained model on this dataset to compute gradients with respect to the image, and use them to produce saliency maps and fooling images.Q4: Image Generation: Classes, Inversion, DeepDream (15 points)In the IPython notebook ImageGeneration.ipynb you will use the pretrained TinyImageNet model to generate images. In particular you will generate class visualizations and implement feature inversion and DeepDream.Q5: Do something extra! (up to +10 points)Given the components of the assignment, try to do something cool. Maybe there is some way to generate images that we did not implement in the assignment?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Software Packages notes]]></title>
      <url>%2F2017%2F04%2F13%2Fcs231n-Software-Packages-notes%2F</url>
      <content type="text"><![CDATA[Software PackagesCaffehttp://caffe.berkeleyvision.orgOverviewFrom U.C. BerkeleyWritten in C++Has Python and Matlab bindingsGood for training or finetuning feedforward modelsTipDon’t be afraid to read the code!Main classesBlob: Stores data and derivativesLayer: Transforms bottom blobs to top blobsNet:Many layersComputes gradients via forward / backwardSolver: Uses gradients to update weightsProtocol Buffers“Typed JSON” from GoogleDefine “message types” in .proto files12345message Person &#123; required string name = 1; required int32 id = 2; optional string email = 3;&#125;Serialize instances to text files (.prototxt)123name: "John Doe"id: 1234email: "jdoe@example.com"Compile classes for different languagesTraining / FinetuningConvert data (run a script)Define net (edit prototxt)Define solver (edit prototxt)Train (with pretrained weights) (run a script)Step1: Convert DataDataLayer reading from LMDB is the easiestCreate LMDB using convert_imagesetNeed text file where each line is“[path/to/image.jpeg][label]”Create HDF5 file yourself using h5py[extras] some methods:ImageDataLayer: Read from image filesWindowDataLayer: For detectionHDF5Layer: Read from HDF5 fileFrom memory, using Python interfaceAll of these are harder to use (except Python)Step2: Define Net123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127name: "ResNet-152"input: "data"input_dim: 1input_dim: 3input_dim: 224input_dim: 224layer &#123; bottom: "data" top: "conv1" name: "conv1" type: "Convolution" convolution_param &#123; num_output: 64 kernel_size: 7 pad: 3 stride: 2 bias_term: false &#125;&#125;layer &#123; bottom: "conv1" top: "conv1" name: "bn_conv1" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "conv1" top: "conv1" name: "scale_conv1" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;layer &#123; top: "conv1" bottom: "conv1" name: "conv1_relu" type: "ReLU"&#125;layer &#123; bottom: "conv1" top: "pool1" name: "pool1" type: "Pooling" pooling_param &#123; kernel_size: 3 stride: 2 pool: MAX &#125;&#125;layer &#123; bottom: "pool1" top: "res2a_branch1" name: "res2a_branch1" type: "Convolution" convolution_param &#123; num_output: 256 kernel_size: 1 pad: 0 stride: 1 bias_term: false &#125;&#125;layer &#123; bottom: "res2a_branch1" top: "res2a_branch1" name: "bn2a_branch1" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "res2a_branch1" top: "res2a_branch1" name: "scale2a_branch1" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;layer &#123; bottom: "pool1" top: "res2a_branch2a" name: "res2a_branch2a" type: "Convolution" convolution_param &#123; num_output: 64 kernel_size: 1 pad: 0 stride: 1 bias_term: false &#125;&#125;layer &#123; bottom: "res2a_branch2a" top: "res2a_branch2a" name: "bn2a_branch2a" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "res2a_branch2a" top: "res2a_branch2a" name: "scale2a_branch2a" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;.prototxt can get ugly for big modelsResNet-152 prototxt is 6775 lines long!Not “compositional”; can’t easily define a residual block and reuseStep2: Define Net (finetuning)Same name: weights copiedDifferent name: weights reinitializedStep3: Define SolverWrite a prototxt file defining a SolverParameter123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166message SolverParameter &#123; ////////////////////////////////////////////////////////////////////////////// // Specifying the train and test networks // // Exactly one train net must be specified using one of the following fields: // train_net_param, train_net, net_param, net // One or more test nets may be specified using any of the following fields: // test_net_param, test_net, net_param, net // If more than one test net field is specified (e.g., both net and // test_net are specified), they will be evaluated in the field order given // above: (1) test_net_param, (2) test_net, (3) net_param/net. // A test_iter must be specified for each test_net. // A test_level and/or a test_stage may also be specified for each test_net. ////////////////////////////////////////////////////////////////////////////// // Proto filename for the train net, possibly combined with one or more // test nets. optional string net = 24; // Inline train net param, possibly combined with one or more test nets. optional NetParameter net_param = 25; optional string train_net = 1; // Proto filename for the train net. repeated string test_net = 2; // Proto filenames for the test nets. optional NetParameter train_net_param = 21; // Inline train net params. repeated NetParameter test_net_param = 22; // Inline test net params. // The states for the train/test nets. Must be unspecified or // specified once per net. // // By default, all states will have solver = true; // train_state will have phase = TRAIN, // and all test_state's will have phase = TEST. // Other defaults are set according to the NetState defaults. optional NetState train_state = 26; repeated NetState test_state = 27; // The number of iterations for each test net. repeated int32 test_iter = 3; // The number of iterations between two testing phases. optional int32 test_interval = 4 [default = 0]; optional bool test_compute_loss = 19 [default = false]; // If true, run an initial test pass before the first iteration, // ensuring memory availability and printing the starting value of the loss. optional bool test_initialization = 32 [default = true]; optional float base_lr = 5; // The base learning rate // the number of iterations between displaying info. If display = 0, no info // will be displayed. optional int32 display = 6; // Display the loss averaged over the last average_loss iterations optional int32 average_loss = 33 [default = 1]; optional int32 max_iter = 7; // the maximum number of iterations optional string lr_policy = 8; // The learning rate decay policy. optional float gamma = 9; // The parameter to compute the learning rate. optional float power = 10; // The parameter to compute the learning rate. optional float momentum = 11; // The momentum value. optional float weight_decay = 12; // The weight decay. // regularization types supported: L1 and L2 // controlled by weight_decay optional string regularization_type = 29 [default = "L2"]; // the stepsize for learning rate policy "step" optional int32 stepsize = 13; // the stepsize for learning rate policy "multistep" repeated int32 stepvalue = 34; // Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm, // whenever their actual L2 norm is larger. optional float clip_gradients = 35 [default = -1]; optional int32 snapshot = 14 [default = 0]; // The snapshot interval optional string snapshot_prefix = 15; // The prefix for the snapshot. // whether to snapshot diff in the results or not. Snapshotting diff will help // debugging but the final protocol buffer size will be much larger. optional bool snapshot_diff = 16 [default = false]; // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default. enum SolverMode &#123; CPU = 0; GPU = 1; &#125; optional SolverMode solver_mode = 17 [default = GPU]; // the device_id will that be used in GPU mode. Use device_id = 0 in default. optional int32 device_id = 18 [default = 0]; // If non-negative, the seed with which the Solver will initialize the Caffe // random number generator -- useful for reproducible results. Otherwise, // (and by default) initialize using a seed derived from the system clock. optional int64 random_seed = 20 [default = -1]; // Solver type enum SolverType &#123; SGD = 0; NESTEROV = 1; ADAGRAD = 2; &#125; optional SolverType solver_type = 30 [default = SGD]; // numerical stability for AdaGrad optional float delta = 31 [default = 1e-8]; // If true, print information about the state of the net that may help with // debugging learning problems. optional bool debug_info = 23 [default = false]; // If false, don't save a snapshot after training finishes. optional bool snapshot_after_train = 28 [default = true];&#125;// A message that stores the solver snapshotsmessage SolverState &#123; optional int32 iter = 1; // The current iteration optional string learned_net = 2; // The file that stores the learned net. repeated BlobProto history = 3; // The history for sgd solvers optional int32 current_step = 4 [default = 0]; // The current step for learning rate&#125;enum Phase &#123; TRAIN = 0; TEST = 1;&#125;message NetState &#123; optional Phase phase = 1 [default = TEST]; optional int32 level = 2 [default = 0]; repeated string stage = 3;&#125;message NetStateRule &#123; // Set phase to require the NetState have a particular phase (TRAIN or TEST) // to meet this rule. optional Phase phase = 1; // Set the minimum and/or maximum levels in which the layer should be used. // Leave undefined to meet the rule regardless of level. optional int32 min_level = 2; optional int32 max_level = 3; // Customizable sets of stages to include or exclude. // The net must have ALL of the specified stages and NONE of the specified // "not_stage"s to meet the rule. // (Use multiple NetStateRules to specify conjunctions of stages.) repeated string stage = 4; repeated string not_stage = 5;&#125;// Specifies training parameters (multipliers on global learning constants,// and the name and other settings used for weight sharing).message ParamSpec &#123; // The names of the parameter blobs -- useful for sharing parameters among // layers, but never required otherwise. To share a parameter between two // layers, give it a (non-empty) name. optional string name = 1; // Whether to require shared weights to have the same shape, or just the same // count -- defaults to STRICT if unspecified. optional DimCheckMode share_mode = 2; enum DimCheckMode &#123; // STRICT (default) requires that num, channels, height, width each match. STRICT = 0; // PERMISSIVE requires only the count (num*channels*height*width) to match. PERMISSIVE = 1; &#125; // The multiplier on the global learning rate for this parameter. optional float lr_mult = 3 [default = 1.0]; // The multiplier on the global weight decay for this parameter. optional float decay_mult = 4 [default = 1.0];&#125;If finetuning, copy existing solver.prototxt fileChange net to be your netChange snapshot_prefix to your outputReduce base learning rate (divide by 100)Maybe change max_iter and snapshotStep 4: Train12345678./build/tools/caffe train \ -gpu 0 \ -model path/to/trainval.prototxt \ -solver path/to/solver.prototxt \ -weights path/to/pretrained_weights.caffemodel # -gpu -1 for CPU mode # -gpu all for multi-GPU data parallelismModel Zoohttps://github.com/BVLC/caffe/wiki/Model-ZooPython InterfaceRead the code! Two most important files:caffe/python/caffe/_caffe.cppExports Blob, Layer, Net, and Solver classescaffe/python/caffe/pycaffe.pyAdds extra methods to Net classGood for:Interfacing with numpyExtract features: Run net forwardCompute gradients: Run net backward (DeepDream, etc)Define layers in Python with numpy (CPU only)Pros / Cons(+) Good for feedforward networks(+) Good for finetuning existing networks(+) Train models without writing any code!(+) Python interface is pretty useful!(-) Need to write C++ / CUDA for new GPU layers(-) Not good for recurrent networks(-) Cumbersome for big networks (GoogLeNet, ResNet)Torchhttp://torch.chOverviewFrom NYU + IDIAPWritten in C and LuaUsed a lot a Facebook, DeepMindLuaLearn Lua in 15 MinutesHigh level scripting language, easy to interface with CSimilar to Javascript:One data structure: table == JS objectPrototypical inheritance: metatable == JS prototypeFirst-class functionsSome gotchas:1-indexed =(Variables global by default =(Small standard libraryTensorTorch tensors are just like numpy arraysDocumentation on GitHub:https://github.com/torch/torch7/blob/master/doc/tensor.mdhttps://github.com/torch/torch7/blob/master/doc/maths.mdnnnn module lets you easily build and train neural nets123456789101112131415-- our optimization procedure will iterate over the modules, so only share-- the parametersmlp = nn.Sequential()linear = nn.Linear(2,2)linear_clone = linear:clone('weight','bias') -- clone sharing the parametersmlp:add(linear)mlp:add(linear_clone)function gradUpdate(mlp, x, y, criterion, learningRate) local pred = mlp:forward(x) local err = criterion:forward(pred, y) local gradCriterion = criterion:backward(pred, y) mlp:zeroGradParameters() mlp:backward(x, gradCriterion) mlp:updateParameters(learningRate)end12345678910111213141516171819-- our optimization procedure will use all the parameters at once, because-- it requires the flattened parameters and gradParameters Tensors. Thus,-- we need to share both the parameters and the gradParametersmlp = nn.Sequential()linear = nn.Linear(2,2)-- need to share the parameters and the gradParameters as welllinear_clone = linear:clone('weight','bias','gradWeight','gradBias')mlp:add(linear)mlp:add(linear_clone)params, gradParams = mlp:getParameters()function gradUpdate(mlp, x, y, criterion, learningRate, params, gradParams) local pred = mlp:forward(x) local err = criterion:forward(pred, y) local gradCriterion = criterion:backward(pred, y) mlp:zeroGradParameters() mlp:backward(x, gradCriterion) -- adds the gradients to all the parameters at once params:add(-learningRate, gradParams)endcunnRunning on GPU is easy123456789101112local model = nn.Sequential()model:add(nn.Linear(2,2))model:add(nn.LogSoftMax())model:cuda() -- convert model to CUDAlocal input = torch.Tensor(32,2):uniform()input = input:cuda()local output = model:forward(input)local input = torch.CudaTensor(32,2):uniform()local output = model:forward(input)optimoptim package implements different update rules: momentum, Adam, etc123456789101112131415161718192021require 'optim'for epoch = 1, 50 do -- local function we give to optim -- it takes current weights as input, and outputs the loss -- and the gradient of the loss with respect to the weights -- gradParams is calculated implicitly by calling 'backward', -- because the model's weight and bias gradient tensors -- are simply views onto gradParams function feval(params) gradParams:zero() local outputs = model:forward(batchInputs) local loss = criterion:forward(outputs, batchLabels) local dloss_doutputs = criterion:backward(outputs, batchLabels) model:backward(batchInputs, dloss_doutputs) return loss, gradParams end optim.sgd(feval, params, optimState)endModulesCaffe has Nets and Layers; Torch just has ModulesModules are classes written in Lua; easy to read and writeForward / backward written in Lua using Tensor methodsSame code runs on CPU / GPU123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122local Linear, parent = torch.class('nn.Linear', 'nn.Module')function Linear:__init(inputSize, outputSize, bias) parent.__init(self) local bias = ((bias == nil) and true) or bias self.weight = torch.Tensor(outputSize, inputSize) self.gradWeight = torch.Tensor(outputSize, inputSize) if bias then self.bias = torch.Tensor(outputSize) self.gradBias = torch.Tensor(outputSize) end self:reset()endfunction Linear:noBias() self.bias = nil self.gradBias = nil return selfendfunction Linear:reset(stdv) if stdv then stdv = stdv * math.sqrt(3) else stdv = 1./math.sqrt(self.weight:size(2)) end if nn.oldSeed then for i=1,self.weight:size(1) do self.weight:select(1, i):apply(function() return torch.uniform(-stdv, stdv) end) end if self.bias then for i=1,self.bias:nElement() do self.bias[i] = torch.uniform(-stdv, stdv) end end else self.weight:uniform(-stdv, stdv) if self.bias then self.bias:uniform(-stdv, stdv) end end return selfendlocal function updateAddBuffer(self, input) local nframe = input:size(1) self.addBuffer = self.addBuffer or input.new() if self.addBuffer:nElement() ~= nframe then self.addBuffer:resize(nframe):fill(1) endendfunction Linear:updateOutput(input) if input:dim() == 1 then self.output:resize(self.weight:size(1)) if self.bias then self.output:copy(self.bias) else self.output:zero() end self.output:addmv(1, self.weight, input) elseif input:dim() == 2 then local nframe = input:size(1) local nElement = self.output:nElement() self.output:resize(nframe, self.weight:size(1)) if self.output:nElement() ~= nElement then self.output:zero() end updateAddBuffer(self, input) self.output:addmm(0, self.output, 1, input, self.weight:t()) if self.bias then self.output:addr(1, self.addBuffer, self.bias) end else error('input must be vector or matrix') end return self.outputendfunction Linear:updateGradInput(input, gradOutput) if self.gradInput then local nElement = self.gradInput:nElement() self.gradInput:resizeAs(input) if self.gradInput:nElement() ~= nElement then self.gradInput:zero() end if input:dim() == 1 then self.gradInput:addmv(0, 1, self.weight:t(), gradOutput) elseif input:dim() == 2 then self.gradInput:addmm(0, 1, gradOutput, self.weight) end return self.gradInput endendfunction Linear:accGradParameters(input, gradOutput, scale) scale = scale or 1 if input:dim() == 1 then self.gradWeight:addr(scale, gradOutput, input) if self.bias then self.gradBias:add(scale, gradOutput) end elseif input:dim() == 2 then self.gradWeight:addmm(scale, gradOutput:t(), input) if self.bias then -- update the size of addBuffer if the input is not the same size as the one we had in last updateGradInput updateAddBuffer(self, input) self.gradBias:addmv(scale, gradOutput:t(), self.addBuffer) end endendfunction Linear:sharedAccUpdateGradParameters(input, gradOutput, lr) -- we do not need to accumulate parameters when sharing: self:defaultAccUpdateGradParameters(input, gradOutput, lr)endfunction Linear:clearState() if self.addBuffer then self.addBuffer:set() end return parent.clearState(self)endfunction Linear:__tostring__() return torch.type(self) .. string.format('(%d -&gt; %d)', self.weight:size(2), self.weight:size(1)) .. (self.bias == nil and ' without bias' or '')endTons of built-in modules and loss functionshttps://github.com/torch/nnContainerContainer modules allow you to combine multiple modulesnngraphA multi-layer network where each layer takes output of previous two layers as input.1234567891011121314input = nn.Identity()()L1 = nn.Tanh()(nn.Linear(10, 20)(input))L2 = nn.Tanh()(nn.Linear(30, 60)(nn.JoinTable(1)(&#123;input, L1&#125;)))L3 = nn.Tanh()(nn.Linear(80, 160)(nn.JoinTable(1)(&#123;L1, L2&#125;)))g = nn.gModule(&#123;input&#125;, &#123;L3&#125;)indata = torch.rand(10)gdata = torch.rand(160)g:forward(indata)g:backward(indata, gdata)graph.dot(g.fg, 'Forward Graph')graph.dot(g.bg, 'Backward Graph')More InfoPretrained Modelsloadcaffe: Load pretrained Caffe models: AlexNet, VGG, some othershttps://github.com/szagoruyko/loadcaffeGoogLeNet v1: https://github.com/soumith/inception.torchGoogLeNet v3: https://github.com/Moodstocks/inception-v3.torchResNet: https://github.com/facebook/fb.resnet.torchPackage ManagementAfter installing torch, use luarocks to install or update Lua packages(Similar to pip install from Python)Other useful packagestorch.cudnn: Bindings for NVIDIA cuDNN kernelshttps://github.com/soumith/cudnn.torchtorch-hdf5: Read and write HDF5 files from Torchhttps://github.com/deepmind/torch-hdf5lua-cjson: Read and write JSON files from Luahttps://luarocks.org/modules/luarocks/lua-cjsoncltorch, clnn: OpenCL backend for Torch, and port of nnhttps://github.com/hughperkins/cltorch, https://github.com/hughperkins/clnntorch-autograd: Automatic differentiation; sort of like more powerful nngraph, similar to Theano or TensorFlowhttps://github.com/twitter/torch-autogradfbcunn: Facebook: FFT conv, multi-GPU (DataParallel, ModelParallel)https://github.com/facebook/fbcunnTypical WorkflowPreprocess data; usually use a Python script to dump data to HDF5Train a model in Lua / Torch; read from HDF5 datafile, save trained model to diskUse trained model for something, often with an evaluation scriptExample: https://github.com/jcjohnson/torch-rnnStep 1: Preprocess data; usually use a Python script to dump data to HDF5 (https://github.com/jcjohnson/torch-rnn/blob/master/scripts/preprocess.py)Step 2: Train a model in Lua / Torch; read from HDF5 datafile, save trained model to disk (https://github.com/jcjohnson/torch-rnn/blob/master/train.lua )Step 3: Use trained model for something, often with an evaluation script (https://github.com/jcjohnson/torch-rnn/blob/master/sample.lua)Pros / Cons(-) Lua(-) Less plug-and-play than CaffeYou usually write your own training code(+) Lots of modular pieces that are easy to combine(+) Easy to write your own layer types and run on GPU(+) Most of the library code is in Lua, easy to read(+) Lots of pretrained models!(-) Not great for RNNsTheanohttp://deeplearning.net/software/theano/OverviewFrom Yoshua Bengio’s group at University of MontrealEmbracing computation graphs, symbolic computationHigh-level wrappers: Keras, LasagneOther TopicsConditionals: The ifelse and switch functions allow conditional control flow in the graphLoops: The scan function allows for (some types) of loops in the computational graph; good for RNNsDerivatives: Efficient Jacobian / vector products with R and L operators, symbolic hessians (gradient of gradient)Sparse matrices, optimizations, etcMulti-GPUExperimental model parallelism:http://deeplearning.net/software/theano/tutorial/using_multi_gpu.htmlData parallelism using platoon:https://github.com/mila-udem/platoonHigh level wrapperLasagneKerasPretrained ModelsLasagne Model Zoo has pretrained common architectures:https://github.com/Lasagne/Recipes/tree/master/modelzooAlexNet with weights: https://github.com/uoguelph-mlrg/theano_alexnetsklearn-theano: Run OverFeat and GoogLeNet forward, but no fine-tuning? http://sklearn-theano.github.iocaffe-theano-conversion: CS 231n project from last year: load models and weights from caffe! Not sure if full-featured https://github.com/kitofans/caffe-theano-conversionPros / Cons(+) Python + numpy(+) Computational graph is nice abstraction(+) RNNs fit nicely in computational graph(-) Raw Theano is somewhat low-level(+) High level wrappers (Keras, Lasagne) ease the pain(-) Error messages can be unhelpful(-) Large models can have long compile times(-) Much “fatter” than Torch; more magic(-) Patchy support for pretrained modelsTensorFlowhttps://www.tensorflow.orgOverviewFrom GoogleVery similar to Theano - all about computation graphsEasy visualizations (TensorBoard)Multi-GPU and multi-node trainingTensorboardTensorboard makes it easy to visualize what’s happening inside your modelsMulti-GPUDistributedPretrained ModelsYou can get a pretrained version of Inception here:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md(In an Android example?? Very well-hidden)The only one I could find =(Pros / Cons(+) Python + numpy(+) Computational graph abstraction, like Theano; great for RNNs(+) Much faster compile times than Theano(+) Slightly more convenient than raw Theano?(+) TensorBoard for visualization(+) Data AND model parallelism; best of all frameworks(+/-) Distributed models, but not open-source yet(-) Slower than other frameworks right now(-) Much “fatter” than Torch; more magic(-) Not many pretrained modelsUse CasesExtract AlexNet or VGG features? Use CaffeFine-tune AlexNet for new classes? Use CaffeImage Captioning with finetuning?-&gt; Need pretrained models (Caffe, Torch, Lasagne)-&gt; Need RNNs (Torch or Lasagne)-&gt; Use Torch or LasagnaSegmentation? (Classify every pixel)-&gt; Need pretrained model (Caffe, Torch, Lasagna)-&gt; Need funny loss function-&gt; If loss function exists in Caffe: Use Caffe-&gt; If you want to write your own loss: Use TorchObject Detection?-&gt; Need pretrained model (Torch, Caffe, Lasagne)-&gt; Need lots of custom imperative code (NOT Lasagne)-&gt; Use Caffe + Python or TorchLanguage modeling with new RNN structure?-&gt; Need easy recurrent nets (NOT Caffe, Torch)-&gt; No need for pretrained models-&gt; Use Theano or TensorFlowImplement BatchNorm?-&gt; Don’t want to derive gradient? Theano or TensorFlow-&gt; Implement efficient backward pass? Use TorchRecommendation:Feature extraction / finetuning existing models: Use CaffeComplex uses of pretrained models: Use Lasagne or TorchWrite your own layers: Use TorchCrazy RNNs: Use Theano or TensorflowHuge model, need model parallelism: Use TensorFlow]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[usr/bin/ld: cannot find -lxxx Solutions]]></title>
      <url>%2F2017%2F04%2F11%2Fusr-bin-ld-cannot-find-lxxx-Solutions%2F</url>
      <content type="text"><![CDATA[在Ubuntu上运行Qt5的过程中报错：1usr/bin/ld: cannot find -lGL最后发现问题是系统中没有对应的库文件 libgl.so那么解决方式也很简单，安装即可：1sudo apt-get install libgl-devTada =)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Reinforcement Learning (GT) Notes]]></title>
      <url>%2F2017%2F04%2F10%2FReinforcement-Learning-GT-Notes%2F</url>
      <content type="text"><![CDATA[Decision Making &amp; Reinforcement LearningSupervised Learning: $y = f(x)$Unsupervised Learning: $f(x)$Reinforcement Learning: $y = f(x), z$Markov Decision ProcessStates: $S$Model: $T(s, a, s^{\prime}) \sim Pr(s^{\prime} | s, a)$Actions: $A(s), A$Reword: $R(s), R(s, a), R(s, a, s^{\prime})$Policy: $\pi(s) \rightarrow a$​ $\pi^{*}$Sequences of Rewards: AssumptionInfinite HorizonsUtility of sequencesif $U(s_0, s_1, s_2, \cdots) &gt; U(s_0, s^{\prime}_1, s^{\prime}_2, \cdots)$then $U(s_1, s_2, \cdots) &gt; U(s^{\prime}_1, s^{\prime}_2, \cdots)$$$U(s_0, s_1, s_2, \cdots)=\sum_{t=0}^{\infty}\gamma^{t}R(s_t), 0 \leq \gamma \leq 1$$$$U\leq\frac{R_{max}}{1 - \gamma}$$Policies$$\pi^{\star}=argmax_{\pi} E[\sum_{t=0}^{\infty}\gamma^{t}R(S_t)|\pi]$$$$U^{\pi}(s)=E[\sum_{t=0}^{\infty}\gamma^{t}R(s_t)|\pi,s_0=s]$$$$\pi^{\star}(s)=argmax_{a}\sum_{s^{\prime}}T(s, a, s^{\prime})U(s^{\prime})$$$$U(s)=R(s)+\gamma \max_{a}\sum_{s^{\prime}}T(s, a, s^{\prime})U(s^{\prime})$$Above is the Bellman Equation.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Lecture 11 Recap]]></title>
      <url>%2F2017%2F04%2F10%2Fcs231n-Lecture-11-Recap%2F</url>
      <content type="text"><![CDATA[Working with CNNs in practiceMaking the most of your dataData augmentationTransfer learningAll about convolutionsHow to arrange themHow to compute them fastImplementation detailsGPU / CPU, bottlenecks, ditributed trainingData AugmentationHorizontal flipsRandom crops/scalesTraining: sample random crops /scalesResNet:Pick random L in range [256, 480]Resize training image, short side = LSample random 224 x 224 patchTesting: average a fixed set of cropsResNet:Resize image at 5 scales: {224, 256, 384, 480, 640}For each size, use 10 224 x 224 crops: 4 corners + center, + flipsColor jitterSimple:Randomly jitter contrastComplex:Apply PCA to all [R, G, B] pixels in training setSample a “color offset” along principal component directionsAdd offset to all pixels of a training image(As seen in [Krizhevsky et al. 2012], ResNet, etc)Transfer Learning“You need a lot of a data if you want to train/use CNNs”some tricks:very similar datasetvery different datasetvery little dataUse Linear Classifer on top layerTry linear classifer from different stagesquite a lot of dataFinetune a few layersFinetune a larger number of layersAll about ConvolutionsHow to stack themReplace large convolutions (5 x 5, 7 x 7) with stacks of 3 x 3 convolutions1 x 1 “bottleneck” convolutions are very efficientCan factor N x N convolutions into 1 x N and N x 1All of the above give fewer parameters, less compute, more nonlinearityHow to compute themim2colBLASFFTCompute FFT of weights: F(W)Compute FFT of image: F(X)Compute elementwise product: F(W) ○ F(X)Compute inverse FFT: Y = F-1(F(W) ○ F(X))FFT convolutions get a big speedup for larger filtersNot much speedup for 3x3 filters =(Fast algorithmsStrassen’s AlgorithmAnd so on…Implementation DetailsGPUs much faster than CPUsDistributed training is sometimes usedNot needed for small problemsBe aware of bottlenecks: CPU / GPU, CPU / diskLow precison makes things faster and still works32 bit is standard now, 16 bit soonIn the future: binary nets?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dynet xor demo [python version]]]></title>
      <url>%2F2017%2F04%2F09%2FDynet-xor-demo-python-version%2F</url>
      <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546import dynet as dyimport random# Parameters of the model and trainingHIDDEN_SIZE = 20NUM_EPOCHS = 20# Define the model and SGD optimizermodel = dy.Model()W_xh_p = model.add_parameters((HIDDEN_SIZE, 2))b_h_p = model.add_parameters(HIDDEN_SIZE)W_hy_p = model.add_parameters((1, HIDDEN_SIZE))b_y_p = model.add_parameters(1)trainer = dy.SimpleSGDTrainer(model)# Define the training data, consisting of (x,y) tuplesdata = [([1,1],1), ([-1,1],-1), ([1,-1],-1), ([-1,-1],1)]# Define the function we would like to calculatedef calc_function(x): dy.renew_cg() w_xh = dy.parameter(W_xh_p) b_h = dy.parameter(b_h_p) W_hy = dy.parameter(W_hy_p) b_y = dy.parameter(b_y_p) x_val = dy.inputVector(x) h_val = dy.tanh(w_xh * x_val + b_h) y_val = W_hy * h_val + b_y return y_val# Perform trainingfor epoch in range(NUM_EPOCHS): epoch_loss = 0 random.shuffle(data) for x, ystar in data: y = calc_function(x) loss = dy.squared_distance(y, dy.scalarInput(ystar)) epoch_loss += loss.value() loss.backward() trainer.update() print("Epoch %d: loss=%f" % (epoch, epoch_loss))# Print results of predictionfor x, ystar in data: y = calc_function(x) print("%r -&gt; %f" % (x, y.value()))Output:123456789101112131415161718192021222324252627[dynet] random seed: 1174664263[dynet] allocating memory: 512MB[dynet] memory allocation done.Epoch 0: loss=12.391680Epoch 1: loss=8.196088Epoch 2: loss=8.103037Epoch 3: loss=8.636450Epoch 4: loss=7.573008Epoch 5: loss=4.910318Epoch 6: loss=3.079966Epoch 7: loss=1.328273Epoch 8: loss=1.171368Epoch 9: loss=0.515850Epoch 10: loss=1.885216Epoch 11: loss=0.568994Epoch 12: loss=0.278629Epoch 13: loss=0.025215Epoch 14: loss=0.018466Epoch 15: loss=0.055305Epoch 16: loss=0.014131Epoch 17: loss=0.010476Epoch 18: loss=0.003893Epoch 19: loss=0.003332[1, 1] -&gt; 1.049703[-1, 1] -&gt; -0.996379[1, -1] -&gt; -0.974599[-1, -1] -&gt; 0.995763]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Installing the Python Dynet module]]></title>
      <url>%2F2017%2F04%2F09%2FInstalling-the-Python-Dynet-module%2F</url>
      <content type="text"><![CDATA[Installing the Python Dynet module(for instructions on installing on a computer with GPU, see below)Python bindings to DyNet are supported for both Python 2.x and 3.x.TL;DR(see below for the details)123456789101112131415161718192021222324# Installing Python DyNet:pip install cython # if you don&apos;t have it already.mkdir dynet-basecd dynet-base# getting dynet and eigengit clone https://github.com/clab/dynet.githg clone https://bitbucket.org/eigen/eigen -r 346ecdb # -r NUM specified a known working revisioncd dynetmkdir buildcd build# without GPU support:cmake .. -DEIGEN3_INCLUDE_DIR=../../eigen -DPYTHON=`which python`# or with GPU support:cmake .. -DEIGEN3_INCLUDE_DIR=../../eigen -DPYTHON=`which python` -DBACKEND=cudamake -j 2 # replace 2 with the number of available corescd pythonpython setup.py install # or `python setup.py install --user` for a user-local install.# this should suffice, but on some systems you may need to add the following line to your# init files in order for the compiled .so files be accessible to Python.# /path/to/dynet/build/dynet is the location in which libdynet.dylib resides.export DYLD_LIBRARY_PATH=/path/to/dynet/build/dynet/:$DYLD_LIBRARY_PATHDetailed InstructionsFirst, get DyNet:1234567cd $HOMEmkdir dynet-basecd dynet-basegit clone https://github.com/clab/dynet.gitcd dynetgit submodule init # To be consistent with DyNet&apos;s installation instructions.git submodule update # To be consistent with DyNet&apos;s installation instructions.Then get Eigen:123cd $HOMEcd dynet-basehg clone https://bitbucket.org/eigen/eigen/ -r 346ecdb(-r NUM specifies a known working revision of Eigen. You can remove this in order to get the bleeding edge Eigen, with the risk of some compile breaks, and the possible benefit of added optimizations.)We also need to make sure the cython module is installed. (you can replace pip with your favorite package manager, such as conda, or install within a virtual environment)1pip install cythonTo simplify the following steps, we can set a bash variable to hold where we have saved the main directories of DyNet and Eigen. In case you have gotten DyNet and Eigen differently from the instructions above and saved them in different location(s), these variables will be helpful:12PATH_TO_DYNET=$HOME/dynet-base/dynet/PATH_TO_EIGEN=$HOME/dynet-base/eigen/Compile DyNet.This is pretty much the same process as compiling DyNet, with the addition of the -DPYTHON= flag, pointing to the location of your Python interpreter.If Boost is installed in a non-standard location, you should add the corresponding flags to the cmake commandline, see the DyNet installation instructions page.123456cd $PATH_TO_DYNETPATH_TO_PYTHON=`which python`mkdir buildcd buildcmake .. -DEIGEN3_INCLUDE_DIR=$PATH_TO_EIGEN -DPYTHON=$PATH_TO_PYTHONmake -j 2Assuming that the cmake command found all the needed libraries and didn’t fail, the make command will take a while, and compile DyNet as well as the Python bindings. You can change make -j 2 to a higher number, depending on the available cores you want to use while compiling.You now have a working Python binding inside of build/dynet. To verify this is working:12cd $PATH_TO_DYNET/build/pythonpythonthen, within Python:123import dynet as dyprint dy.__version__model = dy.Model()In order to install the module so that it is accessible from everywhere in the system, run the following:12cd $PATH_TO_DYNET/build/pythonpython setup.py install --userThe --user switch will install the module in your local site-packages, and works without root privileges. To install the module to the system site-packages (for all users), or to the current virtualenv (if you are on one), run python setup.py installwithout this switch.You should now have a working python binding (the dynet module).Note however that the installation relies on the compiled DyNet library being in $PATH_TO_DYNET/build/dynet, so make sure not to move it from there.Now, check that everything works:1234cd $PATH_TO_DYNETcd examples/pythonpython xor.pypython rnnlm.py rnnlm.pyAlternatively, if the following script works for you, then your installation is likely to be working:12from dynet import *model = Model()If it doesn’t work and you get an error similar to the following:123ImportError: dlopen(/Users/sneharajana/.python-eggs/dyNET-0.0.0-py2.7-macosx-10.11-intel.egg-tmp/_dynet.so, 2): Library not loaded: @rpath/libdynet.dylibReferenced from: /Users/sneharajana/.python-eggs/dyNET-0.0.0-py2.7-macosx-10.11-intel.egg-tmp/_dynet.soReason: image not found``then you may need to run the following (and add it to your shell init files):export DYLD_LIBRARY_PATH=/path/to/dynet/build/dynet/:$DYLD_LIBRARY_PATHUsageThere are two ways to import the dynet module :1import dynetimports dynet and automatically initializes the global dynet parameters with the command line arguments (see the documentation). The amount of memory allocated, GPU/CPU usage is fixed from there on.123import _dynet# orimport _gdynet # For GPUImports dynet for CPU (resp. GPU) and doesn’t initialize the global parameters. These must be initialized manually before using dynet, using one of the following :123# Same as import dynet as dyimport _dynet as dydy.init()123456789101112131415# Same as import dynet as dyimport _dynet as dy# Declare a DynetParams objectdyparams = dy.DynetParams()# Fetch the command line arguments (optional)dyparams.from_args()# Set some parameters manualy (see the command line arguments documentation)dyparams.set_mem(2048)dyparams.set_random_seed(666)dyparams.set_weight_decay(1e-7)dyparams.set_shared_parameters(False)dyparams.set_requested_gpus(1)dyparams.set_gpu_mask([0,1,1,0])# Initialize with the given parametersdyparams.init() # or init_from_params(dyparams)Anaconda SupportAnaconda is a popular package management system for Python. DyNet can be used from within an Anaconda environment, but be sure to activate the environmentsource activate my_environment_namethen install some necessary packages as follows:conda install gcc cmake boost cythonAfter this, the build process should be the same as normal.Note that on some conda environments, people have reported build errors related to the interaction between the icu and boost packages. If you encounter this, try the solution in this comment.Windows SupportYou can also use Python on Windows by following similar steps to the above. For simplicity, we recommend using a Python distribution that already has Cython installed. The following has been tested to work:Install WinPython 2.7.10 (comes with Cython already installed).Run CMake as above with -DPYTHON=/path/to/your/python.exe.Open a command prompt and set VS90COMNTOOLS to the path to your Visual Studio “Common7/Tools” directory. One easy way to do this is a command such as:1set VS90COMNTOOLS=%VS140COMNTOOLS%Open dynet.sln from this command prompt and build the “Release” version of the solution.Follow the rest of the instructions above for testing the build and installing it for other usersNote, currently only the Release version works.GPU/MKL SupportInstalling/running on GPUFor installing on a computer with GPU, first install CUDA. The following instructions assume CUDA is installed.The installation process is pretty much the same, while adding the -DBACKEND=cuda flag to the cmake stage:1cmake .. -DEIGEN3_INCLUDE_DIR=$PATH_TO_EIGEN -DPYTHON=$PATH_TO_PYTHON -DBACKEND=cuda(if CUDA is installed in a non-standard location and cmake cannot find it, you can specify also -DCUDA_TOOLKIT_ROOT_DIR=/path/to/cuda.)Now, build the Python modules (as above, we assume Cython is installed):After running make -j 2, you should have the files _dynet.so and _gdynet.so in the build/python folder.As before, cd build/python followed by python setup.py install --user will install the module.In order to use the GPU support, you can either:Use import _gdynet as dy instead of import dynet as dyOr, (preferred), import dynet as usual, but use the commandline switch --dynet-gpu or the GPU switches detailed herewhen invoking the program. This option lets the same code work with either the GPU or the CPU version depending on how it is invoked.Running with MKLIf you’ve built DyNet to use MKL (using -DMKL or -DMKL_ROOT), Python sometimes has difficulty finding the MKL shared libraries. You can try setting LD_LIBRARY_PATH to point to your MKL library directory. If that doesn’t work, try setting the following environment variable (supposing, for example, your MKL libraries are located at /opt/intel/mkl/lib/intel64):1export LD_PRELOAD=/opt/intel/mkl/lib/intel64/libmkl_def.so:/opt/intel/mkl/lib/intel64/libmkl_avx2.so:/opt/intel/mkl/libSome Errors and correspond Solutions12345678910111213import dynet as dy-------------------------------------------Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;dynet.py&quot;, line 17, in &lt;module&gt; from _dynet import *ImportError: /home/ewan/anaconda2/lib/libstdc++.so.6: version `GLIBCXX_3.4.20&apos; not found (required by /home/ewan/dynet-base/dynet/build/dynet/libdynet.so)-------------------------------------------Solution:conda install libgcc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[lda for news classification]]></title>
      <url>%2F2017%2F03%2F26%2Flda-for-news-classification%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[netease news spider]]></title>
      <url>%2F2017%2F03%2F26%2Fnetease-news-spider%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[LDA]]></title>
      <url>%2F2017%2F03%2F24%2FLDA%2F</url>
      <content type="text"><![CDATA[Scikit-learn example1%matplotlib inlineTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet AllocationThis is an example of applying Non-negative Matrix Factorization and Latent Dirichlet Allocation on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).The default parameters (n_samples / n_features / n_topics) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# Author: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;# Lars Buitinck# Chyi-Kwei Yau &lt;chyikwei.yau@gmail.com&gt;# License: BSD 3 clausefrom __future__ import print_functionfrom time import timefrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizerfrom sklearn.decomposition import NMF, LatentDirichletAllocationfrom sklearn.datasets import fetch_20newsgroupsn_samples = 2000n_features = 1000n_topics = 10n_top_words = 20def print_top_words(model, feature_names, n_top_words): for topic_idx, topic in enumerate(model.components_): print("Topic #%d:" % topic_idx) print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])) print()# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics# to filter out useless terms early on: the posts are stripped of headers,# footers and quoted replies, and common English words, words occurring in# only one document or in at least 95% of the documents are removed.print("Loading dataset...")t0 = time()dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))data_samples = dataset.data[:n_samples]print("done in %0.3fs." % (time() - t0))# Use tf-idf features for NMF.print("Extracting tf-idf features for NMF...")tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')t0 = time()tfidf = tfidf_vectorizer.fit_transform(data_samples)print("done in %0.3fs." % (time() - t0))# Use tf (raw term count) features for LDA.print("Extracting tf features for LDA...")tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')t0 = time()tf = tf_vectorizer.fit_transform(data_samples)print("done in %0.3fs." % (time() - t0))# Fit the NMF modelprint("Fitting the NMF model with tf-idf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features))t0 = time()nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)print("done in %0.3fs." % (time() - t0))print("\nTopics in NMF model:")tfidf_feature_names = tfidf_vectorizer.get_feature_names()print_top_words(nmf, tfidf_feature_names, n_top_words)print("Fitting LDA models with tf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features))lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0)t0 = time()lda.fit(tf)print("done in %0.3fs." % (time() - t0))print("\nTopics in LDA model:")tf_feature_names = tf_vectorizer.get_feature_names()print_top_words(lda, tf_feature_names, n_top_words)1Loading dataset...​1No handlers could be found for logger "sklearn.datasets.twenty_newsgroups"​123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354done in 691.647s.Extracting tf-idf features for NMF...done in 0.454s.Extracting tf features for LDA...done in 0.416s.Fitting the NMF model with tf-idf features, n_samples=2000 and n_features=1000...done in 0.367s.Topics in NMF model:Topic #0:just people don think like know time good make way really say right ve want did ll new use yearsTopic #1:windows use dos using window program os drivers application help software pc running ms screen files version card code workTopic #2:god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religionTopic #3:thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video needTopic #4:car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used boughtTopic #5:edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact bloodTopic #6:file problem files format win sound ftp pub read save site help image available create copy running memory self versionTopic #7:game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teamsTopic #8:drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internalTopic #9:key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standardFitting LDA models with tf features, n_samples=2000 and n_features=1000...done in 2.169s.Topics in LDA model:Topic #0:edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mitTopic #1:don like just know think ve way use right good going make sure ll point got need really time doesnTopic #2:christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believeTopic #3:drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16Topic #4:hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drugTopic #5:god people does just good don jesus say israel way life know true fact time law want believe make thinkTopic #6:55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16Topic #7:car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performanceTopic #8:people said did just didn know time like went think children came come don took years say dead told startedTopic #9:key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology​1data_samples[0]1u"Well i'm not sure about the story nad it did seem biased. What\nI disagree with is your statement that the U.S. Media is out to\nruin Israels reputation. That is rediculous. The U.S. media is\nthe most pro-israeli media in the world. Having lived in Europe\nI realize that incidences such as the one described in the\nletter have occured. The U.S. media as a whole seem to try to\nignore them. The U.S. is subsidizing Israels existance and the\nEuropeans are not (at least not to the same degree). So I think\nthat might be a reason they report more clearly on the\natrocities.\n\tWhat is a shame is that in Austria, daily reports of\nthe inhuman acts commited by Israeli soldiers and the blessing\nreceived from the Government makes some of the Holocaust guilt\ngo away. After all, look how the Jews are treating other races\nwhen they got power. It is unfortunate.\n"1tfidf_vectorizer.get_feature_names()[-10:]12345678910[u'worth', u'wouldn', u'write', u'written', u'wrong', u'xfree86', u'year', u'years', u'yes', u'young']1tfidf.toarray().shape1(2000L, 1000L)1dataset.target_names1234567891011121314151617181920['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']1lda.transform(tf)[1934]12array([ 0.3587206 , 0.00227337, 0.00227317, 0.50146046, 0.00227288, 0.12390701, 0.00227282, 0.00227329, 0.00227343, 0.00227299])ExtrasSome materials can find from Github.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Excel merge the same value that in a column to a cell]]></title>
      <url>%2F2017%2F03%2F16%2FExcel-merge-the-same-value-that-in-a-column-to-a-cell%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Installing ptproc on Ubuntu 16.04 LTS]]></title>
      <url>%2F2017%2F03%2F16%2FInstalling-ptproc-on-Ubuntu-16-04-LTS%2F</url>
      <content type="text"><![CDATA[想对事件数据用点过程进行建模，为了不用重复造轮子，所以找到了一个R包ptproc，但是安装时报错1234install.packages("ptproc")-------------------------------package ‘ptproc’ is not available (as a binary package for R version 3.2.3)既然仓库里没有，那就只好用源码安装了，源码下载地址, 但是依旧报错：123456789101112131415&gt; install.packages("ptproc", repos="http://www.biostat.jhsph.edu/~rpeng/software", type="source")trying URL 'http://www.biostat.jhsph.edu/~rpeng/software/src/contrib/ptproc_1.5-1.tar.gz'Content type 'application/x-gzip' length 282002 bytes (275 KB)opened URL==================================================downloaded 275 KB * installing *source* package ‘ptproc’ ...ERROR: a 'NAMESPACE' file is required* removing ‘/Library/Frameworks/R.framework/Versions/3.1/Resources/library/ptproc’ The downloaded source packages are in ‘/private/var/folders/0b/qdw3f3zn0gq5yy2cjjpm8cgw0000gn/T/RtmpuW1EPA/downloaded_packages’Warning message:In install.packages("ptproc", repos = "http://www.biostat.jhsph.edu/~rpeng/software", : installation of package ‘ptproc’ had non-zero exit status看来只有手动添加一个NAMESPACE,12345cd ptprocecho &apos;exportPattern( &quot;.&quot; )&apos; &gt; NAMESPACEcd ../rm ptproc_1.5-1.tar.gztar cvzf ptproc/ ptproc_1.5-1.tar.gz继续源码安装：1R CMD INSTALL -l &lt;ourRlibrarylocation&gt; &lt;path where I saved the packagename.tar.gz file&gt;Got it.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note Ch12]]></title>
      <url>%2F2017%2F03%2F09%2Fpython-data-analysis-learning-note-Ch12%2F</url>
      <content type="text"><![CDATA[Numpy高级应用123456from __future__ import divisionfrom numpy.random import randnfrom pandas import Seriesimport numpy as npnp.set_printoptions(precision=4)import sys12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"ndarray对象的内部机制NumPy 数据类型体系检测类型是否是某种类型的子类1234ints = np.ones(10, dtype=np.uint16)floats = np.ones(10, dtype=np.float32)np.issubdtype(ints.dtype, np.integer)np.issubdtype(floats.dtype, np.floating)True True 输出某种类型的所有父类1np.float64.mro()[numpy.float64, numpy.floating, numpy.inexact, numpy.number, numpy.generic, float, object] 高级数组操作数组重塑123arr = np.arange(8)arrarr.reshape((4, 2))array([0, 1, 2, 3, 4, 5, 6, 7]) array([[0, 1], [2, 3], [4, 5], [6, 7]]) 1arr.reshape((4, 2)).reshape((2, 4))array([[0, 1, 2, 3], [4, 5, 6, 7]]) -1代表自动选择合适的维度12arr = np.arange(15)arr.reshape((5, -1))array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) 用其他数组的shape进行重塑123other_arr = np.ones((3, 5))other_arr.shapearr.reshape(other_arr.shape)(3, 5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 拉直123arr = np.arange(15).reshape((5, 3))arrarr.ravel()array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) 会产生一个副本1arr.flatten()array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) C vs. Fortran 顺序1234arr = np.arange(12).reshape((3, 4))arrarr.ravel()arr.ravel('F')array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) array([ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11]) 数组的合并以及拆分1234arr1 = np.array([[1, 2, 3], [4, 5, 6]])arr2 = np.array([[7, 8, 9], [10, 11, 12]])np.concatenate([arr1, arr2], axis=0)np.concatenate([arr1, arr2], axis=1)array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]]) 更方便的方法12np.vstack((arr1, arr2))np.hstack((arr1, arr2))array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]]) 1234567from numpy.random import randnarr = randn(5, 2)arrfirst, second, third = np.split(arr, [1, 3])firstsecondthirdarray([[ 0.9659, 1.3079], [-1.7632, 0.0904], [-0.6033, 0.2266], [-0.4417, -1.8609], [-1.2463, -0.6249]]) array([[ 0.9659, 1.3079]]) array([[-1.7632, 0.0904], [-0.6033, 0.2266]]) array([[-0.4417, -1.8609], [-1.2463, -0.6249]]) 堆叠辅助类更…简洁…12345arr = np.arange(6)arr1 = arr.reshape((3, 2))arr2 = randn(3, 2)np.r_[arr1, arr2]np.c_[np.r_[arr1, arr2], arr]array([[ 0. , 1. ], [ 2. , 3. ], [ 4. , 5. ], [ 0.0376, 1.8236], [ 0.9025, -0.053 ], [-0.6849, 1.6728]]) array([[ 0. , 1. , 0. ], [ 2. , 3. , 1. ], [ 4. , 5. , 2. ], [ 0.0376, 1.8236, 3. ], [ 0.9025, -0.053 , 4. ], [-0.6849, 1.6728, 5. ]]) 1np.c_[1:6, -10:-5]array([[ 1, -10], [ 2, -9], [ 3, -8], [ 4, -7], [ 5, -6]]) 元素的重复操作: tile and repeat元素级重复12arr = np.arange(3)arr.repeat(3)array([0, 0, 0, 1, 1, 1, 2, 2, 2]) 指定重复次数1arr.repeat([2, 3, 4])array([0, 0, 1, 1, 1, 2, 2, 2, 2]) 多维数组需要指定axis123arr = randn(2, 2)arrarr.repeat(2, axis=0)array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142], [-0.4628, 1.1142], [ 0.3637, 0.4341], [ 0.3637, 0.4341]]) 12arr.repeat([2, 3], axis=0)arr.repeat([2, 3], axis=1)array([[-0.4628, 1.1142], [-0.4628, 1.1142], [ 0.3637, 0.4341], [ 0.3637, 0.4341], [ 0.3637, 0.4341]]) array([[-0.4628, -0.4628, 1.1142, 1.1142, 1.1142], [ 0.3637, 0.3637, 0.4341, 0.4341, 0.4341]]) 块级重复12arrnp.tile(arr, 2)array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341]]) 123arrnp.tile(arr, (2, 1))np.tile(arr, (3, 2))array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142], [ 0.3637, 0.4341], [-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341], [-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341], [-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341]]) 花式索引的等价函数: take and put123arr = np.arange(10) * 100inds = [7, 1, 2, 6]arr[inds]array([700, 100, 200, 600]) 12345arr.take(inds)arr.put(inds, 42)arrarr.put(inds, [40, 41, 42, 43])arrarray([700, 100, 200, 600]) array([ 0, 42, 42, 300, 400, 500, 42, 42, 800, 900]) array([ 0, 41, 42, 300, 400, 500, 43, 40, 800, 900]) 1234inds = [2, 0, 2, 1]arr = randn(2, 4)arrarr.take(inds, axis=1)array([[ 0.2772, -1.3059, -1.4607, -0.4856], [ 1.5585, -0.4521, -1.6259, -1.6644]]) array([[-1.4607, 0.2772, -1.4607, -1.3059], [-1.6259, 1.5585, -1.6259, -0.4521]]) 广播每一个元素都乘以4123arr = np.arange(5)arrarr * 4array([0, 1, 2, 3, 4]) array([ 0, 4, 8, 12, 16]) 每一维对应减去均值12345arr = randn(4, 3)arr.mean(0)demeaned = arr - arr.mean(0)demeaneddemeaned.mean(0)array([-0.1556, 0.3494, -0.2545]) array([[-0.3753, 0.5353, 1.3534], [-0.4282, 0.5606, 0.8935], [-0.0956, -0.9767, -1.2444], [ 0.899 , -0.1192, -1.0024]]) array([ -5.5511e-17, -1.3878e-17, 0.0000e+00]) 12345arrrow_means = arr.mean(1)row_means.reshape((4, 1))demeaned = arr - row_means.reshape((4, 1))demeaned.mean(1)array([[-0.5308, 0.8848, 1.0989], [-0.5837, 0.91 , 0.639 ], [-0.2511, -0.6273, -1.4989], [ 0.7434, 0.2302, -1.2569]]) array([[ 0.4843], [ 0.3218], [-0.7924], [-0.0944]]) array([ 7.4015e-17, 0.0000e+00, 0.0000e+00, 0.0000e+00]) 沿其他轴向广播维度不对应1arr - arr.mean(1)--------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-31-7b87b85a20b2&gt; in &lt;module&gt;() ----&gt; 1 arr - arr.mean(1) ValueError: operands could not be broadcast together with shapes (4,3) (4,) 1arr - arr.mean(1).reshape((4, 1))array([[-1.0151, 0.4005, 0.6146], [-0.9055, 0.5882, 0.3173], [ 0.5413, 0.1652, -0.7065], [ 0.8378, 0.3246, -1.1625]]) 1234arr = np.zeros((4, 4))arr_3d = arr[:, np.newaxis]arr_3darr_3d.shapearray([[[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]]]) (4, 1, 4) 1234arr_1d = np.random.normal(size=3)arr_1darr_1d[:, np.newaxis]arr_1d[np.newaxis, :]array([-1.1083, 0.5576, 1.2277]) array([[-1.1083], [ 0.5576], [ 1.2277]]) array([[-1.1083, 0.5576, 1.2277]]) 123456arr = randn(3, 4, 5)arrdepth_means = arr.mean(2)depth_meansdemeaned = arr - depth_means[:, :, np.newaxis]demeaned.mean(2)array([[[-1.9966, -0.2431, -0.992 , 0.8283, -0.5073], [-0.3938, -0.1332, -0.7427, 0.3094, -0.9241], [ 1.1069, -0.5383, -0.9288, 0.0233, -0.4678], [-1.2015, 0.6905, 1.6706, -0.1703, -1.3975]], [[-0.3048, -1.7181, -0.189 , 0.6263, 1.1194], [ 0.0823, -0.7132, -0.5162, 1.5305, -1.199 ], [ 0.5777, 1.2935, 0.1547, -1.3637, 0.4251], [ 0.4923, 1.4004, 0.3646, 0.1594, -0.7334]], [[ 1.3836, -0.5313, 0.2826, 0.4739, -1.3435], [-1.141 , -0.3084, 1.1364, 1.1326, 0.3064], [-0.9692, 1.0229, -0.0246, 1.4484, -1.137 ], [ 1.7033, -1.8358, 1.2087, -0.5463, 0.5904]]]) array([[-0.5822, -0.3769, -0.1609, -0.0816], [-0.0932, -0.1631, 0.2174, 0.3367], [ 0.0531, 0.2252, 0.0681, 0.2241]]) array([[ 8.8818e-17, 0.0000e+00, -4.4409e-17, -8.8818e-17], [ 0.0000e+00, 0.0000e+00, 2.7756e-17, 8.8818e-17], [ 4.4409e-17, 5.5511e-17, 4.4409e-17, 0.0000e+00]]) 1234567def demean_axis(arr, axis=0): means = arr.mean(axis) # This generalized things like [:, :, np.newaxis] to N dimensions indexer = [slice(None)] * arr.ndim # like : indexer[axis] = np.newaxis return arr - means[indexer]通过广播设置数组的值123arr = np.zeros((4, 3))arr[:] = 5arrarray([[ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.]]) 12345col = np.array([1.28, -0.42, 0.44, 1.6])arr[:] = col[:, np.newaxis]arrarr[:2] = [[-1.37], [0.509]]arrarray([[ 1.28, 1.28, 1.28], [-0.42, -0.42, -0.42], [ 0.44, 0.44, 0.44], [ 1.6 , 1.6 , 1.6 ]]) array([[-1.37 , -1.37 , -1.37 ], [ 0.509, 0.509, 0.509], [ 0.44 , 0.44 , 0.44 ], [ 1.6 , 1.6 , 1.6 ]]) ufunc高级应用ufunc实例方法reduce通过一系列的二元运算对其值进行聚合（可指明轴向）123arr = np.arange(10)np.add.reduce(arr)arr.sum()45 45 1np.random.seed(12346)这里聚合的是逻辑与操作123456arr = randn(5, 5)arrarr[::2].sort(1) # sort a few rowsarrarr[:, :-1] &lt; arr[:, 1:]np.logical_and.reduce(arr[:, :-1] &lt; arr[:, 1:], axis=1)array([[-0.7066, 0.4268, -0.2776, -0.8283, -2.7628], [ 0.9835, 0.4378, -0.8496, 0.7188, 0.7329], [ 0.5047, -0.7893, 0.5392, 1.2907, 0.8676], [ 0.4113, 0.4459, -0.3172, -1.0493, 1.3459], [ 0.356 , -0.0915, -0.535 , -0.036 , -0.2591]]) array([[-2.7628, -0.8283, -0.7066, -0.2776, 0.4268], [ 0.9835, 0.4378, -0.8496, 0.7188, 0.7329], [-0.7893, 0.5047, 0.5392, 0.8676, 1.2907], [ 0.4113, 0.4459, -0.3172, -1.0493, 1.3459], [-0.535 , -0.2591, -0.0915, -0.036 , 0.356 ]]) array([[ True, True, True, True], [False, False, True, True], [ True, True, True, True], [ True, False, False, True], [ True, True, True, True]], dtype=bool) array([ True, False, True, False, True], dtype=bool) 相对于reduce只输出最后结果，accumulate保留中间结果12arr = np.arange(15).reshape((3, 5))np.add.accumulate(arr, axis=1)array([[ 0, 1, 3, 6, 10], [ 5, 11, 18, 26, 35], [10, 21, 33, 46, 60]], dtype=int32) outer计算两个数组的叉积123arr = np.arange(3).repeat([1, 2, 2])arrnp.multiply.outer(arr, np.arange(5))array([0, 1, 1, 2, 2]) array([[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 2, 4, 6, 8]]) outer输出结果的维度是输入两个数组的维度之和12result = np.subtract.outer(randn(3, 4), randn(5))result.shape(3, 4, 5) 12arr = np.arange(10)np.add.reduceat(arr, [0, 5, 8])array([10, 18, 17], dtype=int32) 123arr = np.multiply.outer(np.arange(4), np.arange(5))arrnp.add.reduceat(arr, [0, 2, 4], axis=1)array([[ 0, 0, 0, 0, 0], [ 0, 1, 2, 3, 4], [ 0, 2, 4, 6, 8], [ 0, 3, 6, 9, 12]]) array([[ 0, 0, 0], [ 1, 5, 4], [ 2, 10, 8], [ 3, 15, 12]], dtype=int32) 自定义 ufuncs两种不同的调用方式1234def add_elements(x, y): return x + yadd_them = np.frompyfunc(add_elements, 2, 1) # 2 input and 1 outputadd_them(np.arange(8), np.arange(8))array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object) 12add_them = np.vectorize(add_elements, otypes=[np.float64])add_them(np.arange(8), np.arange(8))array([ 0., 2., 4., 6., 8., 10., 12., 14.]) 自己实现的还是比不上内置优化过的函数123arr = randn(10000)%timeit add_them(arr, arr)%timeit np.add(arr, arr)100 loops, best of 3: 1.81 ms per loop The slowest run took 16.51 times longer than the fastest. This could mean that an intermediate result is being cached. 100000 loops, best of 3: 3.65 µs per loop 结构化和记录式数组123dtype = [('x', np.float64), ('y', np.int32)]sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)sarrarray([(1.5, 6), (3.141592653589793, -2)], dtype=[(&apos;x&apos;, &apos;&lt;f8&apos;), (&apos;y&apos;, &apos;&lt;i4&apos;)]) 12sarr[0]sarr[0]['y'](1.5, 6) 6 1sarr['x']array([ 1.5 , 3.1416]) 嵌套dtype和多维字段123dtype = [('x', np.int64, 3), ('y', np.int32)]arr = np.zeros(4, dtype=dtype)arrarray([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)], dtype=[(&apos;x&apos;, &apos;&lt;i8&apos;, (3,)), (&apos;y&apos;, &apos;&lt;i4&apos;)]) 1arr[0]['x']array([0, 0, 0], dtype=int64) 1arr['x']array([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=int64) 12345dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)data['x']data['y']data['x']['a']array([(1.0, 2.0), (3.0, 4.0)], dtype=[(&apos;a&apos;, &apos;&lt;f8&apos;), (&apos;b&apos;, &apos;&lt;f4&apos;)]) array([5, 6]) array([ 1., 3.]) 更多有关排序的话题123arr = randn(6)arr.sort()arrarray([-1.3918, -0.2089, 0.2316, 0.728 , 0.8356, 1.9956]) 1234arr = randn(3, 5)arrarr[:, 0].sort() # Sort first column values in-placearrarray([[ -2.9812e-01, 1.2037e+00, -1.5768e-02, 7.4395e-01, 8.6880e-01], [ -4.2865e-01, 7.1886e-01, -1.4510e+00, 1.0510e-01, -1.7942e+00], [ -2.8792e-04, 6.1168e-01, -9.1210e-02, -1.2799e+00, -4.0230e-02]]) array([[ -4.2865e-01, 1.2037e+00, -1.5768e-02, 7.4395e-01, 8.6880e-01], [ -2.9812e-01, 7.1886e-01, -1.4510e+00, 1.0510e-01, -1.7942e+00], [ -2.8792e-04, 6.1168e-01, -9.1210e-02, -1.2799e+00, -4.0230e-02]]) 1234arr = randn(5)arrnp.sort(arr)arrarray([-0.9699, -0.5626, 1.1172, 0.2791, -1.1148]) array([-1.1148, -0.9699, -0.5626, 0.2791, 1.1172]) array([-0.9699, -0.5626, 1.1172, 0.2791, -1.1148]) 1234arr = randn(3, 5)arrarr.sort(axis=1)arrarray([[ 0.2266, 0.3405, 2.6439, -1.6262, -0.3976], [-1.4821, 1.068 , -0.252 , -0.9331, 2.2639], [-0.2311, 1.1472, 0.9287, -0.9023, 1.1761]]) array([[-1.6262, -0.3976, 0.2266, 0.3405, 2.6439], [-1.4821, -0.9331, -0.252 , 1.068 , 2.2639], [-0.9023, -0.2311, 0.9287, 1.1472, 1.1761]]) 1arr[:, ::-1]array([[ 2.6439, 0.3405, 0.2266, -0.3976, -1.6262], [ 2.2639, 1.068 , -0.252 , -0.9331, -1.4821], [ 1.1761, 1.1472, 0.9287, -0.2311, -0.9023]]) 间接排序: argsort and lexsort1234values = np.array([5, 0, 1, 3, 2])indexer = values.argsort()indexervalues[indexer]array([1, 2, 4, 3, 0], dtype=int64) array([0, 1, 2, 3, 5]) 1234arr = randn(3, 5)arr[0] = valuesarrarr[:, arr[0].argsort()]array([[ 5. , 0. , 1. , 3. , 2. ], [ 0.422 , 0.1187, 1.1352, 1.4363, -1.2487], [ 0.1909, -1.0984, 0.7886, -0.5827, 1.1592]]) array([[ 0. , 1. , 2. , 3. , 5. ], [ 0.1187, 1.1352, -1.2487, 1.4363, 0.422 ], [-1.0984, 0.7886, 1.1592, -0.5827, 0.1909]]) 1234first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])sorter = np.lexsort((first_name, last_name))zip(last_name[sorter], first_name[sorter])&lt;zip at 0x1d1284f87c8&gt; 其他排序算法12345values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])key = np.array([2, 2, 1, 1, 1])indexer = key.argsort(kind='mergesort')indexervalues.take(indexer)array([2, 3, 4, 0, 1], dtype=int64) array([&apos;1:first&apos;, &apos;1:second&apos;, &apos;1:third&apos;, &apos;2:first&apos;, &apos;2:second&apos;], dtype=&apos;&lt;U8&apos;) numpy.searchsorted: 在有序数组中查找元素12arr = np.array([0, 1, 7, 12, 15])arr.searchsorted(9)3 1arr.searchsorted([0, 8, 11, 16])array([0, 3, 3, 5], dtype=int64) 123arr = np.array([0, 0, 0, 1, 1, 1, 1])arr.searchsorted([0, 1])arr.searchsorted([0, 1], side='right')array([0, 3], dtype=int64) array([3, 7], dtype=int64) 123data = np.floor(np.random.uniform(0, 10000, size=50))bins = np.array([0, 100, 1000, 5000, 10000])dataarray([ 143., 8957., 309., 2349., 5503., 2754., 4408., 4259., 3313., 3364., 2492., 9977., 4704., 5538., 6089., 5864., 6926., 3677., 8698., 1832., 8931., 6631., 5322., 3712., 9350., 3945., 9514., 3683., 8568., 8247., 7087., 7630., 3392., 8320., 1973., 982., 1672., 7052., 6230., 3894., 1832., 9488., 755., 8522., 1858., 5417., 6162., 7517., 9827., 4458.]) 12labels = bins.searchsorted(data)labelsarray([2, 4, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 4, 4, 3, 3, 4, 2, 4, 3, 4, 4, 4, 4, 3], dtype=int64) 1Series(data).groupby(labels).mean()2 547.250000 3 3178.550000 4 7591.038462 dtype: float64 1np.digitize(data, bins)array([2, 4, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 4, 4, 3, 3, 4, 2, 4, 3, 4, 4, 4, 4, 3], dtype=int64) NumPy matrix class12345678X = np.array([[ 8.82768214, 3.82222409, -1.14276475, 2.04411587], [ 3.82222409, 6.75272284, 0.83909108, 2.08293758], [-1.14276475, 0.83909108, 5.01690521, 0.79573241], [ 2.04411587, 2.08293758, 0.79573241, 6.24095859]])X[:, 0] # one-dimensionaly = X[:, :1] # two-dimensional by slicingXyarray([ 8.8277, 3.8222, -1.1428, 2.0441]) array([[ 8.8277, 3.8222, -1.1428, 2.0441], [ 3.8222, 6.7527, 0.8391, 2.0829], [-1.1428, 0.8391, 5.0169, 0.7957], [ 2.0441, 2.0829, 0.7957, 6.241 ]]) array([[ 8.8277], [ 3.8222], [-1.1428], [ 2.0441]]) 1np.dot(y.T, np.dot(X, y))array([[ 1195.468]]) 12345Xm = np.matrix(X)ym = Xm[:, 0]Xmymym.T * Xm * ymmatrix([[ 8.8277, 3.8222, -1.1428, 2.0441], [ 3.8222, 6.7527, 0.8391, 2.0829], [-1.1428, 0.8391, 5.0169, 0.7957], [ 2.0441, 2.0829, 0.7957, 6.241 ]]) matrix([[ 8.8277], [ 3.8222], [-1.1428], [ 2.0441]]) matrix([[ 1195.468]]) 1Xm.I * Xmatrix([[ 1.0000e+00, 6.9616e-17, -4.0136e-17, 8.1258e-17], [ -2.3716e-17, 1.0000e+00, 2.2230e-17, -2.5721e-17], [ 1.0957e-16, 5.0783e-18, 1.0000e+00, 7.8658e-18], [ -5.7092e-17, -3.7777e-18, 6.2391e-18, 1.0000e+00]]) 高级数组输入输出内存映像文件12mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000))mmapmemmap([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]]) 1section = mmap[:5]1234section[:] = np.random.randn(5, 10000)mmap.flush()mmapdel mmapmemmap([[-1.273 , -0.1547, 0.7817, ..., 0.3421, 1.0272, -1.8742], [-0.3544, -3.1195, 0.1256, ..., -0.4476, 0.4863, -0.8311], [-1.1117, 0.8186, 2.3934, ..., 0.1061, 1.4123, 0.6489], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]]) 12mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))mmapmemmap([[-1.273 , -0.1547, 0.7817, ..., 0.3421, 1.0272, -1.8742], [-0.3544, -3.1195, 0.1256, ..., -0.4476, 0.4863, -0.8311], [-1.1117, 0.8186, 2.3934, ..., 0.1061, 1.4123, 0.6489], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]]) 12%xdel mmap!del mymmapNameError: name &apos;mmap&apos; is not defined C:\Users\Ewan\Downloads\pydata-book-master\mymmap The process cannot access the file because it is being used by another process. ​性能建议连续内存的重要性12345arr_c = np.ones((1000, 1000), order='C')arr_f = np.ones((1000, 1000), order='F')arr_c.flagsarr_f.flagsarr_f.flags.f_contiguous C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False C_CONTIGUOUS : False F_CONTIGUOUS : True OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False True 12%timeit arr_c.sum(1)%timeit arr_f.sum(1)1000 loops, best of 3: 848 µs per loop 1000 loops, best of 3: 582 µs per loop 1arr_f.copy('C').flagsC_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False 12arr_c[:50].flags.contiguousarr_c[:, :50].flagsTrue C_CONTIGUOUS : False F_CONTIGUOUS : False OWNDATA : False WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False 123%xdel arr_c%xdel arr_f%cd ..C:\Users\Ewan\Downloads ​其他加速手段: Cython, f2py, C12345678910from numpy cimport ndarray, float64_tdef sum_elements(ndarray[float64_t] arr): cdef Py_ssize_t i, n = len(arr) cdef float64_t result = 0 for i in range(n): result += arr[i] return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GFW Break]]></title>
      <url>%2F2017%2F03%2F08%2FGFW-Break%2F</url>
      <content type="text"><![CDATA[（Beta版本， 留待以后完善）软件下载百度网盘链接：http://pan.baidu.com/s/1gftCmd1使用方法解压解压后的目录如下：​打开软件：​界面如下：​​​这里需要填一些东西：Server IPServer PortPassword具体值（sscat.txt）我放在了网盘里，链接：http://pan.baidu.com/s/1gftCmd1启动软件：​​右键这个小飞机图标（可能你的颜色看起来不一样，是暗蓝色），会出现如下界面：勾选上第一项，然后将鼠标移到第二项上：选择PAC模式（这个模式会自动检测你所进入的网站是否需要翻墙，所以选择这个模式就可以了，如果不行的话，勾选下面的Global即可）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note Ch10]]></title>
      <url>%2F2017%2F03%2F08%2Fpython-data-analysis-learning-note-Ch10%2F</url>
      <content type="text"><![CDATA[时间序列123456789from __future__ import divisionfrom pandas import Series, DataFrameimport pandas as pdfrom numpy.random import randnimport numpy as nppd.options.display.max_rows = 12np.set_printoptions(precision=4, suppress=True)import matplotlib.pyplot as pltplt.rc('figure', figsize=(12, 4))12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inline日期和时间数据类型及工具123from datetime import datetimenow = datetime.now()nowdatetime.datetime(2017, 3, 8, 14, 47, 50, 32019) 1now.year, now.month, now.day(2017, 3, 8) 返回值（天数，秒数）12delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)deltadatetime.timedelta(926, 56700) 1delta.days926 1delta.seconds56700 timedelta 天数123from datetime import timedeltastart = datetime(2011, 1, 7)start + timedelta(12)datetime.datetime(2011, 1, 19, 0, 0) 1start - 2 * timedelta(12)datetime.datetime(2010, 12, 14, 0, 0) 字符串和datatime的相互转换1stamp = datetime(2011, 1, 3)使用str直接转换1str(stamp)&apos;2011-01-03 00:00:00&apos; 格式化转换1stamp.strftime('%Y-%m-%d')&apos;2011-01-03&apos; 逆转换12value = '2011-01-03'datetime.strptime(value, '%Y-%m-%d')datetime.datetime(2011, 1, 3, 0, 0) 批量转换12datestrs = ['7/6/2011', '8/6/2011'][datetime.strptime(x, '%m/%d/%Y') for x in datestrs][datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)] 总是写格式很麻烦，直接调用parser解析12from dateutil.parser import parseparse('2011-01-03')datetime.datetime(2011, 1, 3, 0, 0) 可以解析任意格式1parse('Jan 31, 1997 10:45 PM')datetime.datetime(1997, 1, 31, 22, 45) 指定格式1parse('6/12/2011', dayfirst=True)datetime.datetime(2011, 12, 6, 0, 0) 1datestrs[&apos;7/6/2011&apos;, &apos;8/6/2011&apos;] pandas的API12pd.to_datetime(datestrs)# note: output changed (no '00:00:00' anymore)DatetimeIndex([&apos;2011-07-06&apos;, &apos;2011-08-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=None) None也可以转换，只不过会变成缺失值12idx = pd.to_datetime(datestrs + [None])idxDatetimeIndex([&apos;2011-07-06&apos;, &apos;2011-08-06&apos;, &apos;NaT&apos;], dtype=&apos;datetime64[ns]&apos;, freq=None) 1idx[2]NaT 1pd.isnull(idx)array([False, False, True], dtype=bool) 时间序列基础将行索引变成时间类型，也就是时间戳12345from datetime import datetimedates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7), datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]ts = Series(np.random.randn(6), index=dates)ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 12type(ts)# note: output changed to "pandas.core.series.Series"pandas.core.series.Series 拥有一个特定的类型1ts.indexDatetimeIndex([&apos;2011-01-02&apos;, &apos;2011-01-05&apos;, &apos;2011-01-07&apos;, &apos;2011-01-08&apos;, &apos;2011-01-10&apos;, &apos;2011-01-12&apos;], dtype=&apos;datetime64[ns]&apos;, freq=None) 可以直接进行加法运算，相同的时间戳会进行匹配1ts + ts[::2]2011-01-02 -0.593708 2011-01-05 NaN 2011-01-07 -0.968984 2011-01-08 NaN 2011-01-10 -0.697394 2011-01-12 NaN dtype: float64 以纳秒形式存储时间戳12ts.index.dtype# note: output changed from dtype('datetime64[ns]') to dtype('&lt;M8[ns]')dtype(&apos;&lt;M8[ns]&apos;) 行索引就会变成时间戳类型123stamp = ts.index[0]stamp# note: output changed from &lt;Timestamp: 2011-01-02 00:00:00&gt; to Timestamp('2011-01-02 00:00:00')Timestamp(&apos;2011-01-02 00:00:00&apos;) 索引、选取、子集构造时间戳索引与正常索引行为一样12stamp = ts.index[2]ts[stamp]-0.4844920247591406 可以直接通过传入与行索引相匹配的时间戳进行索引1ts['1/10/2011']-0.34869693931763396 换个格式也可以，会自动转换为datatime，只要最后转换成的时间戳是相同的，任意格式都可以1ts['20110110']-0.34869693931763396 通过periods参数来指定往后顺延的时间长短123longer_ts = Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))longer_ts2000-01-01 0.871808 2000-01-02 -0.025158 2000-01-03 0.132813 2000-01-04 -2.006494 2000-01-05 -0.988423 2000-01-06 0.775930 ... 2002-09-21 -0.186519 2002-09-22 0.881745 2002-09-23 -1.335826 2002-09-24 0.418774 2002-09-25 0.970405 2002-09-26 0.636320 Freq: D, dtype: float64 时间戳的特殊之处在于可以进行年份以及月份等的选取，相当于一个多维索引1longer_ts['2001']2001-01-01 -1.799866 2001-01-02 0.499890 2001-01-03 -0.409970 2001-01-04 -0.808111 2001-01-05 -1.220433 2001-01-06 0.581235 ... 2001-12-26 -0.312186 2001-12-27 -0.804940 2001-12-28 -0.572741 2001-12-29 -0.175605 2001-12-30 0.693675 2001-12-31 -0.196274 Freq: D, dtype: float64 1longer_ts['2001-05']2001-05-01 -2.783535 2001-05-02 1.386292 2001-05-03 0.153705 2001-05-04 -0.571590 2001-05-05 -0.933012 2001-05-06 0.579244 ... 2001-05-26 0.080809 2001-05-27 0.652650 2001-05-28 0.862616 2001-05-29 -0.967580 2001-05-30 0.907069 2001-05-31 0.551137 Freq: D, dtype: float64 同样可以进行切片，只不过是按照时间的先后度量1ts[datetime(2011, 1, 7):]2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 1ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 而且切片不需要进行索引匹配，只需要指定时间范围即可切片1ts['1/6/2011':'1/11/2011']2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 dtype: float64 一个可以实现同样功能的内置方法1ts.truncate(after='1/9/2011')2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 dtype: float64 这里的freq参数指定了选取的频率，这里的是每一个星期三12345dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')long_df = DataFrame(np.random.randn(100, 4), index=dates, columns=['Colorado', 'Texas', 'New York', 'Ohio'])long_df.ix['5-2001']ColoradoTexasNew YorkOhio2001-05-020.506207-1.1162180.6565750.2126062001-05-09-1.306963-0.054373-1.165053-1.3193612001-05-160.891692-0.4639001.6422670.6449722001-05-23-0.0252832.363886-0.3679880.8278822001-05-30-1.501301-2.5345530.2563690.268207带有重复索引的时间序列直接创建时间戳索引1234dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000', '1/3/2000'])dup_ts = Series(np.arange(5), index=dates)dup_ts2000-01-01 0 2000-01-02 1 2000-01-02 2 2000-01-02 3 2000-01-03 4 dtype: int32 1dup_ts.index.is_uniqueFalse 1dup_ts['1/3/2000'] # not duplicated4 如果有重复的时间索引，则会将满足条件的全部输出1dup_ts['1/2/2000'] # duplicated2000-01-02 1 2000-01-02 2 2000-01-02 3 dtype: int32 因此可以直接根据时间戳进行索引12grouped = dup_ts.groupby(level=0)grouped.mean()2000-01-01 0 2000-01-02 2 2000-01-03 4 dtype: int32 1grouped.count()2000-01-01 1 2000-01-02 3 2000-01-03 1 dtype: int64 日期的范围、频率以及移动pandas中的时间序列一般被认为是不规则的，也就是说没有固定的频率。但是有时候需要以某种相对固定的频率进行分析，比如每日、每月、每15分钟等（这样自然会在时间序列中引入缺失值）。pandas拥有一整套标准时间序列频率以及用于重采样、频率推断、生成固定频率日期范围的工具1ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 例如，我们可以将之前那个时间序列转换为一个具有固定频率（每日）的时间序列。只需要调用resample即可1ts.resample('D').mean()2011-01-02 -0.296854 2011-01-03 NaN 2011-01-04 NaN 2011-01-05 -1.968663 2011-01-06 NaN 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-09 NaN 2011-01-10 -0.348697 2011-01-11 NaN 2011-01-12 0.102276 Freq: D, dtype: float64 生成日期范围data_range函数， 指定始末12index = pd.date_range('4/1/2012', '6/1/2012')indexDatetimeIndex([&apos;2012-04-01&apos;, &apos;2012-04-02&apos;, &apos;2012-04-03&apos;, &apos;2012-04-04&apos;, &apos;2012-04-05&apos;, &apos;2012-04-06&apos;, &apos;2012-04-07&apos;, &apos;2012-04-08&apos;, &apos;2012-04-09&apos;, &apos;2012-04-10&apos;, &apos;2012-04-11&apos;, &apos;2012-04-12&apos;, &apos;2012-04-13&apos;, &apos;2012-04-14&apos;, &apos;2012-04-15&apos;, &apos;2012-04-16&apos;, &apos;2012-04-17&apos;, &apos;2012-04-18&apos;, &apos;2012-04-19&apos;, &apos;2012-04-20&apos;, &apos;2012-04-21&apos;, &apos;2012-04-22&apos;, &apos;2012-04-23&apos;, &apos;2012-04-24&apos;, &apos;2012-04-25&apos;, &apos;2012-04-26&apos;, &apos;2012-04-27&apos;, &apos;2012-04-28&apos;, &apos;2012-04-29&apos;, &apos;2012-04-30&apos;, &apos;2012-05-01&apos;, &apos;2012-05-02&apos;, &apos;2012-05-03&apos;, &apos;2012-05-04&apos;, &apos;2012-05-05&apos;, &apos;2012-05-06&apos;, &apos;2012-05-07&apos;, &apos;2012-05-08&apos;, &apos;2012-05-09&apos;, &apos;2012-05-10&apos;, &apos;2012-05-11&apos;, &apos;2012-05-12&apos;, &apos;2012-05-13&apos;, &apos;2012-05-14&apos;, &apos;2012-05-15&apos;, &apos;2012-05-16&apos;, &apos;2012-05-17&apos;, &apos;2012-05-18&apos;, &apos;2012-05-19&apos;, &apos;2012-05-20&apos;, &apos;2012-05-21&apos;, &apos;2012-05-22&apos;, &apos;2012-05-23&apos;, &apos;2012-05-24&apos;, &apos;2012-05-25&apos;, &apos;2012-05-26&apos;, &apos;2012-05-27&apos;, &apos;2012-05-28&apos;, &apos;2012-05-29&apos;, &apos;2012-05-30&apos;, &apos;2012-05-31&apos;, &apos;2012-06-01&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 只指定起始， 以及长度1pd.date_range(start='4/1/2012', periods=20)DatetimeIndex([&apos;2012-04-01&apos;, &apos;2012-04-02&apos;, &apos;2012-04-03&apos;, &apos;2012-04-04&apos;, &apos;2012-04-05&apos;, &apos;2012-04-06&apos;, &apos;2012-04-07&apos;, &apos;2012-04-08&apos;, &apos;2012-04-09&apos;, &apos;2012-04-10&apos;, &apos;2012-04-11&apos;, &apos;2012-04-12&apos;, &apos;2012-04-13&apos;, &apos;2012-04-14&apos;, &apos;2012-04-15&apos;, &apos;2012-04-16&apos;, &apos;2012-04-17&apos;, &apos;2012-04-18&apos;, &apos;2012-04-19&apos;, &apos;2012-04-20&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 只指定结尾，以及长度1pd.date_range(end='6/1/2012', periods=20)DatetimeIndex([&apos;2012-05-13&apos;, &apos;2012-05-14&apos;, &apos;2012-05-15&apos;, &apos;2012-05-16&apos;, &apos;2012-05-17&apos;, &apos;2012-05-18&apos;, &apos;2012-05-19&apos;, &apos;2012-05-20&apos;, &apos;2012-05-21&apos;, &apos;2012-05-22&apos;, &apos;2012-05-23&apos;, &apos;2012-05-24&apos;, &apos;2012-05-25&apos;, &apos;2012-05-26&apos;, &apos;2012-05-27&apos;, &apos;2012-05-28&apos;, &apos;2012-05-29&apos;, &apos;2012-05-30&apos;, &apos;2012-05-31&apos;, &apos;2012-06-01&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 指定始末，以及采样频率， BM = business end of month1pd.date_range('1/1/2000', '12/1/2000', freq='BM')DatetimeIndex([&apos;2000-01-31&apos;, &apos;2000-02-29&apos;, &apos;2000-03-31&apos;, &apos;2000-04-28&apos;, &apos;2000-05-31&apos;, &apos;2000-06-30&apos;, &apos;2000-07-31&apos;, &apos;2000-08-31&apos;, &apos;2000-09-29&apos;, &apos;2000-10-31&apos;, &apos;2000-11-30&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;BM&apos;) 默认peroids指的是天数1pd.date_range('5/2/2012 12:56:31', periods=5)DatetimeIndex([&apos;2012-05-02 12:56:31&apos;, &apos;2012-05-03 12:56:31&apos;, &apos;2012-05-04 12:56:31&apos;, &apos;2012-05-05 12:56:31&apos;, &apos;2012-05-06 12:56:31&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 可以省略时间戳1pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True)DatetimeIndex([&apos;2012-05-02&apos;, &apos;2012-05-03&apos;, &apos;2012-05-04&apos;, &apos;2012-05-05&apos;, &apos;2012-05-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 频率和日期偏移量偏移量可以采用特定单位的时间对象123from pandas.tseries.offsets import Hour, Minutehour = Hour()hour&lt;Hour&gt; 4个小时，简单粗暴12four_hours = Hour(4)four_hours&lt;4 * Hours&gt; 每隔四个小时进行采样1pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h')DatetimeIndex([&apos;2000-01-01 00:00:00&apos;, &apos;2000-01-01 04:00:00&apos;, &apos;2000-01-01 08:00:00&apos;, &apos;2000-01-01 12:00:00&apos;, &apos;2000-01-01 16:00:00&apos;, &apos;2000-01-01 20:00:00&apos;, &apos;2000-01-02 00:00:00&apos;, &apos;2000-01-02 04:00:00&apos;, &apos;2000-01-02 08:00:00&apos;, &apos;2000-01-02 12:00:00&apos;, &apos;2000-01-02 16:00:00&apos;, &apos;2000-01-02 20:00:00&apos;, &apos;2000-01-03 00:00:00&apos;, &apos;2000-01-03 04:00:00&apos;, &apos;2000-01-03 08:00:00&apos;, &apos;2000-01-03 12:00:00&apos;, &apos;2000-01-03 16:00:00&apos;, &apos;2000-01-03 20:00:00&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;4H&apos;) 两个半小时1Hour(2) + Minute(30)&lt;150 * Minutes&gt; 也可以直接使用这种类似于自然语言的形式1pd.date_range('1/1/2000', periods=10, freq='1h30min')DatetimeIndex([&apos;2000-01-01 00:00:00&apos;, &apos;2000-01-01 01:30:00&apos;, &apos;2000-01-01 03:00:00&apos;, &apos;2000-01-01 04:30:00&apos;, &apos;2000-01-01 06:00:00&apos;, &apos;2000-01-01 07:30:00&apos;, &apos;2000-01-01 09:00:00&apos;, &apos;2000-01-01 10:30:00&apos;, &apos;2000-01-01 12:00:00&apos;, &apos;2000-01-01 13:30:00&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;90T&apos;) Week of month dates （WOM日期）每月第三个星期五12rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')list(rng)[Timestamp(&apos;2012-01-20 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-02-17 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-03-16 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-04-20 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-05-18 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-06-15 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-07-20 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-08-17 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;)] 移动（超前或滞后）数据123ts = Series(np.random.randn(4), index=pd.date_range('1/1/2000', periods=4, freq='M'))ts2000-01-31 1.294798 2000-02-29 -1.907732 2000-03-31 -1.407750 2000-04-30 0.544825 Freq: M, dtype: float64 整体数据前移1ts.shift(2)2000-01-31 NaN 2000-02-29 NaN 2000-03-31 1.294798 2000-04-30 -1.907732 Freq: M, dtype: float64 整体数据后移，有点类似于位运算中的移位操作1ts.shift(-2)2000-01-31 -1.407750 2000-02-29 0.544825 2000-03-31 NaN 2000-04-30 NaN Freq: M, dtype: float64 移位之后数据对齐1ts / ts.shift(1) - 12000-01-31 NaN 2000-02-29 -2.473382 2000-03-31 -0.262082 2000-04-30 -1.387018 Freq: M, dtype: float64 加入freq之后就是在行索引上进行时间前移1ts.shift(2, freq='M')2000-03-31 1.294798 2000-04-30 -1.907732 2000-05-31 -1.407750 2000-06-30 0.544825 Freq: M, dtype: float64 在天数上进行前移1ts.shift(3, freq='D')2000-02-03 1.294798 2000-03-03 -1.907732 2000-04-03 -1.407750 2000-05-03 0.544825 dtype: float64 另一种实现方式1ts.shift(1, freq='3D')2000-02-03 1.294798 2000-03-03 -1.907732 2000-04-03 -1.407750 2000-05-03 0.544825 dtype: float64 换一个频率1ts.shift(1, freq='90T')2000-01-31 01:30:00 1.294798 2000-02-29 01:30:00 -1.907732 2000-03-31 01:30:00 -1.407750 2000-04-30 01:30:00 0.544825 Freq: M, dtype: float64 通过偏移量对日期进行位移123from pandas.tseries.offsets import Day, MonthEndnow = datetime(2011, 11, 17)now + 3 * Day()Timestamp(&apos;2011-11-20 00:00:00&apos;) 直接移位到月末，是一个相对位移1now + MonthEnd()Timestamp(&apos;2011-11-30 00:00:00&apos;) 传入的参数表示第几个月的月末1now + MonthEnd(2)Timestamp(&apos;2011-12-31 00:00:00&apos;) 换一种方式实现，“主语”不同12offset = MonthEnd()offset.rollforward(now)Timestamp(&apos;2011-11-30 00:00:00&apos;) 往回走，上一个月的月末1offset.rollback(now)Timestamp(&apos;2011-10-31 00:00:00&apos;) 对日期进行移位之后分组123ts = Series(np.random.randn(20), index=pd.date_range('1/15/2000', periods=20, freq='4d'))ts.groupby(offset.rollforward).mean()2000-01-31 -0.610639 2000-02-29 0.029121 2000-03-31 -0.089587 dtype: float64 另一种方式也可以达到相同的效果1ts.resample('M', how='mean')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() if __name__ == &apos;__main__&apos;: 2000-01-31 -0.610639 2000-02-29 0.029121 2000-03-31 -0.089587 Freq: M, dtype: float64 时区处理显示一些时区12import pytzpytz.common_timezones[-5:][&apos;US/Eastern&apos;, &apos;US/Hawaii&apos;, &apos;US/Mountain&apos;, &apos;US/Pacific&apos;, &apos;UTC&apos;] 显示某个时区的具体信息12tz = pytz.timezone('US/Eastern')tz&lt;DstTzInfo &apos;US/Eastern&apos; LMT-1 day, 19:04:00 STD&gt; 本地化和转换123rng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')ts = Series(np.random.randn(len(rng)), index=rng)ts2012-03-09 09:30:00 0.065144 2012-03-10 09:30:00 -0.391505 2012-03-11 09:30:00 1.207495 2012-03-12 09:30:00 1.516354 2012-03-13 09:30:00 -0.253149 2012-03-14 09:30:00 -0.768138 Freq: D, dtype: float64 没有指定时区的时候默认时区为None1print(ts.index.tz)None ​指定时区1pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')DatetimeIndex([&apos;2012-03-09 09:30:00+00:00&apos;, &apos;2012-03-10 09:30:00+00:00&apos;, &apos;2012-03-11 09:30:00+00:00&apos;, &apos;2012-03-12 09:30:00+00:00&apos;, &apos;2012-03-13 09:30:00+00:00&apos;, &apos;2012-03-14 09:30:00+00:00&apos;, &apos;2012-03-15 09:30:00+00:00&apos;, &apos;2012-03-16 09:30:00+00:00&apos;, &apos;2012-03-17 09:30:00+00:00&apos;, &apos;2012-03-18 09:30:00+00:00&apos;], dtype=&apos;datetime64[ns, UTC]&apos;, freq=&apos;D&apos;) 进行时区的转换12ts_utc = ts.tz_localize('UTC')ts_utc2012-03-09 09:30:00+00:00 0.065144 2012-03-10 09:30:00+00:00 -0.391505 2012-03-11 09:30:00+00:00 1.207495 2012-03-12 09:30:00+00:00 1.516354 2012-03-13 09:30:00+00:00 -0.253149 2012-03-14 09:30:00+00:00 -0.768138 Freq: D, dtype: float64 1ts_utc.indexDatetimeIndex([&apos;2012-03-09 09:30:00+00:00&apos;, &apos;2012-03-10 09:30:00+00:00&apos;, &apos;2012-03-11 09:30:00+00:00&apos;, &apos;2012-03-12 09:30:00+00:00&apos;, &apos;2012-03-13 09:30:00+00:00&apos;, &apos;2012-03-14 09:30:00+00:00&apos;], dtype=&apos;datetime64[ns, UTC]&apos;, freq=&apos;D&apos;) 继续转换1ts_utc.tz_convert('US/Eastern')2012-03-09 04:30:00-05:00 0.065144 2012-03-10 04:30:00-05:00 -0.391505 2012-03-11 05:30:00-04:00 1.207495 2012-03-12 05:30:00-04:00 1.516354 2012-03-13 05:30:00-04:00 -0.253149 2012-03-14 05:30:00-04:00 -0.768138 Freq: D, dtype: float64 依旧是转换12ts_eastern = ts.tz_localize('US/Eastern')ts_eastern.tz_convert('UTC')2012-03-09 14:30:00+00:00 0.065144 2012-03-10 14:30:00+00:00 -0.391505 2012-03-11 13:30:00+00:00 1.207495 2012-03-12 13:30:00+00:00 1.516354 2012-03-13 13:30:00+00:00 -0.253149 2012-03-14 13:30:00+00:00 -0.768138 Freq: D, dtype: float64 转转转ts_eastern.tz_convert(‘Europe/Berlin’)转换之前必须要进行本地化1ts.index.tz_localize('Asia/Shanghai')操作时区意识型TimeStamp对象初始化时间戳，本地化，时区转换123stamp = pd.Timestamp('2011-03-12 04:00')stamp_utc = stamp.tz_localize('utc')stamp_utc.tz_convert('US/Eastern')Timestamp(&apos;2011-03-11 23:00:00-0500&apos;, tz=&apos;US/Eastern&apos;) 显式地初始化12stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')stamp_moscowTimestamp(&apos;2011-03-12 04:00:00+0300&apos;, tz=&apos;Europe/Moscow&apos;) 自1970年1月1日起计算的纳秒数1stamp_utc.value1299902400000000000 这个值是绝对的1stamp_utc.tz_convert('US/Eastern').value1299902400000000000 1234# 30 minutes before DST transitionfrom pandas.tseries.offsets import Hourstamp = pd.Timestamp('2012-03-12 01:30', tz='US/Eastern')stampTimestamp(&apos;2012-03-12 01:30:00-0400&apos;, tz=&apos;US/Eastern&apos;) 进行时间的位移1stamp + Hour()Timestamp(&apos;2012-03-12 02:30:00-0400&apos;, tz=&apos;US/Eastern&apos;) 123# 90 minutes before DST transitionstamp = pd.Timestamp('2012-11-04 00:30', tz='US/Eastern')stampTimestamp(&apos;2012-11-04 00:30:00-0400&apos;, tz=&apos;US/Eastern&apos;) 1stamp + 2 * Hour()Timestamp(&apos;2012-11-04 01:30:00-0500&apos;, tz=&apos;US/Eastern&apos;) 不同时区之间的运算123rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')ts = Series(np.random.randn(len(rng)), index=rng)ts2012-03-07 09:30:00 -0.461750 2012-03-08 09:30:00 0.947394 2012-03-09 09:30:00 0.703239 2012-03-12 09:30:00 0.266519 2012-03-13 09:30:00 0.302334 2012-03-14 09:30:00 -0.000725 2012-03-15 09:30:00 0.305446 2012-03-16 09:30:00 -1.605358 2012-03-19 09:30:00 1.306474 2012-03-20 09:30:00 0.865511 Freq: B, dtype: float64 最终结果会变成UTC1234ts1 = ts[:7].tz_localize('Europe/London')ts2 = ts1[2:].tz_convert('Europe/Moscow')result = ts1 + ts2result.indexDatetimeIndex([&apos;2012-03-07 09:30:00+00:00&apos;, &apos;2012-03-08 09:30:00+00:00&apos;, &apos;2012-03-09 09:30:00+00:00&apos;, &apos;2012-03-12 09:30:00+00:00&apos;, &apos;2012-03-13 09:30:00+00:00&apos;, &apos;2012-03-14 09:30:00+00:00&apos;, &apos;2012-03-15 09:30:00+00:00&apos;], dtype=&apos;datetime64[ns, UTC]&apos;, freq=&apos;B&apos;) 时期及其算术运算12p = pd.Period(2007, freq='A-DEC')pPeriod(&apos;2007&apos;, &apos;A-DEC&apos;) 1p + 5Period(&apos;2012&apos;, &apos;A-DEC&apos;) 1p - 2Period(&apos;2005&apos;, &apos;A-DEC&apos;) 1pd.Period('2014', freq='A-DEC') - p7 12rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')rngPeriodIndex([&apos;2000-01&apos;, &apos;2000-02&apos;, &apos;2000-03&apos;, &apos;2000-04&apos;, &apos;2000-05&apos;, &apos;2000-06&apos;], dtype=&apos;int64&apos;, freq=&apos;M&apos;) 1Series(np.random.randn(6), index=rng)2000-01 0.061389 2000-02 0.059265 2000-03 0.779627 2000-04 -0.068995 2000-05 -0.451276 2000-06 -1.531821 Freq: M, dtype: float64 123values = ['2001Q3', '2002Q2', '2003Q1']index = pd.PeriodIndex(values, freq='Q-DEC')indexPeriodIndex([&apos;2001Q3&apos;, &apos;2002Q2&apos;, &apos;2003Q1&apos;], dtype=&apos;int64&apos;, freq=&apos;Q-DEC&apos;) 时区的频率转换以十二月为结尾的一个年时期12p = pd.Period('2007', freq='A-DEC')p.asfreq('M', how='start')Period(&apos;2007-01&apos;, &apos;M&apos;) 1p.asfreq('M', how='end')Period(&apos;2007-12&apos;, &apos;M&apos;) 以六月份结尾的一个年时期12p = pd.Period('2007', freq='A-JUN')p.asfreq('M', 'start')Period(&apos;2006-07&apos;, &apos;M&apos;) 1p.asfreq('M', 'end')Period(&apos;2007-06&apos;, &apos;M&apos;) 2007年8月是属于以六月结尾的2008年的时期中12p = pd.Period('Aug-2007', 'M')p.asfreq('A-JUN')Period(&apos;2008&apos;, &apos;A-JUN&apos;) 相当于一个批量操作123rng = pd.period_range('2006', '2009', freq='A-DEC')ts = Series(np.random.randn(len(rng)), index=rng)ts2006 0.634252 2007 -0.738716 2008 0.398145 2009 -1.226529 Freq: A-DEC, dtype: float64 1ts.asfreq('M', how='start')2006-01 0.634252 2007-01 -0.738716 2008-01 0.398145 2009-01 -1.226529 Freq: M, dtype: float64 1ts.asfreq('B', how='end')2006-12-29 0.634252 2007-12-31 -0.738716 2008-12-31 0.398145 2009-12-31 -1.226529 Freq: B, dtype: float64 按季度计算的时间频率以一月为截止的第四个季度12p = pd.Period('2012Q4', freq='Q-JAN')pPeriod(&apos;2012Q4&apos;, &apos;Q-JAN&apos;) 第四个季度的起始日1p.asfreq('D', 'start')Period(&apos;2011-11-01&apos;, &apos;D&apos;) 结束日1p.asfreq('D', 'end')Period(&apos;2012-01-31&apos;, &apos;D&apos;) 截止日前一天的下午四点12p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60p4pmPeriod(&apos;2012-01-30 16:00&apos;, &apos;T&apos;) 转化成时间戳对象1p4pm.to_timestamp()Timestamp(&apos;2012-01-30 16:00:00&apos;) 123rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')ts = Series(np.arange(len(rng)), index=rng)ts2011Q3 0 2011Q4 1 2012Q1 2 2012Q2 3 2012Q3 4 2012Q4 5 Freq: Q-JAN, dtype: int32 批量转化为时间戳123new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60ts.index = new_rng.to_timestamp()ts2010-10-28 16:00:00 0 2011-01-28 16:00:00 1 2011-04-28 16:00:00 2 2011-07-28 16:00:00 3 2011-10-28 16:00:00 4 2012-01-30 16:00:00 5 dtype: int32 将时间戳转化为时期（以及其逆过程）1234rng = pd.date_range('1/1/2000', periods=3, freq='M')ts = Series(randn(3), index=rng)pts = ts.to_period()ts2000-01-31 0.239752 2000-02-29 -0.469201 2000-03-31 2.835243 Freq: M, dtype: float64 默认以月份为单位进行转化1pts2000-01 0.239752 2000-02 -0.469201 2000-03 2.835243 Freq: M, dtype: float64 转化为月份为单位的时期123rng = pd.date_range('1/29/2000', periods=6, freq='D')ts2 = Series(randn(6), index=rng)ts2.to_period('M')2000-01 1.126773 2000-01 -0.979309 2000-01 -0.784376 2000-02 -1.490820 2000-02 1.125043 2000-02 0.421830 Freq: M, dtype: float64 12pts = ts.to_period()pts2000-01 0.239752 2000-02 -0.469201 2000-03 2.835243 Freq: M, dtype: float64 逆向转换1pts.to_timestamp(how='end')2000-01-31 0.239752 2000-02-29 -0.469201 2000-03-31 2.835243 Freq: M, dtype: float64 通过数组创建PeriodIndex12data = pd.read_csv('ch08/macrodata.csv')data.year0 1959.0 1 1959.0 2 1959.0 3 1959.0 4 1960.0 5 1960.0 ... 197 2008.0 198 2008.0 199 2008.0 200 2009.0 201 2009.0 202 2009.0 Name: year, dtype: float64 1data.quarter0 1.0 1 2.0 2 3.0 3 4.0 4 1.0 5 2.0 ... 197 2.0 198 3.0 199 4.0 200 1.0 201 2.0 202 3.0 Name: quarter, dtype: float64 将年份和季度数据统一起来转化为时期索引数据12index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')indexPeriodIndex([&apos;1959Q1&apos;, &apos;1959Q2&apos;, &apos;1959Q3&apos;, &apos;1959Q4&apos;, &apos;1960Q1&apos;, &apos;1960Q2&apos;, &apos;1960Q3&apos;, &apos;1960Q4&apos;, &apos;1961Q1&apos;, &apos;1961Q2&apos;, ... &apos;2007Q2&apos;, &apos;2007Q3&apos;, &apos;2007Q4&apos;, &apos;2008Q1&apos;, &apos;2008Q2&apos;, &apos;2008Q3&apos;, &apos;2008Q4&apos;, &apos;2009Q1&apos;, &apos;2009Q2&apos;, &apos;2009Q3&apos;], dtype=&apos;int64&apos;, length=203, freq=&apos;Q-DEC&apos;) 12data.index = indexdata.infl1959Q1 0.00 1959Q2 2.34 1959Q3 2.74 1959Q4 0.27 1960Q1 2.31 1960Q2 0.14 ... 2008Q2 8.53 2008Q3 -3.16 2008Q4 -8.79 2009Q1 0.94 2009Q2 3.37 2009Q3 3.56 Freq: Q-DEC, Name: infl, dtype: float64 重采样以及频率转换相当于进行了一次分组操作123rng = pd.date_range('1/1/2000', periods=100, freq='D')ts = Series(randn(len(rng)), index=rng)ts.resample('M', how='mean')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() app.launch_new_instance() 2000-01-31 -0.055153 2000-02-29 0.189412 2000-03-31 -0.075940 2000-04-30 -0.239036 Freq: M, dtype: float64 换个索引的形式1ts.resample('M', how='mean', kind='period')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() if __name__ == &apos;__main__&apos;: 2000-01 -0.055153 2000-02 0.189412 2000-03 -0.075940 2000-04 -0.239036 Freq: M, dtype: float64 降采样按照分钟进行采样123rng = pd.date_range('1/1/2000', periods=12, freq='T')ts = Series(np.arange(12), index=rng)ts2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 2000-01-01 00:09:00 9 2000-01-01 00:10:00 10 2000-01-01 00:11:00 11 Freq: T, dtype: int32 每5分钟降采样12ts.resample('5min').sum()# note: output changed (as the default changed from closed='right', label='right' to closed='left', label='left'2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 1ts.resample('5min', closed='left').sum()2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 1ts.resample('5min', closed='left', label='left').sum()2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 加了个时间的偏移1ts.resample('5min', loffset='-1s').sum()1999-12-31 23:59:59 10 2000-01-01 00:04:59 35 2000-01-01 00:09:59 21 Freq: 5T, dtype: int32 Open-High-Low-Close (OHLC) 降采样1ts2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 2000-01-01 00:09:00 9 2000-01-01 00:10:00 10 2000-01-01 00:11:00 11 Freq: T, dtype: int32 以5分钟为单位12ts.resample('5min').ohlc()# note: output changed because of changed defaultsopenhighlowclose2000-01-01 00:00:0004042000-01-01 00:05:0059592000-01-01 00:10:0010111011通过GroupBy进行重采样123rng = pd.date_range('1/1/2000', periods=100, freq='D')ts = Series(np.arange(100), index=rng)ts.groupby(lambda x: x.month).mean()1 15 2 45 3 75 4 95 dtype: int32 1ts.groupby(lambda x: x.weekday).mean()0 47.5 1 48.5 2 49.5 3 50.5 4 51.5 5 49.0 6 50.0 dtype: float64 升采样和插值1234frame = DataFrame(np.random.randn(2, 4), index=pd.date_range('1/1/2000', periods=2, freq='W-WED'), columns=['Colorado', 'Texas', 'New York', 'Ohio'])frameColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-12-0.5871240.612993-0.796000-0.34113812df_daily = frame.resample('D').mean()df_dailyColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-06NaNNaNNaNNaN2000-01-07NaNNaNNaNNaN2000-01-08NaNNaNNaNNaN2000-01-09NaNNaNNaNNaN2000-01-10NaNNaNNaNNaN2000-01-11NaNNaNNaNNaN2000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('D').ffill()ColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-060.3607730.5064291.1664241.4023362000-01-070.3607730.5064291.1664241.4023362000-01-080.3607730.5064291.1664241.4023362000-01-090.3607730.5064291.1664241.4023362000-01-100.3607730.5064291.1664241.4023362000-01-110.3607730.5064291.1664241.4023362000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('D').ffill(limit=2)ColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-060.3607730.5064291.1664241.4023362000-01-070.3607730.5064291.1664241.4023362000-01-08NaNNaNNaNNaN2000-01-09NaNNaNNaNNaN2000-01-10NaNNaNNaNNaN2000-01-11NaNNaNNaNNaN2000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('W-THU').ffill()ColoradoTexasNew YorkOhio2000-01-060.3607730.5064291.1664241.4023362000-01-13-0.5871240.612993-0.796000-0.341138通过时期进行重采样1234frame = DataFrame(np.random.randn(24, 4), index=pd.period_range('1-2000', '12-2001', freq='M'), columns=['Colorado', 'Texas', 'New York', 'Ohio'])frame[:5]ColoradoTexasNew YorkOhio2000-01-0.2543400.401110-0.931350-0.8725522000-020.390968-0.815357-1.656213-2.2516212000-030.2062970.1973940.927518-0.6572572000-04-0.4517090.908598-0.187902-0.4980822000-05-0.215150-0.042141-0.7387332.499246以年为单位12annual_frame = frame.resample('A-DEC').mean()annual_frameColoradoTexasNew YorkOhio2000-0.0493830.037021-0.272851-0.1409842001-0.183766-0.2919930.3409410.209276以季度为单位1234# Q-DEC: Quarterly, year ending in Decemberannual_frame.resample('Q-DEC').ffill()# note: output changed, default value changed from convention='end' to convention='start' + 'start' changed to span-like# also the following cellsColoradoTexasNew YorkOhio2000Q1-0.0493830.037021-0.272851-0.1409842000Q2-0.0493830.037021-0.272851-0.1409842000Q3-0.0493830.037021-0.272851-0.1409842000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.183766-0.2919930.3409410.2092762001Q2-0.183766-0.2919930.3409410.2092762001Q3-0.183766-0.2919930.3409410.2092762001Q4-0.183766-0.2919930.3409410.2092761annual_frame.resample('Q-DEC', fill_method='ffill', convention='start')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &apos;__main__&apos;: ColoradoTexasNew YorkOhio2000Q1-0.0493830.037021-0.272851-0.1409842000Q2-0.0493830.037021-0.272851-0.1409842000Q3-0.0493830.037021-0.272851-0.1409842000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.183766-0.2919930.3409410.2092762001Q2-0.183766-0.2919930.3409410.2092762001Q3-0.183766-0.2919930.3409410.2092762001Q4-0.183766-0.2919930.3409410.2092761annual_frame.resample('Q-MAR', fill_method='ffill')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &apos;__main__&apos;: ColoradoTexasNew YorkOhio2000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.0493830.037021-0.272851-0.1409842001Q2-0.0493830.037021-0.272851-0.1409842001Q3-0.0493830.037021-0.272851-0.1409842001Q4-0.183766-0.2919930.3409410.2092762002Q1-0.183766-0.2919930.3409410.2092762002Q2-0.183766-0.2919930.3409410.2092762002Q3-0.183766-0.2919930.3409410.209276时间序列绘图1234close_px_all = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]close_px = close_px.resample('B', fill_method='ffill')close_px.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; DatetimeIndex: 2292 entries, 2003-01-02 to 2011-10-14 Freq: B Data columns (total 3 columns): AAPL 2292 non-null float64 MSFT 2292 non-null float64 XOM 2292 non-null float64 dtypes: float64(3) memory usage: 71.6 KB C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() app.launch_new_instance() 按年绘图1close_px['AAPL'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb1f85a080&gt; 按月绘图1close_px.ix['2009'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb20c4d550&gt; 按天绘图1close_px['AAPL'].ix['01-2011':'03-2011'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21235668&gt; 按季度绘图12appl_q = close_px['AAPL'].resample('Q-DEC', fill_method='ffill')appl_q.ix['2009':].plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &apos;__main__&apos;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21346c50&gt; 移动窗口函数1close_px = close_px.asfreq('B').fillna(method='ffill')12close_px.AAPL.plot()pd.rolling_mean(close_px.AAPL, 250).plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb213b95f8&gt; C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:2: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,center=False).mean() from ipykernel import kernelapp as app &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb213b95f8&gt; 1plt.figure()&lt;matplotlib.figure.Figure at 0x1fb212fb550&gt; &lt;matplotlib.figure.Figure at 0x1fb212fb550&gt; 12appl_std250 = pd.rolling_std(close_px.AAPL, 250, min_periods=10)appl_std250[5:12]C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,min_periods=10,center=False).std() if __name__ == &apos;__main__&apos;: 2003-01-09 NaN 2003-01-10 NaN 2003-01-13 NaN 2003-01-14 NaN 2003-01-15 0.077496 2003-01-16 0.074760 2003-01-17 0.112368 Freq: B, Name: AAPL, dtype: float64 1appl_std250.plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21466ba8&gt; 12# Define expanding mean in terms of rolling_meanexpanding_mean = lambda x: rolling_mean(x, len(x), min_periods=1)1pd.rolling_mean(close_px, 60).plot(logy=True)C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_mean is deprecated for DataFrame and will be removed in a future version, replace with DataFrame.rolling(window=60,center=False).mean() if __name__ == &apos;__main__&apos;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21571208&gt; 1plt.close('all')指数加权函数更好的拟合1234567891011121314fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(12, 7))aapl_px = close_px.AAPL['2005':'2009']ma60 = pd.rolling_mean(aapl_px, 60, min_periods=50)ewma60 = pd.ewma(aapl_px, span=60)aapl_px.plot(style='k-', ax=axes[0])ma60.plot(style='k--', ax=axes[0])aapl_px.plot(style='k-', ax=axes[1])ewma60.plot(style='k--', ax=axes[1])axes[0].set_title('Simple MA')axes[1].set_title('Exponentially-weighted MA')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:6: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=60,min_periods=50,center=False).mean() C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:7: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with Series.ewm(span=60,ignore_na=False,min_periods=0,adjust=True).mean() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21983b70&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21983b70&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb219c9a20&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb219c9a20&gt; &lt;matplotlib.text.Text at 0x1fb219b10b8&gt; &lt;matplotlib.text.Text at 0x1fb219efcc0&gt; 二元移动窗口函数12close_pxspx_px = close_px_all['SPX']AAPLMSFTXOM2003-01-027.4021.1129.222003-01-037.4521.1429.242003-01-067.4521.5229.962003-01-077.4321.9328.952003-01-087.2821.3128.832003-01-097.3421.9329.44…………2011-10-07369.8026.2573.562011-10-10388.8126.9476.282011-10-11400.2927.0076.272011-10-12402.1926.9677.162011-10-13408.4327.1876.372011-10-14422.0027.2778.112292 rows × 3 columns1234spx_rets = spx_px / spx_px.shift(1) - 1returns = close_px.pct_change()corr = pd.rolling_corr(returns.AAPL, spx_rets, 125, min_periods=100)corr.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: pd.rolling_corr is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=125,min_periods=100).corr(other=&lt;Series&gt;) app.launch_new_instance() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21a93438&gt; 12corr = pd.rolling_corr(returns, spx_rets, 125, min_periods=100)corr.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_corr is deprecated for DataFrame and will be removed in a future version, replace with DataFrame.rolling(window=125,min_periods=100).corr(other=&lt;Series&gt;) if __name__ == &apos;__main__&apos;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb22b21438&gt; 用户自定义移动窗口函数1234from scipy.stats import percentileofscorescore_at_2percent = lambda x: percentileofscore(x, 0.02)result = pd.rolling_apply(returns.AAPL, 250, score_at_2percent)result.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: pd.rolling_apply is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,center=False).apply(kwargs=&lt;dict&gt;,args=&lt;tuple&gt;,func=&lt;function&gt;) app.launch_new_instance() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb3dbdd2e8&gt; 性能和内存使用方面的注意事项123rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms')ts = Series(np.random.randn(len(rng)), index=rng)ts2000-01-01 00:00:00.000 -0.428577 2000-01-01 00:00:00.010 1.650203 2000-01-01 00:00:00.020 -0.064777 2000-01-01 00:00:00.030 -0.219433 2000-01-01 00:00:00.040 1.907433 2000-01-01 00:00:00.050 0.103347 ... 2000-01-02 03:46:39.940 0.989446 2000-01-02 03:46:39.950 2.333137 2000-01-02 03:46:39.960 0.354455 2000-01-02 03:46:39.970 0.353224 2000-01-02 03:46:39.980 -0.862868 2000-01-02 03:46:39.990 2.007468 Freq: 10L, dtype: float64 1ts.resample('15min').ohlc().info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; DatetimeIndex: 11112 entries, 2000-01-01 00:00:00 to 2000-04-25 17:45:00 Freq: 15T Data columns (total 4 columns): open 11112 non-null float64 high 11112 non-null float64 low 11112 non-null float64 close 11112 non-null float64 dtypes: float64(4) memory usage: 434.1 KB 1%timeit ts.resample('15min').ohlc()10 loops, best of 3: 123 ms per loop ​123rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')ts = Series(np.random.randn(len(rng)), index=rng)%timeit ts.resample('15s').ohlc()1 loop, best of 3: 192 ms per loop ​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 two layer net]]></title>
      <url>%2F2017%2F03%2F08%2Fcs231n-Assignment-1-two-layer-net%2F</url>
      <content type="text"><![CDATA[Implementing a Neural NetworkIn this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.1234567891011121314151617181920# A bit of setupimport numpy as npimport matplotlib.pyplot as pltfrom cs231n.classifiers.neural_net import TwoLayerNet%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# for auto-reloading external modules# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2def rel_error(x, y): """ returns relative error """ return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))We will use the class TwoLayerNet in the file cs231n/classifiers/neural_net.py to represent instances of our network. The network parameters are stored in the instance variable self.params where keys are string parameter names and values are numpy arrays. Below, we initialize toy data and a toy model that we will use to develop your implementation.1234567891011121314151617181920# Create a small net and some toy data to check your implementations.# Note that we set the random seed for repeatable experiments.input_size = 4hidden_size = 10num_classes = 3num_inputs = 5def init_toy_model(): np.random.seed(0) return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)def init_toy_data(): np.random.seed(1) X = 10 * np.random.randn(num_inputs, input_size) y = np.array([0, 1, 2, 2, 1]) return X, ynet = init_toy_model()X, y = init_toy_data()Forward pass: compute scoresOpen the file cs231n/classifiers/neural_net.py and look at the method TwoLayerNet.loss. This function is very similar to the loss functions you have written for the SVM and Softmax exercises: It takes the data and weights and computes the class scores, the loss, and the gradients on the parameters.Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs.1234567891011121314151617scores = net.loss(X)print 'Your scores:'print scoresprintprint 'correct scores:'correct_scores = np.asarray([ [-0.81233741, -1.27654624, -0.70335995], [-0.17129677, -1.18803311, -0.47310444], [-0.51590475, -1.01354314, -0.8504215 ], [-0.15419291, -0.48629638, -0.52901952], [-0.00618733, -0.12435261, -0.15226949]])print correct_scoresprint# The difference should be very small. We get &lt; 1e-7print 'Difference between your scores and correct scores:'print np.sum(np.abs(scores - correct_scores))Your scores: [[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]] correct scores: [[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]] Difference between your scores and correct scores: 3.68027204961e-08 Forward pass: compute lossIn the same function, implement the second part that computes the data and regularizaion loss.123456loss, _ = net.loss(X, y, reg=0.1)correct_loss = 1.30378789133# should be very small, we get &lt; 1e-12print 'Difference between your loss and correct loss:'print np.sum(np.abs(loss - correct_loss))Difference between your loss and correct loss: 1.79412040779e-13 Backward passImplement the rest of the function. This will compute the gradient of the loss with respect to the variables W1, b1, W2, and b2. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:12345678910111213from cs231n.gradient_check import eval_numerical_gradient# Use numeric gradient checking to check your implementation of the backward pass.# If your implementation is correct, the difference between the numeric and# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.loss, grads = net.loss(X, y, reg=0.1)# these should all be less than 1e-8 or sofor param_name in grads: f = lambda W: net.loss(X, y, reg=0.1)[0] param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False) print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))W1 max relative error: 3.669858e-09 W2 max relative error: 3.440708e-09 b2 max relative error: 3.865028e-11 b1 max relative error: 2.738422e-09 Train the networkTo train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. Look at the function TwoLayerNet.train and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. You will also have to implement TwoLayerNet.predict, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2.12345678910111213net = init_toy_model()stats = net.train(X, y, X, y, learning_rate=1e-1, reg=1e-5, num_iters=100, verbose=False)print 'Final training loss: ', stats['loss_history'][-1]# plot the loss historyplt.plot(stats['loss_history'])plt.xlabel('iteration')plt.ylabel('training loss')plt.title('Training Loss history')plt.show()Final training loss: 0.0171496079387 ​Load the dataNow that you have implemented a two-layer network that passes gradient checks and works on toy data, it’s time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset.123456789101112131415161718192021222324252627282930313233343536373839404142434445from cs231n.data_utils import load_CIFAR10def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000): """ Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the two-layer neural net classifier. These are the same steps as we used for the SVM, but condensed to a single function. """ # Load the raw CIFAR-10 data cifar10_dir = 'cs231n/datasets/cifar-10-batches-py' X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) # Subsample the data mask = range(num_training, num_training + num_validation) X_val = X_train[mask] y_val = y_train[mask] mask = range(num_training) X_train = X_train[mask] y_train = y_train[mask] mask = range(num_test) X_test = X_test[mask] y_test = y_test[mask] # Normalize the data: subtract the mean image mean_image = np.mean(X_train, axis=0) X_train -= mean_image X_val -= mean_image X_test -= mean_image # Reshape data to rows X_train = X_train.reshape(num_training, -1) X_val = X_val.reshape(num_validation, -1) X_test = X_test.reshape(num_test, -1) return X_train, y_train, X_val, y_val, X_test, y_test# Invoke the above function to get our data.X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTrain data shape: (49000L, 3072L) Train labels shape: (49000L,) Validation data shape: (1000L, 3072L) Validation labels shape: (1000L,) Test data shape: (1000L, 3072L) Test labels shape: (1000L,) Train a networkTo train our network we will use SGD with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate.1234567891011121314input_size = 32 * 32 * 3hidden_size = 50num_classes = 10net = TwoLayerNet(input_size, hidden_size, num_classes)# Train the networkstats = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=1e-4, learning_rate_decay=0.95, reg=0.5, verbose=True)# Predict on the validation setval_acc = (net.predict(X_val) == y_val).mean()print 'Validation accuracy: ', val_acciteration 0 / 1000: loss 2.302954 iteration 100 / 1000: loss 2.302550 iteration 200 / 1000: loss 2.297648 iteration 300 / 1000: loss 2.259602 iteration 400 / 1000: loss 2.204170 iteration 500 / 1000: loss 2.118565 iteration 600 / 1000: loss 2.051535 iteration 700 / 1000: loss 1.988466 iteration 800 / 1000: loss 2.006591 iteration 900 / 1000: loss 1.951473 Validation accuracy: 0.287 Debug the trainingWith the default parameters we provided above, you should get a validation accuracy of about 0.29 on the validation set. This isn’t very good.One strategy for getting insight into what’s wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized.1234567891011121314# Plot the loss function and train / validation accuraciesplt.subplot(3, 1, 1)plt.plot(stats['loss_history'])plt.title('Loss history')plt.xlabel('Iteration')plt.ylabel('Loss')plt.subplot(3, 1, 3)plt.plot(stats['train_acc_history'], label='train')plt.plot(stats['val_acc_history'], label='val')plt.title('Classification accuracy history')plt.xlabel('Epoch')plt.ylabel('Clasification accuracy')plt.show()123456789101112from cs231n.vis_utils import visualize_grid# Visualize the weights of the networkdef show_net_weights(net): W1 = net.params['W1'] W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2) plt.imshow(visualize_grid(W1, padding=3).astype('uint8')) plt.gca().axis('off') plt.show()show_net_weights(net)Tune your hyperparametersWhat’s wrong?. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.Tuning. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.Approximate results. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.Experiment: You goal in this exercise is to get as good of a result on CIFAR-10 as you can, with a fully-connected Neural Network. For every 1% above 52% on the Test set we will award you with one extra bonus point. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.).123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960best_net = None # store the best model into this ################################################################################## TODO: Tune hyperparameters using the validation set. Store your best trained ## model in best_net. ## ## To help debug your network, it may help to use visualizations similar to the ## ones we used above; these visualizations will have significant qualitative ## differences from the ones we saw above for the poorly tuned network. ## ## Tweaking hyperparameters by hand can be fun, but you might find it useful to ## write code to sweep through possible combinations of hyperparameters ## automatically like we did on the previous exercises. ##################################################################################best_val = -1best_stats = Nonelearning_rates = [1e-1, 1e-2, 1e-3, 1e-4]regularization_strengths = [1e-1, 1e-2, 1e-3, 1e-4]batch_sizes = [200, 400, 800]hidden_sizes = [80, 160, 320]results = &#123;&#125;iters = 2000total_size = 144i = 0for lr in learning_rates: for rs in regularization_strengths: for bs in batch_sizes: for hs in hidden_sizes: i += 1 print i, '/', total_size net = TwoLayerNet(input_size, hs, num_classes) # Train the network stats = net.train(X_train, y_train, X_val, y_val, num_iters=iters, batch_size=bs, learning_rate=lr, learning_rate_decay=0.95, reg=rs) y_train_pred = net.predict(X_train) acc_train = np.mean(y_train == y_train_pred) y_val_pred = net.predict(X_val) acc_val = np.mean(y_val == y_val_pred) results[(lr, rs, bs, hs)] = (acc_train, acc_val) if best_val &lt; acc_val: best_stats = stats best_val = acc_val best_net = net# Print out results.# for lr, reg, bs, hs in sorted(results):# train_accuracy, val_accuracy = results[(lr, reg, bs, hs)]# print 'lr %e reg %e bs %e hs %e train accuracy: %f val accuracy: %f' % (# lr, reg, bs, hs, train_accuracy, val_accuracy)print 'best validation accuracy achieved during cross-validation: %f' % best_val################################################################################## END OF YOUR CODE ##################################################################################1 / 144 ​cs231n\classifiers\neural_net.py:104: RuntimeWarning: overflow encountered in exp exp_scores = np.exp(scores) cs231n\classifiers\neural_net.py:105: RuntimeWarning: invalid value encountered in divide a2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) cs231n\classifiers\neural_net.py:107: RuntimeWarning: divide by zero encountered in log correct_log_probs = -np.log(a2[range(N), y]) cs231n\classifiers\neural_net.py:81: RuntimeWarning: invalid value encountered in maximum a1 = np.maximum(0, z1) cs231n\classifiers\neural_net.py:131: RuntimeWarning: invalid value encountered in less_equal dhidden[z1 &lt;= 0] = 0 cs231n\classifiers\neural_net.py:247: RuntimeWarning: invalid value encountered in maximum a1 = np.maximum(0, z1) # pass through ReLU activation function 2 / 144 3 / 144 4 / 144 5 / 144 6 / 144 7 / 144 8 / 144 9 / 144 10 / 144 11 / 144 12 / 144 13 / 144 14 / 144 15 / 144 16 / 144 17 / 144 18 / 144 19 / 144 20 / 144 21 / 144 22 / 144 23 / 144 24 / 144 25 / 144 26 / 144 27 / 144 28 / 144 29 / 144 30 / 144 31 / 144 32 / 144 33 / 144 34 / 144 35 / 144 36 / 144 37 / 144 38 / 144 39 / 144 40 / 144 41 / 144 42 / 144 43 / 144 44 / 144 45 / 144 46 / 144 47 / 144 48 / 144 49 / 144 50 / 144 51 / 144 52 / 144 53 / 144 54 / 144 55 / 144 56 / 144 57 / 144 58 / 144 59 / 144 60 / 144 61 / 144 62 / 144 63 / 144 64 / 144 65 / 144 66 / 144 67 / 144 68 / 144 69 / 144 70 / 144 71 / 144 72 / 144 73 / 144 74 / 144 75 / 144 76 / 144 77 / 144 78 / 144 79 / 144 80 / 144 81 / 144 82 / 144 83 / 144 84 / 144 85 / 144 86 / 144 87 / 144 88 / 144 89 / 144 90 / 144 91 / 144 92 / 144 93 / 144 94 / 144 95 / 144 96 / 144 97 / 144 98 / 144 99 / 144 100 / 144 101 / 144 102 / 144 103 / 144 104 / 144 105 / 144 106 / 144 107 / 144 108 / 144 109 / 144 110 / 144 111 / 144 112 / 144 113 / 144 114 / 144 115 / 144 116 / 144 117 / 144 118 / 144 119 / 144 120 / 144 121 / 144 122 / 144 123 / 144 124 / 144 125 / 144 126 / 144 127 / 144 128 / 144 129 / 144 130 / 144 131 / 144 132 / 144 133 / 144 134 / 144 135 / 144 136 / 144 137 / 144 138 / 144 139 / 144 140 / 144 141 / 144 142 / 144 143 / 144 144 / 144 best validation accuracy achieved during cross-validation: 0.540000 12# visualize the weights of the best networkshow_net_weights(best_net)Run on the test setWhen you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.We will give you extra bonus point for every 1% of accuracy above 52%.12test_acc = (best_net.predict(X_test) == y_test).mean()print 'Test accuracy: ', test_accTest accuracy: 0.531 ​Codes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254import numpy as npimport matplotlib.pyplot as pltclass TwoLayerNet(object): """ A two-layer fully-connected neural network. The net has an input dimension of N, a hidden layer dimension of H, and performs classification over C classes. We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture: input - fully connected layer - ReLU - fully connected layer - softmax The outputs of the second fully-connected layer are the scores for each class. """ def __init__(self, input_size, hidden_size, output_size, std=1e-4): """ Initialize the model. Weights are initialized to small random values and biases are initialized to zero. Weights and biases are stored in the variable self.params, which is a dictionary with the following keys: W1: First layer weights; has shape (D, H) b1: First layer biases; has shape (H,) W2: Second layer weights; has shape (H, C) b2: Second layer biases; has shape (C,) Inputs: - input_size: The dimension D of the input data. - hidden_size: The number of neurons H in the hidden layer. - output_size: The number of classes C. """ self.params = &#123;&#125; self.params['W1'] = std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) def loss(self, X, y=None, reg=0.0): """ Compute the loss and gradients for a two layer fully connected neural network. Inputs: - X: Input data of shape (N, D). Each X[i] is a training sample. - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it is not passed then we only return scores, and if it is passed then we instead return the loss and gradients. - reg: Regularization strength. Returns: If y is None, return a matrix scores of shape (N, C) where scores[i, c] is the score for class c on input X[i]. If y is not None, instead return a tuple of: - loss: Loss (data loss and regularization loss) for this batch of training samples. - grads: Dictionary mapping parameter names to gradients of those parameters with respect to the loss function; has the same keys as self.params. """ # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None ############################################################################# # TODO: Perform the forward pass, computing the class scores for the input. # # Store the result in the scores variable, which should be an array of # # shape (N,C). # ############################################################################# # First layer pre-activation z1 = X.dot(W1) + b1 # First layer activation a1 = np.maximum(0, z1) # Second layer pre-activation z2 = a1.dot(W2) + b2 scores = z2 ############################################################################# # END OF YOUR CODE # ############################################################################# # If the targets are not given then jump out, we're done if y is None: return scores # Compute the loss loss = None ############################################################################# # TODO: Finish the forward pass, and compute the loss. This should include # # both the data loss and L2 regularization for W1 and W2. Store the result # # in the variable loss, which should be a scalar. Use the Softmax # # classifier loss. So that your results match ours, multiply the # # regularization loss by 0.5 # ############################################################################# exp_scores = np.exp(scores) a2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) correct_log_probs = -np.log(a2[range(N), y]) data_loss = np.sum(correct_log_probs) / N reg_loss = 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) loss = data_loss + reg_loss ############################################################################# # END OF YOUR CODE # ############################################################################# # Backward pass: compute gradients grads = &#123;&#125; ############################################################################# # TODO: Compute the backward pass, computing the derivatives of the weights # # and biases. Store the results in the grads dictionary. For example, # # grads['W1'] should store the gradient on W1, and be a matrix of same size # ############################################################################# dscores = a2 dscores[range(N), y] -= 1 dscores /= N grads['W2'] = np.dot(a1.T, dscores) grads['b2'] = np.sum(dscores, axis=0) dhidden = np.dot(dscores, W2.T) dhidden[z1 &lt;= 0] = 0 grads['W1'] = np.dot(X.T, dhidden) grads['b1'] = np.sum(dhidden, axis=0) grads['W2'] += reg * W2 grads['W1'] += reg * W1 ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, grads def train(self, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=0.95, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this neural network using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) giving training data. - y: A numpy array f shape (N,) giving training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - X_val: A numpy array of shape (N_val, D) giving validation data. - y_val: A numpy array of shape (N_val,) giving validation labels. - learning_rate: Scalar giving learning rate for optimization. - learning_rate_decay: Scalar giving factor used to decay the learning rate after each epoch. - reg: Scalar giving regularization strength. - num_iters: Number of steps to take when optimizing. - batch_size: Number of training examples to use per step. - verbose: boolean; if true print progress during optimization. """ num_train = X.shape[0] iterations_per_epoch = max(num_train / batch_size, 1) # Use SGD to optimize the parameters in self.model loss_history = [] train_acc_history = [] val_acc_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: Create a random minibatch of training data and labels, storing # # them in X_batch and y_batch respectively. # ######################################################################### sample_indices = np.random.choice(num_train, batch_size) X_batch = X[sample_indices] y_batch = y[sample_indices] ######################################################################### # END OF YOUR CODE # ######################################################################### # Compute loss and gradients using the current minibatch loss, grads = self.loss(X_batch, y=y_batch, reg=reg) loss_history.append(loss) ######################################################################### # TODO: Use the gradients in the grads dictionary to update the # # parameters of the network (stored in the dictionary self.params) # # using stochastic gradient descent. You'll need to use the gradients # # stored in the grads dictionary defined above. # ######################################################################### self.params['W1'] += -learning_rate * grads['W1'] self.params['b1'] += -learning_rate * grads['b1'] self.params['W2'] += -learning_rate * grads['W2'] self.params['b2'] += -learning_rate * grads['b2'] ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print 'iteration %d / %d: loss %f' % (it, num_iters, loss) # Every epoch, check train and val accuracy and decay learning rate. if it % iterations_per_epoch == 0: # Check accuracy train_acc = (self.predict(X_batch) == y_batch).mean() val_acc = (self.predict(X_val) == y_val).mean() train_acc_history.append(train_acc) val_acc_history.append(val_acc) # Decay learning rate learning_rate *= learning_rate_decay return &#123; 'loss_history': loss_history, 'train_acc_history': train_acc_history, 'val_acc_history': val_acc_history, &#125; def predict(self, X): """ Use the trained weights of this two-layer network to predict labels for data points. For each data point we predict scores for each of the C classes, and assign each data point to the class with the highest score. Inputs: - X: A numpy array of shape (N, D) giving N D-dimensional data points to classify. Returns: - y_pred: A numpy array of shape (N,) giving predicted labels for each of the elements of X. For all i, y_pred[i] = c means that X[i] is predicted to have class c, where 0 &lt;= c &lt; C. """ y_pred = None ########################################################################### # TODO: Implement this function; it should be VERY simple! # ########################################################################### z1 = X.dot(self.params['W1']) + self.params['b1'] a1 = np.maximum(0, z1) # pass through ReLU activation function scores = a1.dot(self.params['W2']) + self.params['b2'] y_pred = np.argmax(scores, axis=1) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note 09]]></title>
      <url>%2F2017%2F03%2F07%2Fpython-data-analysis-learning-note-09%2F</url>
      <content type="text"><![CDATA[数据聚合与分组运算12345678910from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)123pd.options.display.notebook_repr_html = Falsefrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlineGroupBy 机制12345df = DataFrame(&#123;'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np.random.randn(5)&#125;)df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 12grouped = df['data1'].groupby(df['key1'])grouped&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000008BAFA90&gt; 变量groupby是一个GroupBy对象。它实际还没有进行任何计算，只有进行计算之后才能显示结果1grouped.mean()key1 a 0.746672 b -0.537585 Name: data1, dtype: float64 12means = df['data1'].groupby([df['key1'], df['key2']]).mean()meanskey1 key2 a one 0.880536 two 0.478943 b one -0.519439 two -0.555730 Name: data1, dtype: float64 1means.unstack()key2 one two key1 a 0.880536 0.478943 b -0.519439 -0.555730 只要长度相同即可123states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])years = np.array([2005, 2005, 2006, 2005, 2006])df['data1'].groupby([states, years]).mean()California 2005 0.478943 2006 -0.519439 Ohio 2005 -0.380219 2006 1.965781 Name: data1, dtype: float64 只要数值型数据才会出现在结果中1df.groupby('key1').mean() data1 data2 key1 a 0.746672 0.910916 b -0.537585 0.525384 1df.groupby(['key1', 'key2']).mean() data1 data2 key1 key2 a one 0.880536 1.319920 two 0.478943 0.092908 b one -0.519439 0.281746 two -0.555730 0.769023 1df.groupby(['key1', 'key2']).size()key1 key2 a one 2 two 1 b one 1 two 1 dtype: int64 对分组进行迭代显示分组数据（要通过这种迭代的方式才能显示）1df.groupby('key1')&lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000034BC8CF8&gt; 1234dffor name, group in df.groupby('key1'): print(name) print(group) data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one a data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 4 1.965781 1.246435 a one b data1 data2 key1 key2 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 同样进行迭代才能显示结果123for (k1, k2), group in df.groupby(['key1', 'key2']): print((k1, k2)) print(group)(&apos;a&apos;, &apos;one&apos;) data1 data2 key1 key2 0 -0.204708 1.393406 a one 4 1.965781 1.246435 a one (&apos;a&apos;, &apos;two&apos;) data1 data2 key1 key2 1 0.478943 0.092908 a two (&apos;b&apos;, &apos;one&apos;) data1 data2 key1 key2 2 -0.519439 0.281746 b one (&apos;b&apos;, &apos;two&apos;) data1 data2 key1 key2 3 -0.55573 0.769023 b two 将分组结果转化成一个字典（要先转化为一个列表）12pieces = dict(list(df.groupby('key1')))pieces['b'] data1 data2 key1 key2 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 显示每一项的数据类型1df.dtypesdata1 float64 data2 float64 key1 object key2 object dtype: object 对列进行分组…按照数据类型来？！12grouped = df.groupby(df.dtypes, axis=1)dict(list(grouped)){dtype(&apos;float64&apos;): data1 data2 0 -0.204708 1.393406 1 0.478943 0.092908 2 -0.519439 0.281746 3 -0.555730 0.769023 4 1.965781 1.246435, dtype(&apos;O&apos;): key1 key2 0 a one 1 a two 2 b one 3 b two 4 a one} 选择一列或一组列12df.groupby('key1')['data1']df.groupby('key1')[['data2']]&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC2E8&gt; &lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000015EDF2B0&gt; 上述代码是以下代码的语法糖12df['data1'].groupby(df['key1'])df[['data2']].groupby(df['key1'])&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC8D0&gt; &lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000034BDC898&gt; 1df.groupby(['key1', 'key2'])[['data2']].mean() data2 key1 key2 a one 1.319920 two 0.092908 b one 0.281746 two 0.769023 12s_grouped = df.groupby(['key1', 'key2'])['data2']s_grouped&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC6A0&gt; 1s_grouped.mean()key1 key2 a one 1.319920 two 0.092908 b one 0.281746 two 0.769023 Name: data2, dtype: float64 通过字典或Series进行分组12345people = DataFrame(np.random.randn(5, 5), columns=['a', 'b', 'c', 'd', 'e'], index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA valuespeople a b c d e Joe 1.007189 -1.296221 0.274992 0.228913 1.352917 Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570 Wes -0.539741 NaN NaN -1.021228 -0.577087 Jim 0.124121 0.302614 0.523772 0.000940 1.343810 Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 12mapping = &#123;'a': 'red', 'b': 'red', 'c': 'blue', 'd': 'blue', 'e': 'red', 'f' : 'orange'&#125;会跳过NA值12by_column = people.groupby(mapping, axis=1)by_column.sum() blue red Joe 0.503905 1.063885 Steve 1.297183 -1.553778 Wes -1.021228 -1.116829 Jim 0.524712 1.770545 Travis -4.230992 -2.405455 上述功能同样可以通过Series实现12map_series = Series(mapping)map_seriesa red b red c blue d blue e red f orange dtype: object 1people.groupby(map_series, axis=1).count() blue red Joe 2 3 Steve 2 3 Wes 1 2 Jim 2 3 Travis 2 3 通过函数进行分组根据人名长度进行分组1people.groupby(len).sum() a b c d e 3 0.591569 -0.993608 0.798764 -0.791374 2.119639 5 0.886429 -2.001637 -0.371843 1.669025 -0.438570 6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 再加一个分组度量12key_list = ['one', 'one', 'one', 'two', 'two']people.groupby([len, key_list]).min() a b c d e 3 one -0.539741 -1.296221 0.274992 -1.021228 -0.577087 two 0.124121 0.302614 0.523772 0.000940 1.343810 5 one 0.886429 -2.001637 -0.371843 1.669025 -0.438570 6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 根据索引级别分组1234columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'], [1, 3, 5, 1, 3]], names=['cty', 'tenor'])hier_df = DataFrame(np.random.randn(4, 5), columns=columns)hier_dfcty US JP tenor 1 3 5 1 3 0 0.560145 -1.265934 0.119827 -1.063512 0.332883 1 -2.359419 -0.199543 -1.541996 -0.970736 -1.307030 2 0.286350 0.377984 -0.753887 0.331286 1.349742 3 0.069877 0.246674 -0.011862 1.004812 1.327195 1hier_df.groupby(level='cty', axis=1).count()cty JP US 0 2 3 1 2 3 2 2 3 3 2 3 数据聚合1df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 对分组后的数据进行相应操作12grouped = df.groupby('key1')grouped['data1'].quantile(0.9)key1 a 1.668413 b -0.523068 Name: data1, dtype: float64 通过函数进行聚合操作123def peak_to_peak(arr): return arr.max() - arr.min()grouped.agg(peak_to_peak) data1 data2 key1 a 2.170488 1.300498 b 0.036292 0.487276 列出分组后数据的一些常用属性1grouped.describe() data1 data2 key1 a count 3.000000 3.000000 mean 0.746672 0.910916 std 1.109736 0.712217 min -0.204708 0.092908 25% 0.137118 0.669671 50% 0.478943 1.246435 75% 1.222362 1.319920 max 1.965781 1.393406 b count 2.000000 2.000000 mean -0.537585 0.525384 std 0.025662 0.344556 min -0.555730 0.281746 25% -0.546657 0.403565 50% -0.537585 0.525384 75% -0.528512 0.647203 max -0.519439 0.769023 导入一个数据集用于接下来更加高级的聚合操作1234tips = pd.read_csv('ch08/tips.csv')# Add tip percentage of total billtips['tip_pct'] = tips['tip'] / tips['total_bill']tips[:6] total_bill tip sex smoker day time size_ tip_pct 0 16.99 1.01 Female No Sun Dinner 2 0.059447 1 10.34 1.66 Male No Sun Dinner 3 0.160542 2 21.01 3.50 Male No Sun Dinner 3 0.166587 3 23.68 3.31 Male No Sun Dinner 2 0.139780 4 24.59 3.61 Female No Sun Dinner 4 0.146808 5 25.29 4.71 Male No Sun Dinner 4 0.186240 面向列的多函数应用根据性别以及是否吸烟进行分类1grouped = tips.groupby(['sex', 'smoker'])算出不同类型的顾客所给的小费占总花费的比例的平均值12grouped_pct = grouped['tip_pct']grouped_pct.agg('mean')sex smoker Female No 0.156921 Yes 0.182150 Male No 0.160669 Yes 0.152771 Name: tip_pct, dtype: float64 同时算出比例的均值、标准差以及范围大小1grouped_pct.agg(['mean', 'std', peak_to_peak]) mean std peak_to_peak sex smoker Female No 0.156921 0.036421 0.195876 Yes 0.182150 0.071595 0.360233 Male No 0.160669 0.041849 0.220186 Yes 0.152771 0.090588 0.674707 起一个别名1grouped_pct.agg([('foo', 'mean'), ('bar', np.std)]) foo bar sex smoker Female No 0.156921 0.036421 Yes 0.182150 0.071595 Male No 0.160669 0.041849 Yes 0.152771 0.090588 对分组后的数据的两个属性分别做三个不同的操作123functions = ['count', 'mean', 'max']result = grouped['tip_pct', 'total_bill'].agg(functions)result tip_pct total_bill count mean max count mean max sex smoker Female No 54 0.156921 0.252672 54 18.105185 35.83 Yes 33 0.182150 0.416667 33 17.977879 44.30 Male No 97 0.160669 0.291990 97 19.791237 48.33 Yes 60 0.152771 0.710345 60 22.284500 50.81 提取出上述两个属性中的一个1result['tip_pct'] count mean max sex smoker Female No 54 0.156921 0.252672 Yes 33 0.182150 0.416667 Male No 97 0.160669 0.291990 Yes 60 0.152771 0.710345 对多个属性进行多个操作的同时进行起别名的操作12ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]grouped['tip_pct', 'total_bill'].agg(ftuples) tip_pct total_bill Durchschnitt Abweichung Durchschnitt Abweichung sex smoker Female No 0.156921 0.001327 18.105185 53.092422 Yes 0.182150 0.005126 17.977879 84.451517 Male No 0.160669 0.001751 19.791237 76.152961 Yes 0.152771 0.008206 22.284500 98.244673 对不同的列进行不同的操作1grouped.agg(&#123;'tip' : np.max, 'size_' : 'sum'&#125;) size_ tip sex smoker Female No 140 5.2 Yes 74 6.5 Male No 263 9.0 Yes 150 10.0 对不同的列进行数量不同类型不同的操作12grouped.agg(&#123;'tip_pct' : ['min', 'max', 'mean', 'std'], 'size_' : 'sum'&#125;) tip_pct size_ min max mean std sum sex smoker Female No 0.056797 0.252672 0.156921 0.036421 140 Yes 0.056433 0.416667 0.182150 0.071595 74 Male No 0.071804 0.291990 0.160669 0.041849 263 Yes 0.035638 0.710345 0.152771 0.090588 150 以无索引的形式返回聚合数据1tips.groupby(['sex', 'smoker'], as_index=False).mean() sex smoker total_bill tip size_ tip_pct 0 Female No 18.105185 2.773519 2.592593 0.156921 1 Female Yes 17.977879 2.931515 2.242424 0.182150 2 Male No 19.791237 3.113402 2.711340 0.160669 3 Male Yes 22.284500 3.051167 2.500000 0.152771 1tips.groupby(['sex', 'smoker'], as_index=True).mean() total_bill tip size_ tip_pct sex smoker Female No 18.105185 2.773519 2.592593 0.156921 Yes 17.977879 2.931515 2.242424 0.182150 Male No 19.791237 3.113402 2.711340 0.160669 Yes 22.284500 3.051167 2.500000 0.152771 分组级运算和转换1df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 12k1_means = df.groupby('key1').mean().add_prefix('mean_')k1_means mean_data1 mean_data2 key1 a 0.746672 0.910916 b -0.537585 0.525384 保留原索引1pd.merge(df, k1_means, left_on='key1', right_index=True) data1 data2 key1 key2 mean_data1 mean_data2 0 -0.204708 1.393406 a one 0.746672 0.910916 1 0.478943 0.092908 a two 0.746672 0.910916 4 1.965781 1.246435 a one 0.746672 0.910916 2 -0.519439 0.281746 b one -0.537585 0.525384 3 -0.555730 0.769023 b two -0.537585 0.525384 另一个例子，以更简洁的方式实现上述功能1people a b c d e Joe 1.007189 -1.296221 0.274992 0.228913 1.352917 Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570 Wes -0.539741 NaN NaN -1.021228 -0.577087 Jim 0.124121 0.302614 0.523772 0.000940 1.343810 Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 12key = ['one', 'two', 'one', 'two', 'one']people.groupby(key).mean() a b c d e one -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 two 0.505275 -0.849512 0.075965 0.834983 0.452620 将聚合后的结果放回原来数据中合适的位置（标量进行广播）1people.groupby(key).transform(np.mean) a b c d e Joe -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 Steve 0.505275 -0.849512 0.075965 0.834983 0.452620 Wes -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 Jim 0.505275 -0.849512 0.075965 0.834983 0.452620 Travis -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 同时减去均值1234def demean(arr): return arr - arr.mean()demeaned = people.groupby(key).transform(demean)demeaned a b c d e Joe 1.089221 -0.232534 1.322612 1.113271 1.381226 Steve 0.381154 -1.152125 -0.447807 0.834043 -0.891190 Wes -0.457709 NaN NaN -0.136869 -0.548778 Jim -0.381154 1.152125 0.447807 -0.834043 0.891190 Travis -0.631512 0.232534 -1.322612 -0.976402 -0.832448 检验一下1demeaned.groupby(key).mean() a b c d e one 0.000000e+00 -1.110223e-16 0.0 7.401487e-17 0.0 two -2.775558e-17 0.000000e+00 0.0 0.000000e+00 0.0 Apply: 一般性的 “拆分-应用-合并”小费数据，根据某一个属性从大到小进行排序123def top(df, n=5, column='tip_pct'): return df.sort_values(by=column)[-n:]top(tips, n=6) total_bill tip sex smoker day time size_ tip_pct 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 232 11.61 3.39 Male No Sat Dinner 2 0.291990 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 在分组后的数据集上进行上述排序操作，说明分组后的每一组都是一个DataFrame对象1tips.groupby('smoker').apply(top) total_bill tip sex smoker day time size_ tip_pct smoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 1tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill') total_bill tip sex smoker day time size_ \ smoker day No Fri 94 22.75 3.25 Female No Fri Dinner 2 Sat 212 48.33 9.00 Male No Sat Dinner 4 Sun 156 48.17 5.00 Male No Sun Dinner 6 Thur 142 41.19 5.00 Male No Thur Lunch 5 Yes Fri 95 40.17 4.73 Male Yes Fri Dinner 4 Sat 170 50.81 10.00 Male Yes Sat Dinner 3 Sun 182 45.35 3.50 Male Yes Sun Dinner 3 Thur 197 43.11 5.00 Female Yes Thur Lunch 4 tip_pct smoker day No Fri 94 0.142857 Sat 212 0.186220 Sun 156 0.103799 Thur 142 0.121389 Yes Fri 95 0.117750 Sat 170 0.196812 Sun 182 0.077178 Thur 197 0.115982 获取分组后数据某一列的统计数据12result = tips.groupby('smoker')['tip_pct'].describe()resultsmoker No count 151.000000 mean 0.159328 std 0.039910 min 0.056797 25% 0.136906 50% 0.155625 75% 0.185014 max 0.291990 Yes count 93.000000 mean 0.163196 std 0.085119 min 0.035638 25% 0.106771 50% 0.153846 75% 0.195059 max 0.710345 Name: tip_pct, dtype: float64 1result.unstack('smoker')smoker No Yes count 151.000000 93.000000 mean 0.159328 0.163196 std 0.039910 0.085119 min 0.056797 0.035638 25% 0.136906 0.106771 50% 0.155625 0.153846 75% 0.185014 0.195059 max 0.291990 0.710345 grouped 根据性别以及是否吸烟进行分组12f = lambda x: x.describe()grouped.apply(f) total_bill tip size_ tip_pct sex smoker Female No count 54.000000 54.000000 54.000000 54.000000 mean 18.105185 2.773519 2.592593 0.156921 std 7.286455 1.128425 1.073146 0.036421 min 7.250000 1.000000 1.000000 0.056797 25% 12.650000 2.000000 2.000000 0.139708 50% 16.690000 2.680000 2.000000 0.149691 75% 20.862500 3.437500 3.000000 0.181630 max 35.830000 5.200000 6.000000 0.252672 Yes count 33.000000 33.000000 33.000000 33.000000 mean 17.977879 2.931515 2.242424 0.182150 std 9.189751 1.219916 0.613917 0.071595 min 3.070000 1.000000 1.000000 0.056433 25% 12.760000 2.000000 2.000000 0.152439 50% 16.270000 2.880000 2.000000 0.173913 75% 22.120000 3.500000 2.000000 0.198216 max 44.300000 6.500000 4.000000 0.416667 Male No count 97.000000 97.000000 97.000000 97.000000 mean 19.791237 3.113402 2.711340 0.160669 std 8.726566 1.489559 0.989094 0.041849 min 7.510000 1.250000 2.000000 0.071804 25% 13.810000 2.000000 2.000000 0.131810 50% 18.240000 2.740000 2.000000 0.157604 75% 22.820000 3.710000 3.000000 0.186220 max 48.330000 9.000000 6.000000 0.291990 Yes count 60.000000 60.000000 60.000000 60.000000 mean 22.284500 3.051167 2.500000 0.152771 std 9.911845 1.500120 0.892530 0.090588 min 7.250000 1.000000 1.000000 0.035638 25% 15.272500 2.000000 2.000000 0.101845 50% 20.390000 3.000000 2.000000 0.141015 75% 28.572500 3.820000 3.000000 0.191697 max 50.810000 10.000000 5.000000 0.710345 禁止分组键1tips.groupby('smoker', group_keys=True).apply(top) total_bill tip sex smoker day time size_ tip_pct smoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 1tips.groupby('smoker', group_keys=False).apply(top) total_bill tip sex smoker day time size_ tip_pct 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 分位数与桶分析1234frame = DataFrame(&#123;'data1': np.random.randn(1000), 'data2': np.random.randn(1000)&#125;)factor = pd.cut(frame.data1, 4)factor[:10]0 (-1.23, 0.489] 1 (-2.956, -1.23] 2 (-1.23, 0.489] 3 (0.489, 2.208] 4 (-1.23, 0.489] 5 (0.489, 2.208] 6 (-1.23, 0.489] 7 (-1.23, 0.489] 8 (0.489, 2.208] 9 (0.489, 2.208] Name: data1, dtype: category Categories (4, object): [(-2.956, -1.23] &lt; (-1.23, 0.489] &lt; (0.489, 2.208] &lt; (2.208, 3.928]] 12345678def get_stats(group): return &#123;'min': group.min(), 'max': group.max(), 'count': group.count(), 'mean': group.mean()&#125;grouped = frame.data2.groupby(factor)grouped.apply(get_stats).unstack()#ADAPT the output is not sorted in the book while this is the case now (swap first two lines) count max mean min data1 (-2.956, -1.23] 95.0 1.670835 -0.039521 -3.399312 (-1.23, 0.489] 598.0 3.260383 -0.002051 -2.989741 (0.489, 2.208] 297.0 2.954439 0.081822 -3.745356 (2.208, 3.928] 10.0 1.765640 0.024750 -1.929776 12345# Return quantile numbersgrouping = pd.qcut(frame.data1, 10, labels=False)grouped = frame.data2.groupby(grouping)grouped.apply(get_stats).unstack() count max mean min data1 0 100.0 1.670835 -0.049902 -3.399312 1 100.0 2.628441 0.030989 -1.950098 2 100.0 2.527939 -0.067179 -2.925113 3 100.0 3.260383 0.065713 -2.315555 4 100.0 2.074345 -0.111653 -2.047939 5 100.0 2.184810 0.052130 -2.989741 6 100.0 2.458842 -0.021489 -2.223506 7 100.0 2.954439 -0.026459 -3.056990 8 100.0 2.735527 0.103406 -3.745356 9 100.0 2.377020 0.220122 -2.064111 Example: 用特定分组的值填充缺失值填一些缺失值进去123s = Series(np.random.randn(6))s[::2] = np.nans0 NaN 1 -0.125921 2 NaN 3 -0.884475 4 NaN 5 0.227290 dtype: float64 用均值填充缺失值1s.fillna(s.mean())0 -0.261035 1 -0.125921 2 -0.261035 3 -0.884475 4 -0.261035 5 0.227290 dtype: float64 同样，填一些缺失值123456states = ['Ohio', 'New York', 'Vermont', 'Florida', 'Oregon', 'Nevada', 'California', 'Idaho']group_key = ['East'] * 4 + ['West'] * 4data = Series(np.random.randn(8), index=states)data[['Vermont', 'Nevada', 'Idaho']] = np.nandataOhio 0.922264 New York -2.153545 Vermont NaN Florida -0.375842 Oregon 0.329939 Nevada NaN California 1.105913 Idaho NaN dtype: float64 计算分组均值1data.groupby(group_key).mean()East -0.535707 West 0.717926 dtype: float64 这里的g指代调用apply的主体，也就是data.groupby(group_key)分组后的结果12fill_mean = lambda g: g.fillna(g.mean())data.groupby(group_key).apply(fill_mean)Ohio 0.922264 New York -2.153545 Vermont -0.535707 Florida -0.375842 Oregon 0.329939 Nevada 0.717926 California 1.105913 Idaho 0.717926 dtype: float64 由于groupby操作后得到的结果类似于一个字典，字典key是组名，value是一个DataFrame Object1234fill_values = &#123;'East': 0.5, 'West': -1&#125;fill_func = lambda g: g.fillna(fill_values[g.name])data.groupby(group_key).apply(fill_func)Ohio 0.922264 New York -2.153545 Vermont 0.500000 Florida -0.375842 Oregon 0.329939 Nevada -1.000000 California 1.105913 Idaho -1.000000 dtype: float64 Example: 随机采样和排列构造扑克牌红桃Hearts, 黑桃Spades, 梅花Clubs, 方片Diamonds123456789# Hearts, Spades, Clubs, Diamondssuits = ['H', 'S', 'C', 'D']card_val = (list(range(1, 11)) + [10] * 3) * 4base_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']cards = []for suit in ['H', 'S', 'C', 'D']: cards.extend(str(num) + suit for num in base_names)deck = Series(card_val, index=cards)1deck[:13]AH 1 2H 2 3H 3 4H 4 5H 5 6H 6 7H 7 8H 8 9H 9 10H 10 JH 10 KH 10 QH 10 dtype: int64 随机抽牌123def draw(deck, n=5): return deck.take(np.random.permutation(len(deck))[:n])draw(deck)AD 1 8C 8 5H 5 KC 10 2C 2 dtype: int64 分类抽牌12get_suit = lambda card: card[-1] # last letter is suitdeck.groupby(get_suit).apply(draw, n=2)C 2C 2 3C 3 D KD 10 8D 8 H KH 10 3H 3 S 2S 2 4S 4 dtype: int64 去掉分组键12# alternativelydeck.groupby(get_suit, group_keys=False).apply(draw, n=2)KC 10 JC 10 AD 1 5D 5 5H 5 6H 6 7S 7 KS 10 dtype: int64 Example: 分组加权平均数和相关系数1234df = DataFrame(&#123;'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], 'data': np.random.randn(8), 'weights': np.random.rand(8)&#125;)df category data weights 0 a 1.561587 0.957515 1 a 1.219984 0.347267 2 a -0.482239 0.581362 3 a 0.315667 0.217091 4 b -0.047852 0.894406 5 b -0.454145 0.918564 6 b -0.556774 0.277825 7 b 0.253321 0.955905 123grouped = df.groupby('category')get_wavg = lambda g: np.average(g['data'], weights=g['weights'])grouped.apply(get_wavg)category a 0.811643 b -0.122262 dtype: float64 stock数据集12close_px = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)close_px.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; DatetimeIndex: 2214 entries, 2003-01-02 to 2011-10-14 Data columns (total 4 columns): AAPL 2214 non-null float64 MSFT 2214 non-null float64 XOM 2214 non-null float64 SPX 2214 non-null float64 dtypes: float64(4) memory usage: 86.5 KB 1close_px[-4:] AAPL MSFT XOM SPX 2011-10-11 400.29 27.00 76.27 1195.54 2011-10-12 402.19 26.96 77.16 1207.25 2011-10-13 408.43 27.18 76.37 1203.66 2011-10-14 422.00 27.27 78.11 1224.58 计算相关系数1234rets = close_px.pct_change().dropna()spx_corr = lambda x: x.corrwith(x['SPX'])by_year = rets.groupby(lambda x: x.year)by_year.apply(spx_corr) AAPL MSFT XOM SPX 2003 0.541124 0.745174 0.661265 1.0 2004 0.374283 0.588531 0.557742 1.0 2005 0.467540 0.562374 0.631010 1.0 2006 0.428267 0.406126 0.518514 1.0 2007 0.508118 0.658770 0.786264 1.0 2008 0.681434 0.804626 0.828303 1.0 2009 0.707103 0.654902 0.797921 1.0 2010 0.710105 0.730118 0.839057 1.0 2011 0.691931 0.800996 0.859975 1.0 lambda看来有很大用处12# Annual correlation of Apple with Microsoftby_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))2003 0.480868 2004 0.259024 2005 0.300093 2006 0.161735 2007 0.417738 2008 0.611901 2009 0.432738 2010 0.571946 2011 0.581987 dtype: float64 Example: 分组级线型回归1234567import statsmodels.api as smdef regress(data, yvar, xvars): Y = data[yvar] X = data[xvars] X['intercept'] = 1. result = sm.OLS(Y, X).fit() return result.params这样传参1by_year.apply(regress, 'AAPL', ['SPX']) SPX intercept 2003 1.195406 0.000710 2004 1.363463 0.004201 2005 1.766415 0.003246 2006 1.645496 0.000080 2007 1.198761 0.003438 2008 0.968016 -0.001110 2009 0.879103 0.002954 2010 1.052608 0.001261 2011 0.806605 0.001514 透视表和交叉表1tips[:10] total_bill tip sex smoker day time size_ tip_pct 0 16.99 1.01 Female No Sun Dinner 2 0.059447 1 10.34 1.66 Male No Sun Dinner 3 0.160542 2 21.01 3.50 Male No Sun Dinner 3 0.166587 3 23.68 3.31 Male No Sun Dinner 2 0.139780 4 24.59 3.61 Female No Sun Dinner 4 0.146808 5 25.29 4.71 Male No Sun Dinner 4 0.186240 6 8.77 2.00 Male No Sun Dinner 2 0.228050 7 26.88 3.12 Male No Sun Dinner 4 0.116071 8 15.04 1.96 Male No Sun Dinner 2 0.130319 9 14.78 3.23 Male No Sun Dinner 2 0.218539 pivot_table默认情况相当于分组后进行mean()操作1tips.pivot_table(index=['sex', 'smoker']) size_ tip tip_pct total_bill sex smoker Female No 2.592593 2.773519 0.156921 18.105185 Yes 2.242424 2.931515 0.182150 17.977879 Male No 2.711340 3.113402 0.160669 19.791237 Yes 2.500000 3.051167 0.152771 22.284500 指定分组度量12tips.pivot_table(['tip_pct', 'size_'], index=['sex', 'day'], columns='smoker') tip_pct size_ smoker No Yes No Yes sex day Female Fri 0.165296 0.209129 2.500000 2.000000 Sat 0.147993 0.163817 2.307692 2.200000 Sun 0.165710 0.237075 3.071429 2.500000 Thur 0.155971 0.163073 2.480000 2.428571 Male Fri 0.138005 0.144730 2.000000 2.125000 Sat 0.162132 0.139067 2.656250 2.629630 Sun 0.158291 0.173964 2.883721 2.600000 Thur 0.165706 0.164417 2.500000 2.300000 增加ALL列12tips.pivot_table(['tip_pct', 'size_'], index=['sex', 'day'], columns='smoker', margins=True) tip_pct size_ smoker No Yes All No Yes All sex day Female Fri 0.165296 0.209129 0.199388 2.500000 2.000000 2.111111 Sat 0.147993 0.163817 0.156470 2.307692 2.200000 2.250000 Sun 0.165710 0.237075 0.181569 3.071429 2.500000 2.944444 Thur 0.155971 0.163073 0.157525 2.480000 2.428571 2.468750 Male Fri 0.138005 0.144730 0.143385 2.000000 2.125000 2.100000 Sat 0.162132 0.139067 0.151577 2.656250 2.629630 2.644068 Sun 0.158291 0.173964 0.162344 2.883721 2.600000 2.810345 Thur 0.165706 0.164417 0.165276 2.500000 2.300000 2.433333 All 0.159328 0.163196 0.160803 2.668874 2.408602 2.569672 更换一个分组度量12tips.pivot_table('tip_pct', index=['sex', 'smoker'], columns='day', aggfunc=len, margins=True)day Fri Sat Sun Thur All sex smoker Female No 2.0 13.0 14.0 25.0 54.0 Yes 7.0 15.0 4.0 7.0 33.0 Male No 2.0 32.0 43.0 20.0 97.0 Yes 8.0 27.0 15.0 10.0 60.0 All 19.0 87.0 76.0 62.0 244.0 分组计数并填充（可能存在空组合）12tips.pivot_table('size_', index=['time', 'sex', 'smoker'], columns='day', aggfunc='sum', fill_value=0)day Fri Sat Sun Thur time sex smoker Dinner Female No 2 30 43 2 Yes 8 33 10 0 Male No 4 85 124 0 Yes 12 71 39 0 Lunch Female No 3 0 0 60 Yes 6 0 0 17 Male No 0 0 0 50 Yes 5 0 0 23 交叉表: crosstab1234567891011121314from StringIO import StringIOdata = """\Sample Gender Handedness1 Female Right-handed2 Male Left-handed3 Female Right-handed4 Male Right-handed5 Male Left-handed6 Male Right-handed7 Female Right-handed8 Female Left-handed9 Male Right-handed10 Female Right-handed"""data = pd.read_table(StringIO(data), sep='\s+')1data Sample Gender Handedness 0 1 Female Right-handed 1 2 Male Left-handed 2 3 Female Right-handed 3 4 Male Right-handed 4 5 Male Left-handed 5 6 Male Right-handed 6 7 Female Right-handed 7 8 Female Left-handed 8 9 Male Right-handed 9 10 Female Right-handed 交叉表就是在…计数…1pd.crosstab(data.Gender, data.Handedness, margins=True)Handedness Left-handed Right-handed All Gender Female 1 4 5 Male 2 3 5 All 3 7 10 1pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)smoker No Yes All time day Dinner Fri 3 9 12 Sat 45 42 87 Sun 57 19 76 Thur 1 0 1 Lunch Fri 1 6 7 Thur 44 17 61 All 151 93 244 Example: 2012 联邦选举委员会数据库这个数据库包括赞助人的姓名，职业、雇主、地址以及出资额等信息1fec = pd.read_csv('ch09/P00000001-ALL.csv')1fec.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 1001731 entries, 0 to 1001730 Data columns (total 16 columns): cmte_id 1001731 non-null object cand_id 1001731 non-null object cand_nm 1001731 non-null object contbr_nm 1001731 non-null object contbr_city 1001712 non-null object contbr_st 1001727 non-null object contbr_zip 1001620 non-null object contbr_employer 988002 non-null object contbr_occupation 993301 non-null object contb_receipt_amt 1001731 non-null float64 contb_receipt_dt 1001731 non-null object receipt_desc 14166 non-null object memo_cd 92482 non-null object memo_text 97770 non-null object form_tp 1001731 non-null object file_num 1001731 non-null int64 dtypes: float64(1), int64(1), object(14) memory usage: 122.3+ MB 1fec.ix[123456]cmte_id C00431445 cand_id P80003338 cand_nm Obama, Barack contbr_nm ELLMAN, IRA contbr_city TEMPE contbr_st AZ contbr_zip 852816719 contbr_employer ARIZONA STATE UNIVERSITY contbr_occupation PROFESSOR contb_receipt_amt 50 contb_receipt_dt 01-DEC-11 receipt_desc NaN memo_cd NaN memo_text NaN form_tp SA17A file_num 772372 Name: 123456, dtype: object 输出全部候选人名单12unique_cands = fec.cand_nm.unique()unique_candsarray([&apos;Bachmann, Michelle&apos;, &apos;Romney, Mitt&apos;, &apos;Obama, Barack&apos;, &quot;Roemer, Charles E. &apos;Buddy&apos; III&quot;, &apos;Pawlenty, Timothy&apos;, &apos;Johnson, Gary Earl&apos;, &apos;Paul, Ron&apos;, &apos;Santorum, Rick&apos;, &apos;Cain, Herman&apos;, &apos;Gingrich, Newt&apos;, &apos;McCotter, Thaddeus G&apos;, &apos;Huntsman, Jon&apos;, &apos;Perry, Rick&apos;], dtype=object) 1unique_cands[2]&apos;Obama, Barack&apos; 政党映射12345678910111213parties = &#123;'Bachmann, Michelle': 'Republican', 'Cain, Herman': 'Republican', 'Gingrich, Newt': 'Republican', 'Huntsman, Jon': 'Republican', 'Johnson, Gary Earl': 'Republican', 'McCotter, Thaddeus G': 'Republican', 'Obama, Barack': 'Democrat', 'Paul, Ron': 'Republican', 'Pawlenty, Timothy': 'Republican', 'Perry, Rick': 'Republican', "Roemer, Charles E. 'Buddy' III": 'Republican', 'Romney, Mitt': 'Republican', 'Santorum, Rick': 'Republican'&#125;1fec.cand_nm[123456:123461]123456 Obama, Barack 123457 Obama, Barack 123458 Obama, Barack 123459 Obama, Barack 123460 Obama, Barack Name: cand_nm, dtype: object 1fec.cand_nm[123456:123461].map(parties)123456 Democrat 123457 Democrat 123458 Democrat 123459 Democrat 123460 Democrat Name: cand_nm, dtype: object 根据以上创建的映射，在原数据集中添加一列party12# Add it as a columnfec['party'] = fec.cand_nm.map(parties)1fec['party'].value_counts()Democrat 593746 Republican 407985 Name: party, dtype: int64 看看出资额是正是负1(fec.contb_receipt_amt &gt; 0).value_counts()True 991475 False 10256 Name: contb_receipt_amt, dtype: int64 调整出资额为正1fec = fec[fec.contb_receipt_amt &gt; 0]筛选候选人1fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]根据职业和雇主统计赞助信息统计职业信息1fec.contbr_occupation.value_counts()[:10]RETIRED 233990 INFORMATION REQUESTED 35107 ATTORNEY 34286 HOMEMAKER 29931 PHYSICIAN 23432 INFORMATION REQUESTED PER BEST EFFORTS 21138 ENGINEER 14334 TEACHER 13990 CONSULTANT 13273 PROFESSOR 12555 Name: contbr_occupation, dtype: int64 筛选出一些不符合规格的信息映射到正常信息12345678910occ_mapping = &#123; 'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', 'INFORMATION REQUESTED' : 'NOT PROVIDED', 'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED', 'C.E.O.': 'CEO'&#125;# If no mapping provided, return xf = lambda x: occ_mapping.get(x, x)fec.contbr_occupation = fec.contbr_occupation.map(f)以上巧妙运用了get方法12345678910emp_mapping = &#123; 'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', 'INFORMATION REQUESTED' : 'NOT PROVIDED', 'SELF' : 'SELF-EMPLOYED', 'SELF EMPLOYED' : 'SELF-EMPLOYED',&#125;# If no mapping provided, return xf = lambda x: emp_mapping.get(x, x)fec.contbr_employer = fec.contbr_employer.map(f)根据职业以及候选人政党分组，统计出资额总和123by_occupation = fec.pivot_table('contb_receipt_amt', index='contbr_occupation', columns='party', aggfunc='sum')12over_2mm = by_occupation[by_occupation.sum(1) &gt; 2000000]over_2mmparty Democrat Republican contbr_occupation ATTORNEY 11141982.97 7.477194e+06 CEO 2074974.79 4.211041e+06 CONSULTANT 2459912.71 2.544725e+06 ENGINEER 951525.55 1.818374e+06 EXECUTIVE 1355161.05 4.138850e+06 HOMEMAKER 4248875.80 1.363428e+07 INVESTOR 884133.00 2.431769e+06 LAWYER 3160478.87 3.912243e+05 MANAGER 762883.22 1.444532e+06 NOT PROVIDED 4866973.96 2.056547e+07 OWNER 1001567.36 2.408287e+06 PHYSICIAN 3735124.94 3.594320e+06 PRESIDENT 1878509.95 4.720924e+06 PROFESSOR 2165071.08 2.967027e+05 REAL ESTATE 528902.09 1.625902e+06 RETIRED 25305116.38 2.356124e+07 SELF-EMPLOYED 672393.40 1.640253e+06 1over_2mm.plot(kind='barh')&lt;matplotlib.axes._subplots.AxesSubplot at 0x340fb4e0&gt; 12345def get_top_amounts(group, key, n=5): totals = group.groupby(key)['contb_receipt_amt'].sum() # Order totals by key in descending order return totals.sort_values()[-n:]12grouped = fec_mrbo.groupby('cand_nm')grouped.apply(get_top_amounts, 'contbr_occupation', n=7)cand_nm contbr_occupation Obama, Barack CONSULTANT 2459912.71 LAWYER 3160478.87 PHYSICIAN 3735124.94 HOMEMAKER 4248875.80 INFORMATION REQUESTED 4866973.96 ATTORNEY 11141982.97 RETIRED 25305116.38 Romney, Mitt C.E.O. 1968386.11 EXECUTIVE 2300947.03 PRESIDENT 2491244.89 ATTORNEY 5364718.82 HOMEMAKER 8147446.22 INFORMATION REQUESTED PER BEST EFFORTS 11396894.84 RETIRED 11508473.59 Name: contb_receipt_amt, dtype: float64 1grouped.apply(get_top_amounts, 'contbr_employer', n=10)cand_nm contbr_employer Obama, Barack MICROSOFT 215585.36 VOLUNTEER 257104.00 STUDENT 318831.45 SELF EMPLOYED 469290.00 SELF 1076531.20 HOMEMAKER 2605408.54 INFORMATION REQUESTED 5053480.37 NOT EMPLOYED 8586308.70 SELF-EMPLOYED 17080985.96 RETIRED 22694358.85 Romney, Mitt H.I.G. CAPITAL 139500.00 BARCLAYS CAPITAL 162750.00 GOLDMAN SACH &amp; CO. 238250.00 MORGAN STANLEY 267266.00 CREDIT SUISSE 281150.00 STUDENT 496490.94 SELF-EMPLOYED 7409860.98 HOMEMAKER 8147196.22 RETIRED 11506225.71 INFORMATION REQUESTED PER BEST EFFORTS 12059527.24 Name: contb_receipt_amt, dtype: float64 根据出资额分组不出意外果然要用到桶123bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)labels[:10]411 (10, 100] 412 (100, 1000] 413 (100, 1000] 414 (10, 100] 415 (10, 100] 416 (10, 100] 417 (100, 1000] 418 (10, 100] 419 (100, 1000] 420 (10, 100] Name: contb_receipt_amt, dtype: category Categories (8, object): [(0, 1] &lt; (1, 10] &lt; (10, 100] &lt; (100, 1000] &lt; (1000, 10000] &lt; (10000, 100000] &lt; (100000, 1000000] &lt; (1000000, 10000000]] 12grouped = fec_mrbo.groupby(['cand_nm', labels])grouped.size().unstack(0)cand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 493.0 77.0 (1, 10] 40070.0 3681.0 (10, 100] 372280.0 31853.0 (100, 1000] 153991.0 43357.0 (1000, 10000] 22284.0 26186.0 (10000, 100000] 2.0 1.0 (100000, 1000000] 3.0 NaN (1000000, 10000000] 4.0 NaN 12bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)bucket_sumscand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 318.24 77.00 (1, 10] 337267.62 29819.66 (10, 100] 20288981.41 1987783.76 (100, 1000] 54798531.46 22363381.69 (1000, 10000] 51753705.67 63942145.42 (10000, 100000] 59100.00 12700.00 (100000, 1000000] 1490683.08 NaN (1000000, 10000000] 7148839.76 NaN 计算比例12normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)normed_sumscand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 0.805182 0.194818 (1, 10] 0.918767 0.081233 (10, 100] 0.910769 0.089231 (100, 1000] 0.710176 0.289824 (1000, 10000] 0.447326 0.552674 (10000, 100000] 0.823120 0.176880 (100000, 1000000] 1.000000 NaN (1000000, 10000000] 1.000000 NaN 画个图看看1normed_sums[:-2].plot(kind='barh', stacked=True)&lt;matplotlib.axes._subplots.AxesSubplot at 0x14c4db00&gt; 根据州统计赞助信息1234grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)totals = totals[totals.sum(1) &gt; 100000]totals[:10]cand_nm Obama, Barack Romney, Mitt contbr_st AK 281840.15 86204.24 AL 543123.48 527303.51 AR 359247.28 105556.00 AZ 1506476.98 1888436.23 CA 23824984.24 11237636.60 CO 2132429.49 1506714.12 CT 2068291.26 3499475.45 DC 4373538.80 1025137.50 DE 336669.14 82712.00 FL 7318178.58 8338458.81 Mitt is so...poorly...12percent = totals.div(totals.sum(1), axis=0)percent[:10]cand_nm Obama, Barack Romney, Mitt contbr_st AK 0.765778 0.234222 AL 0.507390 0.492610 AR 0.772902 0.227098 AZ 0.443745 0.556255 CA 0.679498 0.320502 CO 0.585970 0.414030 CT 0.371476 0.628524 DC 0.810113 0.189887 DE 0.802776 0.197224 FL 0.467417 0.532583]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Lecture4 note]]></title>
      <url>%2F2017%2F03%2F06%2Fcs231n-Lecture4-note%2F</url>
      <content type="text"><![CDATA[BackPropIntroductionMotivation. In this section we will develop expertise with an intuitive understanding of backpropagation, which is a way of computing gradients of expressions through recursive application of chain rule. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks.Problem statement. The core problem studied in this section is as follows: We are given some function $f(x)$ where $x$ is a vector of inputs and we are interested in computing the gradient of $f$ at $x$ (i.e. $\Delta f(x)$ ).Modularity: Sigmoid example$$f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more:$$f(x) = \frac{1}{x}\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = -1/x^2\\\\f_c(x) = c + x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = 1\\\\f(x) = e^x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = e^x\\\\f_a(x) = ax\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = a$$The full circuit then looks as follows:In the example above, we see a long chain of function applications that operates on the result of the dot product between w,x. The function that these operations implement is called the sigmoid function $\sigma (x)$. It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):$$\sigma(x) = \frac{1}{1+e^{-x}} \\\\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right)= \left( 1 - \sigma(x) \right) \sigma(x)$$As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 ~= 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code:123456789101112w = [2,-3,-3] # assume some random weights and datax = [-1, -2]# forward passdot = w[0]*x[0] + w[1]*x[1] + w[2]f = 1.0 / (1 + math.exp(-dot)) # sigmoid function# backward pass through the neuron (backpropagation)ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivationdx = [w[0] * ddot, w[1] * ddot] # backprop into xdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w# we're done! we have the gradients on the inputs to the circuitImplementation protip: staged backpropagation. As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable dot which holds the output of the dot product between w and x. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. ddot, and ultimately dw, dx) that hold the gradients of those variables.Backprop in practice: Staged computationSuppose that we have a function of the form:$$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$Here is how we would structure the forward pass of such expression:123456789101112x = 3 # example valuesy = -4# forward passsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)num = x + sigy # numerator #(2)sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # denominator #(6)invden = 1.0 / den #(7)f = num * invden # done! #(8)For each row, we also highlight which part of the forward pass it refers to:123456789101112131415161718192021# backprop f = num * invdendnum = invden # gradient on numerator #(8)dinvden = num #(8)# backprop invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# backprop den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# backprop xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# backprop xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# backprop sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# backprop num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# backprop sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# done! phewNotice a few things:Cache forward pass variables. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them.Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.Patterns in backward flowIt is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add,mul,max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit:Looking at the diagram above as an example, we can see that:The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged). In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged.The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero.The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00.Unintuitive effects and their consequences. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted $w^T x_i$ (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples xixi by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.ExtrasSome extra materials provided in here and here.Optional: [1, 2, 3]Neural Network#1Quick introIt is possible to introduce neural networks without appealing to brain analogies. In the section on linear classification we computed scores for different visual categories given the image using the formula $s=Wx$, where $W$ was a matrix and $x$ was an input column vector containing all pixel data of the image. In the case of CIFAR-10, $x$ is a [3072x1] column vector, and $W$ is a [10x3072] matrix, so that the output scores is a vector of 10 class scores.An example neural network would instead compute $s=W_2 \max(0, W_1x)$. Here, $W_1$ could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. The function $\max(0,−)$is a non-linearity that is applied elementwise. There are several choices we could make for the non-linearity (which we’ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. Finally, the matrix $W_2$ would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores. Notice that the non-linearity is critical computationally - if we left it out, the two matrices could be collapsed to a single matrix, and therefore the predicted class scores would again be a linear function of the input. The non-linearity is where we get the wiggle. The parameters $W_2, W_1$ are learned with stochastic gradient descent, and their gradients are derived with chain rule (and computed with backpropagation).A three-layer neural network could analogously look like $s = W_3 \max(0, W_2 \max(0, W_1 x))$, where all of $W_3, W_2, W_1$ are parameters to be learned. The sizes of the intermediate hidden vectors are hyperparameters of the network and we’ll see how we can set them later. Lets now look into how we can interpret these computations from the neuron/network perspective.Modeling one neuronAn example code for forward-propagating a single neuron might look as follows:1234567class Neuron(object): # ... def forward(inputs): """ assume inputs and weights are 1-D numpy arrays and bias is a number """ cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function return firing_rateSingle neuron as a linear classifierThe mathematical form of the model Neuron’s forward computation might look familiar to you. As we saw with linear classifiers, a neuron has the capacity to “like” (activation near one) or “dislike” (activation near zero) certain linear regions of its input space. Hence, with an appropriate loss function on the neuron’s output, we can turn a single neuron into a linear classifier:Binary Softmax classifier. For example, we can interpret $\sigma(\sum_iw_ix_i + b)$ to be the probability of one of the classes $P(y_i = 1 \mid x_i; w)$. The probability of the other class would be $P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w)$, since they must sum to one. With this interpretation, we can formulate the cross-entropy loss as we have seen in the Linear Classification section, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression). Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5.Binary SVM classifier. Alternatively, we could attach a max-margin hinge loss to the output of the neuron and train it to become a binary Support Vector Machine.Regularization interpretation. The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as gradual forgetting, since it would have the effect of driving all synaptic weights ww towards zero after every parameter update.A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers)Commonly used activation functionsEvery activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:Sigmoid. The sigmoid non-linearity has the mathematical form $\sigma(x) = 1 / (1 + e^{-x})$ and is shown in the image. As alluded to in the previous section, it takes a real-valued number and “squashes” it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $x &gt; 0$ elementwise in $f = w^Tx + b$), then the gradient on the weights ww will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $f$). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.​Tanh. The tanh non-linearity is shown on the image. It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\tanh(x) = 2 \sigma(2x) -1$.ReLU. The Rectified Linear Unit has become very popular in the last few years. It computes the function $f(x) = \max(0, x)$. In other words, the activation is simply thresholded at zero (see image). There are several pros and cons to using the ReLUs:(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.(-) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.Leaky ReLU. Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes $f(x) = \mathbb{1}(x &lt; 0) (\alpha x) + \mathbb{1}(x&gt;=0) (x)$ where $\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.Maxout. Other types of units have been proposed that do not have the functional form $f(w^Tx + b)$ where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. The Maxout neuron computes the function $\max(w_1^Tx+b_1, w_2^Tx + b_2)$. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $w_1, b_1 = 0$). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.TLDR: “What neuron type should I use?” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.Neural Network architecturesLayer-wise organizationSizing neural networks. The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture:The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module.Example feed-forward computation123456# forward-pass of a 3-layer neural network:f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)x = np.random.randn(3, 1) # random input vector of three numbers (3x1)h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)out = np.dot(W3, h2) + b3 # output neuron (1x1)The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.Representational powerOne way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a Neural Network?It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function $f(x)$ and some $\epsilon &gt; 0$, there exists a Neural Network $g(x)$ with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that $\forall x, \mid f(x) - g(x) \mid &lt; \epsilon$. In other words, the neural network can approximate any continuous function.If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. In one dimension, the “sum of indicator bumps” function $g(x) = \sum_i c_i \mathbb{1}(a_i &lt; x &lt; b_i)$ where $a, b, c$ are parameter vectors is also a universal approximator, but noone would suggest that we use this functional form in Machine Learning. Neural Networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent). Similarly, the fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.The full story is, of course, much more involved and a topic of much recent research. If you are interested in these topics we recommend for further reading:Deep Learning book in press by Bengio, Goodfellow, Courville, in practicular Chapter 6.4.Do Deep Nets Really Need to be Deep?FitNets: Hints for Thin Deep NetsSetting number of layers and their sizesWe increase the size and number of layers in a Neural Network, the capacity of the network increases:In practice, it is always better to use these methods to control overfitting instead of the number of neurons.The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It’s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, e.g. in a recent paper The Loss Surfaces of Multilayer Networks. In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. We can look at the results achieved by three different settings:You can play with these examples in this ConvNetsJS demo.The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.Additional Referencesdeeplearning.net tutorial with TheanoConvNetJS demos for intuitionsMichael Nielsen’s tutorials]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note ch08]]></title>
      <url>%2F2017%2F03%2F05%2Fpython-data-analysis-learning-note-ch08%2F</url>
      <content type="text"><![CDATA[绘图和可视化12345678910from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlinematplotlib API 入门1import matplotlib.pyplot as pltFigure 和 Subplot1fig = plt.figure()1ax1 = fig.add_subplot(2, 2, 1)12ax2 = fig.add_subplot(2, 2, 2)ax3 = fig.add_subplot(2, 2, 3)12from numpy.random import randnplt.plot(randn(50).cumsum(), 'k--')12_ = ax1.hist(randn(100), bins=20, color='k', alpha=0.3)ax2.scatter(np.arange(30), np.arange(30) + 3 * randn(30))1plt.close('all')12fig, axes = plt.subplots(2, 3)axes调整subplot周围的间距12plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)12345fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)for i in range(2): for j in range(2): axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)plt.subplots_adjust(wspace=0, hspace=0)(array([ 2., 0., 3., 2., 1., 1., 0., 3., 5., 8., 9., 9., 10., 18., 34., 13., 24., 30., 24., 24., 25., 20., 34., 20., 30., 30., 19., 14., 14., 8., 19., 14., 7., 3., 7., 2., 7., 2., 2., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.]), array([-2.9493, -2.8118, -2.6743, -2.5367, -2.3992, -2.2617, -2.1241, -1.9866, -1.849 , -1.7115, -1.574 , -1.4364, -1.2989, -1.1614, -1.0238, -0.8863, -0.7487, -0.6112, -0.4737, -0.3361, -0.1986, -0.0611, 0.0765, 0.214 , 0.3516, 0.4891, 0.6266, 0.7642, 0.9017, 1.0392, 1.1768, 1.3143, 1.4519, 1.5894, 1.7269, 1.8645, 2.002 , 2.1395, 2.2771, 2.4146, 2.5522, 2.6897, 2.8272, 2.9648, 3.1023, 3.2398, 3.3774, 3.5149, 3.6525, 3.79 , 3.9275]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 1., 0., 2., 0., 1., 1., 5., 7., 4., 5., 8., 12., 12., 13., 15., 17., 13., 22., 30., 21., 24., 17., 20., 20., 20., 18., 26., 16., 24., 19., 8., 14., 15., 7., 11., 5., 4., 9., 7., 6., 1., 6., 2., 4., 2., 0., 2., 1., 2.]), array([-2.595 , -2.4898, -2.3845, -2.2793, -2.1741, -2.0688, -1.9636, -1.8584, -1.7531, -1.6479, -1.5427, -1.4374, -1.3322, -1.227 , -1.1217, -1.0165, -0.9112, -0.806 , -0.7008, -0.5955, -0.4903, -0.3851, -0.2798, -0.1746, -0.0694, 0.0359, 0.1411, 0.2463, 0.3516, 0.4568, 0.562 , 0.6673, 0.7725, 0.8777, 0.983 , 1.0882, 1.1935, 1.2987, 1.4039, 1.5092, 1.6144, 1.7196, 1.8249, 1.9301, 2.0353, 2.1406, 2.2458, 2.351 , 2.4563, 2.5615, 2.6667]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 4., 1., 4., 5., 11., 8., 6., 11., 13., 13., 17., 18., 20., 27., 32., 29., 31., 22., 21., 31., 29., 19., 22., 18., 10., 18., 11., 12., 9., 6., 2., 3., 3., 3., 2., 1., 1., 1., 0., 1.]), array([-3.7454, -3.6052, -3.4651, -3.325 , -3.1849, -3.0448, -2.9047, -2.7646, -2.6244, -2.4843, -2.3442, -2.2041, -2.064 , -1.9239, -1.7837, -1.6436, -1.5035, -1.3634, -1.2233, -1.0832, -0.9431, -0.8029, -0.6628, -0.5227, -0.3826, -0.2425, -0.1024, 0.0377, 0.1779, 0.318 , 0.4581, 0.5982, 0.7383, 0.8784, 1.0185, 1.1587, 1.2988, 1.4389, 1.579 , 1.7191, 1.8592, 1.9994, 2.1395, 2.2796, 2.4197, 2.5598, 2.6999, 2.84 , 2.9802, 3.1203, 3.2604]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 2., 5., 9., 8., 6., 2., 11., 17., 10., 13., 10., 14., 12., 27., 17., 28., 27., 25., 14., 24., 25., 38., 13., 24., 15., 10., 17., 14., 13., 8., 7., 10., 3., 7., 2., 5., 2., 0., 1., 1.]), array([-3.4283, -3.3066, -3.185 , -3.0633, -2.9417, -2.8201, -2.6984, -2.5768, -2.4551, -2.3335, -2.2119, -2.0902, -1.9686, -1.847 , -1.7253, -1.6037, -1.482 , -1.3604, -1.2388, -1.1171, -0.9955, -0.8739, -0.7522, -0.6306, -0.5089, -0.3873, -0.2657, -0.144 , -0.0224, 0.0993, 0.2209, 0.3425, 0.4642, 0.5858, 0.7074, 0.8291, 0.9507, 1.0724, 1.194 , 1.3156, 1.4373, 1.5589, 1.6806, 1.8022, 1.9238, 2.0455, 2.1671, 2.2887, 2.4104, 2.532 , 2.6537]), &lt;a list of 50 Patch objects&gt;) 颜色、标记和线型1plt.figure()1plt.plot(randn(30).cumsum(), 'ko--')1plt.close('all')1234data = randn(30).cumsum()plt.plot(data, 'k--', label='Default')plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')plt.legend(loc='best')刻度、标签和图例设置标题、轴标签、刻度以及刻度标签12345678fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)ax.plot(randn(1000).cumsum())ticks = ax.set_xticks([0, 250, 500, 750, 1000])labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')ax.set_title('My first matplotlib plot')ax.set_xlabel('Stages')添加图例123456fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)ax.plot(randn(1000).cumsum(), 'k', label='one')ax.plot(randn(1000).cumsum(), 'k--', label='two')ax.plot(randn(1000).cumsum(), 'k.', label='three')ax.legend(loc='best')注解以及在subplot上绘图123456789101112131415161718192021222324252627from datetime import datetimefig = plt.figure()ax = fig.add_subplot(1, 1, 1)data = pd.read_csv('ch08/spx.csv', index_col=0, parse_dates=True)spx = data['SPX']spx.plot(ax=ax, style='k-')crisis_data = [ (datetime(2007, 10, 11), 'Peak of bull market'), (datetime(2008, 3, 12), 'Bear Stearns Fails'), (datetime(2008, 9, 15), 'Lehman Bankruptcy')]for date, label in crisis_data: ax.annotate(label, xy=(date, spx.asof(date) + 50), xytext=(date, spx.asof(date) + 200), arrowprops=dict(facecolor='black'), horizontalalignment='left', verticalalignment='top')# Zoom in on 2007-2010ax.set_xlim(['1/1/2007', '1/1/2011'])ax.set_ylim([600, 1800])ax.set_title('Important dates in 2008-2009 financial crisis')1234567891011fig = plt.figure()ax = fig.add_subplot(1, 1, 1)rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)ax.add_patch(rect)ax.add_patch(circ)ax.add_patch(pgon)将图表保存到文件1fig1fig.savefig('figpath.svg')1fig.savefig('figpath.png', dpi=400, bbox_inches='tight')1234from io import BytesIObuffer = BytesIO()plt.savefig(buffer)plot_data = buffer.getvalue()&lt;matplotlib.figure.Figure at 0xaebe550&gt; matplotlib 配置1plt.rc('figure', figsize=(10, 10))pandas中的绘图函数线型图1plt.close('all')12s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))s.plot()1234df = DataFrame(np.random.randn(10, 4).cumsum(0), columns=['A', 'B', 'C', 'D'], index=np.arange(0, 100, 10))df.plot()柱状图1234fig, axes = plt.subplots(2, 1)data = Series(np.random.rand(16), index=list('abcdefghijklmnop'))data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7)data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)12345df = DataFrame(np.random.rand(6, 4), index=['one', 'two', 'three', 'four', 'five', 'six'], columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))dfdf.plot(kind='bar')GenusABCDone0.3016860.1563330.3719430.270731two0.7505890.5255870.6894290.358974three0.3815040.6677070.4737720.632528four0.9424080.1801860.7082840.641783five0.8402780.9095890.0100410.653207six0.0628540.5898130.8113180.0602171plt.figure()1df.plot(kind='barh', stacked=True, alpha=0.5)123456tips = pd.read_csv('ch08/tips.csv')party_counts = pd.crosstab(tips.day, tips.size_)party_counts# Not many 1- and 6-person partiesparty_counts = party_counts.ix[:, 2:5]party_countssize_123456dayFri1161100Sat253181310Sun039151831Thur1484513size_2345dayFri16110Sat5318131Sun3915183Thur4845112345# Normalize to sum to 1party_pcts = party_counts.div(party_counts.sum(1).astype(float), axis=0)party_pctsparty_pcts.plot(kind='bar', stacked=True)size_2345dayFri0.8888890.0555560.0555560.000000Sat0.6235290.2117650.1529410.011765Sun0.5200000.2000000.2400000.040000Thur0.8275860.0689660.0862070.017241直方图和密度图1plt.figure()12tips['tip_pct'] = tips['tip'] / tips['total_bill']tips['tip_pct'].hist(bins=50)1plt.figure()1tips['tip_pct'].plot(kind='kde')1plt.figure()12345comp1 = np.random.normal(0, 1, size=200) # N(0, 1)comp2 = np.random.normal(10, 2, size=200) # N(10, 4)values = Series(np.concatenate([comp1, comp2]))values.hist(bins=100, alpha=0.3, color='k', normed=True)values.plot(kind='kde', style='k--')散点图1234macro = pd.read_csv('ch08/macrodata.csv')data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]trans_data = np.log(data).diff().dropna()trans_data[-5:]cpim1tbilrateunemp198-0.0079040.045361-0.3968810.105361199-0.0219790.066753-2.2772670.1397622000.0023400.0102860.6061360.1603432010.0084190.037461-0.2006710.1273392020.0088940.012202-0.4054650.0425601plt.figure()12plt.scatter(trans_data['m1'], trans_data['unemp'])plt.title('Changes in log %s vs. log %s' % ('m1', 'unemp'))1pd.scatter_matrix(trans_data, diagonal='kde', c='k', alpha=0.3)绘制地图：图形化显示海底地震危机数据12data = pd.read_csv('ch08/Haiti.csv')data.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 3593 entries, 0 to 3592 Data columns (total 10 columns): Serial 3593 non-null int64 INCIDENT TITLE 3593 non-null object INCIDENT DATE 3593 non-null object LOCATION 3592 non-null object DESCRIPTION 3593 non-null object CATEGORY 3587 non-null object LATITUDE 3593 non-null float64 LONGITUDE 3593 non-null float64 APPROVED 3593 non-null object VERIFIED 3593 non-null object dtypes: float64(2), int64(1), object(7) memory usage: 280.8+ KB 1data[['INCIDENT DATE', 'LATITUDE', 'LONGITUDE']][:10]INCIDENT DATELATITUDELONGITUDE005/07/2010 17:2618.233333-72.533333128/06/2010 23:0650.2260295.729886224/06/2010 16:2122.278381114.174287320/06/2010 21:5944.4070628.933989418/05/2010 16:2618.571084-72.334671526/04/2010 13:1418.593707-72.310079626/04/2010 14:1918.482800-73.638800726/04/2010 14:2718.415000-73.195000815/03/2010 10:5818.517443-72.236841915/03/2010 11:0018.547790-72.4100101data['CATEGORY'][:6]0 1. Urgences | Emergency, 3. Public Health, 1 1. Urgences | Emergency, 2. Urgences logistiqu... 2 2. Urgences logistiques | Vital Lines, 8. Autr... 3 1. Urgences | Emergency, 4 1. Urgences | Emergency, 5 5e. Communication lines down, Name: CATEGORY, dtype: object 1data.describe()SerialLATITUDELONGITUDEcount3593.0000003593.0000003593.000000mean2080.27748418.611495-72.322680std1171.1003600.7385723.650776min4.00000018.041313-74.45275725%1074.00000018.524070-72.41750050%2163.00000018.539269-72.33500075%3088.00000018.561820-72.293570max4052.00000050.226029114.174287123data = data[(data.LATITUDE &gt; 18) &amp; (data.LATITUDE &lt; 20) &amp; (data.LONGITUDE &gt; -75) &amp; (data.LONGITUDE &lt; -70) &amp; data.CATEGORY.notnull()]12345678910111213def to_cat_list(catstr): stripped = (x.strip() for x in catstr.split(',')) return [x for x in stripped if x]def get_all_categories(cat_series): cat_sets = (set(to_cat_list(x)) for x in cat_series) return sorted(set.union(*cat_sets))def get_english(cat): code, names = cat.split('.') if '|' in names: names = names.split(' | ')[1] return code, names.strip()1get_english('2. Urgences logistiques | Vital Lines')(&apos;2&apos;, &apos;Vital Lines&apos;) 12345all_cats = get_all_categories(data.CATEGORY)# Generator expressionenglish_mapping = dict(get_english(x) for x in all_cats)english_mapping['2a']english_mapping['6c']&apos;Food Shortage&apos; &apos;Earthquake and aftershocks&apos; 1234567def get_code(seq): return [x.split('.')[0] for x in seq if x]all_codes = get_code(all_cats)code_index = pd.Index(np.unique(all_codes))dummy_frame = DataFrame(np.zeros((len(data), len(code_index))), index=data.index, columns=code_index)1dummy_frame.ix[:, :6].info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; Int64Index: 3569 entries, 0 to 3592 Data columns (total 6 columns): 1 3569 non-null float64 1a 3569 non-null float64 1b 3569 non-null float64 1c 3569 non-null float64 1d 3569 non-null float64 2 3569 non-null float64 dtypes: float64(6) memory usage: 195.2 KB 12345for row, cat in zip(data.index, data.CATEGORY): codes = get_code(to_cat_list(cat)) dummy_frame.ix[row, codes] = 1data = data.join(dummy_frame.add_prefix('category_'))1data.ix[:, 10:15].info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; Int64Index: 3569 entries, 0 to 3592 Data columns (total 5 columns): category_1 3569 non-null float64 category_1a 3569 non-null float64 category_1b 3569 non-null float64 category_1c 3569 non-null float64 category_1d 3569 non-null float64 dtypes: float64(5) memory usage: 167.3 KB 1234567891011121314151617from mpl_toolkits.basemap import Basemapimport matplotlib.pyplot as pltdef basic_haiti_map(ax=None, lllat=17.25, urlat=20.25, lllon=-75, urlon=-71): # create polar stereographic Basemap instance. m = Basemap(ax=ax, projection='stere', lon_0=(urlon + lllon) / 2, lat_0=(urlat + lllat) / 2, llcrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon, urcrnrlon=urlon, resolution='f') # draw coastlines, state and country boundaries, edge of map. m.drawcoastlines() m.drawstates() m.drawcountries() return m123456789101112131415161718fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))fig.subplots_adjust(hspace=0.05, wspace=0.05)to_plot = ['2a', '1', '3c', '7a']lllat=17.25; urlat=20.25; lllon=-75; urlon=-71for code, ax in zip(to_plot, axes.flat): m = basic_haiti_map(ax, lllat=lllat, urlat=urlat, lllon=lllon, urlon=urlon) cat_data = data[data['category_%s' % code] == 1] # compute map proj coordinates. x, y = m(cat_data.LONGITUDE.values, cat_data.LATITUDE.values) m.plot(x, y, 'k.', alpha=0.5) ax.set_title('%s: %s' % (code, english_mapping[code]))C:\Users\Ewan\Anaconda3\envs\ipykernel_py2\lib\site-packages\mpl_toolkits\basemap\__init__.py:3260: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0. b = ax.ishold() C:\Users\Ewan\Anaconda3\envs\ipykernel_py2\lib\site-packages\mpl_toolkits\basemap\__init__.py:3269: MatplotlibDeprecationWarning: axes.hold is deprecated. See the API Changes document (http://matplotlib.org/api/api_changes.html) for more details. ax.hold(b) 12345678910111213141516171819202122#街道数据的路径shapefilepath = 'ch08/PortAuPrince_Roads/PortAuPrince_Roads'fig = plt.figure()ax = fig.add_subplot(1,1,1)lat0 = 18.533333;lon0 = -72.333333;change = 0.13;lllat=lat0-change; urlat=lat0+change; lllon=lon0-change; urlon=lon0+change;m = basic_haiti_map(ax, lllat=lllat, urlat=urlat,lllon=lllon, urlon=urlon)m.readshapefile(shapefilepath,'roads') #添加街道数据code = '2a'cat_data = data[data['category_%s' % code] == 1]# compute map proj coordinates.x, y = m(cat_data.LONGITUDE.values, cat_data.LATITUDE.values)m.plot(x, y, 'k.', alpha=0.5)ax.set_title('Food shortages reported in Port-au-Prince')# plt.savefig('myfig.png',dpi=400,bbox_inches='tight')(1583, 3, [-72.749246, 18.409952, 0.0, 0.0], [-71.973789, 18.7147105, 0.0, 0.0],]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 softmax]]></title>
      <url>%2F2017%2F03%2F05%2Fcs231n-Assignment-1-softmax%2F</url>
      <content type="text"><![CDATA[Softmax exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.This exercise is analogous to the SVM exercise. You will:implement a fully-vectorized loss function for the Softmax classifierimplement the fully-vectorized expression for its analytic gradientcheck your implementation with numerical gradientuse a validation set to tune the learning rate and regularization strengthoptimize the loss function with SGDvisualize the final learned weights12345678910111213import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# for auto-reloading extenrnal modules# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500): """ Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the linear classifier. These are the same steps as we used for the SVM, but condensed to a single function. """ # Load the raw CIFAR-10 data cifar10_dir = 'cs231n/datasets/cifar-10-batches-py' X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) # subsample the data mask = range(num_training, num_training + num_validation) X_val = X_train[mask] y_val = y_train[mask] mask = range(num_training) X_train = X_train[mask] y_train = y_train[mask] mask = range(num_test) X_test = X_test[mask] y_test = y_test[mask] mask = np.random.choice(num_training, num_dev, replace=False) X_dev = X_train[mask] y_dev = y_train[mask] # Preprocessing: reshape the image data into rows X_train = np.reshape(X_train, (X_train.shape[0], -1)) X_val = np.reshape(X_val, (X_val.shape[0], -1)) X_test = np.reshape(X_test, (X_test.shape[0], -1)) X_dev = np.reshape(X_dev, (X_dev.shape[0], -1)) # Normalize the data: subtract the mean image mean_image = np.mean(X_train, axis = 0) X_train -= mean_image X_val -= mean_image X_test -= mean_image X_dev -= mean_image # add bias dimension and transform into columns X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]) X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]) X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]) X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))]) return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev# Invoke the above function to get our data.X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeprint 'dev data shape: ', X_dev.shapeprint 'dev labels shape: ', y_dev.shapeTrain data shape: (49000L, 3073L) Train labels shape: (49000L,) Validation data shape: (1000L, 3073L) Validation labels shape: (1000L,) Test data shape: (1000L, 3073L) Test labels shape: (1000L,) dev data shape: (500L, 3073L) dev labels shape: (500L,) Softmax ClassifierYour code for this section will all be written inside cs231n/classifiers/softmax.py.1234567891011121314# First implement the naive softmax loss function with nested loops.# Open the file cs231n/classifiers/softmax.py and implement the# softmax_loss_naive function.from cs231n.classifiers.softmax import softmax_loss_naiveimport time# Generate a random softmax weight matrix and use it to compute the loss.W = np.random.randn(3073, 10) * 0.0001loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As a rough sanity check, our loss should be something close to -log(0.1).print 'loss: %f' % lossprint 'sanity check: %f' % (-np.log(0.1))loss: 2.395985 sanity check: 2.302585 Inline Question 1:Why do we expect our loss to be close to -log(0.1)? Explain briefly.Your answer: Because the W is selected by random, so the probability of select the true class is 1/10. That is, 0.1.1234567891011121314# Complete the implementation of softmax_loss_naive and implement a (naive)# version of the gradient that uses nested loops.loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As we did for the SVM, use numeric gradient checking as a debugging tool.# The numeric gradient should be close to the analytic gradient.from cs231n.gradient_check import grad_check_sparsef = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad, 10)# similar to SVM case, do another gradient check with regularizationloss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad, 10)numerical: 2.368141 analytic: 2.368141, relative error: 2.349797e-08 numerical: 1.324690 analytic: 1.324690, relative error: 7.140560e-08 numerical: 3.170412 analytic: 3.170411, relative error: 1.324741e-08 numerical: 0.249509 analytic: 0.249509, relative error: 2.647240e-08 numerical: 1.536095 analytic: 1.536095, relative error: 4.345856e-08 numerical: 1.075819 analytic: 1.075819, relative error: 3.902323e-08 numerical: -0.198098 analytic: -0.198098, relative error: 5.737134e-08 numerical: -0.089902 analytic: -0.089902, relative error: 8.604010e-07 numerical: -0.339487 analytic: -0.339487, relative error: 3.992996e-08 numerical: -4.819781 analytic: -4.819781, relative error: 3.465667e-09 numerical: 1.869922 analytic: 1.869921, relative error: 7.536693e-08 numerical: 0.783465 analytic: 0.783465, relative error: 6.960291e-08 numerical: -3.206007 analytic: -3.206007, relative error: 2.337350e-09 numerical: 0.532183 analytic: 0.532183, relative error: 1.498128e-07 numerical: 0.900500 analytic: 0.900500, relative error: 6.954913e-09 numerical: -0.353224 analytic: -0.353224, relative error: 1.836960e-07 numerical: -1.331470 analytic: -1.331470, relative error: 2.726426e-08 numerical: -0.082452 analytic: -0.082452, relative error: 7.712355e-07 numerical: -1.322133 analytic: -1.322133, relative error: 5.516628e-09 numerical: 0.345814 analytic: 0.345814, relative error: 1.251858e-07 1234567891011121314151617181920# Now that we have a naive implementation of the softmax loss function and its gradient,# implement a vectorized version in softmax_loss_vectorized.# The two versions should compute the same results, but the vectorized version should be# much faster.tic = time.time()loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.softmax import softmax_loss_vectorizedtic = time.time()loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# As we did for the SVM, we use the Frobenius norm to compare the two versions# of the gradient.grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)print 'Gradient difference: %f' % grad_differencenaive loss: 2.395985e+00 computed in 0.080000s vectorized loss: 2.395985e+00 computed in 0.003000s Loss difference: 0.000000 Gradient difference: 0.000000 1234567891011121314151617181920212223242526272829303132333435363738394041424344# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of over 0.35 on the validation set.from cs231n.classifiers import Softmaxresults = &#123;&#125;best_val = -1best_softmax = Nonelearning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained softmax classifer in best_softmax. #################################################################################iters = 2000for lr in learning_rates: for rs in regularization_strengths: softmax = Softmax() softmax.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=iters) y_train_pred = softmax.predict(X_train) acc_train = np.mean(y_train == y_train_pred) y_val_pred = softmax.predict(X_val) acc_val = np.mean(y_val == y_val_pred) results[(lr, rs)] = (acc_train, acc_val) if best_val &lt; acc_val: best_val = acc_val best_softmax = softmax################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_vallr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.175633 val accuracy: 0.179000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.174102 val accuracy: 0.161000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.203490 val accuracy: 0.210000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.191367 val accuracy: 0.202000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.208000 val accuracy: 0.197000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.203571 val accuracy: 0.215000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.213551 val accuracy: 0.215000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.238347 val accuracy: 0.229000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.245102 val accuracy: 0.242000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.358265 val accuracy: 0.362000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.356306 val accuracy: 0.374000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.347327 val accuracy: 0.362000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.336347 val accuracy: 0.354000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.331490 val accuracy: 0.348000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.320163 val accuracy: 0.336000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.314551 val accuracy: 0.325000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.313082 val accuracy: 0.324000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.303000 val accuracy: 0.315000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.374163 val accuracy: 0.389000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.353184 val accuracy: 0.365000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.340265 val accuracy: 0.359000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.334673 val accuracy: 0.351000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.326531 val accuracy: 0.337000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.319857 val accuracy: 0.336000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.317878 val accuracy: 0.329000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.310449 val accuracy: 0.329000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.316286 val accuracy: 0.315000 best validation accuracy achieved during cross-validation: 0.389000 12345# evaluate on test set# Evaluate the best softmax on test sety_test_pred = best_softmax.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )softmax on raw pixels final test set accuracy: 0.375000 ​123456789101112131415# Visualize the learned weights for each classw = best_softmax.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])Codes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import numpy as npfrom random import shuffledef softmax_loss_naive(W, X, y, reg): """ Softmax loss function, naive implementation (with loops) Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] num_classes = W.shape[1] for i in xrange(num_train): f = X[i, :].dot(W) f -= np.max(f) correct_f = f[y[i]] denom = np.sum(np.exp(f)) p = np.exp(correct_f) / denom loss += -np.log(p) for j in xrange(num_classes): if j == y[i]: dW[:, y[i]] += (np.exp(f[j]) / denom - 1) * X[i, :] else: dW[:, j] += (np.exp(f[j]) / denom) * X[i, :] loss /= num_train loss += 0.5 * reg * np.sum(W * W) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dWdef softmax_loss_vectorized(W, X, y, reg): """ Softmax loss function, vectorized version. Inputs and outputs are the same as softmax_loss_naive. """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using no explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] f = X.dot(W) f = f - np.max(f, axis=1)[:, np.newaxis] loss = -np.sum( np.log(np.exp(f[np.arange(num_train), y]) / np.sum(np.exp(f), axis=1))) loss /= num_train loss += 0.5 * reg * np.sum(W * W) ind = np.zeros_like(f) ind[np.arange(num_train), y] = 1 dW = X.T.dot(np.exp(f) / np.sum(np.exp(f), axis=1, keepdims=True) - ind) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 svm]]></title>
      <url>%2F2017%2F03%2F03%2Fcs231n-Assignment-1-svm%2F</url>
      <content type="text"><![CDATA[Multiclass Support Vector Machine exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.In this exercise you will:​implement a fully-vectorized loss function for the SVMimplement the fully-vectorized expression for its analytic gradientcheck your implementation using numerical gradientuse a validation set to tune the learning rate and regularization strengthoptimize the loss function with SGDvisualize the final learned weights123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the# notebook rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2CIFAR-10 Data Loading and Preprocessing123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTraining data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show()1?np.random.choice1234567891011121314151617181920212223242526272829303132333435363738# Split the data into train, val, and test sets. In addition we will# create a small development set as a subset of the training data;# we can use this for development so our code runs faster.num_training = 49000num_validation = 1000num_test = 1000num_dev = 500# Our validation set will be num_validation points from the original# training set.mask = range(num_training, num_training + num_validation)X_val = X_train[mask]y_val = y_train[mask]# Our training set will be the first num_train points from the original# training set.mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]# We will also make a development set, which is a small subset of# the training set.mask = np.random.choice(num_training, num_dev, replace=False)X_dev = X_train[mask]y_dev = y_train[mask]# We use the first num_test points of the original test set as our# test set.mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTrain data shape: (49000L, 32L, 32L, 3L) Train labels shape: (49000L,) Validation data shape: (1000L, 32L, 32L, 3L) Validation labels shape: (1000L,) Test data shape: (1000L, 32L, 32L, 3L) Test labels shape: (1000L,) 1234567891011# Preprocessing: reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_val = np.reshape(X_val, (X_val.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))# As a sanity check, print out the shapes of the dataprint 'Training data shape: ', X_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Test data shape: ', X_test.shapeprint 'dev data shape: ', X_dev.shapeTraining data shape: (49000L, 3072L) Validation data shape: (1000L, 3072L) Test data shape: (1000L, 3072L) dev data shape: (500L, 3072L) 1234567# Preprocessing: subtract the mean image# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)print mean_image[:10] # print a few of the elementsplt.figure(figsize=(4,4))plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean imageplt.show()[ 130.64189796 135.98173469 132.47391837 130.05569388 135.34804082 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347] 12345# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image12345678# third: append (at the last) the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print X_train.shape, X_val.shape, X_test.shape, X_dev.shape(49000L, 3073L) (1000L, 3073L) (1000L, 3073L) (500L, 3073L) ​SVM ClassifierYour code for this section will all be written inside cs231n/classifiers/linear_svm.py.As you can see, we have prefilled the function compute_loss_naive which uses for loops to evaluate the multiclass SVM loss function.123456789# Evaluate the naive implementation of the loss we provided for you:from cs231n.classifiers.linear_svm import svm_loss_naiveimport time# generate a random SVM weight matrix of small numbersW = np.random.randn(3073, 10) * 0.0001 loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.00001)print 'loss: %f' % (loss, )loss: 8.831645 ​The grad returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function svm_loss_naive. You will find it helpful to interleave your new code inside the existing function.To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:123456789101112131415161718# Once you've implemented the gradient, recompute it with the code below# and gradient check it with the function we provided for you# Compute the loss and its gradient at W.loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)# Numerically compute the gradient along several randomly chosen dimensions, and# compare them with your analytically computed gradient. The numbers should match# almost exactly along all dimensions.from cs231n.gradient_check import grad_check_sparsef = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad)# do the gradient check once again with regularization turned on# you didn't forget the regularization gradient did you?loss, grad = svm_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: svm_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad)numerical: -13.865929 analytic: -13.865929, relative error: 1.283977e-12 numerical: 7.842142 analytic: 7.735021, relative error: 6.876784e-03 numerical: 3.464393 analytic: 3.464393, relative error: 9.040092e-11 numerical: -23.034911 analytic: -23.034911, relative error: 6.876266e-12 numerical: -0.185311 analytic: -0.185311, relative error: 2.538774e-10 numerical: 25.825504 analytic: 25.825504, relative error: 1.336035e-11 numerical: 4.457836 analytic: 4.457836, relative error: 1.015819e-10 numerical: 3.184691 analytic: 3.184691, relative error: 8.849109e-11 numerical: 10.428446 analytic: 10.374317, relative error: 2.601982e-03 numerical: 12.479957 analytic: 12.479957, relative error: 6.825191e-12 numerical: 12.237949 analytic: 12.326308, relative error: 3.597051e-03 numerical: 4.377103 analytic: 4.377103, relative error: 3.904758e-11 numerical: -1.951930 analytic: -1.951930, relative error: 1.432276e-10 numerical: 33.752503 analytic: 33.752503, relative error: 4.254520e-12 numerical: 11.367149 analytic: 11.367149, relative error: 1.682727e-11 numerical: 16.461879 analytic: 16.461879, relative error: 4.766805e-12 numerical: 3.814562 analytic: 3.814562, relative error: 1.087469e-10 numerical: 13.931226 analytic: 13.931226, relative error: 9.578349e-12 numerical: -27.291095 analytic: -27.395406, relative error: 1.907445e-03 numerical: -7.610407 analytic: -7.610407, relative error: 1.015282e-12 Inline Question 1:It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? Hint: the SVM loss function is not strictly speaking differentiableYour Answer: Maybe the SVM loss function is not differentiable on that dimension1?np.max1np.sum(np.maximum(0, X_dev.dot(W) - X_dev.dot(W)[np.arange(len(y_dev)), [y_dev]].T + 1))4915.822409730994 123456789101112131415# Next implement the function svm_loss_vectorized; for now only compute the loss;# we will implement the gradient in a moment.tic = time.time()loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.linear_svm import svm_loss_vectorizedtic = time.time()loss_vectorized, _ = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# The losses should match but your vectorized implementation should be much faster.print 'difference: %f' % (loss_naive - loss_vectorized)Naive loss: 8.831645e+00 computed in 0.071000s Vectorized loss: 8.831645e+00 computed in 0.000000s difference: 0.000000 1234567891011121314151617181920# Complete the implementation of svm_loss_vectorized, and compute the gradient# of the loss function in a vectorized way.# The naive implementation and the vectorized implementation should match, but# the vectorized version should still be much faster.tic = time.time()_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss and gradient: computed in %fs' % (toc - tic)tic = time.time()_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss and gradient: computed in %fs' % (toc - tic)# The loss is a single number, so it is easy to compare the values computed# by the two implementations. The gradient on the other hand is a matrix, so# we use the Frobenius norm to compare them.difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'difference: %f' % differenceNaive loss and gradient: computed in 0.084000s Vectorized loss and gradient: computed in 0.005000s difference: 0.000000 Stochastic Gradient DescentWe now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss.123456789# In the file linear_classifier.py, implement SGD in the function# LinearClassifier.train() and then run it with the code below.from cs231n.classifiers import LinearSVMsvm = LinearSVM()tic = time.time()loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4, num_iters=1500, verbose=True)toc = time.time()print 'That took %fs' % (toc - tic)iteration 0 / 1500: loss 791.772037 iteration 100 / 1500: loss 286.021346 iteration 200 / 1500: loss 107.673095 iteration 300 / 1500: loss 41.812791 iteration 400 / 1500: loss 18.665578 iteration 500 / 1500: loss 10.614984 iteration 600 / 1500: loss 6.664814 iteration 700 / 1500: loss 6.509693 iteration 800 / 1500: loss 5.792204 iteration 900 / 1500: loss 4.986855 iteration 1000 / 1500: loss 5.914691 iteration 1100 / 1500: loss 5.058078 iteration 1200 / 1500: loss 5.491475 iteration 1300 / 1500: loss 5.609450 iteration 1400 / 1500: loss 5.376595 That took 5.454000s 123456# A useful debugging strategy is to plot the loss as a function of# iteration number:plt.plot(loss_hist)plt.xlabel('Iteration number')plt.ylabel('Loss value')plt.show()123456# Write the LinearSVM.predict function and evaluate the performance on both the# training and validation sety_train_pred = svm.predict(X_train)print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )y_val_pred = svm.predict(X_val)print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )training accuracy: 0.364980 validation accuracy: 0.378000 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of about 0.4 on the validation set.learning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]# results is dictionary mapping tuples of the form# (learning_rate, regularization_strength) to tuples of the form# (training_accuracy, validation_accuracy). The accuracy is simply the fraction# of data points that are correctly classified.results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.################################################################################# TODO: ## Write code that chooses the best hyperparameters by tuning on the validation ## set. For each combination of hyperparameters, train a linear SVM on the ## training set, compute its accuracy on the training and validation sets, and ## store these numbers in the results dictionary. In addition, store the best ## validation accuracy in best_val and the LinearSVM object that achieves this ## accuracy in best_svm. ## ## Hint: You should use a small value for num_iters as you develop your ## validation code so that the SVMs don't take much time to train; once you are ## confident that your validation code works, you should rerun the validation ## code with a larger value for num_iters. #################################################################################for learning_rate in learning_rates: for regularization_strength in regularization_strengths: svm = LinearSVM() loss_hist = svm.train( X_train, y_train, learning_rate, \ regularization_strength, num_iters=1500, batch_size=200) y_train_pred = svm.predict(X_train) y_val_pred = svm.predict(X_val) training_accuracy = np.mean(y_train == y_train_pred) validation_accuracy = np.mean(y_val == y_val_pred) results[(learning_rate, regularization_strength)] = \ (training_accuracy, validation_accuracy) if validation_accuracy &gt; best_val: best_val = validation_accuracy best_svm = svm################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_vallr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.221898 val accuracy: 0.247000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.233653 val accuracy: 0.258000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.234694 val accuracy: 0.225000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.255959 val accuracy: 0.249000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.259755 val accuracy: 0.273000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.267408 val accuracy: 0.269000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.269102 val accuracy: 0.287000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.277102 val accuracy: 0.285000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.295306 val accuracy: 0.301000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.369388 val accuracy: 0.374000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.380265 val accuracy: 0.390000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.375490 val accuracy: 0.378000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.375633 val accuracy: 0.385000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.369694 val accuracy: 0.375000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.372469 val accuracy: 0.383000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.356000 val accuracy: 0.370000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.352816 val accuracy: 0.355000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.356796 val accuracy: 0.377000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.393510 val accuracy: 0.395000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.377020 val accuracy: 0.382000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.363857 val accuracy: 0.373000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.368714 val accuracy: 0.372000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.361531 val accuracy: 0.364000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.354714 val accuracy: 0.368000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.348306 val accuracy: 0.365000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.358082 val accuracy: 0.378000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.347898 val accuracy: 0.358000 best validation accuracy achieved during cross-validation: 0.395000 123456789101112131415161718192021222324# Visualize the cross-validation resultsimport mathx_scatter = [math.log10(x[0]) for x in results]y_scatter = [math.log10(x[1]) for x in results]# plot training accuracymarker_size = 100colors = [results[x][0] for x in results]plt.subplot(3, 1, 1)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 training accuracy')# plot validation accuracycolors = [results[x][1] for x in results] # default size of markers is 20plt.subplot(3, 1, 3)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 validation accuracy')plt.show()1234# Evaluate the best svm on test sety_test_pred = best_svm.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'linear SVM on raw pixels final test set accuracy: %f' % test_accuracylinear SVM on raw pixels final test set accuracy: 0.383000 ​12x = np.array([[[0], [1], [2]]])np.squeeze(x)array([0, 1, 2]) 123456789101112131415# Visualize the learned weights for each class.# Depending on your choice of learning rate and regularization strength, these may# or may not be nice to look at.w = best_svm.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])Inline question 2:Describe what your visualized SVM weights look like, and offer a brief explanation for why they look they way that they do.Your answer: fill this inCodeslinear_svm.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import numpy as npfrom random import shuffledef svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: dW[:, y[i]] -= X[i, :] dW[:, j] += X[i, :] loss += margin # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += 0.5 * reg * np.sum(W * W) dW += reg * W ############################################################################# # TODO: # # Compute the gradient of the loss function and store it dW. # # Rather that first computing the loss and then computing the derivative, # # it may be simpler to compute the derivative at the same time that the # # loss is being computed. As a result you may need to modify some of the # # code above to compute the gradient. # ############################################################################# return loss, dWdef svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero ############################################################################# # TODO: # # Implement a vectorized version of the structured SVM loss, storing the # # result in loss. # ############################################################################# num_train = X.shape[0] delta = 1.0 scores = X.dot(W) correct_class_score = scores[np.arange(num_train), y] margins = np.maximum( 0, scores - correct_class_score[:, np.newaxis] + delta) margins[np.arange(num_train), y] = 0 loss = np.sum(margins) loss /= num_train loss += 0.5 * reg * np.sum(W.T.dot(W)) ############################################################################# # END OF YOUR CODE # ############################################################################# ############################################################################# # TODO: # # Implement a vectorized version of the gradient for the structured SVM # # loss, storing the result in dW. # # # # Hint: Instead of computing the gradient from scratch, it may be easier # # to reuse some of the intermediate values that you used to compute the # # loss. # ############################################################################# X_mask = np.zeros(margins.shape) X_mask[margins &gt; 0] = 1 count = np.sum(X_mask, axis=1) X_mask[np.arange(num_train), y] = -count dW = X.T.dot(X_mask) dW /= num_train dW += np.multiply(W, reg) ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dWlinear_classifier123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import numpy as npfrom cs231n.classifiers.linear_svm import *from cs231n.classifiers.softmax import *class LinearClassifier(object): def __init__(self): self.W = None def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this linear classifier using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label 0 &lt;= c &lt; C for C classes. - learning_rate: (float) learning rate for optimization. - reg: (float) regularization strength. - num_iters: (integer) number of steps to take when optimizing - batch_size: (integer) number of training examples to use at each step. - verbose: (boolean) If true, print progress during optimization. Outputs: A list containing the value of the loss function at each training iteration. """ num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: # # Sample batch_size elements from the training data and their # # corresponding labels to use in this round of gradient descent. # # Store the data in X_batch and their corresponding labels in # # y_batch; after sampling X_batch should have shape (dim, batch_size) # # and y_batch should have shape (batch_size,) # # # # Hint: Use np.random.choice to generate indices. Sampling with # # replacement is faster than sampling without replacement. # ######################################################################### mask = np.random.choice(num_train, batch_size, replace=True) X_batch = X[mask] y_batch = y[mask] ######################################################################### # END OF YOUR CODE # ######################################################################### # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) # perform parameter update ######################################################################### # TODO: # # Update the weights using the gradient and the learning rate. # ######################################################################### self.W = self.W - learning_rate * grad ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print 'iteration %d / %d: loss %f' % (it, num_iters, loss) return loss_history def predict(self, X): """ Use the trained weights of this linear classifier to predict labels for data points. Inputs: - X: D x N array of training data. Each column is a D-dimensional point. Returns: - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class. """ X = X.T y_pred = np.zeros(X.shape[1]) ########################################################################### # TODO: # # Implement this method. Store the predicted labels in y_pred. # ########################################################################### scores = X.T.dot(self.W) y_pred = np.argsort(scores, axis=1)[:, -1] ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred def loss(self, X_batch, y_batch, reg): """ Compute the loss function and its derivative. Subclasses will override this. Inputs: - X_batch: A numpy array of shape (N, D) containing a minibatch of N data points; each point has dimension D. - y_batch: A numpy array of shape (N,) containing labels for the minibatch. - reg: (float) regularization strength. Returns: A tuple containing: - loss as a single float - gradient with respect to self.W; an array of the same shape as W """ passclass LinearSVM(LinearClassifier): """ A subclass that uses the Multiclass SVM loss function """ def loss(self, X_batch, y_batch, reg): return svm_loss_vectorized(self.W, X_batch, y_batch, reg)class Softmax(LinearClassifier): """ A subclass that uses the Softmax + Cross-entropy loss function """ def loss(self, X_batch, y_batch, reg): return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 kNN]]></title>
      <url>%2F2017%2F03%2F02%2Fcs231n-Assignment-1-kNN%2F</url>
      <content type="text"><![CDATA[k-Nearest Neighbor (kNN) exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.The kNN classifier consists of two stages:During training, the classifier takes the training data and simply remembers itDuring testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examplesThe value of k is cross-validatedIn this exercise you will implement these steps and understand the basic Image Classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code.123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the notebook# rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTraining data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show()12345678910# Subsample the data for more efficient code execution in this exercisenum_training = 5000mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]num_test = 500mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]1234# Reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1)) # Wow~X_test = np.reshape(X_test, (X_test.shape[0], -1))print X_train.shape, X_test.shape(5000L, 3072L) (500L, 3072L) ​1234567from cs231n.classifiers import KNearestNeighbor# Create a kNN classifier instance. # Remember that training a kNN classifier is a noop: # the Classifier simply remembers the data and does no further processing classifier = KNearestNeighbor()classifier.train(X_train, y_train)We would now like to classify the test data with the kNN classifier. Recall that we can break down this process into two steps:First we must compute the distances between all test examples and all train examples.Given these distances, for each test example we find the k nearest examples and have them vote for the labelLets begin with computing the distance matrix between all training and test examples. For example, if there are Ntr training examples and Nte test examples, this stage should result in a Nte x Ntr matrix where each element (i,j) is the distance between the i-th test and j-th train example.First, open cs231n/classifiers/k_nearest_neighbor.py and implement the function compute_distances_two_loops that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time.123456# Open cs231n/classifiers/k_nearest_neighbor.py and implement# compute_distances_two_loops.# Test your implementation:dists = classifier.compute_distances_two_loops(X_test)print dists.shape(500L, 5000L) ​1234# We can visualize the distance matrix: each row is a single test example and# its distances to training examplesplt.imshow(dists, interpolation='none')plt.show()Inline Question #1: Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)What in the data is the cause behind the distinctly bright rows?What causes the columns?Your Answer: Maybe exists noises in test data set and train data set.12345678# Now implement the function predict_labels and run the code below:# We use k = 1 (which is Nearest Neighbor).y_test_pred = classifier.predict_labels(dists, k=1)# Compute and print the fraction of correctly predicted examplesnum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 137 / 500 correct =&gt; accuracy: 0.274000 ​You should expect to see approximately 27% accuracy. Now lets try out a larger k, say k = 5:1234y_test_pred = classifier.predict_labels(dists, k=5)num_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 142 / 500 correct =&gt; accuracy: 0.284000 ​You should expect to see a slightly better performance than with k = 1.1234567891011121314151617# Now lets speed up distance matrix computation by using partial vectorization# with one loop. Implement the function compute_distances_one_loop and run the# code below:dists_one = classifier.compute_distances_one_loop(X_test)# To ensure that our vectorized implementation is correct, we make sure that it# agrees with the naive implementation. There are many ways to decide whether# two matrices are similar; one of the simplest is the Frobenius norm. In case# you haven't seen it before, the Frobenius norm of two matrices is the square# root of the squared sum of differences of all elements; in other words, reshape# the matrices into vectors and compute the Euclidean distance between them.difference = np.linalg.norm(dists - dists_one, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different'Difference was: 0.000000 Good! The distance matrices are the same 1234567891011# Now implement the fully vectorized version inside compute_distances_no_loops# and run the codedists_two = classifier.compute_distances_no_loops(X_test)# check that the distance matrix agrees with the one we computed before:difference = np.linalg.norm(dists - dists_two, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different'Difference was: 0.000000 Good! The distance matrices are the same 123456789101112131415161718192021# Let's compare how fast the implementations aredef time_function(f, *args): """ Call a function f with args and return the time (in seconds) that it took to execute. """ import time tic = time.time() f(*args) toc = time.time() return toc - tictwo_loop_time = time_function(classifier.compute_distances_two_loops, X_test)print 'Two loop version took %f seconds' % two_loop_timeone_loop_time = time_function(classifier.compute_distances_one_loop, X_test)print 'One loop version took %f seconds' % one_loop_timeno_loop_time = time_function(classifier.compute_distances_no_loops, X_test)print 'No loop version took %f seconds' % no_loop_time# you should see significantly faster performance with the fully vectorized implementationTwo loop version took 27.001000 seconds One loop version took 59.630000 seconds No loop version took 0.205000 seconds Cross-validationWe have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []################################################################################# TODO: ## Split up the training data into folds. After splitting, X_train_folds and ## y_train_folds should each be lists of length num_folds, where ## y_train_folds[i] is the label vector for the points in X_train_folds[i]. ## Hint: Look up the numpy array_split function. #################################################################################X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)################################################################################# END OF YOUR CODE ################################################################################## A dictionary holding the accuracies for different values of k that we find# when running cross-validation. After running cross-validation,# k_to_accuracies[k] should be a list of length num_folds giving the different# accuracy values that we found when using that value of k.k_to_accuracies = &#123;&#125;################################################################################# TODO: ## Perform k-fold cross validation to find the best value of k. For each ## possible value of k, run the k-nearest-neighbor algorithm num_folds times, ## where in each case you use all but one of the folds as training data and the ## last fold as a validation set. Store the accuracies for all fold and all ## values of k in the k_to_accuracies dictionary. #################################################################################for k in k_choices: k_to_accuracies[k] = [] for fold in xrange(num_folds): train_X = np.append( X_train_folds[:fold], X_train_folds[fold+1:]).reshape( (X_train.shape[0] - X_train.shape[0]/num_folds, -1)) train_y = np.append( y_train_folds[:fold], y_train_folds[fold+1:]).reshape( (y_train.shape[0] - y_train.shape[0]/num_folds, -1)).flatten() classifier.train(train_X, train_y) dists = classifier.compute_distances_no_loops(X_train_folds[fold]) y_test_pred = classifier.predict_labels(dists, k) num_correct = np.sum(y_test_pred == y_train_folds[fold]) accuracy = float(num_correct) / len(y_train_folds[fold]) k_to_accuracies[k].append(accuracy) ################################################################################# END OF YOUR CODE ################################################################################## Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print 'k = %d, accuracy = %f' % (k, accuracy)k = 1, accuracy = 0.263000 k = 1, accuracy = 0.257000 k = 1, accuracy = 0.264000 k = 1, accuracy = 0.278000 k = 1, accuracy = 0.266000 k = 3, accuracy = 0.241000 k = 3, accuracy = 0.249000 k = 3, accuracy = 0.243000 k = 3, accuracy = 0.273000 k = 3, accuracy = 0.264000 k = 5, accuracy = 0.258000 k = 5, accuracy = 0.273000 k = 5, accuracy = 0.281000 k = 5, accuracy = 0.290000 k = 5, accuracy = 0.272000 k = 8, accuracy = 0.263000 k = 8, accuracy = 0.288000 k = 8, accuracy = 0.278000 k = 8, accuracy = 0.285000 k = 8, accuracy = 0.277000 k = 10, accuracy = 0.265000 k = 10, accuracy = 0.296000 k = 10, accuracy = 0.278000 k = 10, accuracy = 0.284000 k = 10, accuracy = 0.286000 k = 12, accuracy = 0.260000 k = 12, accuracy = 0.294000 k = 12, accuracy = 0.281000 k = 12, accuracy = 0.282000 k = 12, accuracy = 0.281000 k = 15, accuracy = 0.255000 k = 15, accuracy = 0.290000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.276000 k = 20, accuracy = 0.270000 k = 20, accuracy = 0.281000 k = 20, accuracy = 0.280000 k = 20, accuracy = 0.282000 k = 20, accuracy = 0.284000 k = 50, accuracy = 0.271000 k = 50, accuracy = 0.288000 k = 50, accuracy = 0.278000 k = 50, accuracy = 0.269000 k = 50, accuracy = 0.266000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.270000 k = 100, accuracy = 0.263000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.263000 12345678910111213# plot the raw observationsfor k in k_choices: accuracies = k_to_accuracies[k] plt.scatter([k] * len(accuracies), accuracies)# plot the trend line with error bars that correspond to standard deviationaccuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)plt.title('Cross-validation on k')plt.xlabel('k')plt.ylabel('Cross-validation accuracy')plt.show()12345678910111213# Based on the cross-validation results above, choose the best value for k, # retrain the classifier using all the training data, and test it on the test# data. You should be able to get above 28% accuracy on the test data.best_k = 10classifier = KNearestNeighbor()classifier.train(X_train, y_train)y_test_pred = classifier.predict(X_test, k=best_k)# Compute and display the accuracynum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 139 / 500 correct =&gt; accuracy: 0.278000 ​k_nearest_neighbor.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import numpy as npimport operatorclass KNearestNeighbor(object): """ a kNN classifier with L2 distance """ def __init__(self): pass def train(self, X, y): """ Train the classifier. For k-nearest neighbors this is just memorizing the training data. Inputs: - X: A numpy array of shape (num_train, D) containing the training data consisting of num_train samples each of dimension D. - y: A numpy array of shape (N,) containing the training labels, where y[i] is the label for X[i]. """ self.X_train = X self.y_train = y def predict(self, X, k=1, num_loops=0): """ Predict labels for test data using this classifier. Inputs: - X: A numpy array of shape (num_test, D) containing test data consisting of num_test samples each of dimension D. - k: The number of nearest neighbors that vote for the predicted labels. - num_loops: Determines which implementation to use to compute distances between training points and testing points. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ if num_loops == 0: dists = self.compute_distances_no_loops(X) elif num_loops == 1: dists = self.compute_distances_one_loop(X) elif num_loops == 2: dists = self.compute_distances_two_loops(X) else: raise ValueError('Invalid value %d for num_loops' % num_loops) return self.predict_labels(dists, k=k) def compute_distances_two_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a nested loop over both the training data and the test data. Inputs: - X: A numpy array of shape (num_test, D) containing test data. Returns: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] is the Euclidean distance between the ith test point and the jth training point. """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): for j in xrange(num_train): ############################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ############################################################### dists[i, j] = np.sqrt(np.sum((X[i, :] - self.X_train[j, :]) ** 2)) ############################################################### # END OF YOUR CODE # ############################################################### return dists def compute_distances_one_loop(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a single loop over the test data. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): ################################################################### # TODO: # # Compute the l2 distance between the ith test point and all training # # points, and store the result in dists[i, :]. # ################################################################### dists[i, :] = np.sqrt(np.sum(np.square(X[i, :] - self.X_train), axis=1)) ################################################################### # END OF YOUR CODE # ################################################################### return dists def compute_distances_no_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using no explicit loops. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) ####################################################################### # TODO: # # Compute the l2 distance between all test points and all training # # points without using any explicit loops, and store the result in # # dists. # # # # You should implement this function using only basic array operations; # # in particular you should not use functions from scipy. # # # # HINT: Try to formulate the l2 distance using matrix multiplication # # and two broadcast sums. # ####################################################################### dists = np.sqrt(np.multiply(np.dot(X, self.X_train.T), -2) + np.sum(self.X_train ** 2, axis=1) + np.sum(X ** 2, axis=1)[:, np.newaxis]) ####################################################################### # END OF YOUR CODE # ####################################################################### return dists def predict_labels(self, dists, k=1): """ Given a matrix of distances between test points and training points, predict a label for each test point. Inputs: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] gives the distance betwen the ith test point and the jth training point. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in xrange(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] ################################################################### # TODO: # # Use the distance matrix to find the k nearest neighbors of the ith # # testing point, and use self.y_train to find the labels of these # # neighbors. Store these labels in closest_y. # # Hint: Look up the function numpy.argsort. # ################################################################### k_nearest_index = np.argsort(dists[i, :])[:k] ################################################################### # TODO: # # Now that you have found the labels of the k nearest neighbors, you # # need to find the most common label in the list closest_y of labels. # # Store this label in y_pred[i]. Break ties by choosing the smaller # # label. # ################################################################### closest_y = self.y_train[k_nearest_index] labels_counts = &#123;&#125; for label in closest_y: if label in labels_counts.keys(): labels_counts[label] += 1 else: labels_counts[label] = 0 sorted_labels_counts = sorted( labels_counts.items(), key=operator.itemgetter(1), reverse=True) y_pred[i] = sorted_labels_counts[0][0] ################################################################### # END OF YOUR CODE # ################################################################### return y_pred]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture3 note]]></title>
      <url>%2F2017%2F03%2F02%2FCS231n-Lecture3-note%2F</url>
      <content type="text"><![CDATA[Loss functionSource is here.Multiclass Support Vector Machine lossThe SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$The Multiclass SVM loss for the i-th example is formalized as follows:Example$s = [13, -7, 11]$ and $\Delta = 10$, then,$$L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)$$In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by $\Delta$ (delta). If this is not the case, we will accumulate loss.Note that $f(x_i; W) = W x_i$, so we can also rewrite the loss function in this equivalent form:$$L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)$$A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero $\max(0,−)$ function is often called the hinge loss. You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form $\max(0,−)^2$ that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.The follow image shows the motivation of the SVM loss function”RegularizationThere is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and $L_i=0$ for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters $\lambda W$ where $\lambda &gt; 1$ will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30.We can avoid this by extending the loss function with a regularization penalty $R(W)$. The most common regularization penalty is the L2 norm:$$R(W) = \sum_k\sum_l W_{k,l}^2$$That is, the full Multiclass SVM loss becomes:$$L = \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\$$Or expanding this out in its full form:$$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$$Including the L2 penalty leads to the appealing max margin property in SVMs (See CS229 lecture notes for full details if you are interested).The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector $x=[1,1,1,1]$ and two weight vectors $w_1=[1,0,0,0]$, $w_2=[0.25,0.25,0.25,0.25]$. Then $w^T_1x=w^T_2x=1$ so both weight vectors lead to the same dot product, but the L2 penalty of $w_1$ is 1.0 while the L2 penalty of $w_2$ is only 0.25. Therefore, according to the L2 penalty the weight vector $w_2$ would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in $w_2$ are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting.Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546def L_i(x, y, W): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_idef L_i_vectorized(x, y, W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) """ delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X, y, W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) """ # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignmentPractice ConsiderationsSetting DeltaNote that we brushed over the hyperparameter $\Delta$ and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to $\Delta = 1.0$ in all cases. The hyperparameters $\Delta$ and $\lambda$ seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights W has direct effect on the scores (and hence also their differences): As we shrink all values inside W the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. $\Delta = 10$, or$\Delta = 100$) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength $\lambda$).Softmax classifierIn the Softmax classifier, the function mapping $f(x_i; W) = W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form:$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$$The function $f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$ is called the softmax function.Information theory viewThe cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as:$$H(p,q) = - \sum_x p(x) \log q(x)$$The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( $q = e^{f_{y_i}} / \sum_j e^{f_j}$ as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. $p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the $y_i$ -th position.).Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q) = H(p) + D_{KL}(p||q)$, and the entropy of the delta function $p$ is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.Probabilistic interpretation$$P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }$$can be interpreted as the (normalized) probability assigned to the correct label $y_i$given the image $x_i$ and parameterized by W. To see this, remember that the Softmax classifier interprets the scores inside the output vector $f$ as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term $R(W)$ in the full loss function as coming from a Gaussian prior over the weight matrix W, where instead of MLE we are performing the Maximum a posteriori (MAP) estimation.Numeric stabilityWhen you’re writing code for computing the Softmax function in practice, the intermediate term $e^{f_{y_i}}$ and $\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant C and push it into the sum, we get the following (mathematically equivalent) expression:$$\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}$$We are free to choose the value of C. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for C is to set $\log C = -\max_j f_j$. This simply states that we should shift the values inside the vector ff so that the highest value is zero. In code:123456f = np.array([123, 456, 789]) # example with 3 classes and each having large scoresp = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup# instead: first shift the values of f so that the highest number is 0:f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answerSome tricksHow peaky or diffuse these probabilities are depends directly on the regularization strength $\lambda$ - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute:$$[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]$$Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength $\lambda$ was higher, the weights W would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute:$$[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]$$where the probabilites are now more diffuse.Futher ReadingDeep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.OptimizationStrategy #1: A first very bad idea solution: Random search12345678910111213141516171819202122# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)# assume Y_train are the labels (e.g. 1D array of 50,000)# assume the function L evaluates the loss functionbestloss = float("inf") # Python assigns the highest possible float valuefor num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)# prints:# in attempt 0 the loss was 9.401632, best 9.401632# in attempt 1 the loss was 8.959668, best 8.959668# in attempt 2 the loss was 9.044034, best 8.959668# in attempt 3 the loss was 9.278948, best 8.959668# in attempt 4 the loss was 8.857370, best 8.857370# in attempt 5 the loss was 8.943151, best 8.857370# in attempt 6 the loss was 8.605604, best 8.605604# ... (trunctated: continues for 1000 lines)We can take the best weights W found by this search and try it out on the test set:1234567# Assume X_test is [3073 x 10000], Y_test [10000 x 1]scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples# find the index with max score in each column (the predicted class)Yte_predict = np.argmax(scores, axis = 0)# and calculate accuracy (fraction of predictions that are correct)np.mean(Yte_predict == Yte)# returns 0.1555With the best W this gives an accuracy of about 15.5%.Core idea: iterative refinement. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), but the problem of refining a specific set of weights W to be slightly better is significantly less difficult. In other words, our approach will be to start with a random W and then iteratively refine it, making it slightly better each time.Our strategy will be to start with random weights and iteratively refine them over time to get lower lossStrategy #2: Random Local SearchConcretely, we will start out with a random WW, generate random perturbations $\delta W$ to it and if the loss at the perturbed $W + \delta W$ is lower, we will perform an update. The code for this procedure is as follows:12345678910W = np.random.randn(10, 3073) * 0.001 # generate random starting Wbestloss = float("inf")for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print 'iter %d loss is %f' % (i, bestloss)This approach achieves test set classification accuracy of 21.4%.Strategy #3: Following the GradientThe mathematical expression for the derivative of a 1-D function with respect its input is:$$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.Computing the gradientThere are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). We will now present both.Computing the gradient numerically with finite differences123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): """ a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at """ fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return gradPractical considerations.Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula:$$[f(x+h) - f(x-h)] / 2 h$$See wiki for details.1234567# to use the generic code above we want a function that takes a single argument# (the weights in our case) so we close over X_train and Y_traindef CIFAR10_loss_fun(W): return L(X_train, Y_train, W)W = np.random.rand(10, 3073) * 0.001 # random weight vectordf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradientThen we can use to make an update:12345678910111213141516171819202122loss_original = CIFAR10_loss_fun(W) # the original lossprint 'original loss: %f' % (loss_original, )# lets see the effect of multiple step sizesfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # new position in the weight space loss_new = CIFAR10_loss_fun(W_new) print 'for step size %f new loss: %f' % (step_size, loss_new)# prints:# original loss: 2.200718# for step size 1.000000e-10 new loss: 2.200652# for step size 1.000000e-09 new loss: 2.200057# for step size 1.000000e-08 new loss: 2.194116# for step size 1.000000e-07 new loss: 2.135493# for step size 1.000000e-06 new loss: 1.647802# for step size 1.000000e-05 new loss: 2.844355# for step size 1.000000e-04 new loss: 25.558142# for step size 1.000000e-03 new loss: 254.086573# for step size 1.000000e-02 new loss: 2539.370888# for step size 1.000000e-01 new loss: 25392.214036Effect of step sizeComputing the gradient analytically with CalculusHowever, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check.Lets use the example of the SVM loss function for a single datapoint:$$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]$$$$\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient.$$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch07]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch07%2F</url>
      <content type="text"><![CDATA[数据规整化：清理、转换、合并、重塑1234567891011121314from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandasimport pandas as pdnp.set_printoptions(precision=4, threshold=500)pd.options.display.max_rows = 100from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inline合并数据集数据库风格的DataFrame合并12345df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'd'], 'data2': range(3)&#125;)df1data1key00b11b22a33c44a55a66b1df2data2key00a11b22d默认情况下根据重叠的列名进行合并1pd.merge(df1, df2)data1keydata200b111b126b132a044a055a0最好进行显式地指定1pd.merge(df1, df2, on='key')data1keydata200b111b126b132a044a055a01234df3 = DataFrame(&#123;'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df4 = DataFrame(&#123;'rkey': ['a', 'b', 'd'], 'data2': range(3)&#125;)1df3data1lkey00b11b22a33c44a55a66b1df4data2rkey00a11b22d如果两个对象的列名不同，那么就需要分别指定1pd.merge(df3, df4, left_on='lkey', right_on='rkey')data1lkeydata2rkey00b1b11b1b26b1b32a0a44a0a55a0a默认是进行inner连接（交集）， outer是求取并集1pd.merge(df1, df2, how='outer')data1keydata200.0b1.011.0b1.026.0b1.032.0a0.044.0a0.055.0a0.063.0cNaN7NaNd2.01234df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'a', 'b', 'd'], 'data2': range(5)&#125;)1df1data1key00b11b22a33c44a55b1df2data2key00a11b22a33b44d1pd.merge(df1, df2, on='key', how='left')data1keydata200b1.010b3.021b1.031b3.042a0.052a2.063cNaN74a0.084a2.095b1.0105b3.01pd.merge(df1, df2, how='inner')data1keydata200b110b321b131b345b155b362a072a284a094a2123456left = DataFrame(&#123;'key1': ['foo', 'foo', 'bar'], 'key2': ['one', 'two', 'one'], 'lval': [1, 2, 3]&#125;)right = DataFrame(&#123;'key1': ['foo', 'foo', 'bar', 'bar'], 'key2': ['one', 'one', 'one', 'two'], 'rval': [4, 5, 6, 7]&#125;)121leftkey1key2lval0fooone11footwo22barone31rightkey1key2rval0fooone41fooone52barone63bartwo71pd.merge(left, right, on=['key1', 'key2'], how='outer')key1key2lvalrval0fooone1.04.01fooone1.05.02footwo2.0NaN3barone3.06.04bartwoNaN7.0列名重复问题1pd.merge(left, right, on='key1')key1key2_xlvalkey2_yrval0fooone1one41fooone1one52footwo2one43footwo2one54barone3one65barone3two71pd.merge(left, right, on='key1', suffixes=('_left', '_right'))key1key2_leftlvalkey2_rightrval0fooone1one41fooone1one52footwo2one43footwo2one54barone3one65barone3two7索引上的合并123left1 = DataFrame(&#123;'key': ['a', 'b', 'a', 'a', 'b', 'c'], 'value': range(6)&#125;)right1 = DataFrame(&#123;'group_val': [3.5, 7]&#125;, index=['a', 'b'])1left1keyvalue0a01b12a23a34b45c51right1group_vala3.5b7.01pd.merge(left1, right1, left_on='key', right_index=True)keyvaluegroup_val0a03.52a23.53a33.51b17.04b47.01pd.merge(left1, right1, left_on='key', right_index=True, how='outer')keyvaluegroup_val0a03.52a23.53a33.51b17.04b47.05c5NaN12345678lefth = DataFrame(&#123;'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'key2': [2000, 2001, 2002, 2001, 2002], 'data': np.arange(5.)&#125;)righth = DataFrame(np.arange(12).reshape((6, 2)), index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'], [2001, 2000, 2000, 2000, 2001, 2002]], columns=['event1', 'event2'])lefthdatakey1key200.0Ohio200011.0Ohio200122.0Ohio200233.0Nevada200144.0Nevada20021righthevent1event2Nevada200101200023Ohio200045200067200189200210111pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)datakey1key2event1event200.0Ohio20004500.0Ohio20006711.0Ohio20018922.0Ohio2002101133.0Nevada20010112pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True, how='outer')datakey1key2event1event200.0Ohio2000.04.05.000.0Ohio2000.06.07.011.0Ohio2001.08.09.022.0Ohio2002.010.011.033.0Nevada2001.00.01.044.0Nevada2002.0NaNNaN4NaNNevada2000.02.03.01234left2 = DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'], columns=['Ohio', 'Nevada'])right2 = DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])1left2OhioNevadaa1.02.0c3.04.0e5.06.01right2MissouriAlabamab7.08.0c9.010.0d11.012.0e13.014.01pd.merge(left2, right2, how='outer', left_index=True, right_index=True)OhioNevadaMissouriAlabamaa1.02.0NaNNaNbNaNNaN7.08.0c3.04.09.010.0dNaNNaN11.012.0e5.06.013.014.01left2.join(right2, how='outer')OhioNevadaMissouriAlabamaa1.02.0NaNNaNbNaNNaN7.08.0c3.04.09.010.0dNaNNaN11.012.0e5.06.013.014.01left1.join(right1, on='key')keyvaluegroup_val0a03.51b17.02a23.53a33.54b47.05c5NaN12another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]], index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])New YorkOregona7.08.0c9.010.0e11.012.0f16.017.0相当于三个表进行合并1234left2right2anotherleft2.join([right2, another])OhioNevadaa1.02.0c3.04.0e5.06.0MissouriAlabamab7.08.0c9.010.0d11.012.0e13.014.0New YorkOregona7.08.0c9.010.0e11.012.0f16.017.0OhioNevadaMissouriAlabamaNew YorkOregona1.02.0NaNNaN7.08.0c3.04.09.010.09.010.0e5.06.013.014.011.012.01left2.join([right2, another], how='outer')轴向连接之前指的都是行级别的连接操作1arr = np.arange(12).reshape((3, 4))1arrarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 1np.concatenate([arr, arr], axis=1)array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) 123s1 = Series([0, 1], index=['a', 'b'])s2 = Series([2, 3, 4], index=['c', 'd', 'e'])s3 = Series([5, 6], index=['f', 'g'])1pd.concat([s1, s2, s3])a 0 b 1 c 2 d 3 e 4 f 5 g 6 dtype: int64 1pd.concat([s1, s2, s3], axis=1)012a0.0NaNNaNb1.0NaNNaNcNaN2.0NaNdNaN3.0NaNeNaN4.0NaNfNaNNaN5.0gNaNNaN6.012s4 = pd.concat([s1 * 5, s3])s4a 0 b 5 f 5 g 6 dtype: int64 1pd.concat([s1, s4], axis=1)01a0.00b1.05fNaN5gNaN61pd.concat([s1, s4], axis=1, join='inner')01a00b151pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])01a0.00.0cNaNNaNb1.05.0eNaNNaN在连接轴上建立一个层次化索引123s1s3result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])a 0 b 1 dtype: int64 f 5 g 6 dtype: int64 1resultone a 0 b 1 two a 0 b 1 three f 5 g 6 dtype: int64 12# Much more on the unstack function laterresult.unstack()abfgone0.01.0NaNNaNtwo0.01.0NaNNaNthreeNaNNaN5.06.01pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])onetwothreea0.0NaNNaNb1.0NaNNaNcNaN2.0NaNdNaN3.0NaNeNaN4.0NaNfNaNNaN5.0gNaNNaN6.012345678df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'], columns=['one', 'two'])df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'], columns=['three', 'four'])df1df2pd.concat([df1, df2], keys=['level1', 'level2'])pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])onetwoa01b23c45threefoura56c78fouronethreetwolevel1aNaN0.0NaN1.0bNaN2.0NaN3.0cNaN4.0NaN5.0level2a6.0NaN5.0NaNc8.0NaN7.0NaNlevel1level2onetwothreefoura015.06.0b23NaNNaNc457.08.012pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=0)pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=1)fouronethreetwolevel1aNaN0.0NaN1.0bNaN2.0NaN3.0cNaN4.0NaN5.0level2a6.0NaN5.0NaNc8.0NaN7.0NaNlevel1level2onetwothreefoura015.06.0b23NaNNaNc457.08.012pd.concat([df1, df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower'])upperlevel1level2loweronetwothreefoura015.06.0b23NaNNaNc457.08.012df1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])1df1abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.2962211df2bda00.2749920.2289131.35291710.886429-2.001637-0.371843去除无关的行索引12pd.concat([df1, df2], ignore_index=False)pd.concat([df1, df2], ignore_index=True)abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.29622101.3529170.274992NaN0.2289131-0.3718430.886429NaN-2.001637abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.29622131.3529170.274992NaN0.2289134-0.3718430.886429NaN-2.001637合并重叠数据12345a = Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan], index=['f', 'e', 'd', 'c', 'b', 'a'])b = Series(np.arange(len(a), dtype=np.float64), index=['f', 'e', 'd', 'c', 'b', 'a'])b[-1] = np.nan1af NaN e 2.5 d NaN c 3.5 b 4.5 a NaN dtype: float64 1bf 0.0 e 1.0 d 2.0 c 3.0 b 4.0 a NaN dtype: float64 1np.where(pd.isnull(a), b, a)array([ 0. , 2.5, 2. , 3.5, 4.5, nan]) combine_first, 重叠值合并，且进行数据对其1b[:-2].combine_first(a[2:])a NaN b 4.5 c 3.0 d 2.0 e 1.0 f 0.0 dtype: float64 12345678df1 = DataFrame(&#123;'a': [1., np.nan, 5., np.nan], 'b': [np.nan, 2., np.nan, 6.], 'c': range(2, 18, 4)&#125;)df2 = DataFrame(&#123;'a': [5., 4., np.nan, 3., 7.], 'b': [np.nan, 3., 4., 6., 8.]&#125;)df1df2df1.combine_first(df2)abc01.0NaN21NaN2.0625.0NaN103NaN6.014ab05.0NaN14.03.02NaN4.033.06.047.08.0abc01.0NaN2.014.02.06.025.04.010.033.06.014.047.08.0NaN重塑和轴向旋转重塑层次化索引1234data = DataFrame(np.arange(6).reshape((2, 3)), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['one', 'two', 'three'], name='number'))datanumberonetwothreestateOhio012Colorado345stack将列旋转为行12result = data.stack()resultstate number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 unstack将行旋转为列，默认操作最内层1result.unstack()numberonetwothreestateOhio012Colorado3451result.unstack(0)stateOhioColoradonumberone03two14three251result.unstack('state')stateOhioColoradonumberone03two14three251234567s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])s2 = Series([4, 5, 6], index=['c', 'd', 'e'])s1s2data2 = pd.concat([s1, s2], keys=['one', 'two'])data2data2.unstack()a 0 b 1 c 2 d 3 dtype: int64 c 4 d 5 e 6 dtype: int64 one a 0 b 1 c 2 d 3 two c 4 d 5 e 6 dtype: int64 abcdeone0.01.02.03.0NaNtwoNaNNaN4.05.06.0stack默认会滤除缺失值，因此两者可逆1data2.unstack().stack()one a 0.0 b 1.0 c 2.0 d 3.0 two c 4.0 d 5.0 e 6.0 dtype: float64 1data2.unstack().stack(dropna=False)one a 0.0 b 1.0 c 2.0 d 3.0 e NaN two a NaN b NaN c 4.0 d 5.0 e 6.0 dtype: float64 1234resultdf = DataFrame(&#123;'left': result, 'right': result + 5&#125;, columns=pd.Index(['left', 'right'], name='side'))dfstate number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 sideleftrightstatenumberOhioone05two16three27Coloradoone38two49three510DataFrame作为旋转轴的级别将成为结果中的最低级别（axis=MAX）1df.unstack('state')sideleftrightstateOhioColoradoOhioColoradonumberone0358two1469three25710stack操作将axis-1?1df.unstack('state').stack('side')stateOhioColoradonumbersideoneleft03right58twoleft14right69threeleft25right71012df.unstack('state')df.unstack('state').stack('state')sideleftrightstateOhioColoradoOhioColoradonumberone0358two1469three25710sideleftrightnumberstateoneOhio05Colorado38twoOhio16Colorado49threeOhio27Colorado510将长格式旋转为宽格式12345678910111213data = pd.read_csv('ch07/macrodata.csv')data[:10]periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name='date')periods[:10]data = DataFrame(data.to_records(), columns=pd.Index(['realgdp', 'infl', 'unemp'], name='item'), index=periods.to_timestamp('D', 'end'))data[:10]data.to_records()[:10]data.stack()[:10]ldata = data.stack().reset_index().rename(columns=&#123;0: 'value'&#125;)wdata = ldata.pivot('date', 'item', 'value')yearquarterrealgdprealconsrealinvrealgovtrealdpicpim1tbilrateunemppopinflrealint01959.01.02710.3491707.4286.898470.0451886.928.98139.72.825.8177.1460.000.0011959.02.02778.8011733.7310.859481.3011919.729.15141.73.085.1177.8302.340.7421959.03.02775.4881751.8289.226491.2601916.429.35140.53.825.3178.6572.741.0931959.04.02785.2041753.7299.356484.0521931.329.37140.04.335.6179.3860.274.0641960.01.02847.6991770.5331.722462.1991955.529.54139.63.505.2180.0072.311.1951960.02.02834.3901792.9298.152460.4001966.129.55140.22.685.2180.6710.142.5561960.03.02839.0221785.8296.375474.6761967.829.75140.92.365.6181.5282.70-0.3471960.04.02802.6161788.2259.764476.4341966.629.84141.12.296.3182.2871.211.0881961.01.02819.2641787.7266.405475.8541984.529.81142.12.376.8182.992-0.402.7791961.02.02872.0051814.3286.246480.3282014.429.92142.92.297.0183.6911.470.81PeriodIndex([&apos;1959Q1&apos;, &apos;1959Q2&apos;, &apos;1959Q3&apos;, &apos;1959Q4&apos;, &apos;1960Q1&apos;, &apos;1960Q2&apos;, &apos;1960Q3&apos;, &apos;1960Q4&apos;, &apos;1961Q1&apos;, &apos;1961Q2&apos;], dtype=&apos;int64&apos;, name=&apos;date&apos;, freq=&apos;Q-DEC&apos;) itemrealgdpinflunempdate1959-03-312710.3490.005.81959-06-302778.8012.345.11959-09-302775.4882.745.31959-12-312785.2040.275.61960-03-312847.6992.315.21960-06-302834.3900.145.21960-09-302839.0222.705.61960-12-312802.6161.216.31961-03-312819.264-0.406.81961-06-302872.0051.477.0rec.array([(datetime.datetime(1959, 3, 31, 0, 0), 2710.349, 0.0, 5.8), (datetime.datetime(1959, 6, 30, 0, 0), 2778.801, 2.34, 5.1), (datetime.datetime(1959, 9, 30, 0, 0), 2775.488, 2.74, 5.3), (datetime.datetime(1959, 12, 31, 0, 0), 2785.204, 0.27, 5.6), (datetime.datetime(1960, 3, 31, 0, 0), 2847.699, 2.31, 5.2), (datetime.datetime(1960, 6, 30, 0, 0), 2834.39, 0.14, 5.2), (datetime.datetime(1960, 9, 30, 0, 0), 2839.022, 2.7, 5.6), (datetime.datetime(1960, 12, 31, 0, 0), 2802.616, 1.21, 6.3), (datetime.datetime(1961, 3, 31, 0, 0), 2819.264, -0.4, 6.8), (datetime.datetime(1961, 6, 30, 0, 0), 2872.005, 1.47, 7.0)], dtype=[(&apos;date&apos;, &apos;O&apos;), (&apos;realgdp&apos;, &apos;&lt;f8&apos;), (&apos;infl&apos;, &apos;&lt;f8&apos;), (&apos;unemp&apos;, &apos;&lt;f8&apos;)]) date item 1959-03-31 realgdp 2710.349 infl 0.000 unemp 5.800 1959-06-30 realgdp 2778.801 infl 2.340 unemp 5.100 1959-09-30 realgdp 2775.488 infl 2.740 unemp 5.300 1959-12-31 realgdp 2785.204 dtype: float64 1ldata[:10]dateitemvalue01959-03-31realgdp2710.34911959-03-31infl0.00021959-03-31unemp5.80031959-06-30realgdp2778.80141959-06-30infl2.34051959-06-30unemp5.10061959-09-30realgdp2775.48871959-09-30infl2.74081959-09-30unemp5.30091959-12-31realgdp2785.20412pivoted = ldata.pivot('date', 'item', 'value')pivoted.head()iteminflrealgdpunempdate1959-03-310.002710.3495.81959-06-302.342778.8015.11959-09-302.742775.4885.31959-12-310.272785.2045.61960-03-312.312847.6995.212ldata['value2'] = np.random.randn(len(ldata))ldata[:10]dateitemvaluevalue201959-03-31realgdp2710.349-0.20470811959-03-31infl0.0000.47894321959-03-31unemp5.800-0.51943931959-06-30realgdp2778.801-0.55573041959-06-30infl2.3401.96578151959-06-30unemp5.1001.39340661959-09-30realgdp2775.4880.09290871959-09-30infl2.7400.28174681959-09-30unemp5.3000.76902391959-12-31realgdp2785.2041.24643512pivoted = ldata.pivot('date', 'item')pivoted[:5]valuevalue2iteminflrealgdpunempinflrealgdpunempdate1959-03-310.002710.3495.80.478943-0.204708-0.5194391959-06-302.342778.8015.11.965781-0.5557301.3934061959-09-302.742775.4885.30.2817460.0929080.7690231959-12-310.272785.2045.61.0071891.246435-1.2962211960-03-312.312847.6995.20.2289130.2749921.3529171pivoted['value'][:5]iteminflrealgdpunempdate1959-03-310.002710.3495.81959-06-302.342778.8015.11959-09-302.742775.4885.31959-12-310.272785.2045.61960-03-312.312847.6995.2123ldata.set_index(['date', 'item'])[:9]unstacked = ldata.set_index(['date', 'item']).unstack('item')unstacked[:7]valuevalue2dateitem1959-03-31realgdp2710.349-0.204708infl0.0000.478943unemp5.800-0.5194391959-06-30realgdp2778.801-0.555730infl2.3401.965781unemp5.1001.3934061959-09-30realgdp2775.4880.092908infl2.7400.281746unemp5.3000.769023valuevalue2iteminflrealgdpunempinflrealgdpunempdate1959-03-310.002710.3495.80.478943-0.204708-0.5194391959-06-302.342778.8015.11.965781-0.5557301.3934061959-09-302.742775.4885.30.2817460.0929080.7690231959-12-310.272785.2045.61.0071891.246435-1.2962211960-03-312.312847.6995.20.2289130.2749921.3529171960-06-300.142834.3905.2-2.0016370.886429-0.3718431960-09-302.702839.0225.6-0.4385701.669025-0.539741数据转换移除重复值123data = DataFrame(&#123;'k1': ['one'] * 3 + ['two'] * 4, 'k2': [1, 1, 2, 3, 3, 4, 4]&#125;)datak1k20one11one12one23two34two35two46two41data.duplicated()0 False 1 True 2 False 3 False 4 True 5 False 6 True dtype: bool 1data.drop_duplicates()k1k20one12one23two35two4123data['v1'] = range(7)datadata.drop_duplicates(['k1'])k1k2v10one101one112one223two334two345two456two46k1k2v10one103two3312data.drop_duplicates(['k1', 'k2'], keep='last')data.drop_duplicates(['k1', 'k2'], keep='first')k1k2v11one112one224two346two46k1k2v10one102one223two335two45利用函数或映射进行数据转换12345data = DataFrame(&#123;'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'], 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]&#125;)datafoodounces0bacon4.01pulled pork3.02bacon12.03Pastrami6.04corned beef7.55Bacon8.06pastrami3.07honey ham5.08nova lox6.012345678meat_to_animal = &#123; 'bacon': 'pig', 'pulled pork': 'pig', 'pastrami': 'cow', 'corned beef': 'cow', 'honey ham': 'pig', 'nova lox': 'salmon'&#125;12data['animal'] = data['food'].map(str.lower).map(meat_to_animal)datafoodouncesanimal0bacon4.0pig1pulled pork3.0pig2bacon12.0pig3Pastrami6.0cow4corned beef7.5cow5Bacon8.0pig6pastrami3.0cow7honey ham5.0pig8nova lox6.0salmon1data['food'].map(lambda x: meat_to_animal[x.lower()])0 pig 1 pig 2 pig 3 cow 4 cow 5 pig 6 cow 7 pig 8 salmon Name: food, dtype: object 替换值12data = Series([1., -999., 2., -999., -1000., 3.])data0 1.0 1 -999.0 2 2.0 3 -999.0 4 -1000.0 5 3.0 dtype: float64 1data.replace(-999, np.nan)0 1.0 1 NaN 2 2.0 3 NaN 4 -1000.0 5 3.0 dtype: float64 1data.replace([-999, -1000], np.nan)0 1.0 1 NaN 2 2.0 3 NaN 4 NaN 5 3.0 dtype: float64 1data.replace([-999, -1000], [np.nan, 0])0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 1data.replace(&#123;-999: np.nan, -1000: 0&#125;)0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 重命名轴索引1234data = DataFrame(np.arange(12).reshape((3, 4)), index=['Ohio', 'Colorado', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567New York8910111data.index.map(str.upper)array([&apos;OHIO&apos;, &apos;COLORADO&apos;, &apos;NEW YORK&apos;], dtype=object) 12data.index = data.index.map(str.upper)dataonetwothreefourOHIO0123COLORADO4567NEW YORK8910111data.rename(index=str.title, columns=str.upper)ONETWOTHREEFOUROhio0123Colorado4567New York89101112data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, columns=&#123;'three': 'peekaboo'&#125;)onetwopeekaboofourINDIANA0123COLORADO4567NEW YORK891011123# Always returns a reference to a DataFrame_ = data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, inplace=True)dataonetwothreefourINDIANA0123COLORADO4567NEW YORK891011离散化和面元划分1ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]123bins = [18, 25, 35, 60, 100]cats = pd.cut(ages, bins)cats[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]] Length: 12 Categories (4, object): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]] 1cats.codesarray([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8) 1cats.categoriesIndex([&apos;(18, 25]&apos;, &apos;(25, 35]&apos;, &apos;(35, 60]&apos;, &apos;(60, 100]&apos;], dtype=&apos;object&apos;) 1pd.value_counts(cats)(18, 25] 5 (35, 60] 3 (25, 35] 3 (60, 100] 1 dtype: int64 1pd.cut(ages, [18, 26, 36, 61, 100], right=False)[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)] Length: 12 Categories (4, object): [[18, 26) &lt; [26, 36) &lt; [36, 61) &lt; [61, 100)] 12group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']pd.cut(ages, bins, labels=group_names)[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult] Length: 12 Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior] 12data = np.random.rand(20)pd.cut(data, 4, precision=2)[(0.25, 0.49], (0.25, 0.49], (0.73, 0.98], (0.25, 0.49], (0.25, 0.49], ..., (0.25, 0.49], (0.73, 0.98], (0.49, 0.73], (0.49, 0.73], (0.49, 0.73]] Length: 20 Categories (4, object): [(0.0032, 0.25] &lt; (0.25, 0.49] &lt; (0.49, 0.73] &lt; (0.73, 0.98]] 123data = np.random.randn(1000) # Normally distributedcats = pd.qcut(data, 4) # Cut into quartilescats[(0.636, 3.26], [-3.745, -0.648], (0.636, 3.26], (-0.022, 0.636], (-0.648, -0.022], ..., (0.636, 3.26], (-0.022, 0.636], [-3.745, -0.648], (-0.022, 0.636], (-0.022, 0.636]] Length: 1000 Categories (4, object): [[-3.745, -0.648] &lt; (-0.648, -0.022] &lt; (-0.022, 0.636] &lt; (0.636, 3.26]] 1pd.value_counts(cats)(0.636, 3.26] 250 (-0.022, 0.636] 250 (-0.648, -0.022] 250 [-3.745, -0.648] 250 dtype: int64 1pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])[(-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298], (-1.274, -0.022], ..., (-0.022, 1.298], (-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298]] Length: 1000 Categories (4, object): [[-3.745, -1.274] &lt; (-1.274, -0.022] &lt; (-0.022, 1.298] &lt; (1.298, 3.26]] 检测和过滤异常值123np.random.seed(12345)data = DataFrame(np.random.randn(1000, 4))data.describe()0123count1000.0000001000.0000001000.0000001000.000000mean-0.0676840.0679240.025598-0.002298std0.9980350.9921061.0068350.996794min-3.428254-3.548824-3.184377-3.74535625%-0.774890-0.591841-0.641675-0.64414450%-0.1164010.1011430.002073-0.01361175%0.6163660.7802820.6803910.654328max3.3666262.6536563.2603833.92752812col = data[3]col[np.abs(col) &gt; 3]97 3.927528 305 -3.399312 400 -3.745356 Name: 3, dtype: float64 1data[(np.abs(data) &gt; 3).any(1)]01235-0.5397410.4769853.248944-1.02122897-0.7743630.5529360.1060613.927528102-0.655054-0.5652303.1768730.959533305-2.3155550.457246-0.025907-3.3993123240.0501881.9513123.2603830.9633014000.1463260.508391-0.196713-3.745356499-0.293333-0.242459-3.0569901.918403523-3.428254-0.296336-0.439938-0.8671655860.2751441.179227-3.1843771.369891808-0.362528-3.5488241.553205-2.1863019003.366626-2.3722140.8510101.33284612data[np.abs(data) &gt; 3] = np.sign(data) * 3data.describe()0123count1000.0000001000.0000001000.0000001000.000000mean-0.0676230.0684730.025153-0.002081std0.9954850.9902531.0039770.989736min-3.000000-3.000000-3.000000-3.00000025%-0.774890-0.591841-0.641675-0.64414450%-0.1164010.1011430.002073-0.01361175%0.6163660.7802820.6803910.654328max3.0000002.6536563.0000003.000000排列和随机采样123df = DataFrame(np.arange(5 * 4).reshape((5, 4)))sampler = np.random.permutation(5)samplerarray([1, 0, 2, 3, 4]) 1df0123001231456728910113121314154161718191df.take(sampler)0123145670012328910113121314154161718191df.take(np.random.permutation(len(df))[:3])0123145670012341617181912bag = np.array([5, 7, -1, 6, 4])sampler = np.random.randint(0, len(bag), size=10)1samplerarray([3, 0, 4, 1, 1, 2, 3, 0, 1, 2]) 12draws = bag.take(sampler)drawsarray([ 6, 5, 4, 7, 7, -1, 6, 5, 7, -1]) 计算指标 / 哑变量1234df = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)dfpd.get_dummies(df['key'])data1key00b11b22a33c44a55babc00.01.00.010.01.00.021.00.00.030.00.01.041.00.00.050.01.00.01234dummies = pd.get_dummies(df['key'], prefix='key')dummiesdf_with_dummy = df[['data1']].join(dummies)df_with_dummykey_akey_bkey_c00.01.00.010.01.00.021.00.00.030.00.01.041.00.00.050.01.00.0data1key_akey_bkey_c000.01.00.0110.01.00.0221.00.00.0330.00.01.0441.00.00.0550.01.00.01234mnames = ['movie_id', 'title', 'genres']movies = pd.read_table('ch02/movielens/movies.dat', sep='::', header=None, names=mnames)movies[:10]C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: ParserWarning: Falling back to the &apos;python&apos; engine because the &apos;c&apos; engine does not support regex separators (separators &gt; 1 char and different from &apos;\s+&apos; are interpreted as regex); you can avoid this warning by specifying engine=&apos;python&apos;. app.launch_new_instance() movie_idtitlegenres01Toy Story (1995)Animation|Children’s|Comedy12Jumanji (1995)Adventure|Children’s|Fantasy23Grumpier Old Men (1995)Comedy|Romance34Waiting to Exhale (1995)Comedy|Drama45Father of the Bride Part II (1995)Comedy56Heat (1995)Action|Crime|Thriller67Sabrina (1995)Comedy|Romance78Tom and Huck (1995)Adventure|Children’s89Sudden Death (1995)Action910GoldenEye (1995)Action|Adventure|Thriller123genre_iter = (set(x.split('|')) for x in movies.genres)genres = sorted(set.union(*genre_iter))genres[&apos;Action&apos;, &apos;Adventure&apos;, &apos;Animation&apos;, &quot;Children&apos;s&quot;, &apos;Comedy&apos;, &apos;Crime&apos;, &apos;Documentary&apos;, &apos;Drama&apos;, &apos;Fantasy&apos;, &apos;Film-Noir&apos;, &apos;Horror&apos;, &apos;Musical&apos;, &apos;Mystery&apos;, &apos;Romance&apos;, &apos;Sci-Fi&apos;, &apos;Thriller&apos;, &apos;War&apos;, &apos;Western&apos;] 12dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)dummies[:10].ix[:, :5]ActionAdventureAnimationChildren’sComedy00.00.00.00.00.010.00.00.00.00.020.00.00.00.00.030.00.00.00.00.040.00.00.00.00.050.00.00.00.00.060.00.00.00.00.070.00.00.00.00.080.00.00.00.00.090.00.00.00.00.0123for i, gen in enumerate(movies.genres): dummies.ix[i, gen.split('|')] = 1dummies[:10].ix[:, :5]ActionAdventureAnimationChildren’sComedy00.00.01.01.01.010.01.00.01.00.020.00.00.00.01.030.00.00.00.01.040.00.00.00.01.051.00.00.00.00.060.00.00.00.01.070.01.00.01.00.081.00.00.00.00.091.01.00.00.00.012movies_windic = movies.join(dummies.add_prefix('Genre_'))movies_windic.ix[0]movie_id 1 title Toy Story (1995) genres Animation|Children&apos;s|Comedy Genre_Action 0 Genre_Adventure 0 Genre_Animation 1 Genre_Children&apos;s 1 Genre_Comedy 1 Genre_Crime 0 Genre_Documentary 0 Genre_Drama 0 Genre_Fantasy 0 Genre_Film-Noir 0 Genre_Horror 0 Genre_Musical 0 Genre_Mystery 0 Genre_Romance 0 Genre_Sci-Fi 0 Genre_Thriller 0 Genre_War 0 Genre_Western 0 Name: 0, dtype: object 1np.random.seed(12345)12values = np.random.rand(10)valuesarray([ 0.9296, 0.3164, 0.1839, 0.2046, 0.5677, 0.5955, 0.9645, 0.6532, 0.7489, 0.6536]) 12bins = [0, 0.2, 0.4, 0.6, 0.8, 1]pd.get_dummies(pd.cut(values, bins))(0, 0.2](0.2, 0.4](0.4, 0.6](0.6, 0.8](0.8, 1]00.00.00.00.01.010.01.00.00.00.021.00.00.00.00.030.01.00.00.00.040.00.01.00.00.050.00.01.00.00.060.00.00.00.01.070.00.00.01.00.080.00.00.01.00.090.00.00.01.00.0字符串操作字符串对象方法12val = 'a,b, guido'val.split(',')[&apos;a&apos;, &apos;b&apos;, &apos; guido&apos;] 12pieces = [x.strip() for x in val.split(',')]pieces[&apos;a&apos;, &apos;b&apos;, &apos;guido&apos;] 12first, second, third = piecesfirst + '::' + second + '::' + third&apos;a::b::guido&apos; Surprise :P1'::'.join(pieces)&apos;a::b::guido&apos; 1'guido' in valTrue 1val.index(',')1 1val.find(':')-1 1val.index(':')--------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-110-280f8b2856ce&gt; in &lt;module&gt;() ----&gt; 1 val.index(&apos;:&apos;) ValueError: substring not found 1val.count(',')2 1val.replace(',', '::')&apos;a::b:: guido&apos; 1val.replace(',', '')&apos;ab guido&apos; 正则表达式123import retext = "foo bar\t baz \tqux"re.split('\s+', text)[&apos;foo&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;qux&apos;] 12regex = re.compile('\s+')regex.split(text)[&apos;foo&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;qux&apos;] 1regex.findall(text)[&apos; &apos;, &apos;\t &apos;, &apos; \t&apos;] 123456789text = """Dave dave@google.comSteve steve@gmail.comRob rob@gmail.comRyan ryan@yahoo.com"""pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]&#123;2,4&#125;'# re.IGNORECASE makes the regex case-insensitiveregex = re.compile(pattern, flags=re.IGNORECASE)1regex.findall(text)[&apos;dave@google.com&apos;, &apos;steve@gmail.com&apos;, &apos;rob@gmail.com&apos;, &apos;ryan@yahoo.com&apos;] Search只返回第一项12m = regex.search(text)m&lt;_sre.SRE_Match object; span=(5, 20), match=&apos;dave@google.com&apos;&gt; 1text[m.start():m.end()]&apos;dave@google.com&apos; 只匹配出现在字符串开头的模式1print(regex.match(text))None ​替换1print(regex.sub('REDACTED', text))Dave REDACTED Steve REDACTED Rob REDACTED Ryan REDACTED ​12pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]&#123;2,4&#125;)'regex = re.compile(pattern, flags=re.IGNORECASE)12m = regex.match('wesm@bright.net')m.groups()(&apos;wesm&apos;, &apos;bright&apos;, &apos;net&apos;) 1regex.findall(text)[(&apos;dave&apos;, &apos;google&apos;, &apos;com&apos;), (&apos;steve&apos;, &apos;gmail&apos;, &apos;com&apos;), (&apos;rob&apos;, &apos;gmail&apos;, &apos;com&apos;), (&apos;ryan&apos;, &apos;yahoo&apos;, &apos;com&apos;)] 1print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text))Dave Username: dave, Domain: google, Suffix: com Steve Username: steve, Domain: gmail, Suffix: com Rob Username: rob, Domain: gmail, Suffix: com Ryan Username: ryan, Domain: yahoo, Suffix: com ​123456regex = re.compile(r""" (?P&lt;username&gt;[A-Z0-9._%+-]+) @ (?P&lt;domain&gt;[A-Z0-9.-]+) \. (?P&lt;suffix&gt;[A-Z]&#123;2,4&#125;)""", flags=re.IGNORECASE|re.VERBOSE)12m = regex.match('wesm@bright.net')m.groupdict(){&apos;domain&apos;: &apos;bright&apos;, &apos;suffix&apos;: &apos;net&apos;, &apos;username&apos;: &apos;wesm&apos;} pandas中矢量化的字符串函数123data = &#123;'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan&#125;data = Series(data)1dataDave dave@google.com Rob rob@gmail.com Steve steve@gmail.com Wes NaN dtype: object 1data.isnull()Dave False Rob False Steve False Wes True dtype: bool 1data.str.contains('gmail')Dave False Rob True Steve True Wes NaN dtype: object 1pattern&apos;([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})&apos; 1data.str.findall(pattern, flags=re.IGNORECASE)Dave [(dave, google, com)] Rob [(rob, gmail, com)] Steve [(steve, gmail, com)] Wes NaN dtype: object 12matches = data.str.match(pattern, flags=re.IGNORECASE)matchesC:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: In future versions of pandas, match will change to always return a bool indexer. if __name__ == &apos;__main__&apos;: Dave (dave, google, com) Rob (rob, gmail, com) Steve (steve, gmail, com) Wes NaN dtype: object 1matches.str.get(1)Dave google Rob gmail Steve gmail Wes NaN dtype: object 1matches.str[0]Dave dave Rob rob Steve steve Wes NaN dtype: object 1data.str[:5]Dave dave@ Rob rob@g Steve steve Wes NaN dtype: object Example: USDA Food Database123456789101112131415161718192021222324252627&#123; "id": 21441, "description": "KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,Wing, meat and skin with breading", "tags": ["KFC"], "manufacturer": "Kentucky Fried Chicken", "group": "Fast Foods", "portions": [ &#123; "amount": 1, "unit": "wing, with skin", "grams": 68.0 &#125;, ... ], "nutrients": [ &#123; "value": 20.8, "units": "g", "description": "Protein", "group": "Composition" &#125;, ... ]&#125;123import jsondb = json.load(open('ch07/foods-2011-10-03.json'))len(db)6636 1db[0].keys()dict_keys([&apos;id&apos;, &apos;tags&apos;, &apos;portions&apos;, &apos;nutrients&apos;, &apos;description&apos;, &apos;group&apos;, &apos;manufacturer&apos;]) 1db[0]['nutrients'][0]{&apos;description&apos;: &apos;Protein&apos;, &apos;group&apos;: &apos;Composition&apos;, &apos;units&apos;: &apos;g&apos;, &apos;value&apos;: 25.18} 12nutrients = DataFrame(db[0]['nutrients'])nutrients[:7]descriptiongroupunitsvalue0ProteinCompositiong25.181Total lipid (fat)Compositiong29.202Carbohydrate, by differenceCompositiong3.063AshOtherg3.284EnergyEnergykcal376.005WaterCompositiong39.286EnergyEnergykJ1573.0012info_keys = ['description', 'group', 'id', 'manufacturer']info = DataFrame(db, columns=info_keys)1info[:5]descriptiongroupidmanufacturer0Cheese, carawayDairy and Egg Products10081Cheese, cheddarDairy and Egg Products10092Cheese, edamDairy and Egg Products10183Cheese, fetaDairy and Egg Products10194Cheese, mozzarella, part skim milkDairy and Egg Products10281pd.value_counts(info.group)[:10]Vegetables and Vegetable Products 812 Beef Products 618 Baked Products 496 Breakfast Cereals 403 Legumes and Legume Products 365 Fast Foods 365 Lamb, Veal, and Game Products 345 Sweets 341 Pork Products 328 Fruits and Fruit Juices 328 Name: group, dtype: int64 12345678nutrients = []for rec in db: fnuts = DataFrame(rec['nutrients']) fnuts['id'] = rec['id'] nutrients.append(fnuts)nutrients = pd.concat(nutrients, ignore_index=True)1nutrients[:10]descriptiongroupunitsvalueid0ProteinCompositiong25.1810081Total lipid (fat)Compositiong29.2010082Carbohydrate, by differenceCompositiong3.0610083AshOtherg3.2810084EnergyEnergykcal376.0010085WaterCompositiong39.2810086EnergyEnergykJ1573.0010087Fiber, total dietaryCompositiong0.0010088Calcium, CaElementsmg673.0010089Iron, FeElementsmg0.6410081nutrients.duplicated().sum()14179 1nutrients = nutrients.drop_duplicates()1234col_mapping = &#123;'description' : 'food', 'group' : 'fgroup'&#125;info = info.rename(columns=col_mapping, copy=False)info[:10]foodfgroupidmanufacturer0Cheese, carawayDairy and Egg Products10081Cheese, cheddarDairy and Egg Products10092Cheese, edamDairy and Egg Products10183Cheese, fetaDairy and Egg Products10194Cheese, mozzarella, part skim milkDairy and Egg Products10285Cheese, mozzarella, part skim milk, low moistureDairy and Egg Products10296Cheese, romanoDairy and Egg Products10387Cheese, roquefortDairy and Egg Products10398Cheese spread, pasteurized process, american, …Dairy and Egg Products10489Cream, fluid, half and halfDairy and Egg Products10491234col_mapping = &#123;'description' : 'nutrient', 'group' : 'nutgroup'&#125;nutrients = nutrients.rename(columns=col_mapping, copy=False)nutrients[:10]nutrientnutgroupunitsvalueid0ProteinCompositiong25.1810081Total lipid (fat)Compositiong29.2010082Carbohydrate, by differenceCompositiong3.0610083AshOtherg3.2810084EnergyEnergykcal376.0010085WaterCompositiong39.2810086EnergyEnergykJ1573.0010087Fiber, total dietaryCompositiong0.0010088Calcium, CaElementsmg673.0010089Iron, FeElementsmg0.6410081ndata = pd.merge(nutrients, info, on='id', how='outer')1ndata[:10]nutrientnutgroupunitsvalueidfoodfgroupmanufacturer0ProteinCompositiong25.181008Cheese, carawayDairy and Egg Products1Total lipid (fat)Compositiong29.201008Cheese, carawayDairy and Egg Products2Carbohydrate, by differenceCompositiong3.061008Cheese, carawayDairy and Egg Products3AshOtherg3.281008Cheese, carawayDairy and Egg Products4EnergyEnergykcal376.001008Cheese, carawayDairy and Egg Products5WaterCompositiong39.281008Cheese, carawayDairy and Egg Products6EnergyEnergykJ1573.001008Cheese, carawayDairy and Egg Products7Fiber, total dietaryCompositiong0.001008Cheese, carawayDairy and Egg Products8Calcium, CaElementsmg673.001008Cheese, carawayDairy and Egg Products9Iron, FeElementsmg0.641008Cheese, carawayDairy and Egg Products1ndata.ix[30000]nutrient Glycine nutgroup Amino Acids units g value 0.04 id 6158 food Soup, tomato bisque, canned, condensed fgroup Soups, Sauces, and Gravies manufacturer Name: 30000, dtype: object 123result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)result[:10]result['Zinc, Zn'].sort_values().plot(kind='barh')nutrient fgroup Adjusted Protein Sweets 12.900 Vegetables and Vegetable Products 2.180 Alanine Baby Foods 0.085 Baked Products 0.248 Beef Products 1.550 Beverages 0.003 Breakfast Cereals 0.311 Cereal Grains and Pasta 0.373 Dairy and Egg Products 0.271 Ethnic Foods 1.290 Name: value, dtype: float64 &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba08c9e780&gt; 123456789by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])get_maximum = lambda x: x.xs(x.value.idxmax())get_minimum = lambda x: x.xs(x.value.idxmin())max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]# make the food a little smallermax_foods.food = max_foods.food.str[:50]1max_foods.ix['Amino Acids']['food']nutrient Alanine Gelatins, dry powder, unsweetened Arginine Seeds, sesame flour, low-fat Aspartic acid Soy protein isolate Cystine Seeds, cottonseed flour, low fat (glandless) Glutamic acid Soy protein isolate Glycine Gelatins, dry powder, unsweetened Histidine Whale, beluga, meat, dried (Alaska Native) Hydroxyproline KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINA... Isoleucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Leucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Lysine Seal, bearded (Oogruk), meat, dried (Alaska Na... Methionine Fish, cod, Atlantic, dried and salted Phenylalanine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Proline Gelatins, dry powder, unsweetened Serine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Threonine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Tryptophan Sea lion, Steller, meat with fat (Alaska Native) Tyrosine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Valine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Name: food, dtype: object]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch06]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch06%2F</url>
      <content type="text"><![CDATA[Data loading, storage, and file formats12345678910111213from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport sysimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%pwd&apos;C:\\Users\\Ewan\\Downloads\\pydata-book-master&apos; Reading and Writing Data in Text Format1!more ch06\ex1.csva,b,c,d,message 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12df = pd.read_csv('ch06/ex1.csv')dfabcdmessage01234hello15678world29101112foo1pd.read_table('ch06/ex1.csv', sep=',')abcdmessage01234hello15678world29101112foo1!more ch06\ex2.csv1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12pd.read_csv('ch06/ex2.csv', header=None)pd.read_csv('ch06/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])0123401234hello15678world29101112fooabcdmessage01234hello15678world29101112foo12names = ['a', 'b', 'c', 'd', 'message']pd.read_csv('ch06/ex2.csv', names=names, index_col='message')abcdmessagehello1234world5678foo9101112123!more ch06\csv_mindex.csvparsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])parsedkey1,key2,value1,value2 one,a,1,2 one,b,3,4 one,c,5,6 one,d,7,8 two,a,9,10 two,b,11,12 two,c,13,14 two,d,15,16 value1value2key1key2onea12b34c56d78twoa910b1112c1314d15161list(open('ch06/ex3.txt'))[&apos; A B C\n&apos;, &apos;aaa -0.264438 -1.026059 -0.619500\n&apos;, &apos;bbb 0.927272 0.302904 -0.032399\n&apos;, &apos;ccc -0.264273 -0.386314 -0.217601\n&apos;, &apos;ddd -0.871858 -0.348382 1.100491\n&apos;] 采用正则表达式作为分隔符12result = pd.read_table('ch06/ex3.txt', sep='\s+')resultABCaaa-0.264438-1.026059-0.619500bbb0.9272720.302904-0.032399ccc-0.264273-0.386314-0.217601ddd-0.871858-0.3483821.10049112!more ch06\ex4.csvpd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])# hey! a,b,c,d,message # just wanted to make things more difficult for you # who reads CSV files with computers, anyway? 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo abcdmessage01234hello15678world29101112foo1234!more ch06\ex5.csvresult = pd.read_csv('ch06/ex5.csv')resultpd.isnull(result)something,a,b,c,d,message one,1,2,3,4,NA two,5,6,,8,world three,9,10,11,12,foo somethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foosomethingabcdmessage0FalseFalseFalseFalseFalseTrue1FalseFalseFalseTrueFalseFalse2FalseFalseFalseFalseFalseFalse12result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])resultsomethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foo12sentinels = &#123;'message': ['foo', 'NA'], 'something': ['two']&#125;pd.read_csv('ch06/ex5.csv', na_values=sentinels)somethingabcdmessage0one123.04NaN1NaN56NaN8world2three91011.012NaN逐块读取文本文件12result = pd.read_csv('ch06/ex6.csv')resultonetwothreefourkey00.467976-0.038649-0.295344-1.824726L1-0.3588931.4044530.704965-0.200638B2-0.5018400.659254-0.421691-0.057688G30.2048861.0741341.388361-0.982404R40.354628-0.1331160.283763-0.837063Q51.8174800.7422730.419395-2.251035Q6-0.7767640.935518-0.332872-1.875641U7-0.9131351.530624-0.5726570.477252K80.358480-0.497572-0.3670160.507702S9-1.740877-1.160417-1.6378302.172201G100.240564-0.3282491.2521551.0727968110.7640181.165476-0.6395441.495258R120.571035-0.3105370.582437-0.2987651132.3176580.430710-1.3342160.199679P141.547771-1.119753-2.2776340.329586J15-1.3106080.401719-1.0009871.156708E16-0.0884960.6347120.1533240.415335B17-0.018663-0.247487-1.4465220.750938A18-0.070127-1.5790970.1208920.671432F19-0.194678-0.4920392.3596050.319810H20-0.2486180.868707-0.492226-0.717959W21-1.091549-0.867110-0.647760-0.832562C220.641404-0.138822-0.621963-0.284839C231.2164080.9926870.165162-0.069619V24-0.5644740.7928320.7470530.571675I251.759879-0.515666-0.2304811.362317S260.1262660.3092810.382820-0.239199L271.334360-0.100152-0.840731-0.643967628-0.7376200.278087-0.053235-0.950972J29-1.148486-0.986292-0.1449630.124362Y………………99700.633495-0.1865240.9276270.143164499710.308636-0.1128570.762842-1.07297719972-1.627051-0.9781510.154745-1.229037Z99730.3148470.0979890.1996080.955193P99741.6669070.9920050.496128-0.686391S99750.0106030.708540-1.2587110.226541K99760.118693-0.714455-0.501342-0.254764K99770.302616-2.011527-0.6280850.768827H9978-0.0985721.769086-0.215027-0.053076A9979-0.0190581.9649940.738538-0.883776F9980-0.5953490.001781-1.423355-1.458477M99811.392170-1.396560-1.425306-0.847535H9982-0.896029-0.1522871.9244830.36518469983-2.274642-0.9018741.5003520.996541N9984-0.3018981.0199061.1021602.624526I9985-2.548389-0.5853741.496201-0.718815D9986-0.0645880.759292-1.568415-0.420933E9987-0.143365-1.111760-1.8155810.43527429988-0.070412-1.0559210.338017-0.440763X99890.6491480.994273-1.3842270.485120Q9990-0.3707690.404356-1.051628-1.05089989991-0.4099800.155627-0.8189901.277350W99920.301214-1.1112030.6682580.671922A99931.8211170.4164450.1738740.505118X99940.0688041.3227590.8023460.223618H99952.311896-0.417070-1.409599-0.515821L9996-0.479893-0.6504190.745152-0.646038E99970.5233310.7871120.4860661.093156K9998-0.3625590.598894-1.8432010.887292G9999-0.096376-1.012999-0.657431-0.573315010000 rows × 5 columns1pd.read_csv('ch06/ex6.csv', nrows=5)onetwothreefourkey00.467976-0.038649-0.295344-1.824726L1-0.3588931.4044530.704965-0.200638B2-0.5018400.659254-0.421691-0.057688G30.2048861.0741341.388361-0.982404R40.354628-0.1331160.283763-0.837063Q12chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)chunker&lt;pandas.io.parsers.TextFileReader at 0x2035229de80&gt; 1234567chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)tot = Series([])for piece in chunker: tot = tot.add(piece['key'].value_counts(), fill_value=0)tot = tot.sort_values(ascending=False)1tot[:10]E 368.0 X 364.0 L 346.0 O 343.0 Q 340.0 M 338.0 J 337.0 F 335.0 K 334.0 H 330.0 dtype: float64 将数据写出到文本格式12data = pd.read_csv('ch06/ex5.csv')datasomethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foo12data.to_csv('ch06/out.csv')!more ch06\out.csv,something,a,b,c,d,message 0,one,1,2,3.0,4, 1,two,5,6,,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, sep='|')|something|a|b|c|d|message 0|one|1|2|3.0|4| 1|two|5|6||8|world 2|three|9|10|11.0|12|foo 1data.to_csv(sys.stdout, na_rep='NULL'),something,a,b,c,d,message 0,one,1,2,3.0,4,NULL 1,two,5,6,NULL,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, header=False)one,1,2,3.0,4, two,5,6,,8,world three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])a,b,c 1,2,3.0 5,6, 9,10,11.0 123456dates = pd.date_range('1/1/2000', periods=7)datests = Series(np.arange(7), index=dates)tsts.to_csv('ch06/tseries.csv')!more ch06\tseries.csvDatetimeIndex([&apos;2000-01-01&apos;, &apos;2000-01-02&apos;, &apos;2000-01-03&apos;, &apos;2000-01-04&apos;, &apos;2000-01-05&apos;, &apos;2000-01-06&apos;, &apos;2000-01-07&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 Freq: D, dtype: int32 2000-01-01,0 2000-01-02,1 2000-01-03,2 2000-01-04,3 2000-01-05,4 2000-01-06,5 2000-01-07,6 1Series.from_csv('ch06/tseries.csv', parse_dates=True)2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 dtype: int64 手动处理分隔符格式1!more ch06\ex7.csv&quot;a&quot;,&quot;b&quot;,&quot;c&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot; 1234import csvf = open('ch06/ex7.csv')reader = csv.reader(f)12for line in reader: print(line)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;] [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;] 12345678910lines = list(csv.reader(open('ch06/ex7.csv')))header, values = lines[0], lines[1:]for item in zip(*values): print(item) for h, v in zip(header, zip(*values)): print(h, v)data_dict = &#123;h: v for h, v in zip(header, zip(*values))&#125;data_dict(&apos;1&apos;, &apos;1&apos;) (&apos;2&apos;, &apos;2&apos;) (&apos;3&apos;, &apos;3&apos;) a (&apos;1&apos;, &apos;1&apos;) b (&apos;2&apos;, &apos;2&apos;) c (&apos;3&apos;, &apos;3&apos;) {&apos;a&apos;: (&apos;1&apos;, &apos;1&apos;), &apos;b&apos;: (&apos;2&apos;, &apos;2&apos;), &apos;c&apos;: (&apos;3&apos;, &apos;3&apos;)} 12345class my_dialect(csv.Dialect): lineterminator = '\n' delimiter = ';' quotechar = '"' quoting = csv.QUOTE_MINIMAL123456with open('mydata.csv', 'w') as f: writer = csv.writer(f, dialect=my_dialect) writer.writerow(('one', 'two', 'three')) writer.writerow(('1', '2', '3')) writer.writerow(('4', '5', '6')) writer.writerow(('7', '8', '9'))14 6 6 6 1!more mydata.csvone;two;three 1;2;3 4;5;6 7;8;9 JSON数据12345678obj = """&#123;"name": "Wes", "places_lived": ["United States", "Spain", "Germany"], "pet": null, "siblings": [&#123;"name": "Scott", "age": 25, "pet": "Zuko"&#125;, &#123;"name": "Katie", "age": 33, "pet": "Cisco"&#125;]&#125;"""123import jsonresult = json.loads(obj)result{&apos;name&apos;: &apos;Wes&apos;, &apos;pet&apos;: None, &apos;places_lived&apos;: [&apos;United States&apos;, &apos;Spain&apos;, &apos;Germany&apos;], &apos;siblings&apos;: [{&apos;age&apos;: 25, &apos;name&apos;: &apos;Scott&apos;, &apos;pet&apos;: &apos;Zuko&apos;}, {&apos;age&apos;: 33, &apos;name&apos;: &apos;Katie&apos;, &apos;pet&apos;: &apos;Cisco&apos;}]} 1asjson = json.dumps(result) # convert to json12siblings = DataFrame(result['siblings'], columns=['name', 'age'])siblingsnameage0Scott251Katie33XML和HTML： Web信息收集NB. The Yahoo! Finance API has changed and this example no longer works123456from lxml.html import parsefrom urllib.request import urlopenparsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))doc = parsed.getroot()12links = doc.findall('.//a')links[15:20][&lt;Element a at 0x20352cad598&gt;, &lt;Element a at 0x20352cad5e8&gt;, &lt;Element a at 0x20352cad638&gt;, &lt;Element a at 0x20352cad688&gt;, &lt;Element a at 0x20352cad6d8&gt;] 1234lnk = links[28]lnklnk.get('href')lnk.text_content()&lt;Element a at 0x20352cad9a8&gt; &apos;/quote/NFLX?p=NFLX&apos; &apos;NFLX&apos; 12urls = [lnk.get('href') for lnk in doc.findall('.//a')]urls[-10:][&apos;//finance.yahoo.com/broker-comparison?bypass=true&apos;, &apos;https://help.yahoo.com/kb/index?page=content&amp;y=PROD_MAIL_ML&amp;locale=en_US&amp;id=SLN2310&amp;actp=productlink&apos;, &apos;http://help.yahoo.com/l/us/yahoo/finance/&apos;, &apos;https://yahoo.uservoice.com/forums/382977&apos;, &apos;http://info.yahoo.com/privacy/us/yahoo/&apos;, &apos;http://info.yahoo.com/relevantads/&apos;, &apos;http://info.yahoo.com/legal/us/yahoo/utos/utos-173.html&apos;, &apos;http://twitter.com/YahooFinance&apos;, &apos;http://facebook.com/yahoofinance&apos;, &apos;http://yahoofinance.tumblr.com&apos;] 123tables = doc.findall('.//table')len(tables)calls = tables[0]1 1rows = calls.findall('.//tr')123def _unpack(row, kind='td'): elts = row.findall('.//%s' % kind) return [val.text_content() for val in elts]12_unpack(rows[0], kind='th')_unpack(rows[1], kind='td')[] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-87-7d371ed47023&gt; in &lt;module&gt;() 1 _unpack(rows[0], kind=&apos;th&apos;) ----&gt; 2 _unpack(rows[1], kind=&apos;td&apos;) IndexError: list index out of range 1234567from pandas.io.parsers import TextParserdef parse_options_data(table): rows = table.findall('.//tr') header = _unpack(rows[0], kind='th') data = [_unpack(r) for r in rows[1:]] return TextParser(data, names=header).get_chunk()123call_data = parse_options_data(calls)put_data = parse_options_data(puts)call_data[:10]Parsing XML with lxml.objectify12345from lxml import objectifypath = '.\ch06\mta_perf\Performance_MNR.xml'parsed = objectify.parse(open(path))root = parsed.getroot()12345678910111213data = []skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']for elt in root.INDICATOR: el_data = &#123;&#125; for child in elt.getchildren(): if child.tag in skip_fields: continue el_data[child.tag] = child.pyval data.append(el_data)12perf = DataFrame(data)perf.ix[:10, 5:]INDICATOR_UNITMONTHLY_ACTUALMONTHLY_TARGETPERIOD_MONTHPERIOD_YEARYTD_ACTUALYTD_TARGET0%96.9951200896.9951%95952200896952%96.9953200896.3953%98.3954200896.8954%95.8955200896.6955%94.4956200896.2956%96957200896.2957%96.4958200896.2958%93.7959200895.9959%96.495102008969510%96.99511200896.195二进制数据格式123frame = pd.read_csv('ch06/ex1.csv')frameframe.to_pickle('ch06/frame_pickle')abcdmessage01234hello15678world29101112foo1pd.read_pickle('ch06/frame_pickle')abcdmessage01234hello15678world29101112foo使用HDF5格式1234store = pd.HDFStore('mydata.h5')store['obj1'] = framestore['obj1_col'] = frame['a']store&lt;class &apos;pandas.io.pytables.HDFStore&apos;&gt; File path: mydata.h5 /obj1 frame (shape-&gt;[3,5]) /obj1_col series (shape-&gt;[3]) 1store['obj1']abcdmessage01234hello15678world29101112foo12store.close()os.remove('mydata.h5')使用数据库1234567891011import sqlite3query = """CREATE TABLE test(a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER);"""con = sqlite3.connect(':memory:')con.execute(query)con.commit()&lt;sqlite3.Cursor at 0x2035487c880&gt; 1234567data = [('Atlanta', 'Georgia', 1.25, 6), ('Tallahassee', 'Florida', 2.6, 3), ('Sacramento', 'California', 1.7, 5)]stmt = "INSERT INTO test VALUES(?, ?, ?, ?)"con.executemany(stmt, data)con.commit()&lt;sqlite3.Cursor at 0x2035487c810&gt; 123cursor = con.execute('select * from test')rows = cursor.fetchall()rows[(&apos;Atlanta&apos;, &apos;Georgia&apos;, 1.25, 6), (&apos;Tallahassee&apos;, &apos;Florida&apos;, 2.6, 3), (&apos;Sacramento&apos;, &apos;California&apos;, 1.7, 5)] 12import pandas.io.sql as sqlsql.read_sql('select * from test', con)abcd0AtlantaGeorgia1.2561TallahasseeFlorida2.6032SacramentoCalifornia1.705]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note ch05]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-ch05%2F</url>
      <content type="text"><![CDATA[pandas入门按照以下约定引用相关package12from pandas import Series, DataFrameimport pandas as pd123456789101112from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"pandas数据结构介绍SeriesSeries是一种类似于一维数组的对象，由一组数据以及一组与之相关的数据标签（类似于字典的键）组成，所以可以看成是一个有序的字典12obj = Series([4, 7, -5, 3])obj0 4 1 7 2 -5 3 3 dtype: int64 12obj.valuesobj.indexarray([ 4, 7, -5, 3], dtype=int64) RangeIndex(start=0, stop=4, step=1) 12obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])obj2d 4 b 7 a -5 c 3 dtype: int64 1obj2.indexIndex([&apos;d&apos;, &apos;b&apos;, &apos;a&apos;, &apos;c&apos;], dtype=&apos;object&apos;) 1obj2['a']-5 12obj2['d'] = 6obj2[['c', 'a', 'd']]c 3 a -5 d 6 dtype: int64 各种Numpy运算都是作用在数据上，同时索引与数据的链接会一直保持1obj2[obj2 &gt; 0]d 6 b 7 c 3 dtype: int64 1obj2 * 2d 12 b 14 a -10 c 6 dtype: int64 1np.exp(obj2)d 403.428793 b 1096.633158 a 0.006738 c 20.085537 dtype: float64 1'b' in obj2True 1'e' in obj2False 因此可以直接根据Numpy的Dict来创建Series123sdata = &#123;'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000&#125;obj3 = Series(sdata)obj3Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 如果传入了index参数的话，那么就会与传入的Dict做键匹配，没有匹配上的就设为NaN123states = ['California', 'Ohio', 'Oregon', 'Texas']obj4 = Series(sdata, index=states)obj4California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1pd.isnull(obj4)California True Ohio False Oregon False Texas False dtype: bool 1pd.notnull(obj4)California False Ohio True Oregon True Texas True dtype: bool 1obj4.isnull()California True Ohio False Oregon False Texas False dtype: bool Series有一个非常重要的数据对齐的功能1obj3Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 1obj4California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1obj3 + obj4California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 Series本身以及其索引都有一个叫做name的属性，这个属性十分重要，以后很多高级功能都会用到123obj4.name = 'population'obj4.index.name = 'state'obj4state California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 Name: population, dtype: float64 可以通过直接赋值的方式修改index属性12obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']objBob 4 Steve 7 Jeff -5 Ryan 3 dtype: int64 DataFrameDataFrame是一个表格型的数据结构，可以看成由Series组成的字典，只不过这些Series共用一套索引1234data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = DataFrame(data)数据会被排序1framepopstateyear01.5Ohio200011.7Ohio200123.6Ohio200232.4Nevada200142.9Nevada20021DataFrame(data, columns=['year', 'state', 'pop'])yearstatepop02000Ohio1.512001Ohio1.722002Ohio3.632001Nevada2.442002Nevada2.9123frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'], index=['one', 'two', 'three', 'four', 'five'])frame2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7NaNthree2002Ohio3.6NaNfour2001Nevada2.4NaNfive2002Nevada2.9NaN1frame2.columnsIndex([&apos;year&apos;, &apos;state&apos;, &apos;pop&apos;, &apos;debt&apos;], dtype=&apos;object&apos;) DataFrame每一个Key对应的Value都是一个Series1frame2['state']one Ohio two Ohio three Ohio four Nevada five Nevada Name: state, dtype: object 1frame2.yearone 2000 two 2001 three 2002 four 2001 five 2002 Name: year, dtype: int64 注意到name属性也已经被设置好了ix相当于一个行索引？1frame2.ix['three']year 2002 state Ohio pop 3.6 debt NaN Name: three, dtype: object 可以利用Numpy的广播功能12frame2['debt'] = 16.5frame2yearstatepopdebtone2000Ohio1.516.5two2001Ohio1.716.5three2002Ohio3.616.5four2001Nevada2.416.5five2002Nevada2.916.5也可以赋值一个列表，但是长度必须匹配12frame2['debt'] = np.arange(5.)frame2yearstatepopdebtone2000Ohio1.50.0two2001Ohio1.71.0three2002Ohio3.62.0four2001Nevada2.43.0five2002Nevada2.94.0如果是赋值一个Series，则会匹配上索引，没有匹配上的就是置为NaN123val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])frame2['debt'] = valframe2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7-1.2three2002Ohio3.6NaNfour2001Nevada2.4-1.5five2002Nevada2.9-1.7也可以进行逻辑操作12frame2['eastern'] = frame2.state == 'Ohio'frame2yearstatepopdebteasternone2000Ohio1.5NaNTruetwo2001Ohio1.7-1.2Truethree2002Ohio3.6NaNTruefour2001Nevada2.4-1.5Falsefive2002Nevada2.9-1.7Falsedel用于删除一列12del frame2['eastern']frame2.columnsIndex([&apos;year&apos;, &apos;state&apos;, &apos;pop&apos;, &apos;debt&apos;], dtype=&apos;object&apos;) 只要是通过索引方式进行的操作，都是直接在原数据上进行的操作，不是一个副本嵌套的字典也可直接生成DataFrame，只不过内层的键被当作index，外层的键被当作colums12pop = &#123;'Nevada': &#123;2001: 2.4, 2002: 2.9&#125;, 'Ohio': &#123;2000: 1.5, 2001: 1.7, 2002: 3.6&#125;&#125;12frame3 = DataFrame(pop)frame3NevadaOhio2000NaN1.520012.41.720022.93.6同样可以进行转置，这样的话index和column就会互换1frame3.T200020012002NevadaNaN2.42.9Ohio1.51.73.6显式地指定索引，不匹配的会置为NaN1DataFrame(pop, index=[2001, 2002, 2003])NevadaOhio20012.41.720022.93.62003NaNNaN1frame3NevadaOhio2000NaN1.520012.41.720022.93.6还可以这样构建123pdata = &#123;'Ohio': frame3['Ohio'][:-1], 'Nevada': frame3['Nevada'][:2]&#125;DataFrame(pdata)NevadaOhio2000NaN1.520012.41.71frame3NevadaOhio2000NaN1.520012.41.720022.93.6name属性也会在表格中显示出来12frame3.index.name = 'year'; frame3.columns.name = 'state'frame3stateNevadaOhioyear2000NaN1.520012.41.720022.93.6values方法只返回数据，不返回index以及key1frame3.valuesarray([[ nan, 1.5], [ 2.4, 1.7], [ 2.9, 3.6]]) 1frame2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7-1.2three2002Ohio3.6NaNfour2001Nevada2.4-1.5five2002Nevada2.9-1.71frame2.valuesarray([[2000, &apos;Ohio&apos;, 1.5, nan], [2001, &apos;Ohio&apos;, 1.7, -1.2], [2002, &apos;Ohio&apos;, 3.6, nan], [2001, &apos;Nevada&apos;, 2.4, -1.5], [2002, &apos;Nevada&apos;, 2.9, -1.7]], dtype=object) 索引对象Index是一个可以单独提取出来的对象123obj = Series(range(3), index=['a', 'b', 'c'])index = obj.indexindexIndex([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], dtype=&apos;object&apos;) 1index[1:]Index([&apos;b&apos;, &apos;c&apos;], dtype=&apos;object&apos;) 不可修改~！1index[1] = 'd'--------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-52-676fdeb26a68&gt; in &lt;module&gt;() ----&gt; 1 index[1] = &apos;d&apos; C:\Users\Ewan\Anaconda3\lib\site-packages\pandas\indexes\base.py in __setitem__(self, key, value) 1243 1244 def __setitem__(self, key, value): -&gt; 1245 raise TypeError(&quot;Index does not support mutable operations&quot;) 1246 1247 def __getitem__(self, key): TypeError: Index does not support mutable operations 直接创建Index对象123index = pd.Index(np.arange(3))obj2 = Series([1.5, -2.5, 0], index=index)obj2.index is indexTrue 1frame3stateNevadaOhioyear2000NaN1.520012.41.720022.93.61'Ohio' in frame3.columnsTrue 12003 in frame3.indexFalse 基本功能重新索引12obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])objd 4.5 b 7.2 a -5.3 c 3.6 dtype: float64 重排索引形成新对象12obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])obj2a -5.3 b 7.2 c 3.6 d 4.5 e NaN dtype: float64 1obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)a -5.3 b 7.2 c 3.6 d 4.5 e 0.0 dtype: float64 12obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])obj3.reindex(range(6), method='ffill') # 前向填充0 blue 1 blue 2 purple 3 purple 4 yellow 5 yellow dtype: object 123frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])frameOhioTexasCaliforniaa012c345d67812frame2 = frame.reindex(['a', 'b', 'c', 'd'])frame2OhioTexasCaliforniaa0.01.02.0bNaNNaNNaNc3.04.05.0d6.07.08.012states = ['Texas', 'Utah', 'California']frame.reindex(columns=states)TexasUtahCaliforniaa1NaN2c4NaN5d7NaN8插值只能按行12frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill', columns=states)TexasUtahCaliforniaa1NaN2b1NaN2c4NaN5d7NaN8用ix方法进行重新索引操作会使得代码很简洁1frame.ix[['a', 'b', 'c', 'd'], states]TexasUtahCaliforniaa1.0NaN2.0bNaNNaNNaNc4.0NaN5.0d7.0NaN8.0丢弃指定轴上的项1frame.ix[['a', 'b', 'c', 'd'], states]TexasUtahCaliforniaa1.0NaN2.0bNaNNaNNaNc4.0NaN5.0d7.0NaN8.0123obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])new_obj = obj.drop('c')new_obja 0.0 b 1.0 d 3.0 e 4.0 dtype: float64 1obj.drop(['d', 'c'])a 0.0 b 1.0 e 4.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567Utah891011New York121314151data.drop(['Colorado', 'Ohio'])onetwothreefourUtah891011New York121314151data.drop('two', axis=1)onethreefourOhio023Colorado467Utah81011New York1214151data.drop(['two', 'four'], axis=1)onethreeOhio02Colorado46Utah810New York1214索引，选取和过滤多种索引方式123obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])objobj['b']a 0.0 b 1.0 c 2.0 d 3.0 dtype: float64 1.0 1obj[1]1.0 1obj[2:4]c 2.0 d 3.0 dtype: float64 1obj[['b', 'a', 'd']]b 1.0 a 0.0 d 3.0 dtype: float64 1obj[[1, 3]]b 1.0 d 3.0 dtype: float64 1obj[obj &lt; 2] # 直接对data进行操作a 0.0 b 1.0 dtype: float64 这种切片方式…末端包含1obj['b':'c']b 1.0 c 2.0 dtype: float64 12obj['b':'c'] = 5obja 0.0 b 5.0 c 5.0 d 3.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567Utah891011New York121314151data['two']Ohio 1 Colorado 5 Utah 9 New York 13 Name: two, dtype: int32 1data[['three', 'one']]threeoneOhio20Colorado64Utah108New York14121data[:2] # axis=0onetwothreefourOhio0123Colorado45671data[data['three'] &gt; 5]onetwothreefourColorado4567Utah891011New York121314151data &lt; 5onetwothreefourOhioTrueTrueTrueTrueColoradoTrueFalseFalseFalseUtahFalseFalseFalseFalseNew YorkFalseFalseFalseFalse1data[data &lt; 5] = 01dataonetwothreefourOhio0000Colorado0567Utah891011New York12131415索引的另外一种选择1data.ix['Colorado', ['two', 'three']]two 5 three 6 Name: Colorado, dtype: int32 1data.ix[['Colorado', 'Utah'], [3, 0, 1]]fouronetwoColorado705Utah11891data.ix[2]one 8 two 9 three 10 four 11 Name: Utah, dtype: int32 1data.ix[:'Utah', 'two']Ohio 0 Colorado 5 Utah 9 Name: two, dtype: int32 1data.ix[data.three &gt; 5, :3]onetwothreeColorado056Utah8910New York121314总结一下就是说，DataFrame是一个二维的数组，只不过每一维的索引方式除了序号之外，还可以用name属性来进行索引，且一切行为与序号无异算术运算和数据对齐12s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])1s1a 7.3 c -2.5 d 3.4 e 1.5 dtype: float64 1s2a -2.1 c 3.6 e -1.5 f 4.0 g 3.1 dtype: float64 数据对齐操作就是一种特殊的并集操作1s1 + s2a 5.2 c 1.1 d NaN e 0.0 f NaN g NaN dtype: float64 12345df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])df1bcdOhio0.01.02.0Texas3.04.05.0Colorado6.07.08.01df2bdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.0并且数据对齐操作是在所有维度上同时进行的1df1 + df2bcdeColoradoNaNNaNNaNNaNOhio3.0NaN6.0NaNOregonNaNNaNNaNNaNTexas9.0NaN12.0NaNUtahNaNNaNNaNNaN在算术方法中填充词下面这种定义column的方式值得注意123df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))df1abcd00.01.02.03.014.05.06.07.028.09.010.011.01df2abcde00.01.02.03.04.015.06.07.08.09.0210.011.012.013.014.0315.016.017.018.019.01df1 + df2abcde00.02.04.06.0NaN19.011.013.015.0NaN218.020.022.024.0NaN3NaNNaNNaNNaNNaN要想填充值必须使用add方法1df1.add(df2, fill_value=0)abcde00.02.04.06.04.019.011.013.015.09.0218.020.022.024.014.0315.016.017.018.019.0reindex方法与add方法还是存在差异的1df1.reindex(columns=df2.columns, fill_value=0)abcde00.01.02.03.0014.05.06.07.0028.09.010.011.00DataFrame和Series之间的运算12arr = np.arange(12.).reshape((3, 4))arrarray([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) 1arr[0]array([ 0., 1., 2., 3.]) 广播操作1arr - arr[0]array([[ 0., 0., 0., 0.], [ 4., 4., 4., 4.], [ 8., 8., 8., 8.]]) 1234frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])series = frame.ix[0]framebdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.0Series的name等于DataFrame的切片属性1seriesb 0.0 d 1.0 e 2.0 Name: Utah, dtype: float64 默认情况下，DataFrame和Series之间的算术运算会将Series的index匹配到DataFrame的column, 然后沿着行向下广播1frame - seriesbdeUtah0.00.00.0Ohio3.03.03.0Texas6.06.06.0Oregon9.09.09.0如果Series的index与DataFrame的column不匹配，则进行数据对齐12series2 = Series(range(3), index=['b', 'e', 'f'])frame + series2bdefUtah0.0NaN3.0NaNOhio3.0NaN6.0NaNTexas6.0NaN9.0NaNOregon9.0NaN12.0NaN12series3 = frame['d']framebdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.01series3Utah 1.0 Ohio 4.0 Texas 7.0 Oregon 10.0 Name: d, dtype: float64 匹配行并且在列上进行广播， 就必须要指定axis12frame.sub(series3, axis=0)frame.sub(series3, axis=1)bdeUtah-1.00.01.0Ohio-1.00.01.0Texas-1.00.01.0Oregon-1.00.01.0OhioOregonTexasUtahbdeUtahNaNNaNNaNNaNNaNNaNNaNOhioNaNNaNNaNNaNNaNNaNNaNTexasNaNNaNNaNNaNNaNNaNNaNOregonNaNNaNNaNNaNNaNNaNNaN函数应用和映射12frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])1framebdeUtah-0.2047080.478943-0.519439Ohio-0.5557301.9657811.393406Texas0.0929080.2817460.769023Oregon1.2464351.007189-1.296221Numpy的元素级方法也可以应用到DataFrame上，直接把DataFrame当作二维的Numpy.array即可1np.abs(frame)bdeUtah0.2047080.4789430.519439Ohio0.5557301.9657811.393406Texas0.0929080.2817460.769023Oregon1.2464351.0071891.2962211f = lambda x: x.max() - x.min()apply方法将函数映射到由各行或者各列形成的一维数组上1frame.apply(f) # axis=0b 1.802165 d 1.684034 e 2.689627 dtype: float64 1frame.apply(f, axis=1)Utah 0.998382 Ohio 2.521511 Texas 0.676115 Oregon 2.542656 dtype: float64 1234def f(x): return Series([x.min(), x.max()], index=['min', 'max'])frame.apply(f)frame.apply(f, axis=1)bdemin-0.5557300.281746-1.296221max1.2464351.9657811.393406minmaxUtah-0.5194390.478943Ohio-0.5557301.965781Texas0.0929080.769023Oregon-1.2962211.246435元素级的函数映射applymap， 之所以叫这个名字是因为Series有一个元素级的映射函数map12format = lambda x: '%.2f' % xframe.applymap(format)bdeUtah-0.200.48-0.52Ohio-0.561.971.39Texas0.090.280.77Oregon1.251.01-1.301frame['e'].map(format)Utah -0.52 Ohio 1.39 Texas 0.77 Oregon -1.30 Name: e, dtype: object 排序和排名对索引或者column进行（字典）排序12obj = Series(range(4), index=['d', 'a', 'b', 'c'])obj.sort_index()a 1 b 2 c 3 d 0 dtype: int32 123frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=['d', 'a', 'b', 'c'])frame.sort_index()dabcone4567three01231frame.sort_index(axis=1)abcdthree1230one5674降序1frame.sort_index(axis=1, ascending=False)dcbathree0321one4765按照data进行排序12obj = Series([4, 7, -3, 2])obj.sort_values()2 -3 3 2 0 4 1 7 dtype: int64 在排序时，任何缺失值默认都会被放到Series的末尾12obj = Series([4, np.nan, 7, np.nan, -3, 2])obj.sort_values()4 -3.0 5 2.0 0 4.0 2 7.0 1 NaN 3 NaN dtype: float64 12frame = DataFrame(&#123;'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]&#125;)frameab00411720-3312对指定index或者column进行排序1frame.sort_values(by='b')ab20-3312004117或者根据multi-index亦或multi-column进行排序1frame.sort_values(by=['a', 'b'])ab20-3004312117默认情况下，rank方法通过“为各组分配一个平均排名”的方式破坏平级关系。也就是说，如果有多个相同的值，则这些值的rank就是这些相同值rand的算术平均。12obj = Series([7, -5, 7, 4, 2, 0, 4, 4])obj.rank()0 7.5 1 1.0 2 7.5 3 5.0 4 3.0 5 2.0 6 5.0 7 5.0 dtype: float64 如果想按照一般方式排名1obj.rank(method='first')0 7.0 1 1.0 2 8.0 3 4.0 4 3.0 5 2.0 6 5.0 7 6.0 dtype: float64 使用每个分组的最大排名1obj.rank(ascending=False, method='max')0 2.0 1 8.0 2 2.0 3 5.0 4 6.0 5 7.0 6 5.0 7 5.0 dtype: float64 123frame = DataFrame(&#123;'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]&#125;)frameabc004.3-2.0117.05.020-3.08.0312.0-2.5指定维度1frame.rank(axis=1)abc02.03.01.011.03.02.022.01.03.032.03.01.0带有重复值的轴索引12obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])obja 0 a 1 b 2 b 3 c 4 dtype: int32 1obj.index.is_uniqueFalse 返回一个Series1obj['a']a 0 a 1 dtype: int32 1obj['c']4 12df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])df012a0.2749920.2289131.352917a0.886429-2.001637-0.371843b1.669025-0.438570-0.539741b0.4769853.248944-1.0212281df.ix['b']012b1.669025-0.438570-0.539741b0.4769853.248944-1.021228汇总和计算描述统计12345df = DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]], index=['a', 'b', 'c', 'd'], columns=['one', 'two'])dfonetwoa1.40NaNb7.10-4.5cNaNNaNd0.75-1.31df.sum() # axis=0 skipna=Trueone 9.25 two -5.80 dtype: float64 1df.sum(axis=1) # skipna=Truea 1.40 b 2.60 c 0.00 d -0.55 dtype: float64 1df.mean(axis=1, skipna=False)a NaN b 1.300 c NaN d -0.275 dtype: float64 返回的是索引1df.idxmax()one b two d dtype: object 1df.cumsum()onetwoa1.40NaNb8.50-4.5cNaNNaNd9.25-5.8describe对于数值型和非数值型数据的行为不一样1df.describe()C:\Users\Ewan\Anaconda3\lib\site-packages\numpy\lib\function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning) onetwocount3.0000002.000000mean3.083333-2.900000std3.4936852.262742min0.750000-4.50000025%NaNNaN50%NaNNaN75%NaNNaNmax7.100000-1.300000123obj = Series(['a', 'a', 'b', 'c'] * 4)objobj.describe()0 a 1 a 2 b 3 c 4 a 5 a 6 b 7 c 8 a 9 a 10 b 11 c 12 a 13 a 14 b 15 c dtype: object count 16 unique 3 top a freq 8 dtype: object 相关系数和xi12345678910import pandas_datareader.data as weball_data = &#123;&#125;for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']: all_data[ticker] = web.get_data_yahoo(ticker)price = DataFrame(&#123;tic: data['Adj Close'] for tic, data in all_data.items()&#125;)volume = DataFrame(&#123;tic: data['Volume'] for tic, data in all_data.items()&#125;)1price[:10]AAPLGOOGIBMMSFTDate2010-01-0427.727039313.062468111.40500025.5554852010-01-0527.774976311.683844110.05923225.5637412010-01-0627.333178303.826685109.34428325.4068592010-01-0727.282650296.753749108.96578625.1426342010-01-0827.464034300.709808110.05923225.3160312010-01-1127.221758300.255255108.90690324.9940072010-01-1226.912110294.945572109.77324524.8288662010-01-1327.291720293.252243109.53773525.0600642010-01-1427.133657294.630868111.28724525.5637412010-01-1526.680198289.710772110.84145825.4811721volume[:10]AAPLGOOGIBMMSFTDate2010-01-0412343240039270006155300384091002010-01-0515047620060319006841400497496002010-01-0613804000079871005605300581824002010-01-07119282800128766005840600505597002010-01-0811190270094839004197200511974002010-01-11115557400144798005730400687547002010-01-1214861490097429008081500659121002010-01-13151473000130418006455400518635002010-01-1410822350085119007111800632281002010-01-1514851690010909600849440079913200price.pct_changeSignature: price.pct_change(periods=1, fill_method=’pad’, limit=None, freq=None, **kwargs)Docstring:Percent change over given number of periods.Parametersperiods : int, default 1Periods to shift for forming percent changefill_method : str, default ‘pad’How to handle NAs before computing percent changeslimit : int, default NoneThe number of consecutive NAs to fill before stoppingfreq : DateOffset, timedelta, or offset alias string, optionalIncrement to use from time series API (e.g. ‘M’ or BDay())Returnschg : NDFrameNotesBy default, the percentage change is calculated along the stataxis: 0, or Index, for DataFrame and 1, or minor forPanel. You can change this with the axis keyword argument.12returns = price.pct_change()returns.tail()AAPLGOOGIBMMSFTDate2017-02-210.0072210.004335-0.002269-0.0020122017-02-220.002999-0.0010820.004937-0.0020162017-02-23-0.0042300.0006860.0027600.0040402017-02-240.000952-0.003236-0.0016510.0000002017-02-270.0019760.000772-0.010753-0.0060351returns.MSFT.corr(returns.IBM)0.49525655865062668 1returns.MSFT.cov(returns.IBM)8.5880535146740545e-05 1returns.corr()AAPLGOOGIBMMSFTAAPL1.0000000.4095230.3813740.388875GOOG0.4095231.0000000.4027810.470781IBM0.3813740.4027811.0000000.495257MSFT0.3888750.4707810.4952571.0000001returns.cov()AAPLGOOGIBMMSFTAAPL0.0002690.0001050.0000750.000092GOOG0.0001050.0002440.0000750.000106IBM0.0000750.0000750.0001440.000086MSFT0.0000920.0001060.0000860.0002091returns.corrwith(returns.IBM)AAPL 0.381374 GOOG 0.402781 IBM 1.000000 MSFT 0.495257 dtype: float64 1returns.corrwith(volume)AAPL -0.074055 GOOG -0.009543 IBM -0.194107 MSFT -0.090724 dtype: float64 唯一值， 值计数以及成员资格1obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])12uniques = obj.unique()uniquesarray([&apos;c&apos;, &apos;a&apos;, &apos;d&apos;, &apos;b&apos;], dtype=object) 1obj.value_counts()c 3 a 3 b 2 d 1 dtype: int64 1pd.value_counts(obj.values, sort=False)a 3 d 1 b 2 c 3 dtype: int64 12mask = obj.isin(['b', 'c'])mask0 True 1 False 2 False 3 False 4 False 5 True 6 True 7 True 8 True dtype: bool 1obj[mask]0 c 5 b 6 b 7 c 8 c dtype: object 1234data = DataFrame(&#123;'Qu1': [1, 3, 4, 3, 4], 'Qu2': [2, 3, 1, 2, 3], 'Qu3': [1, 5, 2, 4, 4]&#125;)dataQu1Qu2Qu30121133524123324443412result = data.apply(pd.value_counts).fillna(0)resultQu1Qu2Qu311.01.01.020.02.01.032.02.00.042.00.02.050.00.01.0处理缺失数据12string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])string_data0 aardvark 1 artichoke 2 NaN 3 avocado dtype: object 1string_data.isnull()0 False 1 False 2 True 3 False dtype: bool 12string_data[0] = Nonestring_data.isnull()0 True 1 False 2 True 3 False dtype: bool 滤除缺失数据123from numpy import nan as NAdata = Series([1, NA, 3.5, NA, 7])data.dropna()0 1.0 2 3.5 4 7.0 dtype: float64 1data[data.notnull()]0 1.0 2 3.5 4 7.0 dtype: float64 1234data = DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])cleaned = data.dropna()data01201.06.53.011.0NaNNaN2NaNNaNNaN3NaN6.53.01cleaned01201.06.53.01data.dropna(how='all')01201.06.53.011.0NaNNaN3NaN6.53.012data[4] = NAdata012401.06.53.0NaN11.0NaNNaNNaN2NaNNaNNaNNaN3NaN6.53.0NaN1data.dropna(axis=1, how='all')01201.06.53.011.0NaNNaN2NaNNaNNaN3NaN6.53.0123df = DataFrame(np.random.randn(7, 3))df.ix[:4, 1] = NA; df.ix[:2, 2] = NAdf0120-0.204708NaNNaN1-0.555730NaNNaN20.092908NaNNaN31.246435NaN-1.29622140.274992NaN1.35291750.886429-2.001637-0.37184361.669025-0.438570-0.5397411df.dropna(thresh=2)01231.246435NaN-1.29622140.274992NaN1.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741填充缺失数据1df.fillna(0)0120-0.2047080.0000000.0000001-0.5557300.0000000.00000020.0929080.0000000.00000031.2464350.000000-1.29622140.2749920.0000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.5397411df.fillna(&#123;1: 0.5, 3: -1&#125;)0120-0.2047080.500000NaN1-0.5557300.500000NaN20.0929080.500000NaN31.2464350.500000-1.29622140.2749920.5000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741fillna默认返回新对象，但也可以对现有对象就地修改123# always returns a reference to the filled object_ = df.fillna(0, inplace=True)df0120-0.2047080.0000000.0000001-0.5557300.0000000.00000020.0929080.0000000.00000031.2464350.000000-1.29622140.2749920.0000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741123df = DataFrame(np.random.randn(6, 3))df.ix[2:, 1] = NA; df.ix[4:, 2] = NAdf01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.523772NaN1.3438103-0.713544NaN-2.3702324-1.860761NaNNaN5-1.265934NaNNaN1df.fillna(method='ffill')01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.5237720.1241211.3438103-0.7135440.124121-2.3702324-1.8607610.124121-2.3702325-1.2659340.124121-2.3702321df.fillna(method='ffill', limit=2)01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.5237720.1241211.3438103-0.7135440.124121-2.3702324-1.860761NaN-2.3702325-1.265934NaN-2.37023212data = Series([1., NA, 3.5, NA, 7])data.fillna(data.mean())0 1.000000 1 3.833333 2 3.500000 3 3.833333 4 7.000000 dtype: float64 层次索引1234data = Series(np.random.randn(10), index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'], [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])dataa 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 1data.indexMultiIndex(levels=[[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;], [1, 2, 3]], labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2]]) 1data['b']1 -1.541996 2 -0.970736 3 -1.307030 dtype: float64 1data['b':'c']b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 dtype: float64 1data.ix[['b', 'd']]b 1 -1.541996 2 -0.970736 3 -1.307030 d 2 -0.753887 3 0.331286 dtype: float64 1data[:, 2]a -2.359419 b -0.970736 c 0.377984 d -0.753887 dtype: float64 1data.unstack()123a0.332883-2.359419-0.199543b-1.541996-0.970736-1.307030c0.2863500.377984NaNdNaN-0.7538870.3312861data.unstack().stack()a 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 12345frame = DataFrame(np.arange(12).reshape((4, 3)), index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]], columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])frameOhioColoradoGreenRedGreena10122345b1678291011123frame.index.names = ['key1', 'key2']frame.columns.names = ['state', 'color']framestateOhioColoradocolorGreenRedGreenkey1key2a10122345b16782910111frame['Ohio']colorGreenRedkey1key2a101234b1672910创建MultiIndex对象复用12MultiIndex.from_arrays([[&apos;Ohio&apos;, &apos;Ohio&apos;, &apos;Colorado&apos;], [&apos;Green&apos;, &apos;Red&apos;, &apos;Green&apos;]], names=[&apos;state&apos;, &apos;color&apos;])重排分级顺序1frame.swaplevel('key1', 'key2')stateOhioColoradocolorGreenRedGreenkey2key11a0122a3451b6782b910111frame.sortlevel(1)stateOhioColoradocolorGreenRedGreenkey1key2a1012b1678a2345b2910111frame.swaplevel(0, 1).sortlevel(0)stateOhioColoradocolorGreenRedGreenkey2key11a012b6782a345b91011根据级别汇总统计1frame.sum(level='key2')stateOhioColoradocolorGreenRedGreenkey21681021214161frame.sum(level='color', axis=1)colorGreenRedkey1key2a121284b114722010使用DataFrame的列1234frame = DataFrame(&#123;'a': range(7), 'b': range(7, 0, -1), 'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'], 'd': [0, 1, 2, 0, 1, 2, 3]&#125;)frameabcd007one0116one1225one2334two0443two1552two2661two312frame2 = frame.set_index(['c', 'd'])frame2abcdone007116225two0341432523611frame.set_index(['c', 'd'], drop=False)abcdcdone007one0116one1225one2two034two0143two1252two2361two31frame2.reset_index()cdab0one0071one1162one2253two0344two1435two2526two361拓展话题整数索引12ser = Series(np.arange(3.))ser.iloc[-1]2.0 1ser0 0.0 1 1.0 2 2.0 dtype: float64 12ser2 = Series(np.arange(3.), index=['a', 'b', 'c'])ser2[-1]2.0 1ser.ix[:1]0 0.0 1 1.0 dtype: float64 12ser3 = Series(range(3), index=[-5, 1, 3])ser3.iloc[2]2 123frame = DataFrame(np.arange(6).reshape((3, 2)), index=[2, 0, 1])frameframe.iloc[0]012010231450 0 1 1 Name: 2, dtype: int32 面板数据1234import pandas_datareader.data as webpdata = pd.Panel(dict((stk, web.get_data_yahoo(stk)) for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL']))1pdata&lt;class &apos;pandas.core.panel.Panel&apos;&gt; Dimensions: 4 (items) x 1820 (major_axis) x 6 (minor_axis) Items axis: AAPL to MSFT Major_axis axis: 2010-01-04 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: Open to Adj Close 12pdata = pdata.swapaxes('items', 'minor')pdata['Adj Close'].iloc[:10]AAPLDELLGOOGMSFTDate2010-01-0427.72703914.06528313.06246825.5554852010-01-0527.77497614.38450311.68384425.5637412010-01-0627.33317814.10397303.82668525.4068592010-01-0727.28265014.23940296.75374925.1426342010-01-0827.46403414.36516300.70980825.3160312010-01-1127.22175814.37483300.25525524.9940072010-01-1226.91211014.56830294.94557224.8288662010-01-1327.29172014.57797293.25224325.0600642010-01-1427.13365714.22005294.63086825.5637412010-01-1526.68019813.92985289.71077225.4811721pdata.ix[:, '6/1/2012', :]OpenHighLowCloseVolumeAdj CloseAAPL569.159996572.650009560.520012560.989983130246900.072.681610DELL12.15000012.30000012.04500012.07000019397600.011.675920GOOG571.790972572.650996568.350996570.9810006138700.0285.205295MSFT28.76000028.95999928.44000128.45000156634300.024.9422391pdata.ix['Adj Close', '5/22/2012':, :].iloc[:10]AAPLDELLGOOGMSFTDate2012-05-2272.16078614.58765300.10041226.0907212012-05-2373.92149412.08221304.42610625.5208642012-05-2473.24260712.04351301.52897825.4857952012-05-2572.85003812.05319295.47005025.4770282012-05-28NaN12.05319NaNNaN2012-05-2974.14304112.24666296.87364525.9153802012-05-3075.03700512.14992293.82167425.7225052012-05-3174.85044211.92743290.14035425.5910002012-06-0172.68161011.67592285.20529524.9422392012-06-0473.10915611.60821289.00648025.02990812stacked = pdata.ix[:, '5/30/2012':, :].to_frame()stackedOpenHighLowCloseVolumeAdj CloseDateminor2012-05-30AAPL569.199997579.989990566.559990579.169998132357400.075.037005DELL12.59000012.70000012.46000012.56000019787800.012.149920GOOG588.161028591.901014583.530999588.2309923827600.0293.821674MSFT29.35000029.48000029.12000129.34000041585500.025.7225052012-05-31AAPL580.740021581.499985571.460022577.730019122918600.074.850442DELL12.53000012.54000012.33000012.33000019955600.011.927430GOOG588.720982590.001032579.001013580.8609905958800.0290.140354MSFT29.29999929.42000028.94000129.19000139134000.025.5910002012-06-01AAPL569.159996572.650009560.520012560.989983130246900.072.681610DELL12.15000012.30000012.04500012.07000019397600.011.675920GOOG571.790972572.650996568.350996570.9810006138700.0285.205295MSFT28.76000028.95999928.44000128.45000156634300.024.9422392012-06-04AAPL561.500008567.499985548.499977564.289978139248900.073.109156DELL12.11000012.11250011.80000012.00000017015700.011.608210GOOG570.220958580.491016570.011006578.5909734883500.0289.006480MSFT28.62000128.78000128.32000028.54999947926300.025.0299082012-06-05AAPL561.269989566.470001558.330002562.83002597053600.072.920005DELL11.95000012.24000011.95000012.16000015620900.011.762980GOOG575.451008578.131003566.470986570.4109994697200.0284.920579MSFT28.51000028.75000028.38999928.51000045715400.024.9948412012-06-06AAPL567.770004573.849983565.499992571.460022100363900.074.038104DELL12.21000012.28000012.09000012.21500020779900.011.816190GOOG576.480979581.970971573.611004580.5709664207200.0289.995487MSFT28.87999929.37000128.80999929.35000046860500.025.7312732012-06-07AAPL577.290009577.320023570.500000571.72000194941700.074.071787DELL12.32000012.41000012.12000012.13000020074000.011.733960GOOG587.601014587.891038577.251006578.2309863530100.0288.826666MSFT29.63999929.70000129.17000029.23000037792800.025.6260672012-06-08AAPL571.599998580.580017568.999992580.31998486879100.075.185997DELL12.13000012.22500012.02000012.12000018155600.011.724290……………………2017-02-13AAPL133.080002133.820007132.750000133.28999323035400.0133.289993GOOG816.000000820.958984815.489990819.2399901198100.0819.239990MSFT64.23999864.86000164.12999764.72000122920100.064.3300002017-02-14AAPL133.470001135.089996133.250000135.02000432815500.0135.020004GOOG819.000000823.000000816.000000820.4500121053600.0820.450012MSFT64.41000464.72000164.01999764.57000023065900.064.5700002017-02-15AAPL135.520004136.270004134.619995135.50999535501600.0135.509995GOOG819.359985823.000000818.469971818.9799801304000.0818.979980MSFT64.50000064.57000064.16000464.52999916917000.064.5299992017-02-16AAPL135.669998135.899994134.839996135.35000622118000.0135.350006GOOG819.929993824.400024818.979980824.1599731281700.0824.159973MSFT64.73999865.23999864.44000264.51999720524700.064.5199972017-02-17AAPL135.100006135.830002135.100006135.72000122084500.0135.720001GOOG823.020020828.070007821.655029828.0700071597800.0828.070007MSFT64.47000164.69000264.30000364.62000321234600.064.6200032017-02-21AAPL136.229996136.750000135.979996136.69999724265100.0136.699997GOOG828.659973833.450012828.349976831.6599731247700.0831.659973MSFT64.61000164.94999764.44999764.48999819384900.064.4899982017-02-22AAPL136.429993137.119995136.110001137.11000120745300.0137.110001GOOG828.659973833.250000828.640015830.760010982900.0830.760010MSFT64.33000264.38999964.05000364.36000119259700.064.3600012017-02-23AAPL137.380005137.479996136.300003136.52999920704100.0136.529999GOOG830.119995832.460022822.880005831.3300171470100.0831.330017MSFT64.41999864.73000364.19000264.62000320235200.064.6200032017-02-24AAPL135.910004136.660004135.279999136.66000421690900.0136.660004GOOG827.729980829.000000824.200012828.6400151386600.0828.640015MSFT64.52999964.80000364.13999964.62000321705200.064.6200032017-02-27AAPL137.139999137.440002136.279999136.92999320196400.0136.929993GOOG824.549988830.500000824.000000829.2800291099500.0829.280029MSFT64.54000164.54000164.05000364.23000315850400.064.2300033952 rows × 6 columns1stacked.to_panel()&lt;class &apos;pandas.core.panel.Panel&apos;&gt; Dimensions: 6 (items) x 1207 (major_axis) x 4 (minor_axis) Items axis: Open to Adj Close Major_axis axis: 2012-05-30 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: AAPL to MSFT]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch04]]></title>
      <url>%2F2017%2F02%2F26%2FPython-data-analysis-Learning-note-Ch04%2F</url>
      <content type="text"><![CDATA[Numpy基础：数组和矢量计算1%matplotlib inline1234from __future__ import divisionfrom numpy.random import randnimport numpy as npnp.set_printoptions(precision=4, suppress=True)12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"NumPy ndarray: 一种多维数组对象1data = randn(2, 3)123datadata * 10data + dataarray([[ 0.1584, 0.299 , -0.2555], [ 0.3277, -0.6934, 1.3191]]) array([[ 1.5842, 2.9896, -2.5545], [ 3.2767, -6.9342, 13.1913]]) array([[ 0.3168, 0.5979, -0.5109], [ 0.6553, -1.3868, 2.6383]]) 12data.shapedata.dtype(2, 3) dtype(&apos;float64&apos;) 创建ndarray123data1 = [6, 7.5, 8, 0, 1]arr1 = np.array(data1)arr1array([ 6. , 7.5, 8. , 0. , 1. ]) 12345data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]arr2 = np.array(data2)arr2arr2.ndimarr2.shapearray([[1, 2, 3, 4], [5, 6, 7, 8]]) 2 (2, 4) 除非显示说明，np.array会尝试为新建的数组选择一个合适的类型12arr1.dtypearr2.dtypedtype(&apos;float64&apos;) dtype(&apos;int32&apos;) 123np.zeros(10)np.zeros((3, 6))np.empty((2, 3, 2))array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) array([[ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.]]) array([[[ 0., 0.], [ 0., 0.], [ 0., 0.]], [[ 0., 0.], [ 0., 0.], [ 0., 0.]]]) 1np.arange(15)array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) ones_like, zeros_like, empty_like这三个方法接受一个数组为对象，创建和这个数组形状和dtype一样的全1， 全0和分配的初始空间ndarray的数据类型1234arr1 = np.array([1, 2, 3], dtype=np.float64)arr2 = np.array([1, 2, 3], dtype=np.int32)arr1.dtypearr2.dtypedtype(&apos;float64&apos;) dtype(&apos;int32&apos;) 当需要控制数据在内存和磁盘中的存储方式时（尤其是对大数据集），那就得了解如何控制存储类型1234arr = np.array([1, 2, 3, 4, 5])arr.dtypefloat_arr = arr.astype(np.float64) float_arr.dtypedtype(&apos;int32&apos;) dtype(&apos;float64&apos;) 123arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])arrarr.astype(np.int32)array([ 3.7, -1.2, -2.6, 0.5, 12.9, 10.1]) array([ 3, -1, -2, 0, 12, 10]) 12numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)numeric_strings.astype(float)array([ 1.25, -9.6 , 42. ]) 123int_array = np.arange(10)calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)int_array.astype(calibers.dtype)array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) 12empty_uint32 = np.empty(8, dtype='u4')empty_uint32array([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint32) 调用astype会创建原数组的一份拷贝数组和标量之间的运算1234arr = np.array([[1., 2., 3.], [4., 5., 6.]])arrarr * arrarr - arrarray([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 1., 4., 9.], [ 16., 25., 36.]]) array([[ 0., 0., 0.], [ 0., 0., 0.]]) 121 / arrarr ** 0.5array([[ 1. , 0.5 , 0.3333], [ 0.25 , 0.2 , 0.1667]]) array([[ 1. , 1.4142, 1.7321], [ 2. , 2.2361, 2.4495]]) 基本的索引和切片123456arr = np.arange(10)arrarr[5]arr[5:8]arr[5:8] = 12arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 5 array([5, 6, 7]) array([ 0, 1, 2, 3, 4, 12, 12, 12, 8, 9]) 切片直接在原数组上操作如果想要得到一个复制的版本，需要显示地调用copy()方法12345arr_slice = arr[5:8]arr_slice[1] = 12345arrarr_slice[:] = 64arrarray([ 0, 1, 2, 3, 4, 12, 12345, 12, 8, 9]) array([ 0, 1, 2, 3, 4, 64, 64, 64, 8, 9]) 12arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])arr2d[2]array([7, 8, 9]) 注意下面这种索引方式12arr2d[0][2]arr2d[0, 2]3 3 12arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])arr3darray([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[0]array([[1, 2, 3], [4, 5, 6]]) 12345old_values = arr3d[0].copy()arr3d[0] = 42arr3darr3d[0] = old_valuesarr3darray([[[42, 42, 42], [42, 42, 42]], [[ 7, 8, 9], [10, 11, 12]]]) array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[1, 0]array([7, 8, 9]) 切片索引1arr[1:6]array([ 1, 2, 3, 4, 64]) 12arr2darr2d[:2]array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) array([[1, 2, 3], [4, 5, 6]]) 1arr2d[:2, 1:]array([[2, 3], [5, 6]]) 12arr2d[1, :2]arr2d[2, :1]array([4, 5]) array([7]) 1arr2d[:, :1]array([[1], [4], [7]]) 1arr2d[:2, 1:] = 0布尔型索引1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])data = randn(7, 4)namesdataarray([&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;, &apos;Bob&apos;, &apos;Will&apos;, &apos;Joe&apos;, &apos;Joe&apos;], dtype=&apos;&lt;U4&apos;) array([[-2.9033, 1.4721, 0.9512, 1.7727], [ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 1names == 'Bob'array([ True, False, False, True, False, False, False], dtype=bool) 1data[names == 'Bob']array([[-2.9033, 1.4721, 0.9512, 1.7727], [-1.4289, 0.1665, -1.2874, -1.0817]]) 12data[names == 'Bob', 2:]data[names == 'Bob', 3]array([[ 0.9512, 1.7727], [-1.2874, -1.0817]]) array([ 1.7727, -1.0817]) 12names != 'Bob'data[~(names == 'Bob')]array([False, True, True, False, True, True, True], dtype=bool) array([[ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 123mask = (names == 'Bob') | (names == 'Will')maskdata[mask]array([ True, False, True, True, True, False, False], dtype=bool) array([[-2.9033, 1.4721, 0.9512, 1.7727], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673]]) Python关键字and和or在布尔型数组中无效12data[data &lt; 0] = 0dataarray([[ 0. , 1.4721, 0.9512, 1.7727], [ 2.2303, 0. , 1.0664, 0.534 ], [ 0. , 0.2226, 0. , 0. ], [ 0. , 0.1665, 0. , 0. ], [ 1.3581, 0. , 0. , 0.1673], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) 12data[names != 'Joe'] = 7dataarray([[ 7. , 7. , 7. , 7. ], [ 2.2303, 0. , 1.0664, 0.534 ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) 花式索引花式索引创建新的数组1234arr = np.empty((8, 4))for i in range(8): arr[i] = iarrarray([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], [ 3., 3., 3., 3.], [ 4., 4., 4., 4.], [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]]) 1arr[[4, 3, 0, 6]]array([[ 4., 4., 4., 4.], [ 3., 3., 3., 3.], [ 0., 0., 0., 0.], [ 6., 6., 6., 6.]]) 1arr[[-3, -5, -7]]array([[ 5., 5., 5., 5.], [ 3., 3., 3., 3.], [ 1., 1., 1., 1.]]) 1234# more on reshape in Chapter 12arr = np.arange(32).reshape((8, 4))arrarr[[1, 5, 7, 2], [0, 3, 1, 2]]array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) array([ 4, 23, 29, 10]) 根据以上可知，传入两个索引数组相当于进行了同位置组合注意以下这种方式1arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) np.ix_方法将两个一维数组转换为一个矩形区域的索引选择器1arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])]array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) 数组转置和轴对换123arr = np.arange(15).reshape((3, 5))arrarr.Tarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12arr = np.random.randn(6, 3)np.dot(arr.T, arr)array([[ 3.6804, 0.0133, 1.0388], [ 0.0133, 1.6074, 0.1836], [ 1.0388, 0.1836, 3.5281]]) 123arr = np.arange(16).reshape((2, 2, 4))arrarr.transpose((1, 0, 2))array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) Refered from here.In short: transposing an array means that NumPy just needs to permute the stride and shape information for each axis:&gt;&gt;&gt; arr.strides (64, 32, 8) &gt;&gt;&gt; arr.transpose(1, 0, 2).strides (32, 64, 8) Notice that the strides for the first and second axes were swapped here. This means that no data needs to be copied; NumPy can simply change how it looks at the memory to construct the array.What are strides?The values in a 3D array arr are stored in a contiguous block of memory like this:[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] In the case of arr, each integer takes up 8 bytes of memory (i.e. we’re using the int64 dtype).A stride tells NumPy how many bytes to skip in order to move to the next value along an axis. For example, to get the next value in a row in arr (axis 2), we just need to move 8 bytes (1 number).The strides for arr.transpose(1, 0, 2) are (32, 64, 8). To move along the first axis, instead of 64 bytes (8 numbers) NumPy will now only skip 32 bytes (4 numbers) each time:[[[0 ...] [... ...]] [[4 ...] [... ...]]] Similarly, NumPy will now skip 64 bytes (8 numbers) in order to move along axis 1:[[[0 ...] [8 ...]] [[4 ...] [12 ...]]] The actual code that does the transposing is written in C and can be found here.也可以使用swapaxes方法123arrarr.swapaxes(1, 2)arrarray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 4], [ 1, 5], [ 2, 6], [ 3, 7]], [[ 8, 12], [ 9, 13], [10, 14], [11, 15]]]) array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) 通用函数：快速的元素级数组函数123arr = np.arange(10)np.sqrt(arr)np.exp(arr)array([ 0. , 1. , 1.4142, 1.7321, 2. , 2.2361, 2.4495, 2.6458, 2.8284, 3. ]) array([ 1. , 2.7183, 7.3891, 20.0855, 54.5982, 148.4132, 403.4288, 1096.6332, 2980.958 , 8103.0839]) 12345x = randn(8)y = randn(8)xynp.maximum(x, y) # 对应元素进行比较array([ 0.811 , -0.0214, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.0382]) array([-1.223 , 0.3271, -1.7197, -2.2636, -0.1154, -1.4122, -0.0989, 0.4477]) array([ 0.811 , 0.3271, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.4477]) modf函数挺有意思123arr = randn(7) * 5arrnp.modf(arr)array([ 10.3171, -4.733 , -6.3358, 3.2457, -7.3823, 2.7036, -2.6173]) (array([ 0.3171, -0.733 , -0.3358, 0.2457, -0.3823, 0.7036, -0.6173]), array([ 10., -4., -6., 3., -7., 2., -2.])) 利用数组进行数据处理meshgrid产生两个二维数组，对应points中所有的二元组1234points = np.arange(-5, 5, 0.01) # 1000 equally spaced pointsxs, ys = np.meshgrid(points, points)xsysarray([[-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], ..., [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99]]) array([[-5. , -5. , -5. , ..., -5. , -5. , -5. ], [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99], [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98], ..., [ 4.97, 4.97, 4.97, ..., 4.97, 4.97, 4.97], [ 4.98, 4.98, 4.98, ..., 4.98, 4.98, 4.98], [ 4.99, 4.99, 4.99, ..., 4.99, 4.99, 4.99]]) 1from matplotlib.pyplot import imshow, title12345import matplotlib.pyplot as pltz = np.sqrt(xs ** 2 + ys ** 2)zplt.imshow(z, cmap=plt.cm.gray); plt.colorbar()plt.title("Image plot of $\sqrt&#123;x^2 + y^2&#125;$ for a grid of values")array([[ 7.0711, 7.064 , 7.0569, ..., 7.0499, 7.0569, 7.064 ], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], ..., [ 7.0499, 7.0428, 7.0357, ..., 7.0286, 7.0357, 7.0428], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569]]) &lt;matplotlib.image.AxesImage at 0x23400a22b38&gt; &lt;matplotlib.colorbar.Colorbar at 0x23400a7c7b8&gt; &lt;matplotlib.text.Text at 0x23400a03da0&gt; 1plt.draw()&lt;matplotlib.figure.Figure at 0x23401396eb8&gt; 将条件逻辑表述为数组运算123xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])cond = np.array([True, False, True, True, False])注意下面列表生成式的写法123result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]result[1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5] 上述方法具有一些缺点：大数组处理速度慢（纯Python实现）无法处理多维数组所以可以使用下面这种方法：12result = np.where(cond, xarr, yarr)resultarray([ 1.1, 2.2, 1.3, 1.4, 2.5]) 1234arr = randn(4, 4)arrnp.where(arr &gt; 0, 2, -2)np.where(arr &gt; 0, 2, arr) # set only positive values to 2array([[-0.7355, -0.3188, -0.2358, 0.3137], [-0.6196, -0.5803, -0.5504, -1.1508], [ 0.1719, -1.1599, -0.7115, 1.7869], [-0.2306, 0.2068, 1.5366, 1.6154]]) array([[-2, -2, -2, 2], [-2, -2, -2, -2], [ 2, -2, -2, 2], [-2, 2, 2, 2]]) array([[-0.7355, -0.3188, -0.2358, 2. ], [-0.6196, -0.5803, -0.5504, -1.1508], [ 2. , -1.1599, -0.7115, 2. ], [-0.2306, 2. , 2. , 2. ]]) 显然where还可以应用于更复杂的操作。考虑下面这种逻辑：12345678910result = []for i in range(n): if cond1[i] and cond2[i]: result.append(0) elif cond1[i]: result.append(1) elif cond2[i]: result.append(2) else: result.append(3)用where可以这样实现：123np.where(cond1 &amp; cond2, 0, np.where(cond1, 1, np.where(cond2, 2, 3)))更加magic一点：1result = 1 * cond1 + 2 * cond2 + 3 * -(cond1 | cond2)数学和统计方法123456arr = np.random.randn(5, 4) # 正态分布arr# 下面两种方式都可以使用arr.mean()np.mean(arr)arr.sum()array([[ 1.4513, -0.8225, 0.7011, -0.617 ], [ 1.5872, 1.2937, 1.0151, 0.7123], [-0.2012, -0.0168, -0.3847, 0.5274], [-0.6312, -0.2762, 0.4869, 0.0462], [-0.5268, -1.1071, 1.8642, 0.2282]]) 0.26650934393195791 0.26650934393195791 5.3301868786391582 12arr.mean(axis=1)arr.sum(0) # axis=0array([ 0.1782, 1.1521, -0.0188, -0.0936, 0.1146]) array([ 1.6793, -0.9289, 3.6826, 0.8971]) 1234arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])arrarr.cumsum(0) # axis=0arr.cumprod(1) # axis=1array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) array([[ 0, 1, 2], [ 3, 5, 7], [ 9, 12, 15]], dtype=int32) array([[ 0, 0, 0], [ 3, 12, 60], [ 6, 42, 336]], dtype=int32) 用于布尔数组的方法123arr = randn(100)arr(arr &gt; 0).sum() # Number of positive valuesarray([ 0.7828, 0.1372, -0.6264, 1.8927, -0.2104, 0.2822, -0.3672, -0.3601, 0.5918, 0.9285, 0.1808, -0.4021, 0.4086, -0.2949, 0.5633, -0.7462, -0.1635, 0.1482, -0.3226, -1.2127, -0.9821, 0.0536, -0.1772, -0.4714, -0.9002, -0.0037, 0.7352, 0.5675, -1.1612, 0.5288, 0.3319, 0.7315, 0.6841, -0.6881, 1.5654, -0.4605, -0.5423, 0.0184, -0.8153, -0.1313, 0.4594, 0.0228, 0.255 , -2.2361, 0.8703, -1.5153, -0.9458, 0.2769, 0.9986, 0.7699, -0.7948, -1.2508, 1.7059, 0.1805, -1.0265, -0.0181, -0.9415, 0.1265, -0.2576, 0.6791, 0.3969, 0.8027, -0.6792, -0.7487, -1.9949, -0.9595, 0.5706, -0.5727, -1.0204, 0.1521, -0.9755, -0.4094, 0.67 , 0.212 , 0.4081, -0.1435, 0.3964, -0.1865, -0.6018, -2.6185, -0.5073, -0.6328, -0.2631, 0.6637, -0.5586, 1.3346, -0.5317, 0.8572, 1.1159, 0.9563, -0.0434, -1.0534, 0.5869, 0.0502, -0.0479, -0.8673, 0.1531, 1.0646, -0.2624, -0.3726]) 47 1234bools = np.array([False, False, True, False])boolsbools.any()bools.all()array([False, False, True, False], dtype=bool) True False 排序1234arr = randn(8)arrarr.sort()arrarray([ 1.0584, 1.9062, -0.2923, 0.7169, 0.5186, -0.6089, -2.0444, -0.5661]) array([-2.0444, -0.6089, -0.5661, -0.2923, 0.5186, 0.7169, 1.0584, 1.9062]) 1234arr = randn(5, 3)arrarr.sort(1) # axis=1arrarray([[ 0.0118, -2.8916, -0.4477], [-1.9768, 1.859 , -1.128 ], [-2.6262, 0.5791, 0.7594], [-0.5254, -0.9059, 0.0203], [-1.4029, -1.8566, 0.1892]]) array([[-2.8916, -0.4477, 0.0118], [-1.9768, -1.128 , 1.859 ], [-2.6262, 0.5791, 0.7594], [-0.9059, -0.5254, 0.0203], [-1.8566, -1.4029, 0.1892]]) 12345large_arr = randn(1000)large_arrlarge_arr.sort()large_arrlarge_arr[int(0.05 * len(large_arr))] # 5% quantilearray([ 1.2296, 0.3794, -0.1526, 2.1223, -0.0675, 0.6867, -0.5742, -1.4291, 0.6856, 0.1364, -0.3966, -0.7793, 0.4965, 0.2447, -0.7487, 0.7695, 0.5358, -0.4813, 0.9949, -0.6489, -0.3656, 1.9551, 0.8327, 1.497 , -0.4431, -0.8357, -0.821 , -0.7348, 1.9294, -0.3144, 0.1396, -0.9111, 0.0943, 0.8043, 1.067 , 0.9362, -2.2574, 0.7475, -1.0152, -1.1234, -0.3774, 1.076 , 0.8743, 1.1864, 0.0801, 0.3995, 0.2536, -0.9371, -1.669 , -2.2444, 1.2544, 1.0539, -0.7579, 0.2963, 0.7496, -1.3655, 0.1552, -0.6259, 0.2621, -1.5415, -0.1036, -0.5794, 1.2098, 1.3388, 0.3159, 1.0998, 0.5109, -0.3927, 1.4797, -1.4891, 0.3624, 0.966 , 0.0756, -0.4703, 0.1859, 1.6091, 0.662 , -0.4808, 0.8744, 0.4738, 1.1351, 0.0251, -1.017 , -0.849 , -0.1602, -1.5392, 0.0601, 1.7323, 1.1837, 0.4657, 0.8858, -0.211 , 0.1865, 0.673 , 0.3086, -1.2527, -0.7802, 0.407 , -1.118 , -0.2058, 0.7921, 0.5284, -2.3038, -0.4038, -1.1087, -0.827 , -2.6518, 0.3711, -0.0244, 1.1103, 0.2748, -0.7962, 1.9456, 0.5347, 0.1862, -0.3734, -0.3036, 0.6831, -0.9419, 1.4848, -0.1247, -0.4138, -0.601 , 0.6138, 1.1334, 0.4386, 0.0466, -0.0588, 0.6883, -1.2912, -0.2381, 0.3934, 0.2132, -0.4143, 1.0844, -0.5258, -0.9944, 1.0977, 0.3528, 1.9928, 1.421 , 0.8634, 0.1973, -1.1799, -2.9433, 2.697 , 0.4778, 0.6464, 0.049 , -0.2339, 1.6945, -0.6568, -0.5972, -0.8324, -0.6443, 0.0882, -0.3686, 0.0419, 0.5119, -0.641 , 1.1545, 1.0735, -0.5329, -0.1126, 0.0375, -1.0699, -1.3153, -1.6097, 2.5671, -0.9516, -0.388 , -0.0129, -0.0171, -1.0763, -0.7125, 0.767 , 0.2254, -0.7638, -0.2065, 1.2797, 0.0784, -0.7762, 1.7106, -0.0136, -0.4435, 1.2946, -2.5489, 0.4241, 0.5675, -0.7596, 0.6128, 1.1161, -1.2456, -0.131 , -0.2684, 1.6461, -0.2497, -0.4294, 1.122 , 0.5969, 0.3335, -0.0453, 1.1567, 0.0216, -0.7277, -2.5465, -2.4542, -1.5895, 0.4607, -0.8303, 0.0263, 0.0301, -1.2365, -0.146 , -0.8632, 0.6449, 0.1958, -0.6914, -0.3223, 0.4037, 0.9918, -0.3542, 0.8442, 0.7751, -1.6248, 2.6081, 0.3524, 1.5298, 0.4421, 1.5228, -1.5263, -1.3994, 0.0285, -0.5389, 1.4047, -2.1117, -1.0397, 0.6495, 0.9073, 1.8738, 0.2913, -1.069 , -0.7835, -0.6437, 0.6739, 0.3272, -0.8483, -0.2971, 0.2882, 0.1778, -0.6705, -1.4129, -0.1935, 0.6615, -0.4423, -1.2472, -0.9816, 0.927 , -2.2774, 0.5736, 1.3996, 1.1653, -0.3253, -0.2074, -0.2447, 0.4925, 1.8415, -1.1551, -0.5131, -0.6407, 0.5033, -0.817 , 0.0479, -0.9106, 1.4391, -1.5824, -0.4652, 1.253 , -0.6051, 0.6699, 0.3803, 1.0767, 1.5449, 0.106 , -0.7215, -0.354 , 0.1016, -1.3191, -0.6596, -0.9632, -0.3655, 0.8411, -0.2314, 1.9493, -0.6966, -1.2598, 0.4023, 0.1704, -0.452 , 1.5924, 0.381 , -0.4731, -1.2467, -0.4264, -0.2298, -0.1792, -0.5009, -1.0032, 1.0126, 0.5436, 1.1366, -1.0318, 1.3289, 0.3218, -0.2828, 0.5597, -0.0213, -0.078 , 0.7667, -0.3984, -1.0263, -0.5557, -2.0724, -0.9343, -0.6877, 1.0567, -0.605 , 1.7923, 0.6351, -1.769 , 0.4175, 0.8266, 0.3767, -0.1508, -0.4301, -0.3397, 0.7248, 0.188 , 1.1632, -1.0831, -0.5726, -0.475 , 0.092 , -0.1566, 1.9074, -1.4261, 1.8589, -0.7534, -1.0767, -0.2704, 0.7567, 0.5903, -1.5612, -1.1097, 0.3504, -0.9086, -0.1691, 0.6714, -0.6033, 1.8315, -0.8141, 0.5968, -0.408 , -1.1843, 0.5146, 0.6201, 0.4293, 0.9797, 1.066 , -1.3325, -1.733 , 0.8545, 0.3993, -0.2041, -0.4624, 0.0272, -0.005 , 0.9237, -0.5523, 0.9975, -0.4374, 0.1351, -0.6148, 0.3185, 0.0572, -0.3002, 0.0889, -0.0894, -0.5617, -2.0553, -0.2923, 0.7227, 0.604 , -0.6623, -0.6126, -0.4991, 0.0923, -0.6982, 0.2099, -0.6853, -0.4752, -1.625 , 0.0443, 2.5507, -1.1597, 0.3504, -0.7654, -1.4366, -1.3755, 0.3702, -1.7853, -0.7326, -1.2803, -0.6089, -0.4472, 0.462 , 0.7799, 0.3141, 0.8064, -1.0487, 0.7317, -0.2446, 0.3061, 0.1384, -0.572 , -0.0311, 0.3572, -0.6371, -0.2236, 0.0806, 0.6648, -0.148 , -0.2547, 1.3649, -0.1595, 1.3632, -0.8858, 1.1801, 0.5533, 2.3306, 0.2724, 0.7073, -0.5605, -0.8849, 0.9533, 0.3683, -0.2901, -0.0453, 0.1064, 1.3342, -0.7036, 0.7127, 1.2156, 0.9017, 1.2378, -1.1017, 1.0558, 1.4273, 0.7003, 1.1649, 0.0334, 0.3433, -0.3997, -0.1195, 1.3725, -0.3746, 0.8444, 0.961 , -0.2644, 0.3245, -1.3583, 0.387 , 1.2944, 0.0274, -0.5057, 0.15 , 0.6 , -0.5752, 0.3746, 1.7114, -0.0026, -0.1221, -0.8084, -0.9521, -0.6332, 0.7254, 1.7032, -0.0879, 0.3329, -1.9525, -0.7083, -0.4113, 1.163 , 0.9018, -0.3667, 0.8419, 0.4417, 0.2904, 0.1666, 1.3722, -0.4455, -1.4876, 0.4103, 2.3672, 0.3569, -0.8546, 0.5152, 0.9623, 1.1777, 1.6789, -1.7793, -0.7797, -1.0923, 0.07 , -0.8974, -0.3151, -0.3675, -1.9851, -2.3352, 0.3566, 1.1929, 1.5275, 1.4349, -1.4742, -0.1913, 1.5874, -0.7264, -0.5594, 0.3166, -0.9377, -0.6452, 0.394 , -0.2238, -1.1239, -0.0324, 1.3866, -0.6174, -0.1301, -0.0328, -0.92 , 1.8067, 0.2576, -0.5248, 0.4114, 0.1655, -0.1674, 0.2743, 0.0835, -0.145 , 1.1658, 1.2624, 0.0404, 2.0929, 0.6047, 1.0317, -0.4956, -1.5666, 1.1729, 0.484 , 0.955 , 1.0546, 0.0106, 0.5062, 0.3211, 0.8503, 0.4706, 1.9953, -0.9362, 0.6326, -0.3154, 1.4987, -0.1695, 1.0906, -0.686 , 0.2501, -0.316 , 0.3032, 0.4873, 0.6402, -0.1209, -0.1857, -0.3707, -0.3082, -0.4769, -0.858 , -0.1521, -0.3403, -0.9853, -0.5049, 0.3338, -0.3197, -0.5789, -0.7124, -0.8867, -0.0228, -1.5519, 1.8517, 0.5229, 0.7613, -0.5586, 0.4827, -1.3011, -0.5284, -0.3806, -0.7719, 1.6304, 0.0375, -0.9122, -0.1006, 0.382 , 0.0969, 1.7784, 0.1831, -1.8866, 0.2996, 0.4778, -0.2491, -1.6537, 0.022 , -0.101 , 0.5912, -0.2249, -1.1422, -0.6436, -1.4096, -0.7446, 0.8055, 1.0727, 0.2426, -0.8079, -1.4692, 0.062 , -0.4466, 0.3786, -2.0461, 0.7238, -1.6195, 1.4005, 0.4881, -0.8161, -0.582 , 0.3456, 1.2922, 0.2469, 1.9035, 0.9072, -0.0729, -0.9424, -1.1129, 0.8922, -0.5628, 1.6215, -0.7022, 0.8395, -0.3423, 0.6048, -0.248 , 0.7411, 0.3546, 0.6176, -0.8221, -0.338 , -2.1051, -1.0049, -0.0659, 0.0917, -0.6661, -0.5234, 0.9574, -0.6316, -0.0047, -0.4773, 0.1562, -0.116 , -1.6255, -0.9108, -1.4767, -0.7765, -1.7101, 0.0557, 0.8112, 0.7382, 1.8806, 0.9239, 1.8638, 0.8426, 0.0359, 0.2743, 1.9204, 1.2223, 0.4575, -0.3408, 0.3727, 0.5036, 0.5392, -1.3331, -0.4008, -0.1341, -1.5197, 0.1923, 0.2128, 1.1533, -1.4284, -0.7483, -0.4092, 1.2843, -0.4489, -0.6624, 0.9255, -0.0895, 0.3199, -0.2564, -0.1166, -1.4701, 1.1799, -1.6238, 0.0508, 0.2312, 0.7322, -1.3623, -0.232 , -0.2206, 0.566 , 1.2411, -1.1563, 1.1777, -1.1481, -0.6716, 0.4596, -0.2422, -0.8654, 0.4441, 0.1869, 1.4626, 0.7621, 0.4249, 0.252 , 0.632 , 0.5626, -0.7925, 1.1995, 1.5665, 0.6096, 0.4821, -0.7324, -0.7624, 1.858 , -0.8434, -0.4408, 0.2011, 0.7552, -0.8955, -1.3255, 0.7022, 0.1507, 0.662 , -1.2229, 0.5199, 0.9837, -0.3947, -0.5262, -1.0424, -1.4582, 0.5126, -0.3606, 0.4427, -2.3922, 1.2784, -0.8382, -0.0198, 1.2136, -0.4212, -0.7798, -1.3387, -0.7141, 0.9581, -0.8575, -0.2255, 0.8436, -2.2162, 0.0742, 0.9683, -0.3633, -0.0227, -1.2176, 1.1482, -0.6697, 0.9643, -1.2802, -0.3651, -1.29 , 0.851 , 1.0167, 1.0011, -1.3014, -0.7205, 1.3621, -0.692 , 1.0637, 0.5637, 0.0851, 2.1514, -0.272 , 0.3136, 0.2179, 0.7035, -1.3028, -0.1032, 0.0611, 1.2002, -0.7346, 0.9991, -0.3747, 0.7908, -0.9573, -0.5114, -0.8607, -0.6711, 1.3335, -0.6671, -0.1687, 0.4601, 0.5747, -0.0767, -0.8428, 0.3372, -1.7756, -2.5264, -1.503 , -0.5669, 0.0167, -1.961 , 0.8861, 1.1902, 2.239 , 0.2481, 0.7361, -1.1103, 0.8368, -1.0434, 0.6809, -0.0839, -0.6972, -1.5492, -1.4129, 0.5889, 0.2138, 1.7689, -0.4861, -0.1124, 0.2032, 1.0664, -0.369 , 2.3793, 0.4406, -1.1741, 1.0812, 1.3965, -0.149 , 0.8793, 1.3494, 1.2159, -0.0001, 1.1929, -0.1966, -0.1666, 1.7097, -0.4273, 0.4831, -0.2411, -1.4517, -0.7317, 0.099 , 1.7922, 0.2313, -0.5031, -0.0849, 0.7331, -0.1483, -0.8003, 1.1897, 0.031 , -0.3624, -1.1133, 1.4647, 2.5653, -1.9536, -0.4528, -1.693 , 0.4847, 0.1368, 0.6859, -0.9872, 0.8425, -0.1492, -0.1335, -0.0229, -0.0903, -0.4381, 1.2552, 1.5763, 0.2375, -0.7597, 0.0845, 0.0894, -1.6022, -0.1988, 0.3095, -1.0785, -1.6044, -0.4922, 0.4583, 0.3168, -2.0485, -1.2147, -0.2803, -0.2071, 0.0767, 1.9544, -1.7648, 0.2873, -0.4029, -0.8128, -0.1081, 0.0332, 2.5288, 0.9933, 0.4378, -0.8208, -0.2451, 0.3472, -0.2917, 2.0775, 1.7381, -0.467 , -0.8943, -1.4171, -0.3905, 0.2591, 0.8118, -0.643 , 1.0387, 0.0049, 1.7299, -0.6882, -1.4132, -1.0893, 0.4606, -1.546 , -2.87 , 0.3492, -1.5968, 0.9858, 0.1384, -0.6016, -0.9632, -0.9088, 0.3711, 1.3509, 0.4601, -1.4963, -0.043 , 0.5588, 0.2638, -1.1118, -0.9376, -0.9139, 0.6551, 0.4876, -1.7039, -0.2915, 0.3867, -0.1795, 1.2298, 0.0893, -0.6019, 1.4109, -1.1918, 0.5009, 0.0157, -1.1307, 1.0407, 1.9742, -1.0377, -0.6151, -0.8398, 1.4096, -0.012 , -1.5323, 0.3323, 0.0539, 0.2383, -0.4059, 2.285 , 0.1536, 0.3838, 0.3623, -0.4326, -0.0975, -1.8119]) array([-2.9433, -2.87 , -2.6518, -2.5489, -2.5465, -2.5264, -2.4542, -2.3922, -2.3352, -2.3038, -2.2774, -2.2574, -2.2444, -2.2162, -2.1117, -2.1051, -2.0724, -2.0553, -2.0485, -2.0461, -1.9851, -1.961 , -1.9536, -1.9525, -1.8866, -1.8119, -1.7853, -1.7793, -1.7756, -1.769 , -1.7648, -1.733 , -1.7101, -1.7039, -1.693 , -1.669 , -1.6537, -1.6255, -1.625 , -1.6248, -1.6238, -1.6195, -1.6097, -1.6044, -1.6022, -1.5968, -1.5895, -1.5824, -1.5666, -1.5612, -1.5519, -1.5492, -1.546 , -1.5415, -1.5392, -1.5323, -1.5263, -1.5197, -1.503 , -1.4963, -1.4891, -1.4876, -1.4767, -1.4742, -1.4701, -1.4692, -1.4582, -1.4517, -1.4366, -1.4291, -1.4284, -1.4261, -1.4171, -1.4132, -1.4129, -1.4129, -1.4096, -1.3994, -1.3755, -1.3655, -1.3623, -1.3583, -1.3387, -1.3331, -1.3325, -1.3255, -1.3191, -1.3153, -1.3028, -1.3014, -1.3011, -1.2912, -1.29 , -1.2803, -1.2802, -1.2598, -1.2527, -1.2472, -1.2467, -1.2456, -1.2365, -1.2229, -1.2176, -1.2147, -1.1918, -1.1843, -1.1799, -1.1741, -1.1597, -1.1563, -1.1551, -1.1481, -1.1422, -1.1307, -1.1239, -1.1234, -1.118 , -1.1133, -1.1129, -1.1118, -1.1103, -1.1097, -1.1087, -1.1017, -1.0923, -1.0893, -1.0831, -1.0785, -1.0767, -1.0763, -1.0699, -1.069 , -1.0487, -1.0434, -1.0424, -1.0397, -1.0377, -1.0318, -1.0263, -1.017 , -1.0152, -1.0049, -1.0032, -0.9944, -0.9872, -0.9853, -0.9816, -0.9632, -0.9632, -0.9573, -0.9521, -0.9516, -0.9424, -0.9419, -0.9377, -0.9376, -0.9371, -0.9362, -0.9343, -0.92 , -0.9139, -0.9122, -0.9111, -0.9108, -0.9106, -0.9088, -0.9086, -0.8974, -0.8955, -0.8943, -0.8867, -0.8858, -0.8849, -0.8654, -0.8632, -0.8607, -0.858 , -0.8575, -0.8546, -0.849 , -0.8483, -0.8434, -0.8428, -0.8398, -0.8382, -0.8357, -0.8324, -0.8303, -0.827 , -0.8221, -0.821 , -0.8208, -0.817 , -0.8161, -0.8141, -0.8128, -0.8084, -0.8079, -0.8003, -0.7962, -0.7925, -0.7835, -0.7802, -0.7798, -0.7797, -0.7793, -0.7765, -0.7762, -0.7719, -0.7654, -0.7638, -0.7624, -0.7597, -0.7596, -0.7579, -0.7534, -0.7487, -0.7483, -0.7446, -0.7348, -0.7346, -0.7326, -0.7324, -0.7317, -0.7277, -0.7264, -0.7215, -0.7205, -0.7141, -0.7125, -0.7124, -0.7083, -0.7036, -0.7022, -0.6982, -0.6972, -0.6966, -0.692 , -0.6914, -0.6882, -0.6877, -0.686 , -0.6853, -0.6716, -0.6711, -0.6705, -0.6697, -0.6671, -0.6661, -0.6624, -0.6623, -0.6596, -0.6568, -0.6489, -0.6452, -0.6443, -0.6437, -0.6436, -0.643 , -0.641 , -0.6407, -0.6371, -0.6332, -0.6316, -0.6259, -0.6174, -0.6151, -0.6148, -0.6126, -0.6089, -0.6051, -0.605 , -0.6033, -0.6019, -0.6016, -0.601 , -0.5972, -0.582 , -0.5794, -0.5789, -0.5752, -0.5742, -0.5726, -0.572 , -0.5669, -0.5628, -0.5617, -0.5605, -0.5594, -0.5586, -0.5557, -0.5523, -0.5389, -0.5329, -0.5284, -0.5262, -0.5258, -0.5248, -0.5234, -0.5131, -0.5114, -0.5057, -0.5049, -0.5031, -0.5009, -0.4991, -0.4956, -0.4922, -0.4861, -0.4813, -0.4808, -0.4773, -0.4769, -0.4752, -0.475 , -0.4731, -0.4703, -0.467 , -0.4652, -0.4624, -0.4528, -0.452 , -0.4489, -0.4472, -0.4466, -0.4455, -0.4435, -0.4431, -0.4423, -0.4408, -0.4381, -0.4374, -0.4326, -0.4301, -0.4294, -0.4273, -0.4264, -0.4212, -0.4143, -0.4138, -0.4113, -0.4092, -0.408 , -0.4059, -0.4038, -0.4029, -0.4008, -0.3997, -0.3984, -0.3966, -0.3947, -0.3927, -0.3905, -0.388 , -0.3806, -0.3774, -0.3747, -0.3746, -0.3734, -0.3707, -0.369 , -0.3686, -0.3675, -0.3667, -0.3656, -0.3655, -0.3651, -0.3633, -0.3624, -0.3606, -0.3542, -0.354 , -0.3423, -0.3408, -0.3403, -0.3397, -0.338 , -0.3253, -0.3223, -0.3197, -0.316 , -0.3154, -0.3151, -0.3144, -0.3082, -0.3036, -0.3002, -0.2971, -0.2923, -0.2917, -0.2915, -0.2901, -0.2828, -0.2803, -0.272 , -0.2704, -0.2684, -0.2644, -0.2564, -0.2547, -0.2497, -0.2491, -0.248 , -0.2451, -0.2447, -0.2446, -0.2422, -0.2411, -0.2381, -0.2339, -0.232 , -0.2314, -0.2298, -0.2255, -0.2249, -0.2238, -0.2236, -0.2206, -0.211 , -0.2074, -0.2071, -0.2065, -0.2058, -0.2041, -0.1988, -0.1966, -0.1935, -0.1913, -0.1857, -0.1795, -0.1792, -0.1695, -0.1691, -0.1687, -0.1674, -0.1666, -0.1602, -0.1595, -0.1566, -0.1526, -0.1521, -0.1508, -0.1492, -0.149 , -0.1483, -0.148 , -0.146 , -0.145 , -0.1341, -0.1335, -0.131 , -0.1301, -0.1247, -0.1221, -0.1209, -0.1195, -0.1166, -0.116 , -0.1126, -0.1124, -0.1081, -0.1036, -0.1032, -0.101 , -0.1006, -0.0975, -0.0903, -0.0895, -0.0894, -0.0879, -0.0849, -0.0839, -0.078 , -0.0767, -0.0729, -0.0675, -0.0659, -0.0588, -0.0453, -0.0453, -0.043 , -0.0328, -0.0324, -0.0311, -0.0244, -0.0229, -0.0228, -0.0227, -0.0213, -0.0198, -0.0171, -0.0136, -0.0129, -0.012 , -0.005 , -0.0047, -0.0026, -0.0001, 0.0049, 0.0106, 0.0157, 0.0167, 0.0216, 0.022 , 0.0251, 0.0263, 0.0272, 0.0274, 0.0285, 0.0301, 0.031 , 0.0332, 0.0334, 0.0359, 0.0375, 0.0375, 0.0404, 0.0419, 0.0443, 0.0466, 0.0479, 0.049 , 0.0508, 0.0539, 0.0557, 0.0572, 0.0601, 0.0611, 0.062 , 0.07 , 0.0742, 0.0756, 0.0767, 0.0784, 0.0801, 0.0806, 0.0835, 0.0845, 0.0851, 0.0882, 0.0889, 0.0893, 0.0894, 0.0917, 0.092 , 0.0923, 0.0943, 0.0969, 0.099 , 0.1016, 0.106 , 0.1064, 0.1351, 0.1364, 0.1368, 0.1384, 0.1384, 0.1396, 0.15 , 0.1507, 0.1536, 0.1552, 0.1562, 0.1655, 0.1666, 0.1704, 0.1778, 0.1831, 0.1859, 0.1862, 0.1865, 0.1869, 0.188 , 0.1923, 0.1958, 0.1973, 0.2011, 0.2032, 0.2099, 0.2128, 0.2132, 0.2138, 0.2179, 0.2254, 0.2312, 0.2313, 0.2375, 0.2383, 0.2426, 0.2447, 0.2469, 0.2481, 0.2501, 0.252 , 0.2536, 0.2576, 0.2591, 0.2621, 0.2638, 0.2724, 0.2743, 0.2743, 0.2748, 0.2873, 0.2882, 0.2904, 0.2913, 0.2963, 0.2996, 0.3032, 0.3061, 0.3086, 0.3095, 0.3136, 0.3141, 0.3159, 0.3166, 0.3168, 0.3185, 0.3199, 0.3211, 0.3218, 0.3245, 0.3272, 0.3323, 0.3329, 0.3335, 0.3338, 0.3372, 0.3433, 0.3456, 0.3472, 0.3492, 0.3504, 0.3504, 0.3524, 0.3528, 0.3546, 0.3566, 0.3569, 0.3572, 0.3623, 0.3624, 0.3683, 0.3702, 0.3711, 0.3711, 0.3727, 0.3746, 0.3767, 0.3786, 0.3794, 0.3803, 0.381 , 0.382 , 0.3838, 0.3867, 0.387 , 0.3934, 0.394 , 0.3993, 0.3995, 0.4023, 0.4037, 0.407 , 0.4103, 0.4114, 0.4175, 0.4241, 0.4249, 0.4293, 0.4378, 0.4386, 0.4406, 0.4417, 0.4421, 0.4427, 0.4441, 0.4575, 0.4583, 0.4596, 0.4601, 0.4601, 0.4606, 0.4607, 0.462 , 0.4657, 0.4706, 0.4738, 0.4778, 0.4778, 0.4821, 0.4827, 0.4831, 0.484 , 0.4847, 0.4873, 0.4876, 0.4881, 0.4925, 0.4965, 0.5009, 0.5033, 0.5036, 0.5062, 0.5109, 0.5119, 0.5126, 0.5146, 0.5152, 0.5199, 0.5229, 0.5284, 0.5347, 0.5358, 0.5392, 0.5436, 0.5533, 0.5588, 0.5597, 0.5626, 0.5637, 0.566 , 0.5675, 0.5736, 0.5747, 0.5889, 0.5903, 0.5912, 0.5968, 0.5969, 0.6 , 0.604 , 0.6047, 0.6048, 0.6096, 0.6128, 0.6138, 0.6176, 0.6201, 0.632 , 0.6326, 0.6351, 0.6402, 0.6449, 0.6464, 0.6495, 0.6551, 0.6615, 0.662 , 0.662 , 0.6648, 0.6699, 0.6714, 0.673 , 0.6739, 0.6809, 0.6831, 0.6856, 0.6859, 0.6867, 0.6883, 0.7003, 0.7022, 0.7035, 0.7073, 0.7127, 0.7227, 0.7238, 0.7248, 0.7254, 0.7317, 0.7322, 0.7331, 0.7361, 0.7382, 0.7411, 0.7475, 0.7496, 0.7552, 0.7567, 0.7613, 0.7621, 0.7667, 0.767 , 0.7695, 0.7751, 0.7799, 0.7908, 0.7921, 0.8043, 0.8055, 0.8064, 0.8112, 0.8118, 0.8266, 0.8327, 0.8368, 0.8395, 0.8411, 0.8419, 0.8425, 0.8426, 0.8436, 0.8442, 0.8444, 0.8503, 0.851 , 0.8545, 0.8634, 0.8743, 0.8744, 0.8793, 0.8858, 0.8861, 0.8922, 0.9017, 0.9018, 0.9072, 0.9073, 0.9237, 0.9239, 0.9255, 0.927 , 0.9362, 0.9533, 0.955 , 0.9574, 0.9581, 0.961 , 0.9623, 0.9643, 0.966 , 0.9683, 0.9797, 0.9837, 0.9858, 0.9918, 0.9933, 0.9949, 0.9975, 0.9991, 1.0011, 1.0126, 1.0167, 1.0317, 1.0387, 1.0407, 1.0539, 1.0546, 1.0558, 1.0567, 1.0637, 1.066 , 1.0664, 1.067 , 1.0727, 1.0735, 1.076 , 1.0767, 1.0812, 1.0844, 1.0906, 1.0977, 1.0998, 1.1103, 1.1161, 1.122 , 1.1334, 1.1351, 1.1366, 1.1482, 1.1533, 1.1545, 1.1567, 1.163 , 1.1632, 1.1649, 1.1653, 1.1658, 1.1729, 1.1777, 1.1777, 1.1799, 1.1801, 1.1837, 1.1864, 1.1897, 1.1902, 1.1929, 1.1929, 1.1995, 1.2002, 1.2098, 1.2136, 1.2156, 1.2159, 1.2223, 1.2296, 1.2298, 1.2378, 1.2411, 1.253 , 1.2544, 1.2552, 1.2624, 1.2784, 1.2797, 1.2843, 1.2922, 1.2944, 1.2946, 1.3289, 1.3335, 1.3342, 1.3388, 1.3494, 1.3509, 1.3621, 1.3632, 1.3649, 1.3722, 1.3725, 1.3866, 1.3965, 1.3996, 1.4005, 1.4047, 1.4096, 1.4109, 1.421 , 1.4273, 1.4349, 1.4391, 1.4626, 1.4647, 1.4797, 1.4848, 1.497 , 1.4987, 1.5228, 1.5275, 1.5298, 1.5449, 1.5665, 1.5763, 1.5874, 1.5924, 1.6091, 1.6215, 1.6304, 1.6461, 1.6789, 1.6945, 1.7032, 1.7097, 1.7106, 1.7114, 1.7299, 1.7323, 1.7381, 1.7689, 1.7784, 1.7922, 1.7923, 1.8067, 1.8315, 1.8415, 1.8517, 1.858 , 1.8589, 1.8638, 1.8738, 1.8806, 1.9035, 1.9074, 1.9204, 1.9294, 1.9456, 1.9493, 1.9544, 1.9551, 1.9742, 1.9928, 1.9953, 2.0775, 2.0929, 2.1223, 2.1514, 2.239 , 2.285 , 2.3306, 2.3672, 2.3793, 2.5288, 2.5507, 2.5653, 2.5671, 2.6081, 2.697 ]) -1.5519406239259821 唯一化以及其他的集合逻辑1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])np.unique(names)ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])np.unique(ints)array([&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;], dtype=&apos;&lt;U4&apos;) array([1, 2, 3, 4]) 1sorted(set(names))[&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;] in1d感觉挺有用12values = np.array([6, 0, 0, 3, 2, 5, 6])np.in1d(values, [2, 3, 6])array([ True, False, False, True, True, False, True], dtype=bool) 用于数组的文件输入输出将数组以二进制的形式保存到磁盘12arr = np.arange(10)np.save('some_array', arr)1np.load('some_array.npy')array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 压缩存储，并且可以存储多个1np.savez('array_archive.npz', a=arr[:4], b=arr)123arch = np.load('array_archive.npz')arch['a']arch['b']array([0, 1, 2, 3]) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 存取文本文件1!more ch04\array_ex.txt0.580052,0.186730,1.040717,1.134411 0.194163,-0.636917,-0.938659,0.124094 -0.126410,0.268607,-0.695724,0.047428 -1.484413,0.004176,-0.744203,0.005487 2.302869,0.200131,1.670238,-1.881090 -0.193230,1.047233,0.482803,0.960334 12arr = np.loadtxt('.\\ch04\\array_ex.txt', delimiter=',')arrarray([[ 0.5801, 0.1867, 1.0407, 1.1344], [ 0.1942, -0.6369, -0.9387, 0.1241], [-0.1264, 0.2686, -0.6957, 0.0474], [-1.4844, 0.0042, -0.7442, 0.0055], [ 2.3029, 0.2001, 1.6702, -1.8811], [-0.1932, 1.0472, 0.4828, 0.9603]]) 线性代数12345x = np.array([[1., 2., 3.], [4., 5., 6.]])y = np.array([[6., 23.], [-1, 7], [8, 9]])xyx.dot(y) # equivalently np.dot(x, y)array([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 6., 23.], [ -1., 7.], [ 8., 9.]]) array([[ 28., 64.], [ 67., 181.]]) 1np.dot(x, np.ones(3))array([ 6., 15.]) 1np.random.seed(12345)123456789from numpy.linalg import inv, qrX = randn(5, 5)Xmat = X.T.dot(X)matinv(mat)mat.dot(inv(mat))q, r = qr(mat) # QR分解rarray([[-0.5031, -0.6223, -0.9212, -0.7262, 0.2229], [ 0.0513, -1.1577, 0.8167, 0.4336, 1.0107], [ 1.8249, -0.9975, 0.8506, -0.1316, 0.9124], [ 0.1882, 2.1695, -0.1149, 2.0037, 0.0296], [ 0.7953, 0.1181, -0.7485, 0.585 , 0.1527]]) array([[ 4.2538, -1.0645, 1.4407, 0.9898, 1.7318], [-1.0645, 7.4431, -1.5585, 4.4972, -2.1367], [ 1.4407, -1.5585, 2.8126, 0.243 , 1.2786], [ 0.9898, 4.4972, 0.243 , 5.0897, 0.305 ], [ 1.7318, -2.1367, 1.2786, 0.305 , 1.928 ]]) array([[ 0.4057, -0.1875, -0.0764, 0.1229, -0.541 ], [-0.1875, 2.462 , 0.2537, -2.3367, 3.0984], [-0.0764, 0.2537, 0.5435, -0.2369, 0.0268], [ 0.1229, -2.3367, -0.2369, 2.4239, -2.9264], [-0.541 , 3.0984, 0.0268, -2.9264, 4.8837]]) array([[ 1., 0., -0., -0., -0.], [ 0., 1., -0., -0., -0.], [ 0., 0., 1., 0., -0.], [ 0., -0., 0., 1., 0.], [ 0., 0., -0., 0., 1.]]) array([[-5.0281, 2.7734, -2.8428, -1.0619, -3.0078], [ 0. , -8.7212, 1.2925, -6.5614, 1.622 ], [ 0. , 0. , -2.0873, -1.0487, -0.6291], [ 0. , 0. , 0. , -1.408 , -0.955 ], [ 0. , 0. , 0. , 0. , 0.1537]]) 随机数生成12samples = np.random.normal(size=(4, 4))samplesarray([[-0.5196, 1.297 , 0.9062, 0.5809], [ 1.2233, -1.3301, 1.0483, 0.357 ], [-0.7935, -0.406 , -0.0096, -0.596 ], [ 1.3833, -0.2029, -1.0547, -0.9795]]) 1234from random import normalvariateN = 1000000%timeit samples = [normalvariate(0, 1) for _ in range(N)]%timeit np.random.normal(size=N)1 loop, best of 3: 814 ms per loop 10 loops, best of 3: 28.4 ms per loop 可以看出numpy确实要快很多Example: 随机游走123456789import randomposition = 0walk = [position]steps = 1000for i in range(steps): step = 1 if random.randint(0, 1) else -1 position += step walk.append(position)通过numpy来实现上述过程1np.random.seed(12345)1234nsteps = 1000draws = np.random.randint(0, 2, size=nsteps)steps = np.where(draws &gt; 0, 1, -1)walk = steps.cumsum()1234import matplotlib.pyplot as pltindex = [x + 1 for x in range(len(walk))]plt.plot(index, walk)[&lt;matplotlib.lines.Line2D at 0x234014e9f98&gt;] 12walk.min()walk.max()-3 31 1(np.abs(walk) &gt;= 10).argmax() # the first index37 一次模拟多次随机漫步123456nwalks = 5000 # 5000 random walknsteps = 1000draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1steps = np.where(draws &gt; 0, 1, -1)walks = steps.cumsum(1)walksarray([[ -1, 0, -1, ..., 24, 23, 22], [ -1, 0, -1, ..., -36, -37, -36], [ 1, 2, 3, ..., -42, -41, -40], ..., [ 1, 0, -1, ..., 48, 49, 50], [ -1, -2, -3, ..., -38, -39, -40], [ -1, 0, 1, ..., -48, -47, -48]], dtype=int32) 12walks.max()walks.min()130 -117 123hits30 = (np.abs(walks) &gt;= 30).any(1)hits30hits30.sum() # Number that hit 30 or -30array([ True, True, True, ..., True, True, True], dtype=bool) 3412 12crossing_times = (np.abs(walks[hits30]) &gt;= 30).argmax(1)crossing_times.mean()497.04103165298943]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Naive Bayes]]></title>
      <url>%2F2017%2F02%2F25%2FNaive-Bayes%2F</url>
      <content type="text"><![CDATA[利用朴素贝叶斯进行文本分类准备数据从文本中构建词向量下面写一个词表到向量的转换函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def LoadDataSet(): """ Load a vector-like data set tranfered by a data set list that generated by artifical. Returns: return_vec: the vector-like data set. class_vec: the class label corresponds to the data items. """ posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] class_vec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return posting_list, class_vecdef CreateVocabList(data_set): """ Create vocabulary list from vector-like data set. Arguments: data_set: the data source. Returns: vocab_list: the vocabulary list. """ vocab_set = set([]) for document in data_set: vocab_set = vocab_set | set(document) return list(vocab_set)def SetOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] = 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vec下面对函数的功能进行测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110In [13]: import bayesIn [14]: list_of_posts, list_classes = bayes.LoadDataSet()In [15]: my_vocab_list = bayes.CreateVocabList(list_of_posts)In [16]: my_vocab_listOut[16]:['help', 'worthless', 'I', 'take', 'love', 'maybe', 'stupid', 'to', 'not', 'please', 'quit', 'park', 'posting', 'dog', 'dalmation', 'steak', 'my', 'how', 'food', 'so', 'stop', 'is', 'garbage', 'flea', 'problems', 'has', 'buying', 'ate', 'him', 'licks', 'mr', 'cute']In [17]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[0])Out[17]:[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]In [18]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[3])Out[18]:[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]看上去一切都work，可以进入下一步了。训练从词向量计算概率我们来看看贝叶斯公式：$$p(c_i | w) = \frac{p(w | c_i) p(c_i)}{p(w)}$$这里$w$代表词向量， 可以看出$c_i$的计算十分简单，值得注意的是，根据朴素贝叶斯的假设，有：$$p(w | c_i) = p(w_0, w_1, w_2, \cdots, w_N | c_i) = p(w_0 | c_i)p(w_1 | c_i)p(w_2 | c_i) \cdots p(w_N | c_i)$$当需要预测新样本的类别时：这样一切就很清楚了，下面给出伪代码：12345678910计算每个类别的文档数目for 每一篇文档： for 每一个类别： if 词条出现在文档中： 增加该词条的计数值 增加总词条数的计数值 for 每一个类别： for 每一个词条： 将该词条的数目除以总词条数目得到条件概率 返回每个类别的条件概率注意，这里的$p(w_j | c_i)$是要根据整个训练集来算，代码实现如下：123456789101112131415161718192021222324252627282930313233def TrainNaiveBayes0(train_matrix, train_category): """ the training method. Arguments: train_matrix: The train data. train_category: The train label. Returns: p0_vect: The conditional probability of w by c0 p1_vect: The conditional probability of w by c1 p_abusive: The conditional probability of c1 """ num_train_docs = len(train_matrix) num_words = len(train_matrix[0]) p_abusive = sum(train_category) / num_train_docs p0_num = zeros(num_words) p1_num = zeros(num_words) p0_denom = 0.0 p1_denom = 0.0 for i in range(num_train_docs): if train_category[i] == 1: p1_num += train_matrix[i] p1_denom += sum(train_matrix[i]) else: p0_num += train_matrix[i] p0_denom += sum(train_matrix[i]) p1_vect = p1_num / p1_denom p0_vect = p0_num / p0_denom return p0_vect, p1_vect, p_abusive同样进行一下测试：123456789101112131415161718192021222324252627282930In [25]: train_mat = []In [26]: for post_in_doc in list_of_posts: ...: train_mat.append(bayes.SetOfWords2Vec(my_vocab_list, post_in_doc)) ...:In [27]: p0_v, p1_v, p_ab = bayes.TrainNaiveBayes0(train_mat, list_classes)In [28]: p_abOut[28]: 0.5In [29]: p0_vOut[29]:array([ 0.04166667, 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0. , 0.04166667, 0.04166667, 0.04166667, 0.125 , 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.08333333, 0.04166667, 0.04166667, 0.04166667])In [30]: p1_vOut[30]:array([ 0. , 0.10526316, 0. , 0.05263158, 0. , 0.05263158, 0.15789474, 0.05263158, 0.05263158, 0. , 0.05263158, 0.05263158, 0.05263158, 0.10526316, 0. , 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. ])但是上述代码存在一些缺陷，首先，计算$p(w_j | c_i)$可能会出现结果为0的情况，那么最后的结果就会为0，那么需要进行一些修改 (为什么是2？)：1234p0_num = ones(num_words)p1_num = ones(num_words)p0_denom = 2.0p1_denom = 2.0另外一个就是下溢的问题，所以要改用log函数12p1_vect = log(p1_num / p1_denom)p0_vect = log(p0_num / p0_denom)测试最后写一个分类和测试函数：123456789101112131415161718192021222324252627282930313233343536373839404142def ClassifyNaiveBayes(vec_to_classify, p0_vect, p1_vect, p_abusive): """ Classify. Arguments: vec_to_classify: The vector to classify. p0_vect: The conditional probability of w by c0. p1_vect: The conditional probability of w by c1. p_abusive: The conditional probability of c1. Returns: 0: The predict class is 0 1: The predict class is 1 """ p1 = sum(vec_to_classify * p1_vect) + log(p_abusive) p0 = sum(vec_to_classify * p0_vect) + log(1 - p_abusive) if p1 &gt; p0: return 1 else: return 0def TestNaiveBayes(): """ A test method. """ list_of_posts, list_of_classes = LoadDataSet() my_vocab_list = CreateVocabList(list_of_posts) train_mat = [] for post_in_doc in list_of_posts: train_mat.append(SetOfWords2Vec(my_vocab_list, post_in_doc)) p0_v, p1_v, p_ab = TrainNaiveBayes0( array(train_mat), array(list_of_classes)) test_entry = ['love', 'my', 'dalmation'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab)) test_entry = ['stupid', 'garbage'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab))Test123In [32]: bayes.TestNaiveBayes()['love', 'my', 'dalmation'] classified as: 0['stupid', 'garbage'] classified as: 1Ok, bravo!词袋模型现在有一个问题， 到目前为止，我们将每个词是否出现作为特征，这被称为词集模型。但是如果有一个词在文档中不止出现一次，那么就需要词袋模型进行建模。1234567891011121314151617181920def BagOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] += 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vec高斯朴素贝叶斯一般的朴素贝叶斯算法的输入特征为离散值，那么当输入变量为连续值时就不能处理了，一般这时候假设输入变量服从一个正态分布，这样$p(w_j | c_i)$就可以计算了，所以整个的流程如下：采用sk-learn进行下实验123456789In [33]: from sklearn import datasets ...: iris = datasets.load_iris() ...: from sklearn.naive_bayes import GaussianNB ...: gnb = GaussianNB() ...: y_pred = gnb.fit(iris.data, iris.target).predict(iris.data) ...: print("Number of mislabeled points out of a total %d points : %d" ...: % (iris.data.shape[0],(iris.target != y_pred).sum())) ...:Number of mislabeled points out of a total 150 points : 6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture2 note]]></title>
      <url>%2F2017%2F02%2F24%2FCS231n-Lecture2-note%2F</url>
      <content type="text"><![CDATA[图像分类目标给一张输入图片赋予一个标签， 这个标签属于事先定义好的类别集合中地位计算机视觉的核心问题例子难点拍摄点的视角多样拍摄点的远近距离多样物体的变形物体部分遮挡光线背景相似品种多样方法数据驱动（即包含训练数据）pipeline输入 -&gt; 学习 -&gt; 评估最近邻分类器距离度量L1 distance$$d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$$L2 distance$$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$示例代码数据读取1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072预测及评估123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )基本方法123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred实验结果L1-distance 38.6% on CIFAR-10L2-distance 35.4% on CIFAR-10L1 vs. L2 L2比L1的差异容忍度更小K近邻分类器采用验证集进行超参调参实例代码123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc))交叉验证最近邻分类器的优缺点优点训练速度快缺点测试速度很慢​ 解决方法：1. ANN 2. FANN距离度量不合适​(http://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg)接下来…t-SNE http://lvdmaaten.github.io/tsne/random projection http://scikit-learn.org/stable/modules/random_projection.htmlINTUITION FAILS IN HIGH DIMENSIONS http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdfRecognizing and Learning Object Categories http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html线性分类一个从图像到标签的映射函数$$f(x_i, W, b) = W x_i + b$$x shape is [D x 1], W shape is [K x D], b shape is [K x 1]注意点：W代表K个分类器的参数放在一起，因此整个模型是K个分类器的一个整合向量化能够大大提升计算速度线性分类器的解释权重W表示不同的标签对于图像不同位置不同颜色的重视程度。比如太阳可能对于圆形的区域以及黄颜色比较看重将图像看成高维空间中的点将线性分类器看成模板匹配将W的每一行看成一个模板，通过内积计算，每一张图片张成的列向量都与每一个模板作比较，最后选出最匹配的，这也是一种最近邻算法。从上图可以看出，这里的模板是各个图像的一种折中。将bias项放入W中数据预处理​ 数据中心化]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Qiniu cloud images batch upload and directory synchronization]]></title>
      <url>%2F2017%2F02%2F24%2FQiniu-cloud-images-batch-upload-and-directory-synchronization%2F</url>
      <content type="text"><![CDATA[最近写博客的时候会用到图片，因此用了七牛云的图片外链功能，但是其内容管理不能创建目录，所以图片的命名以及上传都很麻烦，然后去网上查了一下，也看了一下官方文档，发现官方有一个批量上传的工具挺好用，所以记录一下，下面把官方文档中的一些东西贴出来，方便日后查阅。​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Correlation Analysis]]></title>
      <url>%2F2017%2F02%2F24%2FCorrelation-Analysis%2F</url>
      <content type="text"><![CDATA[问题描述最近有一个关于关联分析的小作业，问题描述如下：孩子的情感状况、品行症状、亲社会行为等等与很多因素相关，这个作业主要着眼点在于孩子每天看视频的时间是否对于以上这些指标有着重要的影响，因此需要对输入特征与输出标签之间的相关关系做一个分析。属性描述如下：由以上属性描述表可以看出，属性类型同时包括类别以及数值，因此进行相关分析时针对这两种情况需要采用不同的方法，这里采用的策略如下：对于离散-离散的情况，采用卡方检验对于连续-离散的情况，采用ANOVA方差检验下面对于这两种检验方法进行介绍。卡方检验一维情况假设一条河里有三种鱼，gumpies, sticklebarbs, 以及spotheads。如果这条河的生态环境没有遭到干扰，那么这三种鱼的数量是相等的（下表第三行）。现在从河里进行300次抽样，最后抽样得到的结果如下表第二行所示：gumpiessticklebarbsspothheadsTotals观察到的频数8912091300期望的频数100100100300现在需要解决的问题是，是否这条河的生态环境收到了干扰？我们假设生态环境正常，这是我们的原假设。（注意，期望的频数是根据实际情况自行定义的）很容易想到的是，我们可以建立一种度量来衡量现实情况与原假设的偏离程度，例如：$$\frac{观察值 - 期望值}{期望值}$$把实际的数据带入，可以得到以下的结果：gumpies: $\frac{89 - 100}{100} = -0.11$sticklebarbs: $\frac{120 - 100}{100} = +0.20$spothheads: $\frac{91 - 100}{100} = -0.09$发现结果还不错，但是这只能用来衡量单个类别的偏差程度，而不能用来衡量整体的偏差程度，因为这三者的加和为零。既然这样，很容易想到可以对之前的度量进行简单的修改，变成这样：$$\frac{(观察值 - 期望值)^2}{期望值}$$再把数据带入看看：gumpies: $\frac{(89 - 100)^2}{100} = 1.21$sticklebarbs: $\frac{(120 - 100)^2}{100} = 4.0$spothheads: $\frac{(91 - 100)}{100} = 0.81$sum: $1.21 + 4.0 + 0.81 = 6.02$这样一来问题便得到了解决，而这也正是卡方检验所采取的方式。而这个sum值就是卡方（chi-square），记作$\chi^2$， 为了更加形式化地表示，我们把观察值记为$O$， 期望值记为$E$， 那么有如下等式成立：$$\chi^2 = \sum \frac{(O - E) ^ 2}{E}$$下面的问题是，我们达到了卡方值，但是这个值到底好还是不好呢？是更加支持原假设还是拒绝原假设呢？可以设想这样一种情况，我们假设河里的鱼服从原假设的分布，也就是三种鱼出现的概率相等。然后我们把三百次采样看作一次实验，每一次实验完毕之后记录下采样出的300条鱼中每一种鱼的频数，然后计算卡方值。在进行很多次这样的实验之后，我们可以画一个柱状图，这个图记录下了卡方值的分布情况。然后再把实际的观察值（也就是上面表格的第二行）计算的卡方值（6.02）带入进去，看看大于或者等于这个值在柱状图中所有卡方值中占有多少比例。如果占有的比例很大，说明这个值是由跟原假设很近似的假设得出的，这就证明了原假设是对的；反之。如果这个比例很小，说明如果分布服从原假设，那么所计算出的卡方值基本不可能包含这个观测出的卡方值，表明原假设是不对的，我们就可以拒绝原假设。其实统计检验的基本思想就是这样。但是存在一个问题，我们不可能进行真实的采样（从河里抓鱼），所以一般采用计算机模拟的方式，具体步骤如下所示：等概率的产生a（代表gumpies）, b（代表sticklebarbs）, c（代表spothheads）的序列计算一个大小为300的序列中a, b, c三者的频数，作为观察值，然后将a = b = c = 100作为期望值，计算并记录下算出的卡方值重复1~2步10000次， 画出柱状图如下：可以看出只有5%左右的值大于6.02，说明我们可以以95%的置信度拒绝原假设。二维情况对于二维的情况，卡方检验又被称为卡方关联度检验，也就是检验两个变量之间的相关程度（独立程度），考虑下面这个数据表$O$：Alzhemer’s onset -during 5-year periodnoyesrecieved-yes1479156estrogenno8101589689571671124这是观察值，为了计算卡方值，很明显我们需要计算期望值。为了方便表示，把上表变成如下形式：（A）Alzhemer’s onset -during 5-year periodnoyes(R) recieved-yes[cell a][cell b]156estrogenno[cell c][cell d]9689571671124拿a做例子：$$E_a = \frac{156}{1124} \times \frac{957}{1124} \times 1124$$这个式子如何解释呢？如果这两个变量是独立的，那么$A$变量取$no$值与$R$变量取$yes$值这两个事件之间就是独立的，那么$[cell_a]$事件发生的概率就是两者相乘，也就是上述等式右边前两个数相乘，最后的期望值自然就是概率乘以实验总数。上面的解释比较不正式，换一种较为正式的表达方式。假设我们要求$cell$的期望值，设$R$为$cell$所在列的边缘事件总数，$C$为$cell$所在行的边缘事件总数，$N$为实验总数目，这样就有：$$E_{cell} =\frac{R}{N} \times \frac{C}{N} \times N$$所以就有期望值$E$数据表如下：（A）Alzhemer’s onset -during 5-year periodnoyes(R) recieved-yes$E_a = \frac{156 \times 957}{1124} = 132.82$$E_b = \frac{156 \times 167}{1124} = 23.18$156estrogenno$E_c = \frac{968 \times 957}{1124} = 824.18$$E_d = \frac{968 \times 167}{1124} = 143.82$9689571671124这样就可以调用公式：$$\chi^2 = \sum \frac{(O - E) ^ 2}{E}$$特别地，当行数以及列数都为2时，上述公式需要进行一下修改：$$\chi^2 = \sum \frac{(|O - E| - 0.5) ^ 2}{E}$$这样算出来的卡方值为$11.01$最后涉及到自由度的问题，因为比较简单，所以只写出结论：$$df = (r - 1)(c - 1)$$r = number of rowsc = number of columnsANOVA方差检验为了方便比较，同样采用上述阿尔兹海默病的例子。研究表明，老年痴呆症患者患病之后会经常经历情绪非常不稳定地阶段，原因是因为患者患病之前的生活中经常有恐惧或者焦虑的体验，正是这些一直存在于脑海中的记忆出发了患病后的不稳定情绪的产生。现在我们假设有一个实验团队发明了一种药物，可以缓解这种情绪问题，他们对小白鼠进行了实验。实验设计如下：将小白鼠随机分为四组A， B， C， DA组作为参照组，不给药；B， C， D三组分别注射一个单位，两个单位，三个单位的药剂记录实验结果，数值越低表明实验效果越好实验结果如下：ABCDTotal27.022.821.923.526.223.123.419.628.827.720.123.733.527.627.820.828.824.019.323.9$M_a = 28.86$$M_b = 25.04$$M_c = 22.50$$M_d = 22.30$$M_T = 24.68$下面需要进行一下相关性分析，判断药物是否对症状的缓解产生作用。和卡方检验一样，ANOVA检验最后也有一个衡量指标，记为$F$，定义如下：$$F = \frac{MS_{bg}}{MS_{wg}} = \frac{组间相似度}{组内相似度}$$具体的计算步骤如下所示（推导过程省略）（1） 首先计算出如下值：ABCDTotal$N_A = 5$$N_B = 5$$N_C = 5$$N_D = 5$$N_T = 20$$\sum X_{Ai} = 144.30$$\sum X_{Bi} = 125.20$$\sum X_{Ci} = 112.50$$\sum X_{Di} = 111.50$$\sum X_{Ti} = 493.50$$\sum X^2_{Ai} = 4196.57$$\sum X^2_{Bi} = 3158.50$$\sum X^2_{Ci} = 2576.51$$\sum X^2_{Di} = 2501.95$$\sum X^2_{Ti} = 12433.53$$SS_A = 32.07$$SS_B = 23.49$$SS_C = 45.26$$SS_D = 15.50$$SS_T = 256.42$其中：$$SS = \sum X^2_i - \frac{(\sum X_i)^2}{N}$$（2）计算$SS_{wg}$以及$SS_{bg}$$$SS_{wg} = SS_A + SS_B + SS_C + SS_D$$$$SS_{bg} = SS_T - SS_{wg}$$（4）计算相关自由度$$df_{bg} = k - 1 = 4 - 1 = 3$$$$df_{wg} = (N_A - 1) + (N_B - 1) + (N_C - 1) + (N_D - 1)$$（5）计算 $MS_{bg}$以及$MS_{wg}$$$MS_{bg} = \frac{SS_{bg}}{df_{bg}}$$$$MS_{wg} = \frac{SS_{wg}}{df_{wg}}$$（6）计算$F$最后得出F = 6.42 （df = 3, 16）代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269import osimport xlrdimport numpy as npimport scipy.stats as statsdef GetChildDetailLab(combine=True): """ Get the detail labs of each child data. Returns: child_detail_lab: the detail labs. """ CHILDS_FEATURE_LAB = ['tid', 'age_m', 'female', 'onlychild', 'divorce', 'medu_newcat', 'income_newcat', 'scr_ave', 'edu_ave', 'edu_of_scr', 'scr_h_cat', 'mediacoview', 'mediacontact'] CHILDS_CAT_LAB = ['emo_cat', 'con_cat', 'hyp_cat', 'pee_cat', 'difficulties_cat', 'pro_cat'] if combine: CHILDS_DETAIL_LAB = CHILDS_FEATURE_LAB CHILDS_DETAIL_LAB.extend(CHILDS_CAT_LAB) return CHILDS_DETAIL_LAB else: return CHILDS_FEATURE_LAB, CHILDS_CAT_LABdef ReadChildInfoFromExcel( file_name='屏幕暴露与SDQ.xlsx', sheet_name='data'): """ Read the screen-exposed vs.SDQ detail information of each child from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: child_scr_exp_sdq: A list that contains the detail information of each child. labs: The lab corresponds to each colume of the data. """ CHILDS_FILE_NAME = 'child_scr_exp_sdq.npy' CHILDS_DETAIL_LAB = GetChildDetailLab() # print(CHILDS_DETAIL_LAB) NOT_INT_LAB_INDEIES = [CHILDS_DETAIL_LAB.index('age_m'), CHILDS_DETAIL_LAB.index('scr_ave'), CHILDS_DETAIL_LAB.index('edu_ave'), CHILDS_DETAIL_LAB.index('edu_of_scr')] child_scr_exp_sdq = [] if (os.path.isfile(CHILDS_FILE_NAME)): with open(CHILDS_FILE_NAME, 'rb') as f: child_scr_exp_sdq = np.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): child_row = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = str(cel.value) # tolerant the value error if val == '': val = '-1.0' except Exception as e: print(e) # because of the type is different try: if col in NOT_INT_LAB_INDEIES: val = float(val) else: # in Excel, if cel.value is 1, then str(cel.value) is # '1.0' val = val.split('.')[0] val = int(val) except Exception as e: print(e) val = -1 child_row.append(val) child_scr_exp_sdq.append(child_row) child_scr_exp_sdq = np.array(child_scr_exp_sdq) with open(CHILDS_FILE_NAME, 'wb') as f: np.save(f, child_scr_exp_sdq) return child_scr_exp_sdq, CHILDS_DETAIL_LABdef SplitDataSet(data_set, feature, cat): """ Split the data set to two column, that is feature and cat Arguments: data_set: the source data set. feature: the input vector cat: the correspond category. Returns: splited_data_set: self-explation. """ CHILDS_DETAIL_LAB = GetChildDetailLab() feature_index = CHILDS_DETAIL_LAB.index(feature) cat_index = CHILDS_DETAIL_LAB.index(cat) # print(feature_index, cat_index) return data_set[:, (feature_index, cat_index)]def CalChi2(data_set): """ Calculate the chi-square value and p-value corresponds to the data set. Arguments: data_set: the object data set. Returns: chi2: the chi-square value. p: the p-value. """ rows_number = len(set(data_set[:, -1])) columns_number = len(set(data_set[:, 0])) # print(rows_number, columns_number) counts = np.zeros((rows_number, columns_number)) for row in data_set: if row[-1] != -1: if row[0] != -1: try: counts[int(row[-1])][int(row[0])] += 1 except: pass # drop the row that all item is 0 del_row_index = [] for index, count in enumerate(counts): if not count.any(): del_row_index.append(index) counts = np.delete(counts, tuple(del_row_index), axis=0) # drop the column that all item is 0 del_col_index = [] for index, count in enumerate(counts.T): if not count.any(): del_col_index.append(index) counts = np.delete(counts, tuple(del_col_index), axis=1) # print(counts) # calculate the chi-square value and correspond p-value chi2, p, dof, excepted = stats.chi2_contingency(counts) return chi2, pdef ANOVATest(data_set): """ Implement a ANOVA test on the data set. Arguments: data_set: the object data set. Return: f: The computed F-value of the test. p: The associated p-value from the F-distribution. """ # Initial the three categories normal = [] critical = [] abnormal = [] for data in data_set: if data[0] != -1: if data[-1] == 0: normal.append(data[0]) elif data[-1] == 1: critical.append(data[0]) elif data[-1] == 2: abnormal.append(data[0]) f, p = stats.f_oneway(normal, critical, abnormal) return f, pdef GenerateCoffMatrix(data_set): """ Calculate the chi-square and p-value of each feature-category pair. Arguments: data_set: the source data set. Returns: coff_matrix: the final cofficient matrix. """ coff_matrix = &#123;&#125; CHILDS_FEATURE_LAB, CHILDS_CAT_LAB = GetChildDetailLab(combine=False) NOT_KEEP_FEATURE_LAB = ['tid', 'age_m', 'scr_ave', 'edu_ave', 'edu_of_scr'] for feature in CHILDS_FEATURE_LAB: if feature in NOT_KEEP_FEATURE_LAB: if feature != NOT_KEEP_FEATURE_LAB[0]: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) f, p = ANOVATest(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (f, p) else: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) chi2, p = CalChi2(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (chi2, p) return coff_matrixdef SiftRelativeFeature(coff_matrix, conf=1e-5): """ Sift the feature that satisfy the caonfident condition. Arguemnts: coff_matrix: the calculated cofficient matrix for all features and categories. conf: the confident. Returns: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. """ relative_feature_matrix = &#123;&#125; for key in coff_matrix.keys(): if coff_matrix[key][-1] &lt;= conf: relative_feature_matrix[key] = coff_matrix[key] return relative_feature_matrixdef WriteResult(coff_matrix, file_name='result.txt'): """ Write the result to file. Arguments: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. file_name: the result file name. """ # Sort sorted_coff_matrix = sorted(coff_matrix.items(), key=lambda item: item[1][-1], reverse=False) # print(sorted_coff_matrix) with open(file_name, 'w') as f: for item in sorted_coff_matrix: f.write(str(item)) f.write('\n')if __name__ == '__main__': data_set, labels = ReadChildInfoFromExcel() # splited_data_set = SplitDataSet(data_set, 'female', 'difficulties_cat') # chi2, p = CalChi2(splited_data_set) coff_matrix = GenerateCoffMatrix(data_set) # relative_feature = SiftRelativeFeature(coff_matrix, 1) WriteResult(coff_matrix)因子分析先说说因子分析与主成分分析的区别，下面的话引自知乎(https://www.zhihu.com/question/24524693)具体实现使用SPSS软件进行实现：操作步骤如下(引自 SPSS数据分析从入门到精通-陈胜可)：实验结果如下：2017.2.27 Updates决策树1234567import pandas as pdfrom sklearn import treeimport numpy as npfrom sklearn.model_selection import cross_val_scorefrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"Read data1data = pd.read_excel('./scr_SDQ.xlsx', sheetname='data', index_col=0).dropna().sort_index()Convert the data format1234float_columns = ['scr_ave', 'edu_ave','scr_of_edu']for column in data.columns: if column not in float_columns: data[column] = data[column].astype(int)Split the data123data_feature_with_edu_ave = data.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_feature_with_scr_of_edu = data.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_classes = data.ix[:, 'emo_cat':]12# dtree = tree.DecisionTreeClassifier(min_samples_leaf=500)# cross_val_score(dtree,data_feature_with_edu_ave, data_classes['difficulties_cat'], cv=10)array([ 0.64356436, 0.64356436, 0.64391855, 0.64391855, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713]) difficulties_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_diff.pngwith open('dtree_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_diff.pngwith open('dtree_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.553782medu_newcat0.217218age_m0.085820income_newcat0.048912edu_ave0.029740female0.026161onlychild0.021336mediacontact0.017030divorce0.000000scr_h_cat0.000000mediacoview0.000000Impscr_ave0.525587medu_newcat0.213067age_m0.084708scr_of_edu0.080575mediacontact0.029232female0.025661onlychild0.020929income_newcat0.020240divorce0.000000scr_h_cat0.000000mediacoview0.000000emo_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['emo_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['emo_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_emo.pngwith open('dtree_with_edu_ave_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_emo.pngwith open('dtree_with_scr_of_edu_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.521694medu_newcat0.159798income_newcat0.147978edu_ave0.083233age_m0.040750female0.023781mediacontact0.019127mediacoview0.003639onlychild0.000000divorce0.000000scr_h_cat0.000000Impscr_ave0.520711income_newcat0.155111medu_newcat0.146762scr_of_edu0.115745age_m0.023767female0.021924mediacontact0.011410mediacoview0.004571onlychild0.000000divorce0.000000scr_h_cat0.000000con_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['con_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['con_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_con.pngwith open('dtree_with_edu_ave_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_con.pngwith open('dtree_with_scr_of_edu_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.562325medu_newcat0.098819mediacontact0.095089edu_ave0.077572female0.064213income_newcat0.056334age_m0.023696onlychild0.013513mediacoview0.008440divorce0.000000scr_h_cat0.000000Impscr_ave0.511282scr_of_edu0.110155medu_newcat0.097473mediacontact0.093793female0.082843income_newcat0.055566age_m0.035552onlychild0.013335divorce0.000000scr_h_cat0.000000mediacoview0.000000hyp_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['hyp_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['hyp_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_hyp.pngwith open('dtree_with_edu_ave_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_hyp.pngwith open('dtree_with_scr_of_edu_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impmedu_newcat0.343448scr_ave0.265652onlychild0.165487edu_ave0.076194income_newcat0.054466age_m0.046774mediacontact0.020992mediacoview0.015532female0.011454divorce0.000000scr_h_cat0.000000Impmedu_newcat0.335192scr_ave0.257218onlychild0.161509scr_of_edu0.105471age_m0.065618income_newcat0.033129mediacontact0.022542mediacoview0.012901female0.006421divorce0.000000scr_h_cat0.000000pee_cat1234567891011121314151617dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pee_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pee_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pee.pngwith open('dtree_with_edu_ave_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pee.pngwith open('dtree_with_scr_of_edu_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.441246income_newcat0.210646female0.152568age_m0.151888medu_newcat0.033530mediacoview0.009625edu_ave0.000498onlychild0.000000divorce0.000000scr_h_cat0.000000mediacontact0.000000Impscr_ave0.430946income_newcat0.200188age_m0.151286female0.139104medu_newcat0.033289scr_of_edu0.033259mediacoview0.011927onlychild0.000000divorce0.000000scr_h_cat0.000000mediacontact0.000000pro_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pro_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pro_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pro.pngwith open('dtree_with_edu_ave_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pro.pngwith open('dtree_with_scr_of_edu_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impfemale0.295389scr_ave0.252091mediacontact0.145010age_m0.137453income_newcat0.085421edu_ave0.062697medu_newcat0.021938onlychild0.000000divorce0.000000scr_h_cat0.000000mediacoview0.000000Impfemale0.299316scr_ave0.204496mediacontact0.131972age_m0.125169scr_of_edu0.111909income_newcat0.080072medu_newcat0.047066onlychild0.000000divorce0.000000scr_h_cat0.000000mediacoview0.000000Convert continuous variables to categorical variables1234data_category = data.copy()for column in data_category.columns: if column in float_columns: data_category[column] = pd.cut(data_category[column], 10, labels=np.arange(10))123data_cate_feature_with_edu_ave = data_category.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_cate_feature_with_scr_of_edu = data_category.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_cate_classes = data.ix[:, 'emo_cat':]12dtree_cate = tree.DecisionTreeClassifier(min_samples_leaf=50)cross_val_score(dtree_cate,data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'], cv=10).sum() / 100.63948514409153423 12345678910111213141516dtree_cate_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_edu_ave = dtree_with_edu_ave.fit(data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_cate_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_scr_of_edu = dtree_cate_with_scr_of_edu.fit(data_cate_feature_with_scr_of_edu, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_cate_with_edu_ave_diff.pngwith open('dtree_cate_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_edu_ave, out_file=dot_file, feature_names=data_cate_feature_with_edu_ave.columns)# dtree_cate_with_scr_of_edu_diff.pngwith open('dtree_cate_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_scr_of_edu, out_file=dot_file, feature_names=data_cate_feature_with_scr_of_edu.columns)Impscr_h_cat0.528976medu_newcat0.221779age_m0.092168income_newcat0.057032onlychild0.034724mediacontact0.027193female0.021036scr_ave0.011653mediacoview0.005439divorce0.000000edu_ave0.000000Impscr_h_cat0.507678medu_newcat0.215210age_m0.089438scr_of_edu0.042832mediacontact0.041463income_newcat0.035742onlychild0.033695female0.020413scr_ave0.013527divorce0.000000mediacoview0.000000]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Get Pois uses God-map apis]]></title>
      <url>%2F2017%2F02%2F21%2FGet-Pois-uses-God-map-apis%2F</url>
      <content type="text"><![CDATA[伪代码如下：12345从Excel文件中读出数据对于每一个house: 提取出其location字段（经纬度） 将location字段作为输入参数传给map api 将返回值进行适当筛选最后存入原数据集中代码实现如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import osimport xlrdimport pickleimport requestsdef ReadHousesInfoFromExcel( file_name='houses_nadrop.xls', sheet_name='小区信息'): """ Read the houses detail information from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: houses: A dict that contains the detail information of each house. """ HOUSES_FILE_NAME = 'houses.pkl' HOUSES_DETAIL_TAB = ['name', 'address', 'property_category', 'area', 'avg_price', 'location', 'property_costs', 'volume_rate', 'green_rate'] houses = [] if (os.path.isfile(HOUSES_FILE_NAME)): with open(HOUSES_FILE_NAME, 'rb') as f: houses = pickle.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): house = &#123;&#125; for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass val = str(val) house[HOUSES_DETAIL_TAB[col]] = val houses.append(house) with open(HOUSES_FILE_NAME, 'wb') as f: pickle.dump(houses, f) return housesdef Geocode(location, poi_type): """ A tool that call the God-Map api. Arguments: location: The location of house. poi_type: The poi type. Returns: answer: The JSON-type data that contains pois infomation. """ location = str(location).strip() parameters = &#123;'location': location, 'key': 'e798a5bfb344a09977b79552ae415974', 'types': poi_type, 'offset': 10, 'page': 1, 'extensions': 'base'&#125; base = 'http://restapi.amap.com/v3/place/around' try: response = requests.get(base, parameters) answer = response.json() except Exception as e: print('error!', e) answer = 'null' finally: pass return answerdef GetPOI(houses): """ Get the pois information of the houses according to the location. Arguments: houses: The house detail information. Returns: houses_with_pois: The house detail information that contains the pois information. """ POI_TYPE_LAB = ['subway_station', 'bus_station', 'parking_lot', 'primary_school', 'secondary_school', 'university', 'mall', 'park'] POI_TYPE_CODE = ['150500', '150700', '150904', '141203', '141202', '141201', '060100', '110101'] KEEP_INFO_LAB = ['name', 'location', 'distance'] NO_INFO_NOW = '-' SIZE = len(houses) houses_with_pois = houses.copy() count = 0 for house in houses_with_pois: count = count + 1 if count % 100 == 0: print(count, '', SIZE) house['pois'] = &#123;&#125; for poi_type_index in range(len(POI_TYPE_LAB)): poi_info_json = Geocode(house['location'], POI_TYPE_CODE[poi_type_index]) if poi_info_json == 'null' or poi_info_json['pois'] is None: house['pois'][POI_TYPE_LAB[poi_type_index]] = NO_INFO_NOW else: house['pois'][POI_TYPE_LAB[poi_type_index]] = [] for poi in poi_info_json['pois']: pois_without_useless = &#123;&#125; for key in poi.keys(): if key in KEEP_INFO_LAB: pois_without_useless[key] = poi[key] house['pois'][POI_TYPE_LAB[poi_type_index]].append( pois_without_useless) # return houses_with_pois return houses_with_poisif __name__ == '__main__': houses = ReadHousesInfoFromExcel() # answer = Geocode(houses[0]['location'], '150905') houses_with_pois = GetPOI(houses)总结一下有几个注意点：传给parameters的location参数的格式一定要规范，前后都不能有空格for循环中不能改变字典的大小，这里的大小不仅指其元素的数目，也包括其总占用空间的大小注意pickle的用法从Excel中读出的内容要转成str格式整个过程十分清晰明了，值得注意的是细节问题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch02]]></title>
      <url>%2F2017%2F02%2F19%2FPython%20data%20analysis-Learning%20notes-ch02%2F</url>
      <content type="text"><![CDATA[利用Python内置的JSON模块对数据进行解析并转化为字典数据如下：123456'&#123; "a": "Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/535.11 (KHTML, like Gecko)Chrome\\/17.0.963.78 Safari\\/535.11", "c": "US", "nk": 1, "tz": "America\\/New_York", "gr":"MA", "g": "A6qOVH", "h": "wfLQtf", "l": "orofrog", "al": "en-US,en;q=0.8", "hh": "1.usa.gov","r": "http:\\/\\/www.facebook.com\\/l\\/7AQEFzjSi\\/1.usa.gov\\/wfLQtf", "u":"http:\\/\\/www.ncbi.nlm.nih.gov\\/pubmed\\/22415991", "t": 1331923247, "hc": 1331822918,"cy": "Danvers", "ll": [ 42.576698, -70.954903 ] &#125;\n'核心代码：123import jsonpath = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt'records = [json.loads(line) for line in open(path)]123456789101112131415161718records[0]-----------------------------------&#123;'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'al': 'en-US,en;q=0.8', 'c': 'US', 'cy': 'Danvers', 'g': 'A6qOVH', 'gr': 'MA', 'h': 'wfLQtf', 'hc': 1331822918, 'hh': '1.usa.gov', 'l': 'orofrog', 'll': [42.576698, -70.954903], 'nk': 1, 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 't': 1331923247, 'tz': 'America/New_York', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991'&#125;对时区字段进行计数（pure python vs. pandas）首先从记录中提取时区字段并且放入一个列表中1time_zones = [rec['tz'] for rec in records if 'tz' in rec]123456789101112time_zones[:10]-----------------------------------['America/New_York', 'America/Denver', 'America/New_York', 'America/Sao_Paulo', 'America/New_York', 'America/New_York', 'Europe/Warsaw', '', '', '']使用纯粹的python进行计数12345678def get_counts(sequence): counts = &#123;&#125; for x in sequence: if x in counts: counts[x] += 1 else: counts[x] = 1 return counts使用下列方法更加简洁1234567from collections import defaultdictdef get_counts2(sequence): counts = defaultdict(int) # values will initialize to 0 for x in sequence: counts[x] += 1 return counts如果需要返回前十位的时区及其计数值1234def top_counts(count_dict, n=10): value_key_pairs = [(count, tz) for tz, count in count_dict.items()] value_key_pairs.sort() return value_key_pairs[-n:]123456789101112top_counts(counts)--------------------------------------[(33, 'America/Sao_Paulo'), (35, 'Europe/Madrid'), (36, 'Pacific/Honolulu'), (37, 'Asia/Tokyo'), (74, 'Europe/London'), (191, 'America/Denver'), (382, 'America/Los_Angeles'), (400, 'America/Chicago'), (521, ''), (1251, 'America/New_York')]可以使用python自带的库1from collections import Counter1counts = Counter(time_zones)123456789101112counts.most_common(10)--------------------------------[('America/New_York', 1251), ('', 521), ('America/Chicago', 400), ('America/Los_Angeles', 382), ('America/Denver', 191), ('Europe/London', 74), ('Asia/Tokyo', 37), ('Pacific/Honolulu', 36), ('Europe/Madrid', 35), ('America/Sao_Paulo', 33)]使用pandas进行相同的任务pandas中主要的数据结构是DataFrame， 作用是将数据表示成表格12345from pandas import DataFrame, Seriesimport pandas as pdframe = DataFrame(records)frame12345678910111213frame['tz'][:10]-------------------------------0 America/New_York1 America/Denver2 America/New_York3 America/Sao_Paulo4 America/New_York5 America/New_York6 Europe/Warsaw7 8 9 Name: tz, dtype: object计数·1234567891011121314tz_counts = frame['tz'].value_counts()tz_counts[:10]--------------------------------------------------America/New_York 1251 521America/Chicago 400America/Los_Angeles 382America/Denver 191Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35America/Sao_Paulo 33Name: tz, dtype: int64填补缺失值以及未知值12345678910111213141516clean_tz = frame['tz'].fillna('Missing')clean_tz[clean_tz == ''] = 'Unknown'tz_counts = clean_tz.value_counts()tz_counts[:10]----------------------------------------------America/New_York 1251Unknown 521America/Chicago 400America/Los_Angeles 382America/Denver 191Missing 120Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35Name: tz, dtype: int64画个图展示一下12plt.figure(figsize=(10, 4))tz_counts[:10].plot(kind='barh', rot=0)下面我们对用户使用的浏览器的信息做一些操作Series应该代表的是DataFrame中的一列123456789results = Series([x.split()[0] for x in frame.a.dropna()])results[:5]---------------------------------------------------0 Mozilla/5.01 GoogleMaps/RochesterNY2 Mozilla/4.03 Mozilla/5.04 Mozilla/5.0dtype: object同样可以进行计数1234567891011results.value_counts()[:8]-----------------------------------------Mozilla/5.0 2594Mozilla/4.0 601GoogleMaps/RochesterNY 121Opera/9.80 34TEST_INTERNET_AGENT 24GoogleProducer 21Mozilla/6.0 5BlackBerry8520/5.0.0.681 4dtype: int64根据Windows和Non-Windows用户进行时区的分组操作1cframe = frame[frame.a.notnull()]123456operating_system = np.where(cframe['a'].str.contains('Windows'), 'Windows', 'Not Windows')operating_system[:5]-----------------------------------------------------------------array(['Windows', 'Not Windows', 'Windows', 'Not Windows', 'Windows'], dtype='&lt;U11')1by_tz_os = cframe.groupby(['tz', operating_system])来看看这个by_tz_os长什么样1by_tz_os.size()再来看看unstack()的炫酷效果排下序， 看看排名多少12345678910111213141516# Use to sort in ascending orderindexer = agg_counts.sum(1).argsort()indexer[:10]------------------------------------------------tz 24Africa/Cairo 20Africa/Casablanca 21Africa/Ceuta 92Africa/Johannesburg 87Africa/Lusaka 53America/Anchorage 54America/Argentina/Buenos_Aires 57America/Argentina/Cordoba 26America/Argentina/Mendoza 55dtype: int64取出前十的来看看12count_subset = agg_counts.take(indexer)[-10:]count_subset同样画个图1count_subset.plot(kind='barh', stacked=True)看看两个类别所占的比例是多少12normed_subset = count_subset.div(count_subset.sum(1), axis=0)normed_subset.plot(kind='barh', stacked=True)电影评分数据表连接操作123456789101112131415import pandas as pdimport osencoding = 'latin1'upath = os.path.expanduser('ch02/movielens/users.dat')rpath = os.path.expanduser('ch02/movielens/ratings.dat')mpath = os.path.expanduser('ch02/movielens/movies.dat')unames = ['user_id', 'gender', 'age', 'occupation', 'zip']rnames = ['user_id', 'movie_id', 'rating', 'timestamp']mnames = ['movie_id', 'title', 'genres']users = pd.read_csv(upath, sep='::', header=None, names=unames, encoding=encoding)ratings = pd.read_csv(rpath, sep='::', header=None, names=rnames, encoding=encoding)movies = pd.read_csv(mpath, sep='::', header=None, names=mnames, encoding=encoding)看看数据长什么样1users[:5]1ratings[:5]1movies[:5]多表连接12data = pd.merge(pd.merge(ratings, users), movies)data12345678910111213data.ix[0]--------------------------------------------user_id 1movie_id 1193rating 5timestamp 978300760gender Fage 1occupation 10zip 48067title One Flew Over the Cuckoo's Nest (1975)genres DramaName: 0, dtype: object根据性别计算每部电影的平均评分123mean_ratings = data.pivot_table('rating', index='title', columns='gender', aggfunc='mean')mean_ratings[:5]过滤掉评分数小于250的电影1ratings_by_title = data.groupby('title').size()123456789ratings_by_title[:5]-----------------------------------------title$1,000,000 Duck (1971) 37'Night Mother (1986) 70'Til There Was You (1997) 52'burbs, The (1989) 303...And Justice for All (1979) 199dtype: int641active_titles = ratings_by_title.index[ratings_by_title &gt;= 250]12345678active_titles[:10]-----------------------------------------Index([''burbs, The (1989)', '10 Things I Hate About You (1999)', '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)', '13th Warrior, The (1999)', '2 Days in the Valley (1996)', '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)', '2010 (1984)'], dtype='object', name='title')ix应该是一个交集操作12mean_ratings = mean_ratings.ix[active_titles]mean_ratings按照女性最喜欢的电影进行降序排序12top_female_ratings = mean_ratings.sort_values(by='F', ascending=False)top_female_ratings[:10]​US Baby Names 1880-2010123import pandas as pdnames1880 = pd.read_csv('ch02/names/yob1880.txt', names=['name', 'sex', 'births'])names1880把所有年份的数据合并一下123456789101112131415# 2010 is the last available year right nowyears = range(1880, 2011)pieces = []columns = ['name', 'sex', 'births']for year in years: path = 'ch02/names/yob%d.txt' % year frame = pd.read_csv(path, names=columns) frame['year'] = year pieces.append(frame)# Concatenate everything into a single DataFramenames = pd.concat(pieces, ignore_index=True)进行聚合操作12total_births = names.pivot_table('births', index='year', columns='sex', aggfunc=sum)1total_births.tail()计算一下每个名字的出生比例1234567def add_prop(group): # Integer division floors births = group.births.astype(float) group['prop'] = births / births.sum() return groupnames = names.groupby(['year', 'sex']).apply(add_prop)1names进行一下有效性检查123np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)--------------------------------------------True筛选出每一对year/sex下总数前1000的名字1234def get_top1000(group): return group.sort_values(by='births', ascending=False)[:1000]grouped = names.groupby(['year', 'sex'])top1000 = grouped.apply(get_top1000)加个索引，结合了numpy1top1000.index = np.arange(len(top1000))Analyzing naming trends将数据分为男女12boys = top1000[top1000.sex == 'M']girls = top1000[top1000.sex == 'F']计算每一年每个名字的出生总数123total_births = top1000.pivot_table('births', index='year', columns='name', aggfunc=sum)total_births选出几个名字看看总数随年份的变化情况123subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]subset.plot(subplots=True, figsize=(12, 10), grid=False, title="Number of births per year")Measuring the increase in naming diversity通过统计前1000项名字所占的比例来判断多样性的变化1234table = top1000.pivot_table('prop', index='year', columns='sex', aggfunc=sum)table.plot(title='Sum of table1000.prop by year and sex', yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))另一种方法，计算占出生人数50%的名字的数量也即从开始累加，看加到第几个名字时所占比例为50%先来看看2010年的男孩1df = boys[boys.year == 2010]1234567891011121314prop_cumsum = df.sort_index(by='prop', ascending=False).prop.cumsum()prop_cumsum[:10]--------------------------------------------------260877 0.011523260878 0.020934260879 0.029959260880 0.038930260881 0.047817260882 0.056579260883 0.065155260884 0.073414260885 0.081528260886 0.089621Name: prop, dtype: float64看来是第116个，不过序号从0开始，应该是117123prop_cumsum.values.searchsorted(0.5)---------------------------------------------------116再来看看1900年的男孩儿12345df = boys[boys.year == 1900]in1900 = df.sort_index(by='prop', ascending=False).prop.cumsum()in1900.values.searchsorted(0.5) + 1---------------------------------------------------25所以这样做是可行的把相同的操作赋予整个数据集123456def get_quantile_count(group, q=0.5): group = group.sort_values(by='prop', ascending=False) return group.prop.cumsum().values.searchsorted(q) + 1diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)diversity = diversity.unstack('sex')diversity.head()1diversity.plot(title="Number of popular names in top 50%")The “Last letter” Revolution取出每个名字对应的最后一个字母，同时序号对应1234567# extract last letter from name columnget_last_letter = lambda x: x[-1]last_letters = names.name.map(get_last_letter)last_letters.name = 'last_letter'table = names.pivot_table('births', index=last_letters, columns=['sex', 'year'], aggfunc=sum)单独取出三年的来看看12subtable = table.reindex(columns=[1910, 1960, 2010], level='year')subtable.head()计算一下字母比例12345678910subtable.sum()-------------------------------------sex yearF 1910 396416.0 1960 2022062.0 2010 1759010.0M 1910 194198.0 1960 2132588.0 2010 1898382.0dtype: float641letter_prop = subtable / subtable.sum().astype(float)123456import matplotlib.pyplot as pltfig, axes = plt.subplots(2, 1, figsize=(10, 8))letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female', legend=False)最后看一下所有的年份并生成一个趋势图12letter_prop = table / table.sum().astype(float)dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T1dny_ts.plot()Boy names that became girl names (and vice versa)以lesl开头的名字为例123456all_names = top1000.name.unique()mask = np.array(['lesl' in x.lower() for x in all_names])lesley_like = all_names[mask]lesley_like----------------------------------------------array(['Leslie', 'Lesley', 'Leslee', 'Lesli', 'Lesly'], dtype=object)从原数据集中筛选出来12345678910filtered = top1000[top1000.name.isin(lesley_like)]filtered.groupby('name').births.sum()----------------------------------------------nameLeslee 1082Lesley 35022Lesli 929Leslie 370429Lesly 10067Name: births, dtype: int64做一下聚合操作并计算比例1234table = filtered.pivot_table('births', index='year', columns='sex', aggfunc='sum')table = table.div(table.sum(1), axis=0)table.tail()看一下趋势1table.plot(style=&#123;'M': 'k-', 'F': 'k--'&#125;)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spider the house infomation and save to excel file]]></title>
      <url>%2F2017%2F02%2F18%2FSpider-the-house-infomation-and-save-to-excel-file%2F</url>
      <content type="text"><![CDATA[数据来源http://sh.fang.com/项目目标爬取二手房信息中的小区信息实现步骤【1】爬取小区信息（核心代码，下同）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150""" &lt;A spider to crawl the house information.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;. """class HouseSpider(scrapy.Spider): name = "house" head = "http://esf.sh.fang.com" allowed_domains = ["sh.fang.com"] start_urls = [ "http://esf.sh.fang.com/housing/" ] # 各区对应的编号（由于丧心病狂的url） area_map = &#123;25: 'pudong', 18: 'minhang', 19: 'xuhui', 30: 'baoshan', 28: 'putuo', 20: 'changning', 26: 'yangpu', 586: 'songjiang', 29: 'jiading', 23: 'hongkou', 27: 'zhabei', 21: 'jingan', 24: 'huangpu', 22: 'luwan', 31: 'qingpu', 32: 'fengxian', 35: 'jinshan', 996: 'chongming'&#125; estate_to_area_map = &#123;&#125; seperator = '=\n' def __init__(self): for key, value in self.area_map.items(): self.estate_to_area_map[key] = [] # print(self.estate_to_area_map) def parse(self, response): # 解析出上海市各区的地址 area_lis = response.xpath('//*[@id="houselist_B03_02"]/div[1]') for a in area_lis.xpath('./a'): # areas = items.AreaItem() # areas['name'] = a.xpath('text()').extract()[0] yield Request(self.head + a.xpath('@href').extract()[0], callback=self.parse_area) # print(a.xpath('text()').extract()[0]) def parse_area(self, response): # 确定response来源于哪一个区 area_index = str(response).split('/')[-2].split('_')[0] if area_index == '': return else: # 解析出各区中小区的详情页面地址 detail_str = 'xiangqing' estate_list = response.xpath('/html/body/div[4]/div[5]/div[4]') for a in estate_list.xpath('.//a[@class="plotTit"]'): estate_url = a.xpath('@href').extract()[0] if estate_url.find('esf') != -1: estate_url = estate_url.replace('esf', detail_str) else: estate_url = estate_url + detail_str if estate_url.find('http') != -1: # print(estate_url) self.estate_to_area_map[int(area_index)].append(estate_url) # print(len(self.estate_to_area_map[int(area_index)])) next_page = response.xpath('//*[@id="PageControl1_hlk_next"]') if len(next_page) != 0: yield Request(self.head + next_page.xpath('@href').extract()[0], callback=self.parse_area) else: # print(len(self.estate_to_area_map[int(area_index)])) for url in self.estate_to_area_map[int(area_index)]: request = Request(url, callback=self.parse_house, dont_filter=True) request.meta['index'] = int(area_index) yield request def parse_house(self, response): flag = 0 area_index = response.meta['index'] area_name = self.area_map[area_index] filename = area_name + '.txt' # print(response.xpath('/html')) # 详情页面存在两种，因此分情况讨论 house_name = response.xpath( '/html/body/div[4]/div[2]/div[2]/h1/a/text()') if len(house_name) == 0: # house_name = response.xpath( # '/html/body/div[1]/div[3]/div[2]/h1/a/text()') # flag = 1 return house_name = house_name.extract()[0] # 清洁小区名 house_name = re.sub(r'小区网', '', house_name) result_str = '【小区名称】' + house_name + '\n' if flag == 0: avg_price_xpath = response.xpath( '/html/body/div[4]/div[4]/div[1]/div[1]/dl[1]/dd/span/text()') avg_price = avg_price_xpath.extract()[0] result_str = result_str + '【平均价格】' + avg_price + '\n' detail_block_list = response.xpath( '/html/body/div[4]/div[4]/div[1]') for headline in detail_block_list.xpath('.//h3'): head_str = headline.xpath('./text()').extract()[0] if head_str == '基本信息': result_str = result_str + \ '【' + \ head_str + '】\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): if len(item.xpath('./strong/text()')) != 0: if len(item.xpath('./text()')) != 0: result_str = result_str + \ item.xpath( './strong/text()').extract()[0] result_str = result_str + \ item.xpath('./text()').extract()[0] + '\n' # print(result_str) # elif head_str == '交通状况': # result_str = result_str + \ # '【' + \ # head_str + '】\n' # tempstr = headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt/text()').extract()[0] # result_str = result_str + tempstr + '\n' # # print(result_str) # elif head_str == '周边信息': # result_str = result_str + \ # '【' + \ # head_str + '】\n' # for item in headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt'): # result_str = result_str + \ # item.xpath('./text()').extract()[0] + '\n' # # print(result_str) elif head_str == '就近楼群': result_str = result_str + \ '【' + \ head_str + '】\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): result_str = result_str + \ item.xpath('./a/text()').extract()[0] + '\n' result_str = result_str + self.seperator # print(result_str) with open(filename, 'a', errors='ignore') as f: f.write(result_str)【2】格式化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136""" &lt;A formatter&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def GetDataFileList(path='.'): """ Get the houses file list. Arguments: path: Dir path. Returns: file_list: the list of data file that find houses data. """ file_list = [x for x in os.listdir(path) if os.path.isfile( x) and os.path.splitext(x)[1] == '.txt'] return file_listdef Parse(file_list): """ Parse the txt file that find houses data. Extract some import infomation such as house name, avarage price, address and so on. Arguments: file_list: the list of data file that find houses data. Returns: houses_dict_list: the list that each item find the detail dict of each house. """ HOUSE_NAME = '小区名称' HOUSE_NAME_SPLITOR = '】' HOUSE_ADDRESS = '小区地址' HOUSE_ADDRESS_SPLITOR = '：' HOUSE_AVG_PRICE = '平均价格' HOUSE_AVG_PRICE_SPLITOR = '】' AREA_OF_HOUSE_BELONGS_TO = '所属区域' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1 = '：' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2 = ' ' PROPERTY_CATEGORY = '物业类别' PROPERTY_CATEGORY_SPLITOR = '：' GREEN_RATE = '绿 化 率' GREEN_RATE_SPLITOR = '：' VOLUME_RATE = '容 积 率' VOLUME_RATE_SPLITOR = '：' PROPERTY_COSTS = '物 业 费' PROPERTY_COSTS_SPLITOR = '：' NO_INFO_NOW = '暂无信息' DETAIL_LIST = [HOUSE_NAME, HOUSE_AVG_PRICE, HOUSE_ADDRESS, AREA_OF_HOUSE_BELONGS_TO, PROPERTY_CATEGORY, GREEN_RATE, VOLUME_RATE, PROPERTY_COSTS] houses_dict_list = [] for file_name in file_list: raw_houses_string = '' # read all lines as a string with open(file_name, 'r', errors='ignore') as f: for line in f.readlines(): raw_houses_string += line # split the string to the houses raw info list raw_houses_list = raw_houses_string.split('=\n') raw_houses_details_list = [] for raw_house in raw_houses_list: # format house raw info to lines raw_houses_details = raw_house.split('\n')[:-1] if len(raw_houses_details) == 0: continue # combine the all formated house raw info to a list raw_houses_details_list.append(raw_houses_details) for raw_house_details in raw_houses_details_list: house_details_dict = &#123;&#125; for raw_detail in raw_house_details: # search house name if re.search(HOUSE_NAME, raw_detail): house_details_dict[HOUSE_NAME] = raw_detail.split( HOUSE_NAME_SPLITOR)[-1] # search house avarage price elif re.search(HOUSE_AVG_PRICE, raw_detail): # print(raw_detail) house_details_dict[HOUSE_AVG_PRICE] = raw_detail.split( HOUSE_AVG_PRICE_SPLITOR)[-1] # search house address elif re.search(HOUSE_ADDRESS, raw_detail): house_details_dict[HOUSE_ADDRESS] = raw_detail.split( HOUSE_ADDRESS_SPLITOR)[-1] # search the area of house belongs to elif re.search(AREA_OF_HOUSE_BELONGS_TO, raw_detail): temp_detail_value = raw_detail.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1)[-1] detail_value = temp_detail_value.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2)[0] house_details_dict[AREA_OF_HOUSE_BELONGS_TO] = detail_value # search the property category of house elif re.search(PROPERTY_CATEGORY, raw_detail): house_details_dict[PROPERTY_CATEGORY] = raw_detail.split( PROPERTY_CATEGORY_SPLITOR)[-1] # search the green rate elif re.search(GREEN_RATE, raw_detail): house_details_dict[GREEN_RATE] = raw_detail.split( GREEN_RATE_SPLITOR)[-1] # search the volume rate elif re.search(VOLUME_RATE, raw_detail): house_details_dict[VOLUME_RATE] = raw_detail.split( VOLUME_RATE_SPLITOR)[-1] # search the property costs elif re.search(PROPERTY_COSTS, raw_detail): house_details_dict[PROPERTY_COSTS] = raw_detail.split( PROPERTY_COSTS_SPLITOR)[-1] # Judge if all details are contained. # If not, set to null. house_details_dict_keys = house_details_dict.keys() for detail_name in DETAIL_LIST: if detail_name not in house_details_dict_keys: house_details_dict[detail_name] = NO_INFO_NOW houses_dict_list.append(house_details_dict) return houses_dict_list【3】通过高德地图api获取经纬度信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100""" &lt;A toolto transfer position.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def Geocode(address): """ A tool that call the God-Map api. Arguments: address: the address to transfer. Returns: location: the transfered location. """ CITY_NAME = '上海' parameters = &#123;'address': address, 'key': 'your key', 'city': CITY_NAME&#125; base = 'http://restapi.amap.com/v3/geocode/geo' try: response = requests.get(base, parameters) except Exception as e: print('error!', e) finally: pass answer = response.json() return answerdef GETGodMapLocation(houses): """ Get the location that corresponds to the house name. Use the God-Map api to get the corresponding location. Arguments: houses_dict_list: the houses info. Returns: houses_dict_list_contains_loc: the houses info that contains the location info. """ HOUSE_NAME = '小区名称' HOUSE_LOCATION = '经纬度' NO_INFO_NOW = '暂无信息' houses_dict_list = houses.copy() error_count = 0 count = 0 size = len(houses) for house_dict in houses_dict_list: # Count count = count + 1 # Loading needs if count % 1000 == 0: print(count, '/', size) address = house_dict[HOUSE_NAME] answer = Geocode(address) # print(answer) # If find if len(answer['geocodes']) != 0: # print(address + "的经纬度：", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # remaking the invalid address # print('address remaking...') if re.search(r'别墅', address): re.sub(r'别墅', '', address) else: address = address + '小区' # print('retransfering...') # transfer again answer = Geocode(address) if len(answer['geocodes']) != 0: # print(address + "的经纬度：", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # print(address) error_count += 1 house_dict[HOUSE_LOCATION] = NO_INFO_NOW print('error counts: ', error_count) return houses_dict_list【4】存储成excel文件1234567891011121314151617181920212223242526272829303132333435363738394041424344""" &lt;A tool to save the excel file.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.""" def Save2ExcelFile(houses): """ Save the python based list file to excel file. Arguments: houses: the houses list. """ houses_dict_list = houses.copy() house_list = [] # format the source data to fit the xlwt package keys = houses[0].keys() for key in keys: house = [] house.append(key) for house_dict in houses_dict_list: house.append(house_dict[key]) house_list.append(house) # return house_list xls = ExcelWrite.Workbook() sheet = xls.add_sheet('小区信息') for i in range(len(house_list)): for j in range(len(house_list[0])): sheet.write(j, i, house_list[i][j]) xls.save('houses.xls')结果展示]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python_god_web_api]]></title>
      <url>%2F2017%2F02%2F17%2Fpython-god-web-api%2F</url>
      <content type="text"><![CDATA[http://lbs.amap.com/api/webservice/guide/api/search123456789101112131415161718#!/usr/bin/env python3#-*- coding:utf-8 -*-'''利用高德地图api实现地址和经纬度的转换'''import requestsdef geocode(address): parameters = &#123;'address': address, 'key': 'e798a5bfb344a09977b79552ae415974'&#125; base = 'http://restapi.amap.com/v3/geocode/geo' response = requests.get(base, parameters) answer = response.json() print(address + "的经纬度：", answer['geocodes'][0]['location'])if __name__=='__main__': #address = input("请输入地址:") address = '北京市海淀区' geocode(address)12345678910111213141516171819202122232425262728293031import xlrddef readXlsx(self, filename='CenterBottom2013.xlsx', sheetname='Sheet1'): rawData = [] if (os.path.isfile(self.fn_rawDat)): with open(self.fn_rawDat, 'rb') as f: self.rawDat = np.load(f) else: workBook = xlrd.open_workbook(filename) bookSheet = workBook.sheet_by_name(sheetname) # 从第二行开始读取，因为第一行有标签 for row in range(1, bookSheet.nrows): rowData = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass if type(val) == float: val = float(val) else: val = str(val) rowData.append(val) rawData.append(rowData) self.rawDat = np.array(rawData) with open(self.fn_rawDat, 'wb') as f: np.save(f, self.rawDat) return self.rawDatRead Excel filesTransfer the address to locaion infoPut back]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Decision tree]]></title>
      <url>%2F2017%2F02%2F15%2FDecision-tree%2F</url>
      <content type="text"><![CDATA[决策树（ID3）决策树的构建构造决策树时，所需要解决的第一个问题就是，每划分一个分支时，应该根据哪一维特征进行划分。这时候我们需要定义某种指标，然后对每一维特征进行该指标的评估，最后选择指标值最高的特征进行划分。划分完毕之后，原始数据集就被划分为几个数据子集。如果某一个下的数据属于同一类型，则算法停止；否则，重复划分过程。伪代码（创建分支）1234567891011createbranch()检测数据集中的每个子项是否属于同一分类: If so return 类标签; Else 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子集 调用函数createBranch并增加返回结果到分支节点中 return 分支节点那么接下来的重点便是如何寻找划分数据集的最好特征，在这里我们使用ID3算法中使用的划分数据集的方法，也即根据熵来划分。信息增益熵划分数据的核心思想是将无序的数据变得更加有序。而一个数据有序程度可以进行量化表示，也就是信息，其度量方式就是熵。其显然，数据集划分前后其所含的信息会发生变化，这个变化便称为信息增益。熵定义为信息的期望，其中信息的定义如下，信息一般针对的对象为多个类别中的某一个类别：$$l(x_i) = -log_{2}p(x_i)$$其中$x_i$表示某一类别，$p(x_i)$表示从多个类别中选择该类别的概率。接下来，熵的定义如下：$$H = \sum_{i=1}^{n}p(x_i)l(x_i)=-\sum_{i=1}^{n}p(x_i)log_{2}p(x_i)$$信息增益定义如下：$$IG(S|T) = H(S) - \sum_{value(T)} \frac{|S_v|}{|S|} H(S_v)$$其中$S$ 为全部样本集合，$value(T) $是属性 $T$所有取值的集合，$v$ 是 $T$ 的其中一个属性值，$S_v$是 $S$ 中属性 $T$ 的值为 $v$ 的样例集合，$|S_v|$ 为 $S_v$ 中所含样例数，$|S|$ 为 $S$ 中所含样例数。代码实现：123456789101112131415161718192021222324252627from math import logdef CalcShannonEnt(data_set): """ Calculate the Shannon Entropy. Arguments: data_set: The object dataset. Returns: shannon_ent: The Shannon entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Shannon entropy shannon_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shannon_ent -= prob * log(prob, 2) return shannon_ent为了进行测试，以及之后的算法运行，我们写一个十分naive的数据生成方法：123456789101112131415def CreateDataSet(): """ A naive data generation method. Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return data_set, labels注意，这里的labels并不代表分类标签，yes以及no才是，labels代表特征名。下面进行一个简单的demo:123456789In [22]: import treesIn [23]: my_dat, labels = trees.CreateDataSet()In [24]: my_datOut[24]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [25]: trees.CalcShannonEnt(my_dat)Out[25]: 0.9709505944546686熵越高，表明数据集中类别数越多。另一个度量无序程度的方法是基尼不纯度（Gini impurity）。基尼不纯度基尼不纯度的定义为，对于每一个节点，从所有类别标签中随机选择一个，选择出来的类别标签与其本身的类别标签不一致的概率之和。形式化地定义如下：$$G = \sum_{i \ne j}p(x_i)p(x_j) = \sum_{i}p(x_i)\sum_{j \ne i}p(x_j) = \sum_{i}p(x_i)(1-p(x_i)) = \sum_{i}p(x_i) - \sum_{i}(p(x_i))^2 = 1 - \sum_{i}(p(x_i))^2$$代码实现如下：1234567891011121314151617181920212223242526def CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impurity同样进行一个简单的demo：123456789In [4]: import treesIn [5]: my_dat, labels = trees.CreateDataSet()In [6]: my_datOut[6]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [7]: trees.CalcGiniImpurity(my_dat)Out[7]: 0.48最后再介绍一种度量无序程度的方式，误分类不纯度。误分类不纯度定义如下：$$M = 1 - \max_{i}(p(x_i))$$代码实现如下：123456789101112131415161718192021222324def CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impurity进行一个简单的demo:12345678910In [25]: reload(trees)Out[25]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [26]: my_dat, labels = trees.CreateDataSet()In [27]: my_datOut[27]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [28]: trees.CalcMisClassifyImpurity(my_dat)Out[28]: 0.4最后用一个图来总结一下这三种不纯度度量的函数图像（以二类情况为例）[Ref: http://www.cse.msu.edu/~cse802/DecisionTrees.pdf]：数据划分根据以上，数据划分的思路是，基于每一维特征的每一个值进行划分，并计算划分前后的信息增益，最后选取增益最大的特征及其所对应的值进行划分，由于这里运用的是ID3算法，因此选择的信息度量方式是熵。代码实现如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set): """ Choose the best feature to split. Arguments: data_set: Object data set. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannonEnt(data_set) best_info_gain = 0.0 best_feature = -1 for i in range(num_features): # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) new_entropy = 0.0 # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannonEnt(sub_data_set) # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_feature由以上代码（ID3算法）可以看出，其计算熵的依据是根据最后一个特征，是否这种naive的选取方式能够达到平均的最好结果？另外，其划分依据仅仅根据划分一次后的子数据集的熵之和，属于一种贪心策略，这样是否可以达到最优解？递归构建决策树既然是递归算法，那么必须设定递归结束条件：遍历完所有属性每个分支下的数据都属于相同的分类这里存在一个问题，如果遍历完所有属性后，某些分支下还是存在多个分类，这种情况下一般采用多数表决的方式，代码实现方式如下：12345678910111213141516171819202122def majority_cnt(class_list): """ Decided the final class. When the splited data is not belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count.iteritems(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0]下面进行树的创建：1234567891011121314151617181920212223242526272829303132333435def CreateTree(data_set, labels): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. Returns: my_tree: A dict that represents the decision tree. """ class_list = [example[-1] for example in data_set] # If the classes are fully same if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Get the next step labels parameter del(labels[best_feat]) # Next step start feat_values = [example[best_feat] for example in data_set] unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_tree下面进行一下简单的测试：123456789In [27]: reload(trees)Out[27]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [28]: my_dat, labels = trees.CreateDataSet()In [29]: my_tree = trees.CreateTree(my_dat, labels)In [30]: my_treeOut[30]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;可见决策树已经构造成功（图形化如下所示），但是这显然不够，我们需要的是用决策树进行分类。决策树分类demo如下：123456789101112131415161718192021In [63]: reload(trees)Out[63]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [64]: my_dat, labels = trees.CreateDataSet()In [65]: labelsOut[65]: ['no surfacing', 'flippers']In [66]: my_tree = trees.CreateTree(my_dat, labels)In [67]: labelsOut[67]: ['no surfacing', 'flippers']In [68]: my_treeOut[68]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;In [69]: trees.Classify(my_tree, labels, [1, 0])Out[69]: 'no'In [70]: trees.Classify(my_tree, labels, [1, 1])Out[70]: 'yes'C4.5C4.5算法是由ID3算法引申而来，主要改进有以下两点：选取最优分裂属性时根据信息增益率 (IGR)使算法对连续变量兼容下面分别对分裂信息以及信息增益率进行定义：$$IGR = \frac{IG}{IV}$$因此只需对ID3算法的代码做一些改动即可，为了兼容ID3， 具体实现如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_feature下面需要解决的问题是连续变量的问题，为了实验的方便，我们更改一下naive的数据生成方法（Ref: http://blog.csdn.net/lemon_tree12138/article/details/51840361 ）：1234567891011121314151617181920212223242526272829303132333435363738def CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] else: data_set = [[85, 85, 'no'], [80, 90, 'yes'], [83, 78, 'no'], [70, 96, 'no'], [68, 80, 'no'], [65, 70, 'yes'], [64, 65, 'yes'], [72, 95, 'no'], [69, 70, 'no'], [75, 80, 'no'], [75, 70, 'yes'], [72, 90, 'yes'], [81, 75, 'no'], [71, 80, 'yes']] labels = ['temperature', 'humidity'] return data_set, labels假设我们选择了温度属性，则被提取的关键数据为：[[85, No], [80, No], [83, Yes], [70, Yes], [68, Yes], [65, No], [64, Yes], [72, No], [69, Yes], [75, Yes], [75, Yes], [72, Yes], [81, Yes], [71, No]]现在我们对这批数据进行从小到大进行排序，排序后数据集就变成：[[64, Yes], [65, No], [68, Yes], [69, Yes], [70, Yes], [71, No], [72, No], [72, Yes], [75, Yes], [75, Yes], [80, No], [81, Yes], [83, Yes], [85, No]]绘制成如下图例：当我们拿到一个已经排好序的（温度，结果）的列表之后，分别计算被某个单元分隔的左边和右边的分裂信息，比如现在计算 index = 4 时的分裂信息。则：$$IV(v_4) = IV([4, 1], [5, 4]) = \frac{5}{14}IV([4, 1]) + \frac{9}{14}IV([5, 4])$$$$IV(v_4) = \frac{5}{14}(-\frac{4}{5} \log_{2} \frac{4}{5} - \frac{1}{5} \log_{2} \frac{1}{5}) + \frac{9}{14}(-\frac{5}{9} \log_{2} \frac{5}{9} - \frac{4}{9} \log_{2} \frac{4}{9})$$下图表示了不同分裂位置所得到的分裂信息：最后给出完整的代码实现 (最后的Classify方法还需修改)：trees.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383from math import logimport operatorimport numpy as npdef CalcShannon(data_set): """ Calculate the Shan0n Entropy. Arguments: data_set: The object dataset. Returns: shan0n_ent: The Shan0n entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # print(label_counts) # Calculates the Shan0n entropy shan0n_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shan0n_ent -= prob * log(prob, 2) return shan0n_entdef CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impuritydef CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impuritydef CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 1], [1, 1, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0]] labels = ['0 surfacing', 'flippers'] else: data_set = [[1, 85, 85, 0, 0], [1, 80, 90, 1, 0], [2, 83, 78, 0, 1], [3, 70, 96, 0, 1], [3, 68, 80, 0, 1], [3, 65, 70, 1, 0], [2, 64, 65, 1, 1], [1, 72, 95, 0, 0], [1, 69, 70, 0, 1], [3, 75, 80, 0, 1], [1, 75, 70, 1, 1], [2, 72, 90, 1, 1], [2, 81, 75, 0, 1], [3, 71, 80, 1, 0]] labels = ['outlook', 'temperature', 'humidity', 'windy'] return data_set, labelsdef SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_featuredef majority_cnt(class_list): """ Decided the final class. When the splited data is 0t belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count. items(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0]def CreateTree(data_set, feat_labels, method='ID3'): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. method: The algorithm class. Returns: my_tree: A dict that represents the decision tree. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) labels = feat_labels.copy() class_list = [example[-1] for example in data_set] # print(class_list) # If the classes are fully same print('class_list', class_list) if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) if method == 'ID3': # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] # Get the next step labels parameter del(labels[best_feat]) unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_tree else: flag = 'ID3' # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set, 'C4.5') best_feat_label = labels[best_feat] print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] del(labels[best_feat]) unique_vals = set(feat_values) if len(unique_vals) &gt; 3: flag = 'C4.5' if flag == 'ID3': for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels, 'C4.5') return my_tree else: data_set = np.array(data_set) best_iv = 0.0 best_split_value = -1 sorted_feat = np.argsort(feat_values) for i in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [i + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; best_iv: best_iv = iv best_split_value = feat_values[sorted_feat[i]] print(best_feat, best_split_value) # print(best_split_value) left_data_set = data_set[ data_set[:, best_feat] &lt;= best_split_value] left_data_set = np.delete(left_data_set, best_feat, axis=1) # if len(left_data_set) == 1: # return left_data_set[0][-1] right_data_set = data_set[ data_set[:, best_feat] &gt; best_split_value] right_data_set = np.delete(right_data_set, best_feat, axis=1) # if len(right_data_set) == 1: # return right_data_set[0][-1] sub_labels = labels[:] my_tree[best_feat_label][ '&lt;=' + str(best_split_value)] = CreateTree( left_data_set.tolist(), sub_labels, 'C4.5') my_tree[best_feat_label][ '&gt;' + str(best_split_value)] = CreateTree( right_data_set.tolist(), sub_labels, 'C4.5') # print('continious tree', my_tree) return my_treedef Classify(input_tree, feat_labels, test_vec): """ Classify that uses the given decision tree. Arguments: input_tree: The Given decision tree. feat_labels: The labels of correspond feature. test_vec: The test data. Returns: class_label: The class label that corresponds to the test data. """ # Get the start feature label to split first_str = list(input_tree.keys())[0] # Get the sub-tree that corresponds to the start feature to split second_dict = input_tree[first_str] # Get the feature index that the label is the start feature label feat_index = feat_labels.index(first_str) # Start recurrence search for key in second_dict.keys(): if test_vec[feat_index] == key: if type(second_dict[key]).__name__ == 'dict': # Recurrence calls class_label = Classify(second_dict[key], feat_labels, test_vec) else: class_label = second_dict[key] return class_label一个小demo：123456789101112131415161718192021222324252627282930313233In [108]: reload(trees)Out[108]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [109]: my_dat, labels = trees.CreateDataSet('C4.5')In [110]: my_tree_c = trees.CreateTree(my_dat, labels, 'C4.5')class_list [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]outlookclass_list [0, 0, 0, 1, 1]humidity1 90class_list [0, 0, 1, 1]temperature0 69class_list [1]class_list [0, 0, 1]windyclass_list [0]class_list [0, 1]class_list [0]class_list [1, 1, 1, 1]class_list [1, 1, 0, 1, 0]windyclass_list [1, 1, 1]class_list [0, 0]In [111]: my_tree_cOut[111]:&#123;'outlook': &#123;1: &#123;'humidity': &#123;'&lt;=90': &#123;'temperature': &#123;'&lt;=69': 1, '&gt;69': &#123;'windy': &#123;0: 0, 1: 0&#125;&#125;&#125;&#125;, '&gt;90': 0&#125;&#125;, 2: 1, 3: &#123;'windy': &#123;0: 1, 1: 0&#125;&#125;&#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kNN and kd-tree]]></title>
      <url>%2F2017%2F02%2F15%2FkNN-and-kd-tree%2F</url>
      <content type="text"><![CDATA[k-近邻算法工作原理存在一组带标签的训练集[1]，每当有新的不带标签的样本[2]出现时，将训练集中数据的特征与测试集的特征逐个比较，通过某种测度来提取出与测试集最相似的k个训练集样本，然后将这k个样本中占大多数[4]的标签赋予测试集样本。伪代码对测试集中的每个点依次执行如下操作：计算训练集中的每个点与当前点的距离按照距离递增次序排序在排序好的点中选取前k个点统计出k个点中不同类别的出现频率选择频率最高的类别为当前点的预测分类​代码实现首先创建测试数据集1from numpy import *1234def createDataset(): group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labels返回预测分类123456789101112131415def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndices = distances.argsort() classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndices[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # the return of sorted() is a list and its item is a tuple sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0] # returns the predict class label进一步探索k-近邻算法的缺点在于当数据量很大时，拥有不可接受的空间复杂度以及时间复杂度其次该算法最关键的地方在与超参k的选取。当k选取的过小时容易造成过拟合，反之容易造成欠拟合。考虑两个极端情况，当k=1时，该算法又叫最近邻算法；当k=N[3]时，表示直接从原始数据中选取占比最大的类别，显然这个算法太naive了。为了解决kNN算法时间复杂度的问题，最关键的便是在于如何对数据进行快速的k近邻搜索，一种解决方法是引入kd树来进行加速。kd树简介以二维空间为例，假设有6个二维数据点{(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2)}，可以用下图来表明kd树所能达到的效果:kd树算法主要分为两个部分：kd树数据结构的建立在kd树上进行查找kd树是一种对k维空间上的数据点进行存储以便进行高效查找的树形数据结构，属于二叉树。构造kd树相当于不断用垂直于坐标轴的超平面对k维空间进行划分，构成一系列k维超矩形区域。kd树的每一个结点对应一个超矩形区域，表示一个空间范围。数据结构下表给出每个结点主要包含的数据结构:域名数据类型描述Node-data数据矢量数据集中的某个数据点，k维矢量Split整数垂直于分割超平面的方向轴序号Leftkd树由位于该节点分割超平面左子空间内所有数据点构成的kd树Rightkd树由位于该节点分割超平面右子空间内所有数据点构成的kd树建立树伪代码下面给出构建kd树的伪代码：算法：构建k-d树（createKDTree）输入：数据点集Data-set输出：Kd，类型为k-d tree1. If Data-set为空，则返回空的k-d tree2. 调用节点生成程序： （1）确定split域：对于所有描述子数据（特征矢量），统计它们在每个维上的数据方差。以SURF特征为例，描述子为64维，可计算64个方差。挑选出最大值，对应的维就是split域的值。数据方差大表明沿该坐标轴方向上的数据分散得比较开，在这个方向上进行数据分割有较好的分辨率； （2）确定Node-data域：数据点集Data-set按其第split域的值排序。位于正中间的那个数据点被选为Node-data。此时新的Data-set’ = Data-set\Node-data（除去其中Node-data这一点）。3. dataleft = {d属于Data-set’ &amp;&amp; d[split] ≤ Node-data[split]} dataright = {d属于Data-set’ &amp;&amp; d[split] &gt; Node-data[split]}4. left = 由（dataleft）建立的k-d tree，即递归调用createKDTree（dataleft）并设置left的parent域为Kd； right = 由（dataright）建立的k-d tree，即调用createKDTree（dataleft）并设置right的parent域为Kd。实例用最开始的6个二维数据点的例子，来具体化这个过程：确定split域的首先该取的值。分别计算x，y方向上数据的方差得知x方向上的方差最大，所以split域值首先取0，也就是x轴方向；确定Node-data的域值。根据x轴方向的值2,5,9,4,8,7排序选出中值为7，所以Node-data = (7, 2)。这样，该节点的分割超平面就是通过(7, 2)并垂直于split = 0（x轴）的直线x = 7；确定左子空间和右子空间。分割超平面x = 7将整个空间分为两部分，如下图所示。x &lt; = 7的部分为左子空间，包含3个节点{(2, 3), (5, 4), (4, 7)}；另一部分为右子空间，包含2个节点{(9, 6), (8, 1)}。如算法所述，k-d树的构建是一个递归的过程。然后对左子空间和右子空间内的数据重复根节点的过程就可以得到下一级子节点（5,4）和（9,6）（也就是左右子空间的’根’节点），同时将空间和数据集进一步细分。如此反复直到空间中只包含一个数据点，如图1所示。最后生成的k-d树如下图所示。注意：每一级节点旁边的’x’和’y’表示以该节点分割左右子空间时split所取的值。这里进行一点补充说明，kd树其实就是二叉树，其与普通的二叉查找树不同之处在于，其每一层根据split的维度进行二叉拆分。具体来说，根据上图，第一层的拆分是根据x，那么其左孩子的x值就小于根结点的x值，右孩子则反之。y值则没有规定（这里出现的左大右小只是纯粹的巧合）。第二层是根据y值来进行split，因此第三层的规律显而易见。代码实现运行环境：Windows 10 Pro 64-bit x64-based(Ver. 10.0.14393), Python 3.5.2, Anaconda 4.1.1(64-bit), IPython 5.0.0, Windows CMD,kdTreeCreate.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import numpy as npfrom kdTreeNode import *def createDataSet(): """ Create the test dataset. Returns: A numpy array that contains the test data. """ dataSet = np.array([[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]) return dataSetdef split(dataSet): """ Split the given dataset. Returns: LeftDataSet: A kdTreeNode object. RightDataSet: A kdTreeNode object. NodeData: A tuple. """ # Ensure the dimension to split dimenIndex = np.var(dataSet, axis=0).argmax() partitionDataSet = dataSet[:, dimenIndex] # print(partitionDataSet) # Ensure the position to split partitionDataSetArgSort = partitionDataSet.argsort() # print(partitionDataSetArgSort) lenOfPartitionDataSetArgSort = len(partitionDataSetArgSort) if lenOfPartitionDataSetArgSort % 2 == 0: posIndex = lenOfPartitionDataSetArgSort // 2 splitIndex = partitionDataSetArgSort[posIndex] else: posIndex = (lenOfPartitionDataSetArgSort - 1) // 2 splitIndex = partitionDataSetArgSort[posIndex] # print(splitIndex) # Split nodeData = dataSet[splitIndex] leftIndeies = partitionDataSetArgSort[:posIndex] rightIndeies = partitionDataSetArgSort[posIndex + 1:] leftDataSet = dataSet[leftIndeies] rightDataSet = dataSet[rightIndeies] return nodeData, dimenIndex, leftDataSet, rightDataSetdef createKDTree(dataSet): """ Create the KD tree. Returns: A kdTreeNode object. """ if len(dataSet) == 0: return nodeData, dimenIndex, leftDataSet, rightDataSet = split(dataSet) # print(nodeData, dimenIndex, leftDataSet, rightDataSet) node = kdTreeNode(nodeData, dimenIndex) node.setLeft(createKDTree(leftDataSet)) node.setRight(createKDTree(rightDataSet)) return nodedef midTravel(node): if node is None: return midTravel(node.getLeft()) print(node.getData()) midTravel(node.getRight())if __name__ == "__main__": dataSet = createDataSet() node = createKDTree(dataSet) midTravel(node)kdTreeNode.py123456789101112131415161718192021222324252627282930313233class kdTreeNode(object): """ Class of k-d tree nodes """ def __init__(self, data=None, split=None, left=None, right=None): self.__data = data self.__split = split self.__left = left self.__right = right def getData(self): return self.__data def setData(self, data): self.__data = data def getSplit(self): return self.__split def setSplit(self, split): self.__split = split def getLeft(self): return self.__left def setLeft(self, left): self.__left = left def getRight(self): return self.__right def setRight(self, right): self.__right = right运行结果：12345678In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6]时间复杂度：N个K维数据进行查找操作时时间复杂度为 $t=O(KN^{2})$下面就要在已经建立好的kd树上进行查找操作。查找kd树中进行的查找与普通的查找操作存在较大的差异，其目的是为了找出与查询点距离最近的点。星号表示要查询的点(2.1, 3.1)。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点(2, 3)。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行’回溯’操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从(7, 2)点开始进行二叉查找，然后到达(5, 4)，最后到达(2, 3)，此时搜索路径中的节点为&lt;(7, 2), (5, 4), (2, 3)&gt;，首先以(2, 3)作为当前最近邻点，计算其到查询点(2.1, 3.1)的距离为0.1414，然后回溯到其父节点(5, 4)，并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以(2.1, 3.1)为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入(5, 4)节点右子空间中去搜索。再回溯到(7, 2)，以(2.1, 3.1)为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入(7, 2)右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点(2, 3)，最近距离为0.1414。一个复杂点了例子如查找点为(2, 4.5)。同样先进行二叉查找，先从(7, 2)查找到(5, 4)节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到(4, 7)，形成搜索路径&lt;(7, 2), (5, 4), (4, 7)&gt;，取(4, 7)为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到(5, 4)，计算其与查找点之间的距离为3.041。以(2, 4.5)为圆心，以3.041为半径作圆。可见该圆和y = 4超平面交割，所以需要进入(5, 4)左子空间进行查找。此时需将(2, 3)节点加入搜索路径中得&lt;(7, 2), (2, 3)&gt;。回溯至(2, 3)叶子节点，(2, 3)距离(2, 4.5)比(5, 4)要近，所以最近邻点更新为(2, 3)，最近距离更新为1.5。回溯至(7, 2)，以(2, 4.5)为圆心1.5为半径作圆，并不和x = 7分割超平面交割。至此，搜索路径回溯完。返回最近邻点(2, 3)，最近距离1.5。k-d树查询算法的伪代码如下所示。查找伪代码算法： k-d树最邻近查找输入：Kd， //k-d tree类型target //查询数据点输出：nearest， //最邻近数据点dist //最邻近数据点和查询点间的距离123456789101112131415161718192021222324252627282930311. If Kd为NULL，则设dist为infinite并返回2. //进行二叉查找，生成搜索路径 Kd_point = &amp;Kd； //Kd-point中保存k-d tree根节点地址 nearest = Kd_point -&gt; Node-data； //初始化最近邻点 while（Kd_point） push（Kd_point）到search_path中； //search_path是一个堆栈结构，存储着搜索路径节点指针 /*** If Dist（nearest，target） &gt; Dist（Kd_point -&gt; Node-data，target） nearest = Kd_point -&gt; Node-data； //更新最近邻点 Max_dist = Dist(Kd_point，target）； //更新最近邻点与查询点间的距离 ***/ s = Kd_point -&gt; split； //确定待分割的方向 If target[s] &lt;= Kd_point -&gt; Node-data[s] //进行二叉查找 Kd_point = Kd_point -&gt; left； else Kd_point = Kd_point -&gt;right； nearest = search_path中最后一个叶子节点； //注意：二叉搜索时不比计算选择搜索路径中的最邻近点，这部分已被注释 Max_dist = Dist（nearest，target）； //直接取最后叶子节点作为回溯前的初始最近邻点 3. //回溯查找 while（search_path != NULL） back_point = 从search_path取出一个节点指针； //从search_path堆栈弹栈 s = back_point -&gt; split； //确定分割方向 If Dist（target[s]，back_point -&gt; Node-data[s]） &lt; Max_dist //判断还需进入的子空间 If target[s] &lt;= back_point -&gt; Node-data[s] Kd_point = back_point -&gt; right； //如果target位于左子空间，就应进入右子空间 else Kd_point = back_point -&gt; left; //如果target位于右子空间，就应进入左子空间 将Kd_point压入search_path堆栈； If Dist（nearest，target） &gt; Dist（Kd_Point -&gt; Node-data，target） nearest = Kd_point -&gt; Node-data； //更新最近邻点 Min_dist = Dist（Kd_point -&gt; Node-data,target）； //更新最近邻点与查询点间的距离代码实现kdTreeSearch.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as npdef cal_dist(node, target): """ Calculate the distance between the node and the target. Arguments: node: The kd-tree's one node. target: Search target. Returns: dist: The distance between the two nodes. """ node_data = np.array(node) target_data = np.array(target) square_dist_vector = (node_data - target_data) ** 2 square_dist = np.sum(square_dist_vector) dist = square_dist ** 0.5 return distdef search(root_node, target): """ Search the nearest node of the target node in the kd-tree that root node is the root_node Arguments: root_node: The kd-tree's root node. target: Search target. Returns: nearest: The nearest node of the target node in the kd-tree. min_dist: The nearest distance. """ if root_node is None: min_dist = float('inf') return min_dist # Two-fork search kd_point = root_node # Save the root node nearest = kd_point.getData() # Initial the nearest node search_path = [] # Initial the search stack while kd_point: search_path.append(kd_point) split_index = kd_point.getSplit() # Ensure the split path if target[split_index] &lt;= kd_point.getData()[split_index]: kd_point = kd_point.getLeft() else: kd_point = kd_point.getRight() nearest = search_path.pop().getData() min_dist = cal_dist(nearest, target) # Retrospect search while search_path: back_point = search_path.pop() # Ensure the back-split path back_split_index = back_point.getSplit() # Judge if needs to enter the subspace if cal_dist(target[back_split_index], back_point.getData()[back_split_index]) &lt; min_dist: # If the target is in the left subspace, then enter the right if target[back_split_index] &lt;= back_point.getData()[back_split_index]: kd_point = back_point.getRight() # Otherwise enter the left else: kd_point = back_point.getLeft() # Add the node to the search path if kd_point is not None: search_path.append(kd_point) if cal_dist(nearest, target) &gt; cal_dist(kd_point.getData(), target): # Update the nearest node nearest = kd_point.getData() # Update the maximum distance min_dist = cal_dist(kd_point.getData(), target) return nearest, min_dist运行结果：12345678910111213141516171819202122232425262728293031323334In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6]In [2]: node-------------------------------------Out [2]: &lt;kdTreeNode.kdTreeNode at 0x26bff22f160&gt;In [3]: import kdTreeSearchIn [4]: nearest, min_dist = kdTreeSearch.search(node, [2.1, 3.1])In [5]: nearest-------------------------------------Out [5]: array([2, 3])In [6]: min_dist-------------------------------------Out [6]: 0.14142135623730964In [7]: nearest, min_dist = kdTreeSearch.search(node, [2, 4.5])In [8]: nearest-------------------------------------Out [8]: array([2, 3])In [9]: min_dist-------------------------------------Out [9]: 1.5时间复杂度：N个结点的K维kd树进行查找操作时最坏时间复杂度为 $t_{worst}=O(KN^{1-1/k})$根据相关研究，当数据维度为K时，只有当数据量N满足 $N&gt;&gt;2^K$ 时，才能达到高效的搜索（K&lt;20，超过20维时可采用ball-tree算法），所以引出了一系列的改进算法（BBF算法，和一系列M树、VP树、MVP树等高维空间索引树），留待后续补充。采用kd树的k-近邻算法接下来便是将两者相结合。[1] 说是训练集其实是不准确的，因为k-近邻算法是一个无参数方法，只存在一个超参k，因此其不存在一个训练的过程[2] 测试集[3] N代表训练集的数目[4] 多数表决]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F02%2F15%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
    </entry>

    
  
  
</search>
